"""Lock file generation utilities."""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING

import narwhals as nw
import tomli_w

from metaxy.metadata_store.exceptions import FeatureNotFoundError

if TYPE_CHECKING:
    from metaxy.metadata_store import MetadataStore
    from metaxy.models.feature_definition import FeatureDefinition


def generate_lock_file(
    store: MetadataStore,
    feature_keys: list[str],
    output_path: Path,
    *,
    exclude_project: str | None = None,
) -> int:
    """Generate a lock file with feature definitions from a metadata store.

    Args:
        store: Metadata store to fetch features from.
        feature_keys: List of feature keys to lock.
        output_path: Path to write the lock file.
        exclude_project: Optional project name to exclude from the lock file.
            Features belonging to this project will be skipped.

    Returns:
        Number of features locked.

    Raises:
        FeatureNotFoundError: If any requested features are not found in the store.
    """
    from contextlib import nullcontext

    from metaxy.metadata_store.system.storage import SystemTableStorage

    # Handle empty features list
    if not feature_keys:
        lock_data = _build_lock_data([])
        _write_lock_file(output_path, lock_data)
        return 0

    # Capture features already in the graph as non-external BEFORE loading from store
    from metaxy.models.feature import FeatureGraph

    graph = FeatureGraph.get_active()
    existing_non_external_keys = {key for key, defn in graph.feature_definitions_by_key.items() if not defn.is_external}

    # Use nullcontext if store is already open, otherwise open it
    cm = nullcontext(store) if store._is_open else store.open("read")
    with cm:
        storage = SystemTableStorage(store)

        # First load all requested features to check for missing ones
        all_definitions = storage._load_feature_definitions_raw(
            filters=[nw.col("feature_key").is_in(feature_keys)],
        )

    # Check for missing features
    loaded_keys = {d.key.to_string() for d in all_definitions}
    missing = [k for k in feature_keys if k not in loaded_keys]
    if missing:
        raise FeatureNotFoundError(
            f"Features not found in metadata store: {', '.join(missing)}. "
            f"Run `metaxy push` in the project that defines these features."
        )

    # Filter out features from the excluded project
    if exclude_project:
        definitions = [d for d in all_definitions if d.project != exclude_project]
    else:
        definitions = all_definitions

    # Filter out features that were already in the graph as non-external before loading
    definitions = [d for d in definitions if d.key not in existing_non_external_keys]

    # Build and write lock file
    lock_data = _build_lock_data(definitions)
    _write_lock_file(output_path, lock_data)

    return len(definitions)


def _build_lock_data(definitions: list[FeatureDefinition]) -> dict:
    """Build dict for the lock file."""
    features = {}
    for defn in sorted(definitions, key=lambda d: d.key.to_string()):
        feature_key = defn.key.to_string()
        feature_data: dict = {
            "project": defn.project,
        }
        if defn.feature_class_path:
            feature_data["feature_class_path"] = defn.feature_class_path

        # Serialize spec and schema as JSON strings
        feature_data["spec"] = json.dumps(defn.spec.model_dump(mode="json"), separators=(",", ":"))
        feature_data["feature_schema"] = json.dumps(defn.feature_schema, separators=(",", ":"))

        features[feature_key] = feature_data

    return {"features": features}


def _write_lock_file(output_path: Path, lock_data: dict) -> None:
    """Write lock file with header comment."""
    header = "# Generated by `metaxy lock`.\n\n"
    content = header + tomli_w.dumps(lock_data)
    output_path.write_text(content)
