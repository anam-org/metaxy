{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metaxy \ud83c\udf0c","text":"<p>Metaxy is a metadata layer for multi-modal Data and ML pipelines that tracks feature versions, dependencies, and data lineage across complex computation graphs.</p> <p>Metaxy builds versioned graphs from feature definitions and tracks dependency changes:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: f3f57b8e\n        example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc9ca835)&lt;/small&gt;&lt;br/&gt;---&lt;br/&gt;\u2022 audio &lt;small&gt;(v:\n22742381)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: 794116a9)&lt;/small&gt;&lt;/div&gt;\"]\n        overview_sst[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;overview/sst&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc3cae5c)&lt;/small&gt;&lt;br/&gt;---&lt;br/&gt;\u2022 transcription &lt;small&gt;(v:\n99b97ac1)&lt;/small&gt;&lt;/div&gt;\"]\n        example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 3ac04df8)&lt;/small&gt;&lt;br/&gt;---&lt;br/&gt;\u2022 audio &lt;small&gt;(v:\n76c8bdc9)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: abc79017)&lt;/small&gt;&lt;/div&gt;\"]\n        overview_embeddings[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;overview/embeddings&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 944318e4)&lt;/small&gt;&lt;br/&gt;---&lt;br/&gt;\u2022 embedding\n&lt;small&gt;(v: b3f81f9e)&lt;/small&gt;&lt;/div&gt;\"]\n        example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: fbe130cc)&lt;/small&gt;&lt;br/&gt;---&lt;br/&gt;\u2022\ndetections &lt;small&gt;(v: ab065369)&lt;/small&gt;&lt;/div&gt;\"]\n        example_video --&gt; example_crop\n        example_crop --&gt; example_face_detection\n        example_video --&gt; overview_sst\n        example_crop --&gt; overview_embeddings</code></pre> <p>Metaxy supports incremental computations, sample-level versioning, field-level versioning, and more.</p> <p>Giga Alpha</p> <p>This project is as raw as a steak still saying \u2018moo.\u2019</p> <p>Metaxy is:</p> <ul> <li> <p>\ud83e\udde9 composable --- bring your own everything!</p> <ul> <li>supports DuckDB, ClickHouse, and 20+ databases via Ibis</li> <li>supports lakehouse storage formats such as DeltaLake or DuckLake</li> <li>is agnostic to tabular compute engines: Polars, Spark, Pandas, and databases thanks to Narwhals</li> <li>we totally don't care how is the multi-modal data produced or where is it stored: Metaxy is responsible for yielding input metadata and writing output metadata</li> </ul> </li> <li> <p>\ud83e\udd38 flexible to work around restrictions consciously:</p> <ul> <li>features are defined as Pydantic models, leveraging Pydantic's type safety guarantees, rich validation system, and allowing inheritance patterns to stay DRY</li> <li>has a migrations system to compensate for reconciling data versions and metadata when computations are not desired</li> </ul> </li> <li> <p>\ud83e\udea8 rock solid when it matters:</p> <ul> <li>data versioning is guaranteed to be consistent across DBs or in-memory compute engines. We really have tested this very well!</li> <li>changes to topology, feature versioning, or individual samples ruthlessly propagate downstream</li> <li>unique field-level dependency system prevents unnecessary recomputations for features that depend on partial data</li> <li>metadata is append-only to ensure data integrity and immutability. Users can perform cleanup if needed (Metaxy provides tools for this).</li> </ul> </li> <li> <p>\ud83d\udcc8 scalable:</p> <ul> <li>supports feature organization and discovery patterns such as packaging entry points. This enables collaboration across teams and projects.</li> <li>is built with performance in mind: all operations default to run in the DB, Metaxy does not stand in the way of metadata flow</li> </ul> </li> <li> <p>\ud83e\uddd1\u200d\ud83d\udcbb dev friendly:</p> <ul> <li>clean, intuitive Python API that stays out of your way when you don't need it</li> <li>feature discovery system for effortless dependency management</li> <li>comprehensive type hints and Pydantic integration for excellent IDE support</li> <li>first-class support for local development, testing, preview environments, CI/CD</li> <li>CLI tool for easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats</li> <li>integrations with popular tools such as SQLModel, Dagster, and Ray.</li> <li>testing helpers that you're going to appreciate</li> </ul> </li> </ul>"},{"location":"#feature-dependencies","title":"Feature Dependencies","text":"<p>Features form a DAG where each feature declares its upstream dependencies. Consider an video processing pipeline:</p> <pre><code>class Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"video\",\n        fields=[\n            FieldSpec(name=\"frames\", code_version=1),\n            FieldSpec(name=\"audio\", code_version=1),\n        ],\n    ),\n):\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n\n\nclass VoiceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"voice_detection\",\n        deps=[FeatureDep(feature=Video)],\n    ),\n):\n    path: str = Field(description=\"Path to the voice detection json file\")\n</code></pre> <p>Note</p> <p>This API will be improved with more ergonomic alternatives. See issue #70 for details.</p> <p>When <code>Video</code> changes, Metaxy automatically identifies that <code>VoiceDetection</code> requires recomputation.</p>"},{"location":"#versioned-change-propagation","title":"Versioned Change Propagation","text":"<p>Every feature definition produces a deterministic version hash computed from its dependencies, fields, and code versions. When you modify a feature\u2014whether changing its dependencies, adding fields, or updating transformation logic, Metaxy detects the change and propagates it downstream. This is done on multiple levels: <code>Feature</code> (class) level, field (class attribute) level, and of course on row level: each sample in the metadata store tracks the version of each field and the overall (class-level) feature version.</p> <p>This ensures that when feature definitions evolve, every feature that transitively depends on it can be systematically updated. Because Metaxy supports declaring dependencies on fields, it can identify when a feature does not require recomputation, even if one of its parents has been changed (but only irrelevant fields did). This is a huge factor in improving efficiency and reducing unnecessary computations (and costs!).</p> <p>Because Metaxy feature graphs are static, Metaxy can calculate data version changes ahead of the actual computation. This enables patterns such as computation preview and computation cost prediction.</p>"},{"location":"#typical-user-workflow","title":"Typical User Workflow","text":"<ol> <li>Record Metaxy feature graph in CI/CD (not necessary in non-production environments)</li> </ol> <pre><code>metaxy graph push\n</code></pre> <ol> <li>Use <code>metaxy.MetadataStore.resolve_update</code> to identify samples requiring recomputation:</li> </ol> <pre><code>from metaxy import init_metaxy\n\n# discover and load Metaxy features\ninit_metaxy()\n\nstore = (\n    ...\n)  # can be DuckDBMetadataStore locally and ClickHouseMetadataStore in production\ndiff = store.resolve_update(VoiceDetection)\n</code></pre> <p><code>metaxy.MetadataStore.resolve_update</code> runs in the database unless it doesn't support the required hash functions, otherwise it fallbacks to in-memory Polars computation (results are guaranteed to be consistent). The returned object provides Narwhals (backend-agnostic) DataFrames with all the data versions already computed.</p> <ol> <li>Handle the computation, this is entirely user-defined, Metaxy is not involved in this step.</li> </ol> <pre><code>if (len(diff.added) + len(diff.changed)) &gt; 0:\n    # run your computation, this can be done in a distributed manner\n    results = run_voice_detection(diff, ...)\n</code></pre> <ol> <li>Record metadata for computed samples, this can be done in a distributed manner as well</li> </ol> <pre><code>store.write_metadata(VoiceDetection, results)\n</code></pre> <p>We have successfully recorded the metadata for the computed samples.</p> <p>No Uniqueness Checks!</p> <p>Metaxy doesn't attempt to perform any deduplication or uniqueness checks for performance reasons. While <code>MetadataStore.resolve_update</code> is guaranteed to never return the same versioned sample twice (hey that's the whole point of Metaxy), it's up to the user to ensure that samples are not written multiple times to the metadata store. Configuring deduplication or uniqueness checks in the store (database) is a good idea.</p>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Learn more about Data Versioning</li> <li>Take a look at CLI reference</li> </ul>"},{"location":"learn/data-versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static, deterministic and idempotent.</p>"},{"location":"learn/data-versioning/#versioning_1","title":"Versioning","text":"<p>Feature and field versions are defined by the feature graph topology and the user-provided code versions of fields. Sample versions are defined by upstream sample versions and the code versions of the fields defined on the sample's feature.</p> <p>All versions are computed ahead of time: feature and field versions can be immediately derived from code (and we keep historical graph snapshots for them), and calculating sample versions requires access to the metadata store.</p> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"learn/data-versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them.</p>"},{"location":"learn/data-versioning/#field-level","title":"Field Level","text":"<ul> <li>Field Code Version is defined on the field and must be provided by the user (defaults to <code>\"0\"</code>).</li> </ul> <p>Code Version Value</p> <p>The value can be arbitrary, but in the future we might implement something around semantic versioning.</p> <ul> <li>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</li> </ul>"},{"location":"learn/data-versioning/#feature-level","title":"Feature Level","text":"<ul> <li>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</li> <li>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</li> </ul>"},{"location":"learn/data-versioning/#graph-level","title":"Graph Level","text":"<ul> <li>Snapshot Version: is computed from the Feature Versions of all features defined on the graph.</li> </ul> <p>Why Do We Need Snapshot Version?</p> <p>This value is used to uniquely encode versioned feature graph topology in historical snapshots.</p>"},{"location":"learn/data-versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to compute them.</p> <ul> <li>Sample Version By Field is computed from the upstream Sample Version By Fields (with respect to defined field-level dependencies and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</li> </ul> sample_uid metaxy_sample_version_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <ul> <li>Sample Version is derived from the Sample Version By Field by simply hashing it.</li> </ul> <p>This is the end game of the versioning system. It ensures that only the necessary samples are recomputed when a feature version changes. It acts as source of truth for resolving incremental updates for feature metadata.</p>"},{"location":"learn/data-versioning/#practical-example","title":"Practical Example","text":"<p>Consider a video processing pipeline with these features:</p> <pre><code>from metaxy import (\n    Feature,\n    FeatureDep,\n    FeatureSpec,\n    FieldDep,\n    FieldSpec,\n)\n\n\nclass Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/video\",\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=1,\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=1,\n            ),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/crop\",\n        deps=[FeatureDep(feature=Video)],\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=1,\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=1,\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass  # omit columns for the sake of simplicity\n\n\nclass FaceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/face_detection\",\n        deps=[\n            FeatureDep(\n                feature=Crop,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"faces\",\n                code_version=1,\n                deps=[\n                    FieldDep(\n                        feature=Crop,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n\n\nclass SpeechToText(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/stt\",\n        deps=[\n            FeatureDep(\n                feature=Video,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"transcription\",\n                code_version=1,\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n</code></pre> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: 8468950d\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc9ca835)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 22742381)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: 794116a9)&lt;/small&gt;&lt;/div&gt;\"]\n        example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 3ac04df8)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 76c8bdc9)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: abc79017)&lt;/small&gt;&lt;/div&gt;\"]\n        example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 1ac83b07)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces &lt;small&gt;(v: 2d75f0bd)&lt;/small&gt;&lt;/div&gt;\"]\n        example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: c83a754a)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 transcription &lt;small&gt;(v: ac412b3c)&lt;/small&gt;&lt;/div&gt;\"]\n        example_video --&gt; example_crop\n        example_crop --&gt; example_face_detection\n        example_video --&gt; example_stt</code></pre>"},{"location":"learn/data-versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>Video</code> feature changes (perhaps denoising was applied):</p> <pre><code>         key=\"example/video\",\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=1,\n+                code_version=2,\n             ),\n</code></pre> <p>Run <code>metaxy graph diff</code> to see what changed:</p> <pre><code>---\ntitle: Merged Graph Diff\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>Notice:</p> <ul> <li><code>Video</code>, <code>Crop</code>, and <code>SpeechToText</code> changed (highlighted)</li> <li><code>FaceDetection</code> remained unchanged (depends only on <code>frames</code>, not <code>audio</code>)</li> <li>Audio field versions changed throughout the graph</li> <li>Frame field versions stayed the same</li> </ul>"},{"location":"learn/data-versioning/#incremental-computation","title":"Incremental Computation","text":"<p>The metadata store's <code>calculate_and_write_data_versions()</code> method:</p> <ol> <li>Joins upstream feature metadata</li> <li>Computes sample versions</li> <li>Compares against existing metadata</li> <li>Returns diff: added, changed, removed samples</li> </ol> <p>Typically, steps 1-3 can be run directly in the database. Analytical databases such as ClickHouse or Snowflake can efficiently handle these operations.</p> <p>The Python pipeline then processes only the delta</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes data_version and identifies changes\n    diff = store.resolve_update(MyFeature)\n\n    # Process only changed samples\n</code></pre> <p>The <code>diff</code> object has attributes for new upstream samples, samples with new versions, and samples that have been removed from upstream metadata.</p> <p>This approach avoids expensive recomputation when nothing changed, while ensuring correctness when dependencies update.</p>"},{"location":"learn/feature-definitions/","title":"Feature System","text":"<p>Metaxy has a declarative (defined statically at class level), expressive, flexible feature system. It has been inspired by Software-Defined Assets in Dagster.</p> <p>Features represent tabular metadata, typically containing references to external multi-modal data such as files, images, or videos. But it can be just pure metadata as well.</p> <p>I will highlight data and metadata with bold so it really stands out.</p> <p>Metaxy is responsible for providing correct metadata to users. During incremental processing, Metaxy will automatically resolve added, changed and deleted metadata rows and calculate the right sample versions for them. Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage (it's a good idea to inject the sample version into the data sample identifier). Metaxy is designed to be used with systems that do not overwrite existing metadata (Metaxy only appends metadata) and therefore data as well (while we cannot enforce that since the user is responsible for writing the data, it's easily achievable by including the sample version into the data sample identifier).</p> <p>I hope we can stop using bold for data and metadata from now on, hopefully we've made our point.</p> <p>Include Sample Version In Your Data Path</p> <p>Include the sample version in your data path to ensure strong consistency guarantees. I mean it. Really do it!</p> <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly). Features are bound to a specific Metaxy project, but can be moved between projects over time. Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p>"},{"location":"learn/feature-definitions/#feature-specs","title":"Feature Specs","text":"<p>Before we can define a <code>Feature</code>, we must first create a <code>FeatureSpec</code> object. But before we get to an example, it's necessary to understand the concept of ID columns. Metaxy must know how to uniquely identify feature samples and join metadata tables, therefore, you need to attach one or more ID columns to your <code>FeatureSpec</code>. Very often these ID columns would stay the same across many feature specs, therefore it makes a lot of sense to define them on a shared base class.</p> <p>Some boilerplate with typing is involved (this is typically a good thing):</p> <pre><code>from typing import TypeAlias\n\nfrom metaxy import BaseFeatureSpec\n\n\nVideoIds: TypeAlias = tuple[str]\n\n\nclass VideoFeatureSpec(BaseFeatureSpec[VideoIds]):\n    id_columns: VideoIds = (\"video_id\",)\n</code></pre> <p><code>BaseFeatureSpec</code> is a Pydantic model, so all normal Pydantic features apply.</p> <p>With our <code>VideoFeatureSpec</code> in place, we can proceed to defining features that would be using it.</p>"},{"location":"learn/feature-definitions/#feature-definitions","title":"Feature Definitions","text":"<p>Metaxy provides a <code>BaseFeature</code> class that can be extended to make user-defined features. It's a Pydantic model as well. User-defined <code>BaseFeature</code> classes must have fields matching ID columns of the <code>FeatureSpec</code> they are using.</p> <p>With respect to the same DRY principle, we can define a shared base class for features that use the <code>VideoFeatureSpec</code>.</p> <pre><code>from metaxy import BaseFeature\n\n\nclass BaseVideoFeature(\n    BaseFeature, spec=None\n):  # spec=None is important to tell Metaxy this is a base class\n    video_id: str\n</code></pre> <p>Now we are finally ready to define an actual feature.</p> <pre><code>class VideoFeature(BaseVideoFeature, spec=VideoFeatureSpec(key=\"/raw/video\")):\n    path: str\n</code></pre> <p>That's it! That's a roow feature, it doesn't have any dependencies. Easy.</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    BaseVideoFeature,\n    spec=VideoFeatureSpec(key=\"/processed/transcript\", deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> <p>Hurray! You get the idea.</p>"},{"location":"learn/feature-definitions/#field-level-dependencies","title":"Field-Level Dependencies","text":"<p>A core (I'be straight: a killer) feature of Metaxy is the concept of field-level dependencies. These are used to define dependencies between logical fields of features.</p> <p>A field is not to be confused with metadata column (Pydantic fields). Fields are completely independent from them.</p> <p>Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy.</p> <p>Fields refer to data and are logical -- users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>Video</code> feature -- an <code>.mp4</code> file -- may have <code>frames</code> and <code>audio</code> fields.</p> <p>Downstream features can depend on specific fields of upstream features. This enables fine-grained control over data versioning, avoiding unnecessary reprocessing.</p> <p>At this point, careful readers have probably noticed that the <code>Transcript</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express that with Metaxy:</p> <pre><code>from metaxy import FieldDep, FieldSpec\n\nvideo_spec = VideoFeatureSpec(key=\"/raw/video\", fields=[FieldSpec(key=\"audio\"], FieldSpec(key=\"frames\"))\n\nclass VideoFeature(BaseVideoFeature, spec=video_spec):\n    path: str\n\n\ntranscript_spec = TranscriptFeatureSpec(key=\"/raw/transcript\", fields=[FieldSpec(key=\"text\", deps=[FieldDep(feature=VideoFeature.spec().key, fields=[\"audio\"])])])\n\nclass TranscriptFeature(BaseTranscriptFeature, spec=transcript_spec):\n    path: str\n</code></pre> <p>Voil\u00e0!</p> <p>The Data Versioning docs explain more about this system.</p>"},{"location":"learn/feature-definitions/#fully-qualified-field-key","title":"Fully Qualified Field Key","text":"<p>A fully qualified field key (FQFK) is an identifier that uniquely identifies a field within the whole feature graph. It consists of the feature key and the field key, separated by a colon, for example: <code>/raw/video:frames</code>, <code>/raw/video:audio/english</code>.</p>"},{"location":"learn/feature-definitions/#a-note-on-type-coercion-for-metaxy-types","title":"A Note on Type Coercion for Metaxy types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent feature keys, their fields, and the dependencies between them.</p> <p>To avoid boilerplate, Metaxy also has syntactic sugar for construction of these classes. Different ways to provide them are automatically coerced into canonical internal models. This is fully typed and only affects constructor arguments, so accessing attributes on Metaxy models will always return only the canonical types.</p> <p>Some examples:</p> <pre><code>from metaxy import FeatureKey\n\nkey = FeatureKey(\"prefix/feature\")\nkey = FeatureKey([\"prefix\", \"feature\"])\nkey = FeatureKey(\"prefix\", \"feature\")\nsame_key = FeatureKey(key)\n</code></pre> <p>Metaxy really loves you, the user! See syntactic sugar for more details.</p>"},{"location":"learn/feature-definitions/#syntactic-sugar","title":"Syntactic Sugar","text":""},{"location":"learn/feature-definitions/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> accept:</p> <ul> <li>String format: <code>FeatureKey(\"prefix/feature\")</code></li> <li>Sequence format: <code>FeatureKey([\"prefix\", \"feature\"])</code></li> <li>Variadic format: <code>FeatureKey(\"prefix\", \"feature\")</code></li> <li>Same type: <code>FeatureKey(another_feature_key)</code> -- for full Inception mode</li> </ul> <p>All formats produce equivalent keys, internally represented as a sequence of parts</p>"},{"location":"learn/feature-discovery/","title":"Feature Discovery","text":"<p>Metaxy provides automatic feature discovery through Python's entrypoint system. This enables modular architecture patterns essential for scaling Metaxy projects.</p>"},{"location":"learn/feature-discovery/#why-feature-discovery","title":"Why Feature Discovery?","text":"<p>Manual feature registration doesn't scale. As your system grows, you need:</p> <ul> <li>Plugin architectures - Third-party teams contribute features without modifying core code</li> <li>Feature collections - Package and distribute related features as installable units</li> <li>Monorepo support - Discover features across multiple packages in a monorepo</li> <li>Internal packages - Share features between projects via private package registries</li> </ul> <p>Feature discovery solves these problems through automatic registration at import time.</p>"},{"location":"learn/feature-discovery/#package-entry-points","title":"Package Entry Points","text":"<p>The most powerful discovery mechanism uses Python's standard entry point system via <code>project.entry-points.\"metaxy.features\"</code> in <code>pyproject.toml</code>.</p>"},{"location":"learn/feature-discovery/#creating-a-feature-plugin","title":"Creating a Feature Plugin","text":"<p>Structure your feature package:</p> <pre><code>my-video-features/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_video_features/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 detection.py\n        \u2514\u2500\u2500 transcription.py\n</code></pre> <p>Declare entry points in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-video-features\"\nversion = \"1.0.0\"\ndependencies = [\"metaxy\"]\n\n[project.entry-points.\"metaxy.features\"]\ndetection = \"my_video_features.detection\"\ntranscription = \"my_video_features.transcription\"\n</code></pre> <p>Define features in the modules:</p> <pre><code># my_video_features/detection.py\nfrom metaxy import Feature, FeatureSpec, FeatureKey\n\n\nclass FaceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=FeatureKey([\"video\", \"face_detection\"]),\n        # ... spec details\n    ),\n):\n    pass\n\n\nclass ObjectDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=FeatureKey([\"video\", \"object_detection\"]),\n        # ... spec details\n    ),\n):\n    pass\n</code></pre>"},{"location":"learn/feature-discovery/#installing-and-using-plugins","title":"Installing and Using Plugins","text":"<p>Install the package:</p> <pre><code>pip install my-video-features\n# Or in a monorepo:\npip install -e ./packages/my-video-features\n</code></pre> <p>UV Package Manager: Entry Point Changes</p> <p>If you're using <code>uv</code> and modify entry points in <code>pyproject.toml</code>, <code>uv sync</code> will not recreate the editable package metadata. You must explicitly reinstall:</p> <pre><code>uv sync --reinstall-package my-video-features\n</code></pre> <p>Installed features will be automatically discovered and loaded:</p> <pre><code>from metaxy import init_metaxy\n\n# Automatically discovers and loads all features\n# from installed packages with metaxy.features entry points\ninit_metaxy()\n\n# Features are now available in the global graph\nfrom metaxy import FeatureGraph\n\ngraph = FeatureGraph.get_active()\nprint(f\"Loaded {len(graph.features_by_key)} features\")\n</code></pre>"},{"location":"learn/feature-discovery/#monorepo-patterns","title":"Monorepo Patterns","text":"<p>In monorepos, use entry points to manage feature collections across teams:</p>"},{"location":"learn/feature-discovery/#team-owned-feature-packages","title":"Team-Owned Feature Packages","text":"<pre><code>monorepo/\n\u251c\u2500\u2500 packages/\n\u2502   \u251c\u2500\u2500 core-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u251c\u2500\u2500 ml-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u2514\u2500\u2500 experimental-features/\n\u2502       \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2514\u2500\u2500 apps/\n    \u2514\u2500\u2500 main-pipeline/\n        \u2514\u2500\u2500 pyproject.toml  # depends on feature packages\n</code></pre> <p>Each team maintains their features independently:</p> <pre><code># packages/ml-features/pyproject.toml\n[project.entry-points.\"metaxy.features\"]\nembeddings = \"ml_features.embeddings\"\nclassifiers = \"ml_features.classifiers\"\n</code></pre> <p>The main application discovers all features.</p>"},{"location":"learn/feature-discovery/#config-based-discovery","title":"Config-Based Discovery","text":"<p>For simpler use cases, load features directly from module paths specified in Metaxy configuration:</p> metaxy.tomlpyproject.toml <pre><code>entrypoints = [\n    \"myapp.features.video\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nentrypoints = [\n    \"myapp.features.video\",\n]\n</code></pre> <pre><code>from metaxy import init_metaxy\n\n# Discovers features from configured entrypoints\ninit_metaxy()\n</code></pre>"},{"location":"learn/feature-discovery/#best-practices","title":"Best Practices","text":"<ol> <li>Use entry points for distribution - Any features intended for reuse should use entry points</li> <li>Version your feature packages - Use semantic versioning for feature collections</li> <li>Test in isolation - Load feature packages into test graphs to verify behavior</li> </ol> <p>The entry point system transforms feature management from a manual process to an automatic, scalable system that grows with your organization.</p>"},{"location":"learn/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: <code>MetadataStore</code>.</p> <p>Metadata stores expose methods for reading, writing, and deleting metadata. Metaxy intentionally does not support mutating metadata in-place for performance reasons. Deletes are not required during normal operations, but they are still supported since users would want to eventually delete stale metadata and data.</p> <p>Metadata reads/writes are not guaranteed to be ACID: Metaxy is designed to interact with analytical databases which lack ACID guarantees by definition and design (for performance reasons). However, Metaxy guarantees to never attempt to retrieve the same sample version twice, so as long as users do not write it twice (or have deduplication configured inside the metadata store) we should be all good.</p> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store. This includes joining upstream features, hashing their versions, and filtering out samples that have already been processed.</p> <p>There are 3 cases where this is done in-memory instead (with the help of polars-hash):</p> <ol> <li>The metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</li> <li>The user explicitly requested to keep the computations in-memory (<code>MetadataStore(..., prefer_native=False)</code>)</li> <li>When having to use a fallback store to retrieve one of the parent features.</li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p> <p>Learn about configuring metadata stores here</p>"},{"location":"learn/metadata-stores/#fallback-stores","title":"Fallback Stores","text":"<p>Fallback stores are a powerful feature that allow stores to read feature metadata from other stores (only if it's missing in the primary store). This is very useful for development, as production data can be retrieved immediately without populating the development environment. This is especially useful for ephemeral environments such as branch/preview deployments (typically created by CI/CD for pull requests) or integration testing environments.</p>"},{"location":"learn/metadata-stores/#project-write-validation","title":"Project Write Validation","text":"<p>The MetadataStore automatically validates project-level writes to prevent accidental cross-project writes in multi-project setups. The validation compares the Feature's <code>project</code> attribute with the project from the global <code>MetaxyConfig</code> instance.</p> <p>When a write is attempted, the store checks if the feature's project matches the expected project from <code>MetaxyConfig.get().project</code>. If they don't match, a <code>ValueError</code> is raised with a clear error message.</p> <p>For legitimate cross-project operations (such as migrations that need to update features across multiple projects), an escape hatch is provided via the <code>allow_cross_project_writes()</code> context manager:</p> <pre><code># Normal operation - writes are validated against expected project\nwith store:\n    store.write_metadata(feature_from_my_project, metadata)  # OK\n    store.write_metadata(feature_from_other_project, metadata)  # Raises ValueError\n\n# Migration scenario - temporarily allow cross-project writes\nwith store:\n    with store.allow_cross_project_writes():\n        store.write_metadata(feature_from_project_a, metadata_a)  # OK\n        store.write_metadata(feature_from_project_b, metadata_b)  # OK\n</code></pre> <p>This design ensures data isolation between projects while providing flexibility for administrative operations that legitimately need to work across project boundaries.</p>"},{"location":"learn/testing/","title":"Testing Metaxy Features","text":"<p>This guide covers patterns for testing your features when using Metaxy.</p>"},{"location":"learn/testing/#graph-isolation","title":"Graph Isolation","text":"<p>By default, Metaxy uses a single global feature graph where all features register themselves automatically. In testing, you need isolated graphs to prevent test interference.</p>"},{"location":"learn/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>def test_my_feature():\n    test_graph = FeatureGraph()\n    with test_graph.use():\n\n        class MyFeature(Feature, spec=...):\n            pass\n\n        # Test operations here\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one.</p>"},{"location":"learn/testing/#graph-context-management","title":"Graph Context Management","text":"<p>The active graph uses context variables to support multiple graphs:</p> <pre><code># Default global graph (used in production)\ngraph = FeatureGraph()\n\n# Get active graph\nactive = FeatureGraph.get_active()\n\n# Use custom graph temporarily\nwith custom_graph.use():\n    # All operations use custom_graph\n    pass\n</code></pre> <p>This enables:</p> <ul> <li>Isolated testing: Each test gets its own feature registry</li> <li>Migration testing: Load historical graphs for migration scenarios</li> <li>Multi-environment testing: Test different feature configurations</li> </ul>"},{"location":"learn/testing/#testing-metadata-store-operations","title":"Testing Metadata Store Operations","text":""},{"location":"learn/testing/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Stores must be used as context managers to ensure proper resource cleanup:</p> <pre><code>def test_metadata_operations():\n    with InMemoryMetadataStore() as store:\n        # Create test data\n        df = pl.DataFrame(\n            {\n                \"sample_uid\": [1, 2, 3],\n                \"data_version\": {...},\n                \"feature_version\": \"abc123\",\n            }\n        )\n\n        # Write metadata\n        store.write_metadata(MyFeature, df)\n\n        # Read and verify\n        result = store.read_metadata(MyFeature)\n        assert len(result) == 3\n</code></pre>"},{"location":"learn/testing/#testing-with-different-backends","title":"Testing with Different Backends","text":"<p>Use parametrized tests to verify behavior across backends:</p> <pre><code>import pytest\n\n\n@pytest.mark.parametrize(\n    \"store_cls\",\n    [\n        InMemoryMetadataStore,\n        DuckDBMetadataStore,\n    ],\n)\ndef test_store_behavior(store_cls, tmp_path):\n    # Use tmp_path for file-based stores\n    store_kwargs = {}\n    if store_cls != InMemoryMetadataStore:\n        store_kwargs[\"path\"] = tmp_path / \"test.db\"\n\n    with store_cls(**store_kwargs) as store:\n        # Test your feature operations\n        pass\n</code></pre>"},{"location":"learn/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing AUTO_CREATE_TABLES Warnings","text":"<p>When testing with <code>auto_create_tables=True</code>, Metaxy emits warnings to remind you not to use this in production. These warnings are important for production safety, but can clutter test output.</p> <p>To suppress these warnings in your test suite, use pytest's <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but pytest filters it from test output.</p> <p>Testing the Warning Itself</p> <p>If you need to verify that the warning is actually emitted, use <code>pytest.warns()</code>:</p> <pre><code>import pytest\n\n\ndef test_auto_create_tables_warning():\n    with pytest.warns(\n        UserWarning, match=r\"AUTO_CREATE_TABLES is enabled.*do not use in production\"\n    ):\n        with DuckDBMetadataStore(\":memory:\", auto_create_tables=True) as store:\n            pass  # Warning is emitted and captured\n</code></pre> <p>This works even with <code>filterwarnings</code> configured, because <code>pytest.warns()</code> explicitly captures and verifies the warning.</p>"},{"location":"learn/testing/#testing-custom-alignment","title":"Testing Custom Alignment","text":"<p>If your feature overrides <code>load_input()</code> for custom alignment, test it thoroughly:</p> <pre><code>def test_custom_alignment():\n    # Prepare test data\n    current = pl.DataFrame({\"sample_uid\": [1, 2, 3], \"custom_field\": [\"a\", \"b\", \"c\"]})\n\n    upstream = {\n        \"video_feature\": pl.DataFrame({\"sample_uid\": [2, 3, 4], \"data_version\": {...}})\n    }\n\n    # Test alignment logic\n    result = MyFeature.load_input(current, upstream)\n\n    # Verify behavior\n    assert set(result[\"sample_uid\"].to_list()) == {2, 3}  # Inner join\n    assert \"custom_field\" in result.columns  # Custom fields preserved\n</code></pre>"},{"location":"learn/testing/#testing-feature-dependencies","title":"Testing Feature Dependencies","text":"<p>Verify that dependencies are correctly defined:</p> <pre><code>def test_feature_dependencies():\n    test_graph = FeatureGraph()\n\n    with test_graph.use():\n        # Define upstream feature\n        class UpstreamFeature(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"upstream\"]),\n                fields=[FieldSpec(key=FieldKey([\"data\"]), code_version=1)],\n            ),\n        ):\n            pass\n\n        # Define downstream feature with dependency\n        class DownstreamFeature(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"downstream\"]),\n                deps=[FeatureDep(feature=FeatureKey([\"upstream\"]))],\n                fields=[\n                    FieldSpec(\n                        key=FieldKey([\"processed\"]),\n                        code_version=1,\n                        deps=[\n                            FieldDep(\n                                feature=FeatureKey([\"upstream\"]),\n                                fields=[FieldKey([\"data\"])],\n                            )\n                        ],\n                    )\n                ],\n            ),\n        ):\n            pass\n\n        # Verify graph structure\n        assert len(test_graph.features_by_key) == 2\n        assert UpstreamFeature in test_graph.get_downstream(DownstreamFeature)\n</code></pre>"},{"location":"learn/testing/#testing-migrations","title":"Testing Migrations","text":""},{"location":"learn/testing/#simulating-feature-changes","title":"Simulating Feature Changes","text":"<p>Test how your features behave when definitions change:</p> <pre><code>def test_migration_scenario():\n    # Initial version\n    graph_v1 = FeatureGraph()\n    with graph_v1.use():\n\n        class MyFeatureV1(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"my_feature\"]),\n                fields=[FieldSpec(key=FieldKey([\"field1\"]), code_version=1)],\n            ),\n        ):\n            pass\n\n    # Record initial state\n    with InMemoryMetadataStore() as store:\n        store.record_feature_graph_snapshot(graph_v1)\n\n        # Modified version\n        graph_v2 = FeatureGraph()\n        with graph_v2.use():\n\n            class MyFeatureV2(\n                Feature,\n                spec=FeatureSpec(\n                    key=FeatureKey([\"my_feature\"]),\n                    fields=[FieldSpec(key=FieldKey([\"field1\"]), code_version=2)],\n                ),\n            ):\n                pass\n\n        # Verify version change detected\n        with graph_v2.use():\n            changes = store.detect_feature_changes(MyFeatureV2)\n            assert changes is not None\n</code></pre>"},{"location":"learn/testing/#testing-migration-idempotency","title":"Testing Migration Idempotency","text":"<p>Ensure migrations can be safely re-run:</p> <pre><code>def test_migration_idempotency():\n    with InMemoryMetadataStore() as store:\n        # Apply migration twice\n        apply_migration(store, migration)\n        result1 = store.read_metadata(MyFeature)\n\n        apply_migration(store, migration)  # Should be no-op\n        result2 = store.read_metadata(MyFeature)\n\n        # Results should be identical\n        assert result1.equals(result2)\n</code></pre>"},{"location":"learn/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Always use isolated graphs - Never rely on the global graph in tests</li> <li>Use context managers - Ensure proper cleanup of stores and resources</li> <li>Test across backends - Verify features work with different metadata stores</li> <li>Test edge cases - Empty data, missing dependencies, version conflicts</li> <li>Mock external dependencies - Isolate tests from external services</li> <li>Verify determinism - Feature versions should be consistent across runs</li> </ol>"},{"location":"learn/integrations/sqlmodel/","title":"SQLModel Integration","text":"<p>The SQLModel integration enables Metaxy features to function as both metadata-tracked features and SQLAlchemy ORM models.</p> <p>This integration combines Metaxy's versioning and dependency tracking with SQLModel's database mapping and query capabilities.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores. The benefits of using SQLModel are mostly in the ability to use migration systems such as Alembic that can ensure schema consistency with Metaxy features, and provide the tools for schema evolution as the features change over time.</p>"},{"location":"learn/integrations/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install metaxy[sqlmodel]\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#basic-usage","title":"Basic Usage","text":"<p>The integration has to be enabled in the configuration file:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_ENABLE=true\n</code></pre> <p>This will expose Metaxy's system tables to SQLAlchemy.</p> <p>First, as always with Metaxy features, we would have to define our ID columns:</p> <pre><code>from metaxy import BaseFeatureSpec\nfrom metaxy.ext.sqlmodel import BaseSQLModelFeature\n\n\nclass SampleFeatureSpec(BaseFeatureSpec):\n    id_columns: tuple[str] = \"sample_id\"\n\n\nclass SampleFeature(BaseSQLModelFeature, table=False, spec=None):\n    sample_id: str\n</code></pre> <p>Note that ID columns cannot be server-generated, so the cannot include a Primary Key.</p> <p>Now we can define feature class that inherits from <code>SampleFeature</code> and specify both Metaxy's <code>spec</code> parameter and SQLModel's <code>table=True</code> parameter:</p> <pre><code>from metaxy import FeatureKey, FieldSpec, FieldKey\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    SampleFeature,\n    table=True,\n    spec=SampleFeatureSpec(\n        key=FeatureKey([\"video\"]),\n        # Root feature with no dependencies\n        fields=[\n            FieldSpec(key=FieldKey([\"frames\"]), code_version=1),\n            FieldSpec(key=FieldKey([\"duration\"]), code_version=1),\n        ],\n    ),\n):\n    # User-defined metadata columns\n    path: str\n    duration: float\n</code></pre> <p>This class serves dual purposes:</p> <ul> <li>Metaxy feature: Tracks feature version, field versions, and dependencies</li> <li>SQLModel table: Maps to database schema with ORM functionality</li> </ul> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\"])</code>, the table name becomes <code>\"video\"</code>. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in Metaxy's configuration.</p>"},{"location":"learn/integrations/sqlmodel/#system-managed-columns","title":"System-Managed Columns","text":"<p>Metaxy's metadata store automatically manages versioning columns:</p> <ul> <li><code>data_version</code>: Struct column mapping field keys to hashes</li> <li><code>feature_version</code>: Hash of feature specification</li> <li><code>snapshot_version</code>: Hash of entire graph state</li> </ul> <p>These columns need not be defined in your SQLModel class. The metadata store injects them during write and read operations.</p>"},{"location":"learn/integrations/sqlmodel/#id-columns","title":"ID Columns","text":"<p>ID columns must exist before database insertion</p> <p>ID columns are used for joins between features, so their values must exist before insertion into the database. This means you cannot use server-generated values (autoincrement, sequences, server_default) for ID columns.</p> <pre><code>Metaxy validates against autoincrement primary keys but cannot detect all server-generated patterns. Ensure your ID columns use client-provided values.\n</code></pre> <p>Example:</p> <pre><code># \u2705 Good: Client-generated ID columns\nclass UserActivity(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"user\", \"activity\"]),\n        id_columns=[\"user_id\", \"session_id\"],  # Client provides these\n        ...\n    ),\n):\n    user_id: str = Field(primary_key=True)  # Client-generated\n    session_id: str = Field(primary_key=True)  # Client-generated\n    created_at: str = Field(sa_column_kwargs={\"server_default\": \"NOW()\"})  # OK - not an ID column\n\n# \u274c Bad: Autoincrement ID column\nclass BadFeature(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"bad\"]),\n        id_columns=[\"id\"],  # This is listed as an ID column\n        ...\n    ),\n):\n    id: int = Field(primary_key=True, sa_column_kwargs={\"autoincrement\": True})  # Will raise error\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#loading-features-and-populating-metadata","title":"Loading Features and Populating Metadata","text":"<p>When using <code>metaxyload_features()</code> to discover and import feature modules, all <code>SQLModelFeature</code> classes are automatically registered in SQLModel's metadata:</p> <pre><code>from metaxy importload_features\nfrom sqlmodel import SQLModel\n\n# Load all features from configured entrypoints\ngraph =load_features()\n\n# All SQLModelFeature tables are now registered in SQLModel.metadata\n# This metadata can be used with Alembic for migrations\nprint(f\"Tables registered: {list(SQLModel.metadata.tables.keys())}\")\n</code></pre> <p>This is particularly useful when:</p> <ul> <li>Generating Alembic migrations that need to discover all tables</li> <li>Setting up database connections that require the complete schema</li> <li>Using SQLModel's <code>create_all()</code> for development/testing (Metaxy's <code>auto_create_tables</code> setting should be preferred over <code>create_all()</code>)</li> </ul> <p>Migration Generation</p> <p>After calling <code>load_features()</code>, you can use Alembic to automatically detect all your SQLModelFeature tables and generate migration scripts.</p>"},{"location":"learn/integrations/sqlmodel/#configuration","title":"Configuration","text":"<p>Configure automatic table naming behavior:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#database-migrations-with-alembic","title":"Database Migrations with Alembic","text":"<p>Metaxy provides SQLModel definitions for its system tables that integrate with Alembic for database migrations. This allows you to version control schema changes alongside your application code. Note that you might want to keep separate migrations per each DB-backed <code>MetadataStore</code> used with Metaxy.</p>"},{"location":"learn/integrations/sqlmodel/#separate-migration-management","title":"Separate Migration Management","text":"<p>Metaxy system tables and user application tables should be managed in separate Alembic migration directories. This separation provides critical safety guarantees:</p> <p>System Table Isolation: Metaxy system tables (<code>metaxy-system__feature_versions</code>, <code>metaxy-system__migration_events</code>) have schemas managed by the framework. User migrations cannot accidentally modify these internal structures.</p> <p>Independent Evolution: Metaxy can evolve its system table schemas independently through framework updates without conflicts with user migrations.</p> <p>Failure Isolation: User migration failures remain isolated from metaxy's internal state tracking. A failed user migration leaves system tables intact for debugging and recovery.</p> <p>Clear Audit Trail: Separate migration histories make it trivial to distinguish framework schema changes from application schema changes. This clarity is essential during rollbacks and incident investigation.</p>"},{"location":"learn/integrations/sqlmodel/#setup","title":"Setup","text":"<p>Enable SQLModel system tables in your metaxy configuration and set up two Alembic directories:</p> <pre><code># Standard structure\nproject/\n\u251c\u2500\u2500 alembic/              # User application migrations\n\u2502   \u251c\u2500\u2500 versions/\n\u2502   \u2514\u2500\u2500 env.py\n\u251c\u2500\u2500 .metaxy/\n\u2502   \u2514\u2500\u2500 alembic-system/   # Metaxy system table migrations\n\u2502       \u251c\u2500\u2500 versions/\n\u2502       \u2514\u2500\u2500 env.py\n\u2514\u2500\u2500 metaxy.toml\n</code></pre> <p>Initialize both Alembic directories:</p> <pre><code># Initialize user migrations\nalembic init alembic\n\n# Initialize metaxy system migrations\nalembic init .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#metaxy-system-tables-configuration","title":"Metaxy System Tables Configuration","text":"<p>Configure <code>.metaxy/alembic-system/env.py</code> to manage only metaxy system tables:</p> <pre><code># typical Alembic boilerplate\nfrom metaxy.ext.alembic import get_metaxy_metadata\n\nmetaxy_system_metadata = get_metaxy_metadata()\n\n# metaxy_system_metadata has system tables\n\n# continue with alembic boilerplate\n</code></pre> <p>Configure <code>.metaxy/alembic-system/alembic.ini</code> with your database URL:</p> <pre><code>[alembic]\nscript_location = .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#user-application-tables-configuration","title":"User Application Tables Configuration","text":"<p>Configure <code>alembic/env.py</code> to manage user tables, excluding metaxy system tables:</p> <pre><code># standard Alembic boilerplate\nfrom sqlmodel import SQLModel\nfrom metaxy importload_features\nload_features()\n\n# SQLModel.metadata now has user-defined Metaxy tables\n\n\n# continue with alembic boilerplate\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#migration-workflow","title":"Migration Workflow","text":"<p>Generate and apply migrations separately for each concern:</p> <pre><code># 1. Create metaxy system tables (run once during initial setup)\nalembic -c .metaxy/alembic-system/alembic.ini revision --autogenerate -m \"create metaxy system tables\"\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head\n\n# 2. Create and apply user table migrations\nalembic revision --autogenerate -m \"add video feature table\"\nalembic upgrade head\n\n# 3. When modifying user tables, only user migrations change\nalembic revision --autogenerate -m \"add processing timestamp\"\nalembic upgrade head\n</code></pre> <p>When deploying to production, always apply system table migrations before user migrations:</p> <pre><code># Production deployment order\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head  # System tables first\nalembic upgrade head                                 # Then user tables\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#disabling-sqlmodel-system-tables","title":"Disabling SQLModel System Tables","text":"<p>If required, disable SQLModel system tables in <code>metaxy.toml</code>:</p> <pre><code>[ext.sqlmodel]\nenabled = true\nsystem_tables = false\n</code></pre>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Metaxy - Feature Metadata Management</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>migrations</code></li> <li><code>generate</code></li> <li><code>apply</code></li> <li><code>status</code></li> <li><code>list</code></li> <li><code>explain</code></li> <li><code>describe</code></li> <li><code>graph</code></li> <li><code>push</code></li> <li><code>history</code></li> <li><code>describe</code></li> <li><code>render</code></li> <li><code>graph-diff</code></li> <li><code>render</code></li> <li><code>list</code></li> <li><code>features</code></li> <li><code>metadata</code></li> <li><code>copy</code></li> <li><code>drop</code></li> </ul> <p>Usage:</p> <pre><code>$ metaxy COMMAND\n</code></pre> <p>Arguments:</p> <p>Options:</p> <ul> <li><code>--config-file</code>: Global option. Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Global option. Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Global option. Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: --no-all-projects]</li> </ul> <p>Commands:</p> <ul> <li><code>graph</code>: Manage feature graphs</li> <li><code>graph-diff</code>: Compare and visualize graph snapshots</li> <li><code>list</code>: List Metaxy entities</li> <li><code>metadata</code>: Manage Metaxy metadata</li> <li><code>migrations</code>: Metadata migration commands</li> <li><code>shell</code>: Start interactive shell.</li> </ul>"},{"location":"reference/cli/#shell","title":"<code>shell</code>","text":"<p>Start interactive shell.</p> <p>Usage:</p> <pre><code>$ shell\n</code></pre>"},{"location":"reference/cli/#migrations","title":"<code>migrations</code>","text":"<p>Metadata migration commands</p> <p>Usage:</p> <pre><code>$ migrations COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>apply</code>: Apply migration(s) from YAML files.</li> <li><code>describe</code>: Show verbose description of migration(s).</li> <li><code>explain</code>: Show detailed diff for a migration.</li> <li><code>generate</code>: Generate migration from detected feature changes.</li> <li><code>list</code>: List all migrations in chain order as defined in code.</li> <li><code>status</code>: Show migration chain and execution status.</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-generate","title":"<code>metaxy migrations generate</code>","text":"<p>Generate migration from detected feature changes.</p> <p>Compares the latest snapshot in the store (or specified from_snapshot) with the current active graph to detect changes.</p> <p>The migration is recorded in the system tables (not a YAML file).</p> <p>Usage:</p> <pre><code>$ metaxy migrations generate --op LIST[STR] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>--OP</code>: Operation class path to use (can be repeated). Example: metaxy.migrations.ops.DataVersionReconciliation  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--name</code>: Migration name (creates {timestamp}_{name} ID)</li> <li><code>--store</code>: Store name (defaults to default)</li> <li><code>--from-snapshot</code>: Compare from this historical snapshot version (defaults to latest)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-apply","title":"<code>metaxy migrations apply</code>","text":"<p>Apply migration(s) from YAML files.</p> <p>Reads migration definitions from .metaxy/migrations/ directory (git). Follows parent chain to ensure correct order.  Tracks execution state in database (events).</p> <p>Usage:</p> <pre><code>$ metaxy migrations apply [OPTIONS] [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to apply (applies all unapplied if not specified)</li> <li><code>STORE, --store</code>: Metadata store to use.</li> <li><code>--dry-run, --no-dry-run</code>: Preview changes without executing  [default: --no-dry-run]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-status","title":"<code>metaxy migrations status</code>","text":"<p>Show migration chain and execution status.</p> <p>Reads migration definitions from YAML files (git). Shows execution status from database events. Displays the parent  chain in order.</p> <p>Usage:</p> <pre><code>$ metaxy migrations status\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-list","title":"<code>metaxy migrations list</code>","text":"<p>List all migrations in chain order as defined in code.</p> <p>Displays a simple table showing migration ID, creation time, and operations.</p> <p>Usage:</p> <pre><code>$ metaxy migrations list\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-explain","title":"<code>metaxy migrations explain</code>","text":"<p>Show detailed diff for a migration.</p> <p>Reads migration from YAML file. Computes and displays the GraphDiff between the two snapshots on-demand.</p> <p>Usage:</p> <pre><code>$ metaxy migrations explain [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to explain (explains latest if not specified)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-describe","title":"<code>metaxy migrations describe</code>","text":"<p>Show verbose description of migration(s).</p> <p>Displays detailed information about what the migration will do: - Migration metadata (ID, parent, snapshots, created  timestamp) - Operations to execute - Affected features with row counts - Execution status if already run</p> <p>Usage:</p> <pre><code>$ metaxy migrations describe [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-IDS, --migration-ids, --empty-migration-ids</code>: Migration IDs to describe (default: all migrations in order)  [default: []]</li> <li><code>STORE, --store</code>: Metadata store to use.</li> </ul>"},{"location":"reference/cli/#graph","title":"<code>graph</code>","text":"<p>Manage feature graphs</p> <p>Usage:</p> <pre><code>$ graph COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>describe</code>: Describe a graph snapshot.</li> <li><code>history</code>: Show history of recorded graph snapshots.</li> <li><code>push</code>: Record all feature versions (push graph snapshot).</li> <li><code>render</code>: Render feature graph visualization.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-push","title":"<code>metaxy graph push</code>","text":"<p>Record all feature versions (push graph snapshot).</p> <p>Records all features in the active graph to the metadata store with a deterministic snapshot version. This should be run after deploying new feature definitions.</p> <p>Usage:</p> <pre><code>$ metaxy graph push [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"<code>metaxy graph history</code>","text":"<p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing snapshot versions, when they were recorded, and  feature counts.</p> <p>Usage:</p> <pre><code>$ metaxy graph history [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-describe","title":"<code>metaxy graph describe</code>","text":"<p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph  depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no  dependents) - Project breakdown (if multi-project)</p> <p>Usage:</p> <pre><code>$ metaxy graph describe [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"<code>metaxy graph render</code>","text":"<p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:</p> <pre><code>graph (default): Hierarchical tree view  cards: Panel/card-based view with dependency edges\n</code></pre> <p>\u2022 mermaid: Mermaid flowchart markup  \u2022 graphviz: Graphviz DOT format</p> <p>Usage:</p> <pre><code>$ metaxy graph render [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>SHOW-FIELDS, --show-fields, --no-show-fields</code>: Render configuration  [default: --show-fields]</li> <li><code>SHOW-FEATURE-VERSIONS, --show-feature-versions, --no-show-feature-versions</code>: Render configuration  [default: --show-feature-versions]</li> <li><code>SHOW-FIELD-VERSIONS, --show-field-versions, --no-show-field-versions</code>: Render configuration  [default: --show-field-versions]</li> <li><code>SHOW-CODE-VERSIONS, --show-code-versions, --no-show-code-versions</code>: Render configuration  [default: --no-show-code-versions]</li> <li><code>SHOW-SNAPSHOT-VERSION, --show-snapshot-version, --no-show-snapshot-version</code>: Render configuration  [default: --show-snapshot-version]</li> <li><code>HASH-LENGTH, --hash-length</code>: Render configuration  [default: 8]</li> <li><code>DIRECTION, --direction</code>: Render configuration  [default: TB]</li> <li><code>FEATURE, --feature</code>: Render configuration</li> <li><code>UP, --up</code>: Render configuration</li> <li><code>DOWN, --down</code>: Render configuration</li> <li><code>PROJECT, --project</code>: Render configuration</li> <li><code>SHOW-PROJECTS, --show-projects, --no-show-projects</code>: Render configuration  [default: --show-projects]</li> <li><code>-f, --format</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>-t, --type</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to render (default: current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>MINIMAL, --minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>VERBOSE, --verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> </ul>"},{"location":"reference/cli/#graph-diff","title":"<code>graph-diff</code>","text":"<p>Compare and visualize graph snapshots</p> <p>Usage:</p> <pre><code>$ graph-diff COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>render</code>: Render merged graph visualization comparing two snapshots.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-diff-render","title":"<code>metaxy graph-diff render</code>","text":"<p>Render merged graph visualization comparing two snapshots.</p> <p>Shows all features color-coded by status (added/removed/changed/unchanged). Uses the unified rendering system - same  renderers as 'metaxy graph render'.</p> <p>Special snapshot literals: - \"latest\": Most recent snapshot in the store - \"current\": Current graph state from code</p> <p>Output formats: - terminal: Hierarchical tree view (default) - cards: Panel/card-based view - mermaid: Mermaid flowchart diagram - graphviz: Graphviz DOT format</p> <p>Usage:</p> <pre><code>$ metaxy graph-diff render FROM-SNAPSHOT [ARGS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>FROM-SNAPSHOT</code>: First snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>TO-SNAPSHOT, --to-snapshot</code>: Second snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [default: current]</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-f, --format</code>: Output format: terminal, cards, mermaid, graphviz, json, or yaml  [choices: terminal, cards, mermaid, graphviz, json, yaml] [default: terminal]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>SHOW-FIELDS, --show-fields, --no-show-fields</code>: Render configuration  [default: --show-fields]</li> <li><code>SHOW-FEATURE-VERSIONS, --show-feature-versions, --no-show-feature-versions</code>: Render configuration  [default: --show-feature-versions]</li> <li><code>SHOW-FIELD-VERSIONS, --show-field-versions, --no-show-field-versions</code>: Render configuration  [default: --show-field-versions]</li> <li><code>SHOW-CODE-VERSIONS, --show-code-versions, --no-show-code-versions</code>: Render configuration  [default: --no-show-code-versions]</li> <li><code>SHOW-SNAPSHOT-VERSION, --show-snapshot-version, --no-show-snapshot-version</code>: Render configuration  [default: --show-snapshot-version]</li> <li><code>HASH-LENGTH, --hash-length</code>: Render configuration  [default: 8]</li> <li><code>DIRECTION, --direction</code>: Render configuration  [default: TB]</li> <li><code>FEATURE, --feature</code>: Render configuration</li> <li><code>UP, --up</code>: Render configuration</li> <li><code>DOWN, --down</code>: Render configuration</li> <li><code>PROJECT, --project</code>: Render configuration</li> <li><code>SHOW-PROJECTS, --show-projects, --no-show-projects</code>: Render configuration  [default: --show-projects]</li> <li><code>MINIMAL, --minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>VERBOSE, --verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> </ul>"},{"location":"reference/cli/#list","title":"<code>list</code>","text":"<p>List Metaxy entities</p> <p>Usage:</p> <pre><code>$ list COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>features</code>: List Metaxy features.</li> </ul>"},{"location":"reference/cli/#metaxy-list-features","title":"<code>metaxy list features</code>","text":"<p>List Metaxy features.</p> <p>Usage:</p> <pre><code>$ metaxy list features\n</code></pre>"},{"location":"reference/cli/#metadata","title":"<code>metadata</code>","text":"<p>Manage Metaxy metadata</p> <p>Usage:</p> <pre><code>$ metadata COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>copy</code>: Copy metadata between stores.</li> <li><code>drop</code>: Drop metadata from a store.</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-copy","title":"<code>metaxy metadata copy</code>","text":"<p>Copy metadata between stores.</p> <p>Copies metadata for specified features from one store to another, optionally using a historical version. Useful for: -  Migrating data between environments - Backfilling metadata - Copying specific feature versions</p> <p>Incremental Mode (default):     By default, performs an anti-join on sample_uid to skip rows that already exist in the destination for the same  snapshot_version. This prevents duplicate writes.  Disabling incremental (--no-incremental) may improve performance  when: - The destination store is empty or has no overlap with source - The destination store has eventual deduplication</p> <p>Usage:</p> <pre><code>$ metaxy metadata copy FROM TO [ARGS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>FROM</code>: Source store name (must be configured in metaxy.toml)  [required]</li> <li><code>TO</code>: Destination store name (must be configured in metaxy.toml)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to copy (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses  --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Copy all features from source store  [default: --no-all-features]</li> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to copy (defaults to latest in source store). The snapshot_version is preserved in the destination.</li> <li><code>INCREMENTAL, --incremental, --no-incremental</code>: Use incremental copy (compare data_version to skip existing rows). Disable for better performance if destination is  empty or uses deduplication.  [default: --incremental]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-drop","title":"<code>metaxy metadata drop</code>","text":"<p>Drop metadata from a store.</p> <p>Removes metadata for specified features from the store. This is a destructive operation and requires --confirm flag.</p> <p>Useful for: - Cleaning up test data - Re-computing feature metadata from scratch - Removing obsolete features</p> <p>Usage:</p> <pre><code>$ metaxy metadata drop [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Store name to drop metadata from (defaults to configured default store)</li> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to drop (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses  --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Drop metadata for all features in the store  [default: --no-all-features]</li> <li><code>CONFIRM, --confirm, --no-confirm</code>: Confirm the drop operation (required to prevent accidental deletion)  [default: --no-confirm]</li> </ul>"},{"location":"reference/cli/#examples","title":"Examples","text":""},{"location":"reference/cli/#recording-a-graph-snapshot","title":"Recording a graph snapshot","text":"<pre><code># Push the current feature graph to the metadata store\nmetaxy graph push\n</code></pre> <p>The recommendation is to run this command in your CD pipeline.</p>"},{"location":"reference/cli/#generating-and-applying-migrations","title":"Generating and applying migrations","text":"<pre><code># Generate a migration for detected changes\nmetaxy migrations generate --op metaxy.migrations.ops.DataVersionReconciliation\n\n# Apply pending migrations\nmetaxy migrations apply\n</code></pre>"},{"location":"reference/cli/#visualizing-the-feature-graph","title":"Visualizing the feature graph","text":"<pre><code># Render as terminal tree view\nmetaxy graph render\n\n# Render as Mermaid diagram\nmetaxy graph render --format mermaid\n\n# Compare two snapshots\nmetaxy graph-diff render &lt;snapshot-id&gt; current --format mermaid\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Metaxy can be configured using TOML configuration files or environment variables.</p>"},{"location":"reference/configuration/#default-configuration","title":"Default Configuration","text":"<p>Here is the complete default configuration with all available options:</p> !metaxy.tomlpyproject.toml <pre><code># Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy]\n# Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[tool.metaxy.ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre>"},{"location":"reference/configuration/#configuration-fields","title":"Configuration Fields","text":"<p>Each field can be set via TOML configuration or environment variables.</p>"},{"location":"reference/configuration/#store","title":"<code>store</code>","text":"<p>Default metadata store to use</p> <p>Type: <code>str</code>  | Default: <code>\"dev\"</code></p> !metaxy.tomlpyproject.toml <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORE=dev\n</code></pre>"},{"location":"reference/configuration/#stores","title":"<code>stores</code>","text":"<p>Named store configurations</p> <p>Type: dict[str, metaxy.config.StoreConfig]</p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# stores = {}\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# stores = {}\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORES=...\n</code></pre>"},{"location":"reference/configuration/#migrations_dir","title":"<code>migrations_dir</code>","text":"<p>Directory where migration files are stored</p> <p>Type: <code>str</code>  | Default: <code>\".metaxy/migrations\"</code></p> !metaxy.tomlpyproject.toml <pre><code>migrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>[tool.metaxy]\nmigrations_dir = \".metaxy/migrations\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_MIGRATIONS_DIR=.metaxy/migrations\n</code></pre>"},{"location":"reference/configuration/#entrypoints","title":"<code>entrypoints</code>","text":"<p>List of Python module paths to load for feature discovery</p> <p>Type: <code>list[str]</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# entrypoints = []\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_ENTRYPOINTS=...\n</code></pre>"},{"location":"reference/configuration/#theme","title":"<code>theme</code>","text":"<p>Graph rendering theme for CLI visualization</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>theme = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\ntheme = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_THEME=default\n</code></pre>"},{"location":"reference/configuration/#hash_truncation_length","title":"<code>hash_truncation_length</code>","text":"<p>Truncate hash values to this length (minimum 8 characters). None = no truncation.</p> <p>Type: <code>int | None</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# hash_truncation_length = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# hash_truncation_length = null\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=...\n</code></pre>"},{"location":"reference/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre>"},{"location":"reference/configuration/#project","title":"<code>project</code>","text":"<p>Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>project = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_PROJECT=default\n</code></pre>"},{"location":"reference/configuration/#ext-sqlmodel-configuration","title":"Ext &gt; Sqlmodel Configuration","text":""},{"location":"reference/configuration/#extsqlmodelenable","title":"<code>ext.sqlmodel.enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__ENABLE=false\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelinfer_db_table_names","title":"<code>ext.sqlmodel.infer_db_table_names</code>","text":"<p>Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's tablename value.</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelsystem_tables","title":"<code>ext.sqlmodel.system_tables</code>","text":"<p>Whether to use SQLModel definitions for system tables (for Alembic migrations).</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nsystem_tables = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__SYSTEM_TABLES=true\n</code></pre>"},{"location":"reference/configuration/#configuration-types","title":"Configuration Types","text":""},{"location":"reference/configuration/#storeconfig","title":"StoreConfig","text":"<p>Configuration for a single metadata store backend.</p> <p>Fields:</p> <ul> <li><code>type</code> (str): Full import path to the store class</li> <li><code>config</code> (dict[str, Any]): Store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#extconfig","title":"ExtConfig","text":"<p>Configuration for Metaxy integrations with third-party tools.</p> <p>Fields:</p> <ul> <li><code>sqlmodel</code> (SQLModelConfig): SQLModel integration configuration</li> </ul>"},{"location":"reference/configuration/#sqlmodelconfig","title":"SQLModelConfig","text":"<p>Configuration for SQLModel integration.</p> <p>Fields:</p> <ul> <li><code>enable</code> (bool): Whether to enable the plugin (default: <code>false</code>)</li> <li><code>infer_db_table_names</code> (bool): Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's <code>__tablename__</code> value (default: <code>true</code>)</li> <li><code>system_tables</code> (bool): Whether to use SQLModel definitions for system tables (default: <code>true</code>)</li> </ul>"},{"location":"reference/configuration/#store-configuration","title":"Store Configuration","text":"<p>The <code>stores</code> field configures metadata store backends. Each store is defined by:</p> <ul> <li><code>type</code>: Full import path to the store class (e.g., <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code>)</li> <li><code>config</code>: Dictionary of store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#example-multiple-stores-with-fallback-chain","title":"Example: Multiple Stores with Fallback Chain","text":"!metaxy.tomlpyproject.toml <pre><code># Default store to use\nstore = \"dev\"\n\n# Development store (in-memory) with fallback to production\n[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n# Production store\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n\n[tool.metaxy.stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n[tool.metaxy.stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre>"},{"location":"reference/configuration/#available-store-types","title":"Available Store Types","text":"Store Type Import Path Description DuckDB <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code> File-based or in-memory DuckDB backend ClickHouse <code>metaxy.metadata_store.clickhouse.ClickHouseMetadataStore</code> ClickHouse database backend In-Memory <code>metaxy.metadata_store.memory.InMemoryMetadataStore</code> In-memory backend for testing"},{"location":"reference/configuration/#getting-a-store-instance","title":"Getting a Store Instance","text":"<pre><code>from metaxy.config import MetaxyConfig\n\nconfig = MetaxyConfig.load()\n\n# Get the default store\nwith config.get_store() as store:\n    # Use store\n    pass\n\n# Get a specific store by name\nwith config.get_store(\"prod\") as store:\n    # Use store\n    pass\n</code></pre>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol> <p>This allows you to override file-based configuration with environment variables, which is useful for CI/CD pipelines and different deployment environments.</p>"},{"location":"reference/configuration/#loading-configuration","title":"Loading Configuration","text":""},{"location":"reference/configuration/#auto-discovery","title":"Auto-Discovery","text":"<pre><code>from metaxy.config import MetaxyConfig\n\n# Auto-discover config file in current or parent directories\nconfig = MetaxyConfig.load()\n</code></pre>"},{"location":"reference/configuration/#explicit-file","title":"Explicit File","text":"<pre><code># Load from specific file\nconfig = MetaxyConfig.load(\"path/to/metaxy.toml\")\n</code></pre>"},{"location":"reference/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code># Create configuration programmatically\nconfig = MetaxyConfig(\n    store=\"prod\",\n    migrations_dir=\".migrations\",\n)\n</code></pre>"}]}