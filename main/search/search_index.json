{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"Metaxy"},{"location":"#metaxy","title":"Metaxy","text":"<p>Metaxy is a pluggable metadata layer for building multi-modal Data and ML pipelines that manages and tracks metadata across complex computational graphs, including row-level versions (1), while allowing the codebase to evolve over time without friction. Metaxy gives you quite a few superpowers:</p> <ul> <li>Cache every single sample in the data pipeline. Millions of cache keys can be calculated in under a second. (2)</li> <li>Freedom from storage lock-in. Swap storage backends in development and production environments without breaking a sweat (3).</li> <li>Use the <code>mx</code> CLI to observe and manage metadata without leaving the comfort of your terminal.</li> <li>Metaxy is composable and extensible (4): use it to build custom integrations and workflows!</li> </ul> <ol> <li>And even more granular partial data versions</li> <li>Our experience at Anam with ClickHouse</li> <li>For example, develop against DeltaLake and scale production with ClickHouse without code changes.</li> <li>See our official integrations here</li> </ol> <p>Granular Data Versioning</p> <p>The feature that makes Metaxy really stand out is the ability to track partial data dependencies (1) and skip downstream updates unless the exactly required subset of upstream data has changed. At the moment of writing, Metaxy is the only available tool that tackles these problems.</p> <ol> <li>which are very common in multi-modal pipelines, for example when you only need to process video frames and not the audio tracks</li> </ol> <p>All of this is possible thanks to (1) Narwhals, Ibis, and a few clever tricks.</p> <ol> <li>we really do stand on the shoulders of giants</li> </ol> Data vs Metadata Clarifications <p>Metaxy manages metadata while data typically (1) lives elsewhere: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Metadata (Metaxy)          \u2502          \u2502   Data (e.g., S3)       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u2502                         \u2502\n\u2502  ID  \u2502   path   \u2502 size \u2502version \u2502          \u2502  \ud83d\udce6 s3://my-bucket/     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524          \u2502                         \u2502\n\u2502 img1 \u2502 s3://... \u2502 2.1M \u2502a3fdsf  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502    \u251c\u2500 img1.jpg          \u2502\n\u2502 img2 \u2502 s3://... \u2502 1.8M \u2502b7e123  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502    \u251c\u2500 img2.jpg          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> Subject Description Data The actual multi-modal data itself, such as images, audio files, video files, text documents, and other raw content that your pipelines process and transform. Metadata Information about the data, typically including references to where data is stored (e.g., object store keys) plus additional descriptive entries such as video length, file size, format, version, and other attributes. <ol> <li>Unless you are a LanceDB fan, in which case we got you covered</li> </ol>"},{"location":"#reliability","title":"Reliability","text":"<p>Metaxy is fanatically tested across all supported metadata stores, Python versions and platforms <sup>1</sup>. We guarantee versioning consistency across the supported metadata stores.</p> <p>Metaxy was built to handle large amounts of big metadata in distributed environments, makes very little assumptions about usage patterns and cannot enter an inconsistent state.</p> <p>We have been dogfooding Metaxy since December 2025 at Anam. We are running it in production with ClickHouse, Dagster, and Ray (1).</p> <ol> <li>and integrations with these tools are probably the most complete at the moment</li> </ol>"},{"location":"#installation","title":"Installation","text":"<p>Warning</p> <p>Metaxy hasn't been publicly released yet, but you can try the latest dev release:</p> <pre><code>pip install --pre metaxy\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Tip</p> <p>Urging to get your hands dirty? Head to Quickstart (WIP!).</p>"},{"location":"#what-is-the-problem-again","title":"What is the problem again?","text":"<p>Info</p> <p>Data, ML and AI workloads processing large amounts of images, videos, audios, or texts (1) can be very expensive to run. In contrast to traditional data engineering, re-running the whole pipeline on changes is no longer an option. Therefore, it becomes crucially important to correctly implement incremental processing and sample-level versioning.</p> <ol> <li>or really any kind of data</li> </ol> <p>These workloads often aren't stale: they evolve all the time, with new data being shipped, bugfixes or algorithm changes introduced, and new features added to the pipeline. This means the pipeline has to be re-computed frequently, but at the same time it's important to avoid unnecessary recomputations for individual data samples.</p> <p>Here are some of the cases where re-computing would be undesirable:</p> <ul> <li>merging two consecutive steps into one (refactoring the graph topology)</li> <li>partial data updates, e.g. changing only the audio track inside a video file</li> <li>backfilling metadata from another source</li> </ul> <p>Correctly identifying these scenarios while also re-computing the feature when it should be is surprisingly challenging, and tracking and propagating these changes correctly to the right subset of samples and features can become incredibly complicated and time-consuming.</p>"},{"location":"#so-what-can-we-do-about-this","title":"So what can we do about this?","text":"<p>Sounds really bad, right? Yes, and it is (1). Until recently, a general solution for this problem did not exist, but not anymore  !</p> <ol> <li>I cannot overestimate the amount of hair pulling I've endured before making Metaxy</li> </ol> <p>Just Use Metaxy</p> <p>Metaxy solves this!</p> <ol> <li> <p>Metaxy builds a versioned graph from declarative feature definitions and tracks version changes across individual samples. These computations can be scaled to run on millions of samples.</p> </li> <li> <p>Metaxy introduces a unique field-level dependency system to express partial data dependencies and avoiding unnecessary downstream recomputations. Each sample holds a dictionary mapping its fields to their respective versions. (1)</p> </li> <li> <p>Metaxy implements a general <code>MetadataStore</code> interface that enables users to interact with storage systems -- be it an analytical database or a LakeHouse -- in the same way.</p> </li> </ol> <ol> <li>for example, a <code>video</code> sample could independently version the frames and the audio track: <code>{\"audio\": \"asddsa\", \"frames\": \"joasdb\"}</code></li> </ol>"},{"location":"#more-information","title":"More Information","text":"<p>Here are a few more useful links:</p> <ul> <li>Take your time and read a bit more (1) about Metaxy here</li> <li>Jump to Quickstart if you just can't wait (WIP!)</li> <li>Abstract Metaxy concepts are discussed here</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Use Metaxy from the command line</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul> <ol> <li>just one more page, I promise, just one more page</li> </ol> <ol> <li> <p>The CLI is not tested on Windows yet.\u00a0\u21a9</p> </li> </ol>"},{"location":"overview/","title":"About Metaxy","text":"<p>Metaxy is...</p>"},{"location":"overview/#composable","title":"\ud83e\udde9 Composable","text":"<p>Bring your own... really everything. Metaxy is a universal glue for metadata. Use it with:</p> <ul> <li>Your database or storage format of choice to keep metadata where you want. DuckDB, ClickHouse and 20+ databases via Ibis (1), lakehouse storage formats such as DeltaLake or DuckLake, and other solutions such as LanceDB. All of this is available through a unified interface.</li> <li>Your favorite dataframe library: Polars, Pandas, or even run all Metaxy computations in the DB thanks to Narwhals</li> <li>Orchestrators: see the excellent Dagster integration </li> <li>Compute frameworks like Ray. We totally don't care how is data (2) produced or where is it stored.</li> <li>Version tracking methods. By default, Metaxy uses Merkle Trees to track changes in metadata. However, users can provide their own data versions and use content-based hashing techniques (3) if needed.</li> </ul> <ol> <li>while we don't (yet) ship native support for all these databases, the base <code>IbisMetadataStore</code> can be easily extended to handle additional databases</li> <li>so not the tables but the actual stuff: images, videos, texts, etc.</li> <li>from naive <code>sha256</code> to more sophisticated semantic hashing</li> </ol>"},{"location":"overview/#reliable","title":"\ud83e\udea8 Reliable","text":"<p>Metaxy is obsessively tested across all supported tabular compute engines. We guarantee to produce versioning hashes that are consistent across DBs and local compute engines. We really have tested this very well! (1)</p> <ol> <li>At the moment of writing our test suite contains more than 2000 tests executed against Linux, Windows and MacOS on all supported Python versions</li> </ol> <p>Metadata is organized in append-only tables. Metaxy never attempts to modify historical metadata, (1) ensuring that data integrity is maintained and historical metadata can be easily retrieved and analyzed.</p> <ol> <li>But provides the ability to perform hard and soft deletions</li> </ol>"},{"location":"overview/#scalable","title":"\ud83d\udcc8 Scalable","text":"<p>Metaxy is built with performance in mind: all operations default to run in the DB, the storage layout is designed with the goal of supporting parallel writers and bulk insertions.</p> <p>Feature definitions can be split across independent Python modules and packages and automatically loaded via packaging entry points. This enables collaboration across teams and projects.</p> <p>We also have a Ray integration which simplifies working with Metaxy from distributed workflows.</p>"},{"location":"overview/#developer-friendly","title":"\ud83e\uddd1\u200d\ud83d\udcbb Developer Friendly","text":"<p>Metaxy provides a clean, intuitive Python API with syntactic sugar that simplifies common feature definitions. The feature discovery system enables effortless feature dependency management.</p> <p>The library includes comprehensive type hints (1), and utilizes Pydantic for feature definitions. There's first-class support for local development (2), testing, preview environments, and CI/CD workflows.</p> <ol> <li>with all the typing shenanigans you would expect from a project as serious as ours</li> <li>the reference local versioning engine is implemented in Polars and <code>polars-hash</code></li> </ol> <p>The included CLI tool allows easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats. You can even drop your database in one command! (1)</p> <ol> <li>that's a joke, it can only be truncated from CLI</li> </ol> <p>Hopefully this was impressive enough and has sparked some interest in Metaxy!</p>"},{"location":"overview/#whats-next","title":"\ud83d\ude80 What's Next?","text":"<ul> <li>Itching to write some Metaxy code? Continue to Quickstart (WIP!).</li> <li>Learn more about feature definitions or versioning</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Use Metaxy from the command line</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul>"},{"location":"examples/","title":"Metaxy Examples","text":"<p>Here you can find complete example Metaxy projects. Each example is a self-contained Python project that demonstrates a specific feature or use case of Metaxy.</p> <p>Verified</p> <p>Examples are continuously tested in CI and are guaranteed to work.</p> <ul> <li>Aggregation</li> <li>Basic Example</li> <li>Expansion</li> </ul>"},{"location":"examples/aggregation/","title":"Aggregation","text":""},{"location":"examples/aggregation/#overview","title":"Overview","text":"<p> View Example Source on GitHub</p> <p>This example demonstrates how to implement aggregation (<code>N:1</code>) relationships with Metaxy. In such relationships multiple parent samples produce a single child sample.</p> <p>These relationships can be modeled with LineageRelationship.aggregation lineage type.</p> <p>We will use a speaker embedding pipeline as an example, where multiple audio recordings from the same speaker are aggregated to compute a single speaker embedding.</p>"},{"location":"examples/aggregation/#the-pipeline","title":"The Pipeline","text":"<p>Let's define a pipeline with two features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    audio[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;audio&lt;/b&gt;&lt;br/&gt;3dac67c8&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- default (80200592)&lt;/div&gt;\"]\n    speaker_embedding[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;speaker/embedding&lt;/b&gt;&lt;br/&gt;a485fa5b&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- embedding (27a15391)&lt;/div&gt;\"]\n    audio --&gt; speaker_embedding\n</code></pre>"},{"location":"examples/aggregation/#defining-features-audio","title":"Defining features: <code>Audio</code>","text":"<p>Each audio recording has an <code>audio_id</code> (unique identifier) and a <code>speaker_id</code> (which speaker it belongs to). Multiple audio recordings can belong to the same speaker.</p> src/example_aggregation/features.py<pre><code>import metaxy as mx\n\n\nclass Audio(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"audio\",\n        id_columns=[\"audio_id\"],\n        fields=[\"default\"],\n    ),\n):\n    \"\"\"Audio recordings of different speakers.\"\"\"\n\n    audio_id: str\n    speaker_id: str\n    duration_seconds: float\n    path: str\n</code></pre>"},{"location":"examples/aggregation/#defining-features-speakerembedding","title":"Defining features: <code>SpeakerEmbedding</code>","text":"<p><code>SpeakerEmbedding</code> aggregates all audio recordings from a speaker into a single embedding. The key configuration is the <code>lineage</code> parameter which tells Metaxy that multiple <code>Audio</code> records with the same <code>speaker_id</code> are aggregated into one <code>SpeakerEmbedding</code>.</p> src/example_aggregation/features.py<pre><code>class SpeakerEmbedding(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"speaker/embedding\",\n        id_columns=[\"speaker_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=Audio,\n                lineage=mx.LineageRelationship.aggregation(on=[\"speaker_id\"]),\n            )\n        ],\n        fields=[\n            mx.FieldSpec(key=\"embedding\", code_version=\"1\"),\n        ],\n    ),\n):\n    \"\"\"Speaker embedding aggregated from all their audio recordings.\n\n    This demonstrates N:1 aggregation lineage where multiple audio recordings\n    from the same speaker are aggregated into a single speaker embedding.\n    \"\"\"\n\n    speaker_id: str\n    n_dim: int\n    path: str\n</code></pre> <p>The <code>LineageRelationship.aggregation(on=[\"speaker_id\"])</code> declaration is the key part. It tells Metaxy:</p> <ol> <li>Multiple <code>Audio</code> rows are aggregated into one <code>SpeakerEmbedding</code> row</li> <li>The aggregation is keyed on <code>speaker_id</code> - all audio with the same speaker_id contributes to one embedding</li> <li>When any audio for a speaker changes, the aggregated provenance changes, triggering recomputation of that speaker's embedding</li> </ol>"},{"location":"examples/aggregation/#walkthrough","title":"Walkthrough","text":"<p>Here is the pipeline code that processes audio and computes speaker embeddings:</p> <code>pipeline.py</code> pipeline.py<pre><code>import metaxy as mx\nimport polars as pl\n\nfrom example_aggregation.features import Audio, SpeakerEmbedding\n\n# Audio samples: 2 speakers with 2 recordings each\nAUDIO_SAMPLES = pl.DataFrame(\n    [\n        {\n            \"audio_id\": \"a1\",\n            \"speaker_id\": \"s1\",\n            \"duration_seconds\": 30.5,\n            \"path\": \"audio/s1_recording1.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a1_v1\"},\n        },\n        {\n            \"audio_id\": \"a2\",\n            \"speaker_id\": \"s1\",\n            \"duration_seconds\": 45.2,\n            \"path\": \"audio/s1_recording2.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a2_v1\"},\n        },\n        {\n            \"audio_id\": \"a3\",\n            \"speaker_id\": \"s2\",\n            \"duration_seconds\": 60.0,\n            \"path\": \"audio/s2_recording1.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a3_v1\"},\n        },\n        {\n            \"audio_id\": \"a4\",\n            \"speaker_id\": \"s2\",\n            \"duration_seconds\": 35.8,\n            \"path\": \"audio/s2_recording2.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a4_v1\"},\n        },\n    ]\n)\n\n\ndef main():\n    cfg = mx.init_metaxy()\n    store = cfg.get_store(\"dev\")\n\n    # Step 1: Write audio metadata\n    with store:\n        diff = store.resolve_update(Audio, samples=AUDIO_SAMPLES)\n        if len(diff.added) &gt; 0:\n            print(f\"Found {len(diff.added)} new audio recordings\")\n            store.write(Audio, diff.added)\n        elif len(diff.changed) &gt; 0:\n            print(f\"Found {len(diff.changed)} changed audio recordings\")\n            store.write(Audio, diff.changed)\n        else:\n            print(\"No new or changed audio recordings\")\n\n    # Step 2: Compute speaker embeddings\n    with store:\n        diff = store.resolve_update(SpeakerEmbedding)\n\n        added_df = diff.added.to_polars()\n        changed_df = diff.changed.to_polars()\n\n        speakers_to_process = (\n            pl.concat([added_df, changed_df])\n            .select(\"speaker_id\")\n            .unique()\n            .sort(\"speaker_id\")\n            .to_series()\n            .to_list()\n        )\n\n        print(\n            f\"Found {len(speakers_to_process)} speakers that need embedding computation\"\n        )\n\n        if speakers_to_process:\n            embedding_data = []\n\n            for speaker_id in speakers_to_process:\n                speaker_rows = pl.concat([added_df, changed_df]).filter(\n                    pl.col(\"speaker_id\") == speaker_id\n                )\n\n                provenance_by_field = speaker_rows[\"metaxy_provenance_by_field\"][0]\n                provenance = speaker_rows[\"metaxy_provenance\"][0]\n\n                n_audio = len(speaker_rows)\n                print(\n                    f\"  Computing embedding for speaker {speaker_id} from {n_audio} audio recordings\"\n                )\n\n                embedding_data.append(\n                    {\n                        \"speaker_id\": speaker_id,\n                        \"n_dim\": 512,\n                        \"path\": f\"embeddings/{speaker_id}.npy\",\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            embedding_df = pl.DataFrame(embedding_data)\n            print(f\"Writing embeddings for {len(embedding_data)} speakers\")\n            store.write(SpeakerEmbedding, embedding_df)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/aggregation/#step-1-initial-run","title":"Step 1: Initial Run","text":"<p>Run the pipeline to create audio recordings and speaker embeddings:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 4 new audio recordings\nFound 2 speakers that need embedding computation\n  Computing embedding for speaker s1 from 2 audio recordings\n  Computing embedding for speaker s2 from 2 audio recordings\nWriting embeddings for 2 speakers\n</code></pre> <p>All features have been materialized:</p> <ul> <li>4 audio recordings (2 per speaker)</li> <li>2 speaker embeddings (one per speaker)</li> </ul>"},{"location":"examples/aggregation/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>No new or changed audio recordings\nFound 0 speakers that need embedding computation\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/aggregation/#step-3-update-one-audio-recording","title":"Step 3: Update One Audio Recording","text":"<p>Now let's update the provenance of audio <code>a1</code> (belonging to speaker <code>s1</code>):</p> <code>patches/01_update_audio_provenance.patch</code> patches/01_update_audio_provenance.patch<pre><code>--- a/pipeline.py\n+++ b/pipeline.py\n@@ -22,7 +22,7 @@ AUDIO_SAMPLES = pl.DataFrame(\n             \"speaker_id\": \"s1\",\n             \"duration_seconds\": 30.5,\n             \"path\": \"audio/s1_recording1.wav\",\n-            \"metaxy_provenance_by_field\": {\"default\": \"a1_v1\"},\n+            \"metaxy_provenance_by_field\": {\"default\": \"a1_v2\"},\n         },\n         {\n             \"audio_id\": \"a2\",\n</code></pre> <p>This represents a change to one audio recording (perhaps it was re-processed or updated).</p>"},{"location":"examples/aggregation/#step-4-observe-selective-recomputation","title":"Step 4: Observe Selective Recomputation","text":"<p>Run the pipeline again after the audio change:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 1 changed audio recordings\nFound 1 speakers that need embedding computation\n  Computing embedding for speaker s1 from 2 audio recordings\nWriting embeddings for 1 speakers\n</code></pre> <p>Key observation:</p> <ul> <li>Only speaker <code>s1</code>'s embedding is recomputed (because audio <code>a1</code> belongs to <code>s1</code>)</li> <li>Speaker <code>s2</code>'s embedding is not recomputed (none of their audio changed)</li> </ul> <p>This demonstrates that Metaxy correctly tracks aggregation lineage - when any audio for a speaker changes, only that speaker's embedding needs recomputation.</p>"},{"location":"examples/aggregation/#step-5-add-new-audio","title":"Step 5: Add New Audio","text":"<p>Now let's add a new audio recording for speaker <code>s1</code>:</p> <code>patches/02_add_audio.patch</code> patches/02_add_audio.patch<pre><code>--- a/pipeline.py\n+++ b/pipeline.py\n@@ -45,6 +45,13 @@\n             \"path\": \"audio/s2_recording2.wav\",\n             \"metaxy_provenance_by_field\": {\"default\": \"a4_v1\"},\n         },\n+        {\n+            \"audio_id\": \"a5\",\n+            \"speaker_id\": \"s1\",\n+            \"duration_seconds\": 25.0,\n+            \"path\": \"audio/s1_recording3.wav\",\n+            \"metaxy_provenance_by_field\": {\"default\": \"a5_v1\"},\n+        },\n     ]\n )\n</code></pre>"},{"location":"examples/aggregation/#step-6-observe-aggregation-update","title":"Step 6: Observe Aggregation Update","text":"<p>Run the pipeline again:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 1 new audio recordings\nFound 1 speakers that need embedding computation\n  Computing embedding for speaker s1 from 3 audio recordings\nWriting embeddings for 1 speakers\n</code></pre> <p>Key observation:</p> <ul> <li>Only speaker <code>s1</code>'s embedding is recomputed (the new audio belongs to <code>s1</code>)</li> <li>Speaker <code>s1</code> now has 3 audio recordings (up from 2)</li> <li>Speaker <code>s2</code> remains unchanged</li> </ul>"},{"location":"examples/aggregation/#how-it-works","title":"How It Works","text":"<p>Metaxy uses window functions to compute aggregated provenance without reducing rows. When resolving updates for <code>SpeakerEmbedding</code>:</p> <ol> <li>All audio rows for the same speaker get identical aggregated provenance values</li> <li>The aggregated provenance is computed from the individual audio provenances</li> <li>When any audio for a speaker changes, the aggregated provenance changes</li> <li>This triggers recomputation of only the affected speaker's embedding</li> </ol> <p>The user's pipeline code performs the actual aggregation (grouping by <code>speaker_id</code>). Metaxy only tracks the provenance and determines what needs recomputation.</p>"},{"location":"examples/aggregation/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling aggregation relationships: LineageRelationship.aggregation. Other Metaxy features continue to seamlessly work with aggregation relationships.</p>"},{"location":"examples/aggregation/#related-materials","title":"Related Materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>One-to-Many Expansion (the inverse relationship)</li> </ul>"},{"location":"examples/basic/","title":"Basic Example","text":""},{"location":"examples/basic/#overview","title":"Overview","text":"<p> View Example Source on GitHub</p> <p>This example demonstrates how Metaxy automatically detects changes in upstream features and triggers recomputation of downstream features. It shows the core value proposition of Metaxy: avoiding unnecessary recomputation while ensuring data consistency.</p> <p>We will build a simple two-feature pipeline where a child feature depends on a parent feature. When the parent's algorithm changes (represented by <code>code_version</code>), the child feature is automatically recomputed.</p>"},{"location":"examples/basic/#the-pipeline","title":"The Pipeline","text":"<p>Let's define a pipeline with two features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    examples_parent[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/parent&lt;/b&gt;&lt;br/&gt;7de0f5e8&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- embeddings (05e66510)&lt;/div&gt;\"]\n    examples_child[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/child&lt;/b&gt;&lt;br/&gt;b10ea448&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- predictions (9cd1c608)&lt;/div&gt;\"]\n    examples_parent --&gt; examples_child\n</code></pre>"},{"location":"examples/basic/#defining-features-parentfeature","title":"Defining features: <code>ParentFeature</code>","text":"<p>The parent feature represents raw embeddings computed from source data. It has a single field <code>embeddings</code> with a <code>code_version</code> that tracks the algorithm version.</p> src/example_basic/features.py<pre><code>import metaxy as mx\n\n\nclass ParentFeature(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"examples/parent\",\n        fields=[\n            mx.FieldSpec(\n                key=\"embeddings\",\n                code_version=\"1\",\n            ),\n        ],\n        id_columns=(\"sample_uid\",),\n    ),\n):\n    \"\"\"Parent feature that generates embeddings from raw data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"examples/basic/#defining-features-childfeature","title":"Defining features: <code>ChildFeature</code>","text":"<p>The child feature depends on the parent and produces predictions. The key configuration is the <code>FeatureDep</code> which declares that <code>ChildFeature</code> depends on <code>ParentFeature</code>.</p> src/example_basic/features.py<pre><code>class ChildFeature(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"examples/child\",\n        deps=[ParentFeature],\n        fields=[\"predictions\"],\n        id_columns=(\"sample_uid\",),\n    ),\n):\n    \"\"\"Child feature that uses parent embeddings to generate predictions.\"\"\"\n\n    pass\n</code></pre> <p>The <code>FeatureDep</code> declaration tells Metaxy:</p> <ol> <li><code>ChildFeature</code> depends on <code>ParentFeature</code></li> <li>When the parent's field provenance changes, the child must be recomputed</li> <li>This dependency is tracked automatically, enabling incremental recomputation</li> </ol>"},{"location":"examples/basic/#walkthrough","title":"Walkthrough","text":""},{"location":"examples/basic/#step-1-initial-run","title":"Step 1: Initial Run","text":"<p>Run the pipeline to create parent embeddings and child predictions:</p> <pre><code>$ python -m example_basic.pipeline\n</code></pre> <pre><code>Graph snapshot_version: 740b3420\nWritten 3 rows for feature examples/parent\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph snapshot_version: 740b3420\n\n\ud83d\udcca Computing examples/child...\n  feature_version: b10ea448\nIdentified: 3 new samples, 0 samples with new provenance_by_field\n\u2713 Materialized 3 new samples\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\n\n\u2705 Pipeline complete!\n</code></pre> <p>The pipeline materialized 3 samples for the child feature. Each sample has its provenance tracked.</p>"},{"location":"examples/basic/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>$ python -m example_basic.pipeline\n</code></pre> <pre><code>Graph snapshot_version: 740b3420\nMetadata already exists for feature examples/parent (feature_version: 7de0f5e8...)\nSkipping write to avoid duplicates\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph snapshot_version: 740b3420\n\n\ud83d\udcca Computing examples/child...\n  feature_version: b10ea448\nIdentified: 0 new samples, 0 samples with new provenance_by_field\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\nNo changes detected (idempotent)\n\n\u2705 Pipeline complete!\n</code></pre> <p>Key observation: No recomputation occurred.</p>"},{"location":"examples/basic/#step-3-update-parent-algorithm","title":"Step 3: Update Parent Algorithm","text":"<p>Now let's simulate an algorithm improvement by changing the parent's <code>code_version</code> from <code>\"1\"</code> to <code>\"2\"</code>:</p> PatchFeature Graph Changes patches/01_update_parent_algorithm.patch<pre><code>--- a/src/example_basic/features.py\n+++ b/src/example_basic/features.py\n@@ -15,7 +15,7 @@ class ParentFeature(\n         fields=[\n             FieldSpec(\n                 key=\"embeddings\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n         ],\n         id_columns=(\"sample_uid\",),\n</code></pre> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    examples_parent[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/parent&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;7de0f5e8&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;68827f3e&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;embeddings&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;05e66510&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;3c8d3e9b&lt;/font&gt;)&lt;/div&gt;\"]\n    examples_child[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/child&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;b10ea448&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e5b92b18&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;predictions&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;9cd1c608&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;7cef6acb&lt;/font&gt;)&lt;/div&gt;\"]\n    examples_parent --&gt; examples_child\n\n\n    style examples_child stroke:#FFAA00,stroke-width:2px\n    style examples_parent stroke:#FFAA00,stroke-width:2px</code></pre> <p>This change means that the existing embeddings and the downstream feature have to be recomputed.</p>"},{"location":"examples/basic/#step-4-observe-automatic-recomputation","title":"Step 4: Observe Automatic Recomputation","text":"<p>Run the pipeline again after the algorithm change:</p> <pre><code>$ python -m example_basic.pipeline\n</code></pre> <pre><code>Graph snapshot_version: 165989b6\nWritten 3 rows for feature examples/parent\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph snapshot_version: 165989b6\n\n\ud83d\udcca Computing examples/child...\n  feature_version: e5b92b18\nIdentified: 3 new samples, 0 samples with new provenance_by_field\n\u2713 Materialized 3 new samples\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\n\n\u2705 Pipeline complete!\n</code></pre> <p>Key observation: The child feature was automatically recomputed because:</p> <ol> <li>The parent's <code>code_version</code> changed from <code>\"1\"</code> to <code>\"2\"</code></li> <li>This changed the parent's <code>metaxy_feature_version</code></li> <li>The child's field dependency on <code>embeddings</code> detected the change</li> <li>All child samples were marked for recomputation</li> </ol>"},{"location":"examples/basic/#how-it-works","title":"How It Works","text":"<p>Metaxy tracks provenance at the field level using content hashes:</p> <ol> <li>Feature Version: A hash of the feature specification (including <code>code_version</code> of all fields)</li> <li>Field Provenance: A hash combining the field's <code>code_version</code> and upstream provenance</li> <li>Dependency Resolution: When resolving updates, Metaxy computes what the provenance would be and compares it to what's stored</li> </ol> <p>The <code>resolve_update()</code> method returns:</p> <ul> <li><code>added</code>: New samples that don't exist in the store</li> <li><code>changed</code>: Existing samples whose computed provenance differs from stored provenance</li> </ul> <p>This enables precise, incremental recomputation without re-processing unchanged data.</p>"},{"location":"examples/basic/#conclusion","title":"Conclusion","text":"<p>Metaxy provides automatic change detection and incremental recomputation through:</p> <ul> <li>Feature dependency tracking via <code>FeatureDep</code></li> <li>Algorithm versioning via <code>code_version</code></li> <li>Provenance-based change detection via <code>resolve_update()</code></li> </ul> <p>This ensures your pipelines are efficient data stays up to date.</p>"},{"location":"examples/basic/#related-materials","title":"Related Materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Data Versioning</li> <li>Relationships</li> </ul>"},{"location":"examples/expansion/","title":"Expansion","text":""},{"location":"examples/expansion/#overview","title":"Overview","text":"<p> View Example Source on GitHub</p> <p>This example demonstrates how to implement expansion (<code>1:N</code>) transformations with Metaxy. In such relationships a single parent sample can map into multiple child samples.</p> <p>These relationships can be modeled with LineageRelationship.expansion lineage type.</p> <p>We will use a hypothetical video chunking pipeline as an example. We are also going to demonstrate that other Metaxy features such as fields mapping work with non-standard lineage types.</p>"},{"location":"examples/expansion/#the-pipeline","title":"The Pipeline","text":"<p>We are going to define a typical video processing pipeline with three features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces&lt;/div&gt;\"]\n        video_raw --&gt; video_chunk\n        video_chunk --&gt; video_faces</code></pre>"},{"location":"examples/expansion/#defining-features-video","title":"Defining features: <code>Video</code>","text":"<p>Each video-like feature in our pipeline is going to have two fields: <code>audio</code> and <code>frames</code>.</p> <p>Let's set the code version of <code>audio</code> to <code>\"1\"</code> in order to change it in the future. <code>frames</code> field will have a default version.</p> src/example_one_to_many/features.py<pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video/raw\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            \"frames\",\n        ],\n    ),\n):\n    video_id: str\n    path: str  # where the video is stored\n</code></pre>"},{"location":"examples/expansion/#defining-features-videochunk","title":"Defining features: <code>VideoChunk</code>","text":"<p><code>VideoChunk</code> represents a piece of the upstream <code>Video</code> feature. Since each <code>Video</code> sample can be split into multiple chunks, we need to tell Metaxy how to map each chunk to its parent video.</p> src/example_one_to_many/features.py<pre><code>class VideoChunk(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"chunk\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=Video,\n                lineage=mx.LineageRelationship.expansion(on=[\"video_id\"]),\n            )\n        ],\n        fields=[\"audio\", \"frames\"],\n    ),\n):\n    video_id: str  # points to the parent video\n    video_chunk_id: str\n    path: str  # where the video chunk is stored\n</code></pre> <p>We do not specify custom versions on its fields. Metaxy will automatically assign field-level dependencies by matching on field names: <code>VideoChunk.frames</code> depends on <code>Video.frames</code> and <code>VideoChunk.audio</code> depends on <code>Video.audio</code>.</p>"},{"location":"examples/expansion/#defining-features-facerecognition","title":"Defining features: <code>FaceRecognition</code>","text":"<p><code>FaceRecognition</code> processes video chunks and only depends on the <code>frames</code> field. This can be expressed with a <code>SpecificFieldsMapping</code>.</p> src/example_one_to_many/features.py<pre><code>class FaceRecognition(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"faces\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=VideoChunk,\n                fields_mapping=mx.FieldsMapping.specific(\n                    mapping={mx.FieldKey(\"faces\"): {mx.FieldKey(\"frames\")}}\n                ),\n            )\n        ],\n        fields=[\"faces\"],\n    ),\n):\n    video_chunk_id: str\n    num_faces: int  # number of faces detected\n</code></pre> <p>This completes the feature definitions. Let's proceed to running the pipeline.</p>"},{"location":"examples/expansion/#walkthrough","title":"Walkthrough","text":"<p>Here is a toy pipeline for computing the feature graph described above:</p> <code>pipeline.py</code> pipeline.py<pre><code>import os\nimport random\n\nimport metaxy as mx\nimport narwhals as nw\nimport polars as pl\n\nfrom example_one_to_many.features import FaceRecognition, Video, VideoChunk\nfrom example_one_to_many.utils import split_video_into_chunks\n\n\ndef main():\n    # Set random seed from environment if provided (for deterministic testing)\n    if seed_str := os.environ.get(\"RANDOM_SEED\"):\n        random.seed(int(seed_str))\n    cfg = mx.init_metaxy()\n    store = cfg.get_store(\"dev\")\n\n    # let's pretend somebody has already created the videos for us\n    samples = pl.DataFrame(\n        {\n            \"video_id\": [1, 2, 3],\n            \"path\": [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"],\n            \"metaxy_provenance_by_field\": [\n                {\"audio\": \"v1\", \"frames\": \"v1\"},\n                {\"audio\": \"v2\", \"frames\": \"v2\"},\n                {\"audio\": \"v3\", \"frames\": \"v3\"},\n            ],\n        }\n    )\n\n    with store:\n        # showcase: resolve incremental update for a root feature\n        diff = store.resolve_update(Video, samples=nw.from_native(samples))\n        if len(diff.added) &gt; 0:\n            print(f\"Found {len(diff.added)} new videos\")\n            store.write(Video, diff.added)\n\n    # Resolve videos that need to be split into chunks\n    with store:\n        diff = store.resolve_update(VideoChunk)\n        # the DataFrame dimensions matches Video (with ID column renamed)\n\n        print(\n            f\"Found {len(diff.added)} videos and {len(diff.changed)} videos that need chunking\"\n        )\n\n        for row_dict in pl.concat(\n            [diff.added.to_polars(), diff.changed.to_polars()]\n        ).iter_rows(named=True):\n            print(f\"Processing video: {row_dict}\")\n            # let's split each video to 3-5 chunks randomly\n\n            video_id = row_dict[\"video_id\"]\n            path = row_dict[\"path\"]\n\n            provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n            provenance = row_dict[\"metaxy_provenance\"]\n\n            # pretend we split the video into chunks\n            chunk_paths = split_video_into_chunks(path)\n\n            # Generate chunk IDs based on the parent video ID\n            chunk_ids = [f\"{video_id}_{i}\" for i in range(len(chunk_paths))]\n\n            # write the chunks to the store\n            # CRUSIAL: all the chunks **must share the same provenance values**\n            chunk_df = pl.DataFrame(\n                {\n                    \"video_id\": [video_id] * len(chunk_paths),\n                    \"video_chunk_id\": chunk_ids,\n                    \"path\": chunk_paths,\n                    \"metaxy_provenance_by_field\": [provenance_by_field]\n                    * len(chunk_paths),\n                    \"metaxy_provenance\": [provenance] * len(chunk_paths),\n                }\n            )\n            print(f\"Writing {len(chunk_paths)} chunks for video {video_id}\")\n            store.write(VideoChunk, nw.from_native(chunk_df))\n\n    # Process face recognition on video chunks\n    with store:\n        diff = store.resolve_update(FaceRecognition)\n        print(\n            f\"Found {len(diff.added)} video chunks and {len(diff.changed)} video chunks that need face recognition\"\n        )\n\n        if len(diff.added) &gt; 0:\n            # simulate face detection on each chunk\n            face_data = []\n            for row_dict in pl.concat(\n                [diff.added.to_polars(), diff.changed.to_polars()]\n            ).iter_rows(named=True):\n                video_chunk_id = row_dict[\"video_chunk_id\"]\n                provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n                provenance = row_dict[\"metaxy_provenance\"]\n\n                # simulate detecting random number of faces\n                num_faces = random.randint(0, 10)\n\n                face_data.append(\n                    {\n                        \"video_chunk_id\": video_chunk_id,\n                        \"num_faces\": num_faces,\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            face_df = pl.DataFrame(face_data)\n            print(f\"Writing face recognition results for {len(face_data)} chunks\")\n            store.write(FaceRecognition, nw.from_native(face_df))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/expansion/#step-1-launch-initial-run","title":"Step 1: Launch Initial Run","text":"<p>Run the pipeline to create videos, chunks, and face recognition results:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 4 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 3 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 5 chunks for video 3\nFound 12 video chunks and 0 video chunks that need face recognition\nWriting face recognition results for 12 chunks\n</code></pre> <p>All three features have been materialized. Note that the <code>VideoChunk</code> feature may dynamically create as many samples as needed: Metaxy doesn't need to know anything about this in advance, except the relationship type.</p>"},{"location":"examples/expansion/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 0 videos and 0 videos that need chunking\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/expansion/#step-3-change-audio-code-version","title":"Step 3: Change Audio Code Version","text":"<p>Now let's bump the code version on the <code>audio</code> field of <code>Video</code> feature:</p> PatchFeature Graph Changes patches/01_update_video_code_version.patch<pre><code>--- a/src/example_one_to_many/features.py\n+++ b/src/example_one_to_many/features.py\n@@ -9,6 +9,6 @@ class Video(\n         id_columns=[\"video_id\"],\n         fields=[\n-            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n+            mx.FieldSpec(key=\"audio\", code_version=\"2\"),\n             \"frames\",\n         ],\n     ),\n</code></pre> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;d842b740&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;6e5a4ab8&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;7132721c&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;f8e35296&lt;/font&gt;)&lt;br/&gt;- frames (3f6f401c)&lt;/div&gt;\"]\n    video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;7dc3712a&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;504507a9&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;1549b1fa&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;7948d163&lt;/font&gt;)&lt;br/&gt;- frames (df8943d3)&lt;/div&gt;\"]\n    video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;52f055e3&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (0561bd8c)&lt;/div&gt;\"]\n    video_chunk --&gt; video_faces\n    video_raw --&gt; video_chunk\n\n\n    style video_raw stroke:#FFAA00,stroke-width:2px\n    style video_faces stroke:#808080\n    style video_chunk stroke:#FFAA00,stroke-width:2px</code></pre> <p>This represents updating the audio processing algorithm, and therefore the audio data, while frame data is kept the same.</p>"},{"location":"examples/expansion/#step-4-observe-field-level-tracking","title":"Step 4: Observe Field-Level Tracking","text":"<p>Run the pipeline again after the code change:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 3 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 5 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 4 chunks for video 3\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Key observation:</p> <ul> <li><code>VideoChunk</code> has been recomputed since the <code>audio</code> field on it has been affected by the upstream change</li> <li><code>FaceRecognition</code> did not require a recompute, because it only depends on the <code>frames</code> field (which did not change)</li> </ul>"},{"location":"examples/expansion/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling expansion relationships: LineageRelationship.expansion. Other Metaxy features such as field-level versioning continue to work seamlessly when declaring expansion relationships.</p>"},{"location":"examples/expansion/#related-materials","title":"Related materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>Fields Mapping</li> </ul>"},{"location":"guide/learn/","title":"Concepts","text":"<p>Metaxy is built around a few core ideas that work together to solve the problem of incremental processing in multi-modal pipelines.</p>"},{"location":"guide/learn/#the-big-picture","title":"The Big Picture","text":"<pre><code>graph LR\n    A[Feature Definitions] --&gt; B[Incremental Updates]\n    B --&gt; C[Metadata Store]</code></pre> <p>Feature definitions declare what data you have and how it depends on other data. Metaxy builds a feature graph from these definitions and uses it to track versions at the sample level. When upstream data changes, Metaxy identifies exactly which downstream samples need recomputation and resolves incremental updates. All of this is persisted in a metadata store. Feature definitions may optionally define custom metadata columns (such as file path, size, etc.) which are stored alongside the versioning information.</p>"},{"location":"guide/learn/#core-concepts","title":"Core Concepts","text":"<ul> <li> Metadata Stores</li> </ul> <p>Unified interface for storing and retrieving metadata across different backends.</p> <ul> <li> Feature Definitions</li> </ul> <p>Declarative specifications that define your data schema, (partial) dependencies, and how versions are calculated.</p> <ul> <li> Versioning</li> </ul> <p>Sample-level version tracking that detects changes and determines what needs recomputation.</p> <ul> <li> Feature Discovery</li> </ul> <p>Automatic registration and graph building from feature definitions in your codebase.</p>"},{"location":"guide/learn/#dependencies-and-lineage","title":"Dependencies and Lineage","text":"<ul> <li> Lineage Relationship</li> </ul> <p>How features relate to upstream dependencies: one-to-one, one-to-many, or many-to-one.</p> <ul> <li> Optional Dependencies</li> </ul> <p>Handle missing upstream data gracefully without blocking downstream processing.</p> <ul> <li> Filters</li> </ul> <p>Select subsets of samples for processing based on metadata conditions.</p>"},{"location":"guide/learn/#advanced-topics","title":"Advanced Topics","text":"<ul> <li> Deletions</li> </ul> <p>Propagate sample deletions through the feature graph correctly.</p> <ul> <li> System Columns</li> </ul> <p>Reserved columns used internally by Metaxy for versioning and deduplication.</p> <ul> <li> Testing</li> </ul> <p>Patterns and utilities for testing Metaxy features.</p>"},{"location":"guide/learn/data-versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static, deterministic and idempotent.</p>"},{"location":"guide/learn/data-versioning/#versioning_1","title":"Versioning","text":"<p>Feature and field versions are defined by the feature graph topology and the user-provided code versions of fields. Sample versions are defined by upstream sample versions and the code versions of the fields defined on the sample's feature.</p> <p>All versions are computed ahead of time: feature and field versions can be immediately derived from code (and we keep historical graph snapshots for them), and calculating sample versions requires access to the metadata store.</p> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"guide/learn/data-versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them.</p>"},{"location":"guide/learn/data-versioning/#field-level","title":"Field Level","text":"<ul> <li>Field Code Version is defined on the field and is provided by the user (defaults to <code>\"__metaxy_initial__\"</code>)</li> </ul> <p>Code Version Value</p> <p>The value can be arbitrary, but in the future we might implement something around semantic versioning.</p> <ul> <li>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</li> </ul>"},{"location":"guide/learn/data-versioning/#feature-level","title":"Feature Level","text":"<ul> <li>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</li> <li>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</li> </ul>"},{"location":"guide/learn/data-versioning/#graph-level","title":"Graph Level","text":"<ul> <li>Snapshot Version: is computed from the Feature Versions of all features defined on the graph.</li> </ul> How is snapshot version used? <p>This value is used to uniquely encode versioned feature graph topology. <code>metaxy push</code> CLI can be used to keep track of previous versions of the feature graph, enabling features such as data version reconciliation migrations.</p>"},{"location":"guide/learn/data-versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to compute them.</p> <ul> <li>Provenance By Field is computed from the upstream Provenance By Field (with respect to defined field-level dependencies and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</li> </ul> sample_uid metaxy_provenance_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <ul> <li>Sample Version is derived from the Provenance By Field by simply hashing it.</li> </ul> <p>Computing this value is the goal of the entire versioning engine. It ensures that only the necessary samples are recomputed when a feature version changes. It acts as source of truth for resolving incremental updates for feature metadata.</p> <p>Customizing Sample Versions</p> <p>Users can override the computed sample-level versions by setting <code>metaxy_data_version_by_field</code> on their metadata. This can be used for eliminating false-positives (e.g. content-based hashing), when sometimes data stays the same even after upstream has changed. This customization only affects how downstream increments are calculated.</p>"},{"location":"guide/learn/data-versioning/#practical-example","title":"Practical Example","text":"<p>Consider a video processing pipeline with these features:</p> Simplified Metaxy Definitions <p>This example uses Metaxy's syntactic sugar for cleaner code. Feature classes can be passed directly to <code>deps</code> instead of wrapping in <code>FeatureDep</code>, and field names matching upstream fields automatically create field-level dependencies.</p> <pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            mx.FieldSpec(key=\"frames\", code_version=\"1\"),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    video_id: str\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/crop\",\n        id_columns=[\"video_id\"],\n        deps=[Video],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),  # (1)!\n            mx.FieldSpec(key=\"frames\", code_version=\"1\"),  # (2)!\n        ],\n    ),\n):\n    video_id: str  # ID column\n\n\nclass FaceDetection(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/face_detection\",\n        id_columns=[\"video_id\"],\n        deps=[Crop],\n        fields=[\n            mx.FieldSpec(\n                key=\"faces\",\n                code_version=\"1\",\n                deps=[mx.FieldDep(feature=Crop, fields=[\"frames\"])],\n            ),\n        ],\n    ),\n):\n    video_id: str\n\n\nclass SpeechToText(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/stt\",\n        id_columns=[\"video_id\"],\n        deps=[Video],\n        fields=[\n            mx.FieldSpec(\n                key=\"transcription\",\n                code_version=\"1\",\n                deps=[mx.FieldDep(feature=Video, fields=[\"audio\"])],\n            ),\n        ],\n    ),\n):\n    video_id: str\n</code></pre> <p>{ .annotated }</p> <ol> <li> <p>This <code>audio</code> field automatically depends on the <code>audio</code> field of the <code>Video</code> feature, because their names match.</p> </li> <li> <p>This <code>frames</code> field automatically depends on the <code>frames</code> field of the <code>Video</code> feature, because their names match.</p> </li> </ol> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;c2ac395f&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (22742381)&lt;br/&gt;- frames (794116a9)&lt;/div&gt;\"]\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;34d75856&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (4c726c4b)&lt;br/&gt;- frames (2419e09d)&lt;/div&gt;\"]\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;f1526ee0&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (006efeef)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;d953dea4&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- transcription (3ec3826d)&lt;/div&gt;\"]\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt\n</code></pre>"},{"location":"guide/learn/data-versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>Video</code> feature changes (1):</p> <ol> <li>Perhaps, something like denoising has been applied externally</li> </ol> <code>patches/01_update_audio_version.patch</code> patches/01_update_audio_version.patch<pre><code>--- a/src/example_overview/features.py\n+++ b/src/example_overview/features.py\n@@ -14,7 +14,7 @@ class Video(\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n             FieldSpec(\n                 key=\"frames\",\n</code></pre> <p>Run <code>metaxy graph diff</code> to see what changed:</p> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;c2ac395f&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;2faffb98&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;22742381&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;09c8398b&lt;/font&gt;)&lt;br/&gt;- frames (794116a9)&lt;/div&gt;\"]\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;34d75856&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;fe237dc9&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;4c726c4b&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e2b6ce39&lt;/font&gt;)&lt;br/&gt;- frames (2419e09d)&lt;/div&gt;\"]\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;f1526ee0&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (006efeef)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;d953dea4&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e57e7555&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;3ec3826d&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;9f7ea40c&lt;/font&gt;)&lt;/div&gt;\"]\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt\n\n\n    style example_crop stroke:#FFAA00,stroke-width:2px\n    style example_video stroke:#FFAA00,stroke-width:2px\n    style example_face_detection stroke:#808080\n    style example_stt stroke:#FFAA00,stroke-width:2px</code></pre> <p>Info</p> <ul> <li> <p><code>Video</code>, <code>Crop</code>, and <code>SpeechToText</code> have changed</p> </li> <li> <p><code>FaceDetection</code> remained unchanged (depends only on <code>frames</code> and not on <code>audio</code>)</p> </li> <li> <p>Audio field versions have changed throughout the graph</p> </li> <li> <p>Frame field versions have stayed the same</p> </li> </ul>"},{"location":"guide/learn/data-versioning/#incremental-computation","title":"Incremental Computation","text":"<p>The single most important piece of code in Metaxy is the <code>resolve_update</code> method. It handles the following:</p> <ol> <li> <p>Joins upstream feature metadata</p> </li> <li> <p>Computes sample versions</p> </li> <li> <p>Compares against existing metadata</p> </li> <li> <p>Returns diff: added, changed, removed samples</p> </li> </ol> <p>Typically, steps 1-3 can be run directly in the database. Analytical databases such as ClickHouse or Snowflake can efficiently handle these operations.</p> <p>The Python pipeline then only handles the increment.</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes provenance_by_field and identifies changes\n    increment = store.resolve_update(DownstreamFeature)\n\n    # Process only changed samples\n</code></pre> <p>The <code>increment</code> object has attributes for new upstream samples, samples with new versions, and samples that have been removed from upstream metadata.</p>"},{"location":"guide/learn/deletions/","title":"Metadata Deletion","text":"<p>Metaxy supports two deletion modes: soft deletes that preserve history and hard deletes that permanently remove records. Soft deletes are the default behavior and preferred for most use cases since they maintain audit trails while allowing records to be filtered out from normal queries.</p>"},{"location":"guide/learn/deletions/#soft-deletes","title":"Soft deletes","text":"<p>Soft deletes mark records as deleted without physically removing them. When you call <code>delete</code>, Metaxy appends a new row with the <code>metaxy_deleted_at</code> system column set to the deletion timestamp. This preserves your full history\u2014nothing is lost, and you can always query for soft-deleted records if needed.</p> <p>By default, <code>read</code> filters out soft-deleted records automatically. You only see active data. Behind the scenes, Metaxy keeps the latest version of each record by coalescing deletion and creation timestamps, so even if you've updated a record multiple times, queries return only the current state.</p> <pre><code>import narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=nw.col(\"status\") == \"pending\",\n    )\n\nwith store:\n    active = store.read(MyFeature)\n    all_rows = store.read(MyFeature, include_soft_deleted=True)\n</code></pre> <p>If you track custom deletion flags in your feature schema, filter them through <code>read</code> filters.</p>"},{"location":"guide/learn/deletions/#hard-deletes","title":"Hard deletes","text":"<p>Hard deletes permanently remove rows from storage. Use them when you need to physically delete data, such as for compliance requirements or to reclaim space. Pass <code>soft=False</code> to <code>delete</code> and specify which records to remove with a filter expression.</p> <pre><code>import narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=nw.col(\"quality\") &lt; 0.8,\n        soft=False,\n    )\n</code></pre> <p>Not all metadata stores support hard deletes. If your store doesn't support them, you'll get a <code>NotImplementedError</code>.</p>"},{"location":"guide/learn/deletions/#cli-workflows","title":"CLI workflows","text":"<p>Run cleanups from the command line using <code>metaxy metadata delete</code>:</p> <pre><code># Soft delete by default\nmetaxy metadata delete --feature predictions --filter \"confidence &lt; 0.3\"\n\n# Hard delete\nmetaxy metadata delete --feature predictions --filter \"created_at &lt; '2024-01-01'\" --soft=false\n</code></pre> <p>Learn more in the CLI reference</p>"},{"location":"guide/learn/feature-definitions/","title":"Feature System","text":"<p>Metaxy has a declarative (defined statically at class level), expressive, flexible feature system. It has been inspired by Dagster's Software-Defined Assets and Nix.</p> <p>Abstract</p> <p>Features represent tabular metadata, typically containing references to external multi-modal data such as files, images, or videos.</p> Subject Description Data The actual multi-modal data itself, such as images, audio files, video files, text documents, and other raw content that your pipelines process and transform. Metadata Information about the data, typically including references to where data is stored (e.g., object store keys) plus additional descriptive entries such as video length, file size, format, version, and other attributes. <p>As an edge case, Metaxy features may also be pure metadata without references to external data.</p> <p>I will highlight data and metadata with bold so it really stands out.</p> <p>Metaxy is responsible for providing correct metadata to users.</p> <p>During incremental processing, Metaxy will automatically resolve added, changed and deleted metadata rows and calculate the right sample versions for them.</p> <p>Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage.</p> <p>Keeping Historical Data</p> <p>Include <code>metaxy_data_version</code> in your data path to avoid collisions between different versions of the same data sample. Doing this will ensure that newer samples are never written over older ones.</p> <p>I hope we can stop using bold for data and metadata from now on, hopefully we've made our point.</p>"},{"location":"guide/learn/feature-definitions/#feature-definitions","title":"Feature Definitions","text":"<p>Metaxy provides a <code>BaseFeature</code> class that can be extended to create user-defined features. It's a Pydantic model.</p> <p>Abstract</p> <p>Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p> <p>Users must provide one or more ID columns (1) to <code>FeatureSpec</code>, telling Metaxy how to uniquely identify feature samples.</p> <ol> <li>ID columns are almost a primary key. The difference is quite subtle: Metaxy may interact with storage systems which do not technically have the concept of a primary key and may allow multiple rows to have the same ID columns (which are deduplicated by Metaxy).</li> </ol> <pre><code>import metaxy as mx\n\n\nclass VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"raw/video\", id_columns=[\"video_id\"])):\n    path: str\n</code></pre> <p>Since <code>VideoFeature</code> is a root feature, it doesn't have any dependencies.</p> <p>That's it! Easy.</p> <p>Tip</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(key=\"processed/transcript\", id_columns=[\"video_id\"], deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> The God <code>FeatureGraph</code> object <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly).</p> <p>Hurray! You get the idea.</p>"},{"location":"guide/learn/feature-definitions/#field-level-dependencies","title":"Field-Level Dependencies","text":"<p>A core (1) feature of Metaxy is the concept of field-level dependencies. These are used to define dependencies between logical fields of features.</p> <ol> <li>really a killer </li> </ol> <p>Abstract</p> <p>A Metaxy field is not to be confused with metadata column. Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy. (1)</p> <ol> <li>columns can be defined with Pydantic fields </li> </ol> <p>Fields refer to data and are purely logical - users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>Video</code> feature - an <code>.mp4</code> file - may have <code>frames</code> and <code>audio</code> fields.</p> <p>At this point, careful readers have probably noticed that the <code>Transcript</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express this with Metaxy:</p> <pre><code>import metaxy as mx\n\nvideo_spec = mx.FeatureSpec(key=\"raw/video2\", id_columns=[\"video_id\"], fields=[\"audio\", \"frames\"])\n\n\nclass VideoFeature2(mx.BaseFeature, spec=video_spec):\n    path: str\n\n\ntranscript_spec = mx.FeatureSpec(\n    key=\"raw/transcript\",\n    id_columns=[\"video_id\"],\n    deps=[VideoFeature2],\n    fields=[\n        mx.FieldSpec(\n            key=\"text\",\n            deps=[mx.FieldDep(feature=VideoFeature2, fields=[\"audio\"])],\n        )\n    ],\n)\n\n\nclass TranscriptFeature(mx.BaseFeature, spec=transcript_spec):\n    path: str\n</code></pre> <p>Voil\u00e0!</p> <p>Use boilerplate-free API</p> <p>Metaxy allows passing simplified types to some of the models like <code>FeatureSpec</code> or <code>FeatureKey</code>. See syntactic sugar for more details.</p> <p>The Data Versioning docs explain more about how Metaxy calculates versions for different components of a feature graph.</p>"},{"location":"guide/learn/feature-definitions/#attaching-user-defined-metadata","title":"Attaching user-defined metadata","text":"<p>Users can attach arbitrary JSON-like metadata dictionary to feature specs, typically used for declaring ownership, providing information to third-party tooling, or documentation purposes. This metadata does not influence graph topology or the versioning system.</p>"},{"location":"guide/learn/feature-definitions/#fully-qualified-field-key","title":"Fully Qualified Field Key","text":"<p>Abstract</p> <p>A fully qualified field key (FQFK) is an identifier that uniquely identifies a field within the whole feature graph.</p> <p>It consists of the feature key and the field key, separated by a colon.</p> <p>Example</p> <ul> <li> <p><code>/raw/video:frames</code></p> </li> <li> <p><code>/raw/video:audio/english</code></p> </li> </ul>"},{"location":"guide/learn/feature-definitions/#external-features","title":"External Features","text":"<p>External features are stubs pointing at features actually defined in other projects and not available in Python at runtime. They can be used if the actual feature class cannot be imported, for example due to dependency conflicts or for other reasons.</p> <p>Externals features can be defined with:</p> <pre><code>import metaxy as mx\n\nexternal_feature = mx.FeatureDefinition.external(\n    spec=mx.FeatureSpec(key=\"a/b/c\", id_columns=[\"id\"]),\n    project=\"external-project\",\n)\n</code></pre> <p>External features only exist until the actual feature definitions are loaded from the metadata store and replace them. This can be done with <code>metaxy.sync_external_features</code>.</p> <pre><code>import metaxy as mx\n\n# Sync external features from the metadata store\nmx.sync_external_features(store)\n</code></pre> <p>Pydantic Schema Limitation</p> <p>Features loaded from the metadata store have their JSON schema preserved from when they were originally saved. However, the Pydantic model class is not available. Operations that require the actual Python class, such as model instantiation or validation, will not work for these features.</p> <p>Metaxy has a few safe guards in order to combat incorrect versioning information on external feature definitions. By default, Metaxy emits warnings when an external feature appears to have a different version (or field versions) than the actual feature definition loaded from the other project. These warnings can be turned into errors by:</p> <ul> <li>passing <code>on_conflict=\"raise\"</code> to <code>sync_external_features</code></li> <li>passing <code>--locked</code> to Metaxy CLI commands</li> <li>setting <code>locked</code> to <code>True</code> in the global Metaxy configuration. This can be done either in the config file or via the <code>METAXY_LOCKED</code> environment variable.</li> </ul> <p>Tip</p> <p>We recommend setting <code>METAXY_LOCKED=1</code> in production</p> <p>Additionally, the following actions always trigger a sync for external feature definitions:</p> <ul> <li>pushing feature definitions to the metadata store (e.g. <code>metaxy push</code> CLI)</li> <li><code>MetadataStore.read</code></li> <li><code>MetadataStore.resolve_update</code></li> </ul> <p>And some other places where it's appropriate and doesn't create an additional overhead.</p> <p>Tip</p> <p>This behavior can be disabled by setting <code>sync=False</code> in the global Metaxy configuration. However, we advise to keep it enabled, because <code>sync_external_features</code> is very lightweight on the first call and a no-op on subsequent calls. It only does anything if the current feature graph does not contain any external features.</p>"},{"location":"guide/learn/feature-discovery/","title":"Feature Discovery","text":"<p>Warning</p> <p>This page is WIP</p>"},{"location":"guide/learn/feature-discovery/#config-based-discovery","title":"Config-Based Discovery","text":"<p>Specify paths for modules containing Metaxy features in Metaxy configuration:</p> metaxy.tomlpyproject.toml <pre><code>project = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre>"},{"location":"guide/learn/feature-discovery/#external-features","title":"External Features","text":"<p>Use <code>sync_external_features</code> to load feature definitions from a metadata store without requiring the original Python classes to be importable. This enables depending on features from external projects or historical snapshots where the source code is not available at runtime.</p> <pre><code>import metaxy as mx\n\n# Sync external features from the metadata store\nmx.sync_external_features(store)\n</code></pre> <p>Pydantic Schema Limitation</p> <p>Features loaded from the metadata store have their JSON schema preserved from when they were originally saved. However, the Pydantic model class is not available. Operations that require the actual Python class, such as model instantiation or validation, will not work for these features.</p> <p>Metaxy has a few safe guards in order to combat incorrect versioning information on external feature definitions. By default, Metaxy emits warnings when an external feature appears to have a different version (or field versions) than the actual feature definition loaded from the other project. These warnings can be turned into errors by:</p> <ul> <li>passing <code>on_conflict=\"raise\"</code> to <code>sync_external_features</code></li> <li>passing <code>--locked</code> to Metaxy CLI commands</li> <li>setting <code>locked</code> to <code>True</code> in the global Metaxy configuration. This can be done either in the config file or via the <code>METAXY_LOCKED</code> environment variable.</li> </ul> <p>Tip</p> <p>We recommend setting <code>METAXY_LOCKED=1</code> in production</p> <p>Info</p> <p><code>MetadataStore.resolve_update] always calls [</code>sync_external_features` internally.</p>"},{"location":"guide/learn/filters/","title":"Specifying Filters As Text","text":"<p>There are a few occasions with Metaxy where users may want to define custom filter expressions via text, mainly being CLI arguments or configuration files. For this purpose, Metaxy implements <code>parse_filter_string</code>, which converts SQL-like <code>WHERE</code> clauses into Narwhals filter expressions.</p> <p>The following syntax is supported:</p> <ul> <li> <p>Comparisons: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></p> </li> <li> <p>Logical operators: <code>AND</code>, <code>OR</code>, <code>NOT</code></p> </li> <li> <p>Set membership: <code>IN</code>, <code>NOT IN</code></p> </li> <li> <p>Null checks: <code>IS NULL</code>, <code>IS NOT NULL</code></p> </li> <li> <p>Parentheses for grouping</p> </li> <li> <p>Column references (identifiers or dotted paths)</p> </li> <li> <p>Literals: strings (<code>'value'</code>), numbers, booleans (<code>TRUE</code>/<code>FALSE</code>), and <code>NULL</code></p> </li> <li> <p>Implicit boolean columns (e.g., <code>NOT is_active</code>)</p> </li> </ul> <p>Example</p> <pre><code>import polars as pl\nimport narwhals as nw\nfrom metaxy.models.filter_expression import parse_filter_string\n\n# Create a sample Polars DataFrame\npdf = pl.DataFrame({\"age\": [10, 20, 30], \"status\": [\"active\", \"deleted\", \"active\"]})\ndf = nw.from_native(pdf)\n\n# Parse a SQL WHERE clause into a backend-agnostic Narwhals expression\nexpr = parse_filter_string(\"(age &gt; 25 OR age &lt; 18) AND status != 'deleted'\")\n\nresult = df.filter(expr)\n\nassert result[\"age\"].to_list() == [10, 30]\n</code></pre>"},{"location":"guide/learn/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: <code>MetadataStore</code>.</p> <p>Metadata stores expose methods for reading, writing, deleting metadata, and the most important one: resolve_update for receiving a metadata increment.</p> <p>It looks more or less like this:</p> <p>Example</p> <pre><code>with store:\n    df = store.read(MyFeature)\n\nwith store.open(\"w\"):\n    store.write(MyFeature, df)\n</code></pre> <p>Metadata stores implement an append-only storage model and rely on Metaxy system columns.</p> <p>Note</p> <p>Metaxy never mutates metadata in-place (1)</p> <ol> <li> safety and performance reasons</li> </ol> <p>Forged About ACID</p> <p>Metadata reads/writes are not guaranteed to be ACID: Metaxy is designed to interact with analytical databases which lack ACID guarantees by definition and design. (1)</p> <ol> <li>for - you've guessed it right -  performance reasons</li> </ol> <p>However, Metaxy never retrieves the same sample version twice, and performs read-time deduplication (1) by the combination of the feature version, ID columns, and <code>metaxy_created_at</code>.</p> <ol> <li>also known as merge-on-read</li> </ol> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store. This includes joining upstream features, hashing their versions, and filtering out samples that have already been processed - everything is pushed into the DB.</p> <p>When can local computations happen instead</p> <p>Metaxy's versioning engine runs locally instead:</p> <p>Info</p> <p>The local versioning engine is implemented with <code>polars-hash</code> and benefits from parallelism, predicate pushdown, and other features of Polars.</p> <ol> <li> <p>If the metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</p> </li> <li> <p>If the user explicitly requested to keep the computations local by setting <code>versioning_engine=\"polars\"</code> when instantiating the metadata store.</p> </li> <li> <p>If a fallback store had to be used to retrieve one of the parent features missing in the current store.</p> </li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p>"},{"location":"guide/learn/metadata-stores/#deletions","title":"Deletions","text":"<p>Deletes are typically not required during normal operations, but they are still supported for cleanup purposes. (1)</p> <ol> <li>deletions might be necessary when working with expansion linear relationships.</li> </ol> <p>Here is an example of how a deletion would look like:</p> <pre><code>from datetime import datetime, timedelta, timezone\n\nimport narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=[nw.col(\"metaxy_created_at\") &lt; datetime.now(timezone.utc) - timedelta(days=30)],\n    )\n</code></pre>"},{"location":"guide/learn/metadata-stores/#metadata-store-implementations","title":"Metadata Store Implementations","text":"<p>Metaxy provides ready <code>MetadataStore</code> implementations for popular databases and storage systems.</p>"},{"location":"guide/learn/optional-dependencies/","title":"Optional Dependencies","text":"<p>By default, feature dependencies use inner joins - a sample only exists in the downstream feature if it exists in all upstream dependencies. Optional dependencies allow you to preserve samples even when some upstream data is missing by using left joins instead.</p>"},{"location":"guide/learn/optional-dependencies/#overview","title":"Overview","text":"<p>When building features with multiple dependencies, you often want certain dependencies to be \"optional\" - meaning the downstream feature should still include samples even if the optional dependency has no matching data. This is controlled by the <code>optional</code> parameter on <code>FeatureDep</code>.</p> <pre><code>import metaxy as mx\n\n\nclass RawVideo(mx.BaseFeature, spec=mx.FeatureSpec(key=\"raw/video\", id_columns=[\"video_id\"])):\n    path: str\n\n\nclass AudioTranscript(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/transcript\", id_columns=[\"video_id\"], deps=[RawVideo])\n):\n    text: str\n\n\nclass EnrichedVideo(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"enriched/video\",\n        id_columns=[\"video_id\"],\n        deps=[\n            mx.FeatureDep(feature=RawVideo),  # Required (default)\n            mx.FeatureDep(feature=AudioTranscript, optional=True),  # Optional\n        ],\n        fields=[\"analysis\"],\n    ),\n):\n    pass\n</code></pre>"},{"location":"guide/learn/optional-dependencies/#join-behavior","title":"Join Behavior","text":"Dependency Type Join Type Behavior Required (<code>optional=False</code>) Inner join Sample must exist in both features Optional (<code>optional=True</code>) Left join Sample preserved even if no match in optional dep All optional Outer join Any row from any dependency can pass through <p>Required dependencies are joined first (inner join), then optional dependencies are joined (left join). If all dependencies are optional, outer joins are used instead to allow any row to pass through.</p>"},{"location":"guide/learn/optional-dependencies/#when-to-use-optional-dependencies","title":"When to Use Optional Dependencies","text":""},{"location":"guide/learn/optional-dependencies/#enrichment-pattern","title":"Enrichment Pattern","text":"<p>Use optional dependencies when you have a base feature that should always be present, with optional enrichment data that may or may not exist:</p> <pre><code>class AudioTranscription(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/transcription\", id_columns=[\"video_id\"], deps=[RawVideo])\n):\n    transcript: str\n\n\nclass ManualAnnotations(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"manual/annotations\", id_columns=[\"video_id\"], deps=[RawVideo])\n):\n    labels: str\n\n\nclass VideoAnalysis(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video/analysis\",\n        id_columns=[\"video_id\"],\n        deps=[\n            mx.FeatureDep(feature=RawVideo),  # Required - defines sample universe\n            mx.FeatureDep(feature=AudioTranscription, optional=True),  # Optional enrichment\n            mx.FeatureDep(feature=ManualAnnotations, optional=True),  # Optional enrichment\n        ],\n        fields=[\"analysis\"],\n    ),\n):\n    pass\n</code></pre> <p>In this example:</p> <ul> <li>All samples from <code>RawVideo</code> are preserved</li> <li>If <code>AudioTranscription</code> exists for a sample, it's included; otherwise, those columns are <code>NULL</code></li> <li>If <code>ManualAnnotations</code> exists for a sample, it's included; otherwise, those columns are <code>NULL</code></li> </ul>"},{"location":"guide/learn/optional-dependencies/#multi-source-fusion","title":"Multi-Source Fusion","text":"<p>Use optional dependencies when combining data from multiple sources where not all sources have all samples:</p> <pre><code>import metaxy as mx\n\n\nclass CoreUserData(mx.BaseFeature, spec=mx.FeatureSpec(key=\"user/core\", id_columns=[\"user_id\"])):\n    name: str\n\n\nclass SocialMediaData(mx.BaseFeature, spec=mx.FeatureSpec(key=\"user/social\", id_columns=[\"user_id\"])):\n    followers: int\n\n\nclass PurchaseHistory(mx.BaseFeature, spec=mx.FeatureSpec(key=\"user/purchases\", id_columns=[\"user_id\"])):\n    total_spent: float\n\n\nclass UnifiedUserProfile(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"user/unified\",\n        id_columns=[\"user_id\"],\n        deps=[\n            mx.FeatureDep(feature=CoreUserData),  # Required base\n            mx.FeatureDep(feature=SocialMediaData, optional=True),\n            mx.FeatureDep(feature=PurchaseHistory, optional=True),\n        ],\n        fields=[\"profile\"],\n    ),\n):\n    pass\n</code></pre>"},{"location":"guide/learn/optional-dependencies/#provenance-with-optional-dependencies","title":"Provenance with Optional Dependencies","text":"<p>Metaxy tracks provenance correctly even when optional dependencies have missing data. When an optional dependency has no match, an empty string is used in provenance calculation. This means samples with vs without optional data will have different provenance hashes, which is correct behavior since the sample's lineage genuinely differs based on what upstream data contributed to it.</p>"},{"location":"guide/learn/relationship/","title":"Lineage Relationships","text":"<p>Metaxy supports a few common mappings from parent to child samples out of the box. These include:</p> <ul> <li> <p><code>1:1</code> mapping with <code>LineageRelationship.identity</code> (the default one)</p> </li> <li> <p><code>1:N</code> mapping with <code>LineageRelationship.expansion</code></p> </li> <li> <p><code>N:1</code> mapping with <code>LineageRelationship.aggregation</code></p> </li> </ul> <p>Tip</p> <p>Always use these classmethods to create instances of lineage relationships. They use Pydantic's discriminated unions under the hood to ensure correct type construction.</p>"},{"location":"guide/learn/relationship/#examples","title":"Examples","text":"<ul> <li>1:N expansion</li> <li>N:1 aggregation</li> </ul>"},{"location":"guide/learn/syntactic-sugar/","title":"Syntactic Sugar","text":""},{"location":"guide/learn/syntactic-sugar/#type-coercion-for-input-types","title":"Type Coercion For Input Types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent feature keys, their fields, and the dependencies between them.</p> <p>To avoid boilerplate, Metaxy also has syntactic sugar for construction of these classes. Different ways to provide them are automatically coerced into canonical internal models. This is fully typed and only affects constructor arguments, so accessing attributes on Metaxy models will always return only the canonical types.</p> <p>Some examples:</p> <pre><code>import metaxy as mx\n\nkey = mx.FeatureKey(\"prefix/feature\")\nkey = mx.FeatureKey([\"prefix\", \"feature\"])\nsame_key = mx.FeatureKey(key)\n</code></pre> <p>Metaxy really loves you, the user!</p>"},{"location":"guide/learn/syntactic-sugar/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> accept:</p> <ul> <li> <p>String format: <code>FeatureKey(\"prefix/feature\")</code></p> </li> <li> <p>Sequence format: <code>FeatureKey([\"prefix\", \"feature\"])</code></p> </li> <li> <p>Same type: <code>FeatureKey(another_feature_key)</code> -- for full Inception mode</p> </li> </ul> <p>All formats produce equivalent keys, internally represented as a sequence of parts.</p>"},{"location":"guide/learn/syntactic-sugar/#feature-dep","title":"Feature Dep","text":"<p><code>FeatureDep</code> accepts types coercible to <code>FeatureKey</code> and additionally subclasses of <code>BaseFeature</code>:</p> <pre><code>import metaxy as mx\n\ndep = mx.FeatureDep(feature=MyFeature)\n</code></pre>"},{"location":"guide/learn/syntactic-sugar/#feature-spec","title":"Feature Spec","text":"<p><code>FeatureSpec</code> has some syntactic sugar implemented as well.</p>"},{"location":"guide/learn/syntactic-sugar/#deps","title":"Deps","text":"<p>The <code>deps</code> argument accepts a sequence of types coercible to <code>FeatureDep</code>:</p> <pre><code>import metaxy as mx\n\nspec = mx.FeatureSpec(\n    key=\"example/spec\",\n    id_columns=[\"id\"],\n    deps=[\n        MyFeature,\n        mx.FeatureDep(feature=[\"my\", \"feature\", \"key\"]),  # sequence format\n        [\"another\", \"key\"],  # also sequence format\n        \"very/nice\",  # string format with slash separator\n    ],\n)\n</code></pre>"},{"location":"guide/learn/syntactic-sugar/#fields","title":"Fields","text":"<p><code>fields</code> elements can omit the full <code>FieldsSpec</code> and be strings (field keys) instead:</p> <pre><code>import metaxy as mx\n\nspec = mx.FeatureSpec(\n    key=\"example/fields\",\n    id_columns=[\"id\"],\n    fields=[\"my/field\", mx.FieldSpec(key=\"field/with/version\", code_version=\"v1.2.3\")],\n)\n</code></pre>"},{"location":"guide/learn/syntactic-sugar/#fields-mapping","title":"Fields Mapping","text":"<p>Metaxy uses a bunch of common sense heuristics automatically find parent fields by matching on their names. This is enabled by default. For example, using the same field names in upstream and downstream features will automatically create a dependency between these fields:</p> <pre><code>import metaxy as mx\n\n\nclass Parent(mx.BaseFeature, spec=mx.FeatureSpec(key=\"parent/feature\", id_columns=[\"id\"], fields=[\"my_field\"])):\n    id: str\n\n\nclass Child(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"child/feature\", id_columns=[\"id\"], deps=[Parent], fields=[\"my_field\"])\n):\n    id: str\n</code></pre> <p>is equivalent to:</p> <pre><code>import metaxy as mx\n\n\nclass Grandchild(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"grandchild/feature\",\n        id_columns=[\"id\"],\n        deps=[Child],\n        fields=[mx.FieldSpec(key=\"my_field\", deps=[mx.FieldDep(feature=Parent, fields=[\"my_field\"])])],\n    ),\n):\n    id: str\n</code></pre>"},{"location":"guide/learn/system-columns/","title":"System Column Registry","text":"<p>Metaxy reserves a set of system-managed columns that it attaches to user-defined feature metadata tables. These columns are part of the platform contract and are used by the metadata store, versioning engine, and migration tooling.</p> <p>All system column names start with the <code>metaxy_</code> prefix.</p>"},{"location":"guide/learn/system-columns/#canonical-column-names","title":"Canonical column names","text":"Canonical name Explanation Level Type <code>metaxy_provenance_by_field</code> Derived from upstream data versions and code version per field sample struct <code>metaxy_provenance</code> Hash of <code>metaxy_provenance_by_field</code> sample string <code>metaxy_data_version_by_field</code> Defaults to <code>metaxy_provenance_by_field</code>, can be user-defined sample struct <code>metaxy_data_version</code> Hash of <code>metaxy_data_version_by_field</code> sample string <code>metaxy_feature_version</code> Derived from versions of relevant upstream fields feature string <code>metaxy_snapshot_version</code> Derived from the entire Metaxy feature graph graph string <code>metaxy_definition_version</code> Hash of the feature spec and Pydantic model schema (excludes project) feature string <code>metaxy_created_at</code> Timestamp when the metadata row was created sample string <code>metaxy_updated_at</code> Timestamp when the metadata row was last written to the store sample string <code>metaxy_deleted_at</code> Timestamp when the metadata row was soft-deleted (null if active) sample string <code>metaxy_materialization_id</code> External orchestration run ID (e.g., Dagster, Airflow) for tracking run string"},{"location":"guide/learn/testing/","title":"Testing Metaxy Features","text":"<p>This guide covers patterns for testing your features when using Metaxy.</p>"},{"location":"guide/learn/testing/#graph-isolation","title":"Graph Isolation","text":"<p>By default, Metaxy uses a single global feature graph where all features register themselves automatically. During testing, you might want to construct your own, clean and isolated graphs.</p>"},{"location":"guide/learn/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>import pytest\nimport metaxy as mx\nfrom metaxy.models.feature import FeatureGraph\n\n\n@pytest.fixture(autouse=True)\ndef isolated_graph():\n    with FeatureGraph().use() as g:\n        yield g\n\n\ndef test_my_feature(isolated_graph: FeatureGraph):\n    class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test/feature\", id_columns=[\"id\"])):\n        id: str\n\n    # Test operations here\n    assert isolated_graph.get_spec(\"test/feature\") is not None\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one. Multiple graphs can exist at the same time, but only one will be used for feature registration.</p>"},{"location":"guide/learn/testing/#graph-context-management","title":"Graph Context Management","text":"<p>The active graph uses context variables to support multiple graphs:</p> <pre><code>from metaxy.models.feature import FeatureGraph\n\n# Create a new graph instance\ncustom_graph = FeatureGraph()\n\n# Get active graph (returns the currently active context)\nactive = FeatureGraph.get_active()\n\n# Use custom graph temporarily\nwith custom_graph.use():\n    # All operations use custom_graph\n    assert FeatureGraph.get_active() is custom_graph\n</code></pre> <p>This enables:</p> <ul> <li>Isolated testing: Each test gets its own feature registry</li> <li>Migration testing: Load historical graphs for migration scenarios</li> <li>Multi-environment testing: Test different feature configurations</li> </ul>"},{"location":"guide/learn/testing/#testing-metadata-store-operations","title":"Testing Metadata Store Operations","text":""},{"location":"guide/learn/testing/#testing-with-different-backends","title":"Testing with Different Backends","text":"<p>Use parametrized tests to verify behavior across backends:</p> <pre><code>import pytest\nfrom metaxy.metadata_store.delta import DeltaMetadataStore\nfrom metaxy.metadata_store.duckdb import DuckDBMetadataStore\n\n\n@pytest.mark.parametrize(\n    \"store_cls\",\n    [\n        DeltaMetadataStore,\n        DuckDBMetadataStore,\n    ],\n)\ndef test_store_behavior(store_cls, tmp_path):\n    # Use tmp_path for file-based stores\n    if store_cls == DeltaMetadataStore:\n        store_kwargs = {\"root_path\": tmp_path / \"delta_store\"}\n    else:\n        store_kwargs = {\"database\": tmp_path / \"test.db\"}\n\n    with store_cls(**store_kwargs) as test_store:\n        # Test your feature operations\n        pass\n</code></pre>"},{"location":"guide/learn/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing AUTO_CREATE_TABLES Warnings","text":"<p>When testing with <code>auto_create_tables=True</code>, Metaxy emits warnings to remind you not to use this in production. These warnings are important for production safety, but can clutter test output.</p> <p>To suppress these warnings in your test suite, use pytest's <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but pytest filters it from test output.</p> <p>Testing the Warning Itself</p> <p>If you need to verify that the warning is actually emitted, use <code>pytest.warns()</code>:</p> <pre><code>import pytest\n\n\ndef test_auto_create_tables_warning():\n    with pytest.warns(UserWarning, match=r\"AUTO_CREATE_TABLES is enabled.*do not use in production\"):\n        with DuckDBMetadataStore(\":memory:\", auto_create_tables=True) as store:\n            pass  # Warning is emitted and captured\n</code></pre> <p>This works even with <code>filterwarnings</code> configured, because <code>pytest.warns()</code> explicitly captures and verifies the warning.</p>"},{"location":"guide/learn/testing/#programmatic-metaxy-configuration","title":"Programmatic Metaxy Configuration","text":"<p>When configuring Metaxy programmatically in Python code, use the following pattern (we also showcase plugin configuration):</p> <pre><code>from metaxy.config import MetaxyConfig\nfrom metaxy.ext.sqlmodel import SQLModelPluginConfig\n\nwith MetaxyConfig(\n    ext={\n        \"sqlmodel\": SQLModelPluginConfig(\n            enable=True,\n            inject_primary_key=True,\n        )\n    }\n).use() as cfg:\n    sqlmodel_config = MetaxyConfig.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    assert sqlmodel_config.inject_primary_key is True\n</code></pre> <p>When testing Metaxy code, it's best if this setup is performed via <code>pytest</code> fixtures.</p> <p>The plugin configuration is accessed via a dictionary where keys are plugin names and values are plugin-specific configuration objects.</p>"},{"location":"guide/overview/feature-dependencies/","title":"Feature Dependencies","text":"<p>Back to quickstart</p> <p>Now let's add a downstream feature. We can use <code>deps</code> field on <code>FeatureSpec</code> in order to do that.</p> features.py<pre><code>import metaxy as mx\nfrom pydantic import Field\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            \"audio\",\n            \"frames\",\n        ],\n    ),\n):\n    # define DB columns\n    video_id: str = Field(description=\"Unique identifier for the video\")\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n\n\nclass CroppedVideo(\n    Video,  # inheritance is a good way to automatically get matching DB columns\n    spec=mx.FeatureSpec(\n        key=\"cropped_video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            \"audio\",\n            \"frames\",\n        ],\n        deps=[Video],\n    ),\n):\n    # additional columns\n    height: int = Field(description=\"Height of the video in pixels\")\n    width: int = Field(description=\"Width of the video in pixels\")\n</code></pre>"},{"location":"guide/overview/quickstart/","title":"Quickstart","text":""},{"location":"guide/overview/quickstart/#installation","title":"Installation","text":"<p>Install Metaxy with <code>deltalake</code> - an easy way to setup a <code>MetadataStore</code> locally:</p> <pre><code>pip install 'metaxy[delta]'\n</code></pre>"},{"location":"guide/overview/quickstart/#drop-a-metaxytoml-file","title":"Drop a <code>metaxy.toml</code> file","text":"metaxy.toml<pre><code>project = \"quickstart\"\nentrypoints = [\"features.py\"]\n\n[stores.dev]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\nconfig = { root_path = \"${HOME}/.metaxy/deltalake\" }\n</code></pre>"},{"location":"guide/overview/quickstart/#define-a-root-feature","title":"Define a root feature","text":"<p>Every Metaxy project must define at least one root feature. Such features do not have upstream dependencies and act as inputs to the feature graph.</p> features.py<pre><code>import metaxy as mx\nfrom pydantic import Field\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            \"audio\",\n            \"frames\",\n        ],\n    ),\n):\n    # define DB columns\n    video_id: str = Field(description=\"Unique identifier for the video\")\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n</code></pre>"},{"location":"guide/overview/quickstart/#create-feature-materialization-script","title":"Create feature materialization script","text":"<p>Use <code>MetadataStore.resolve_update</code> to compute an increment for materialization:</p> script.py<pre><code>import metaxy as mx\n\nfrom .features import Video\n\n# discover and load Metaxy features\ncfg = mx.init_metaxy()\n\n# instantiate the MetadataStore\nstore = cfg.get_store(\"dev\")\n\n# somehow prepare a DataFrame with incoming metadata\n# this can be a Pandas, Polars, Ibis, or any other DataFrame supported by Narwhals\nsamples = ...\n\nwith store:\n    increment = store.resolve_update(Video, samples=samples)\n</code></pre>"},{"location":"guide/overview/quickstart/#3-run-user-defined-computation-over-the-increment","title":"3. Run user-defined computation over the increment","text":"<p>Metaxy is not involved in this step at all.</p> script.py<pre><code>if (len(increment.added) + len(increment.changed)) &gt; 0:\n    # run your computation, this can be done in a distributed manner\n    results = run_custom_pipeline(diff, ...)\n</code></pre>"},{"location":"guide/overview/quickstart/#4-record-metadata-for-processed-samples","title":"4. Record metadata for processed samples","text":"script.py<pre><code>with store.open(\"w\"):\n    store.write(VoiceDetection, results)\n</code></pre> <p>We have now successfully recorded the metadata for the computed samples! Processed samples will no longer be returned by <code>MetadataStore.resolve_update</code> during future pipeline runs.</p>"},{"location":"guide/overview/quickstart/#next-steps","title":"Next Steps","text":"<p>Continue to next section to learn how to add more features and define feature dependencies.</p>"},{"location":"guide/overview/quickstart/#additional-info","title":"Additional info","text":"<ul> <li> <p>Learn more about feature definitions or versioning</p> </li> <li> <p>Explore Metaxy integrations</p> </li> <li> <p>Use Metaxy from the command line</p> </li> <li> <p>Learn how to configure Metaxy</p> </li> <li> <p>Get lost in our API Reference</p> </li> </ul>"},{"location":"integrations/","title":"Metaxy Integrations","text":""},{"location":"integrations/#orchestration","title":"Orchestration","text":"<ul> <li> <p> Dagster</p> <p> Orchestration \u2022 Data Platform</p> <p>Seamlessly integrate Metaxy with Dagster with the power of <code>@metaxify</code> and the <code>MetaxyIOManager</code>.</p> <p>Recommended</p> <p>Metaxy has been built with Dagster in mind. This integration is the best way to organize, materialize and observe multiple Metaxy features at scale.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#metadata-stores","title":"Metadata Stores","text":"<p>Learn more about metadata stores here.</p> <ul> <li> <p>Google BigQuery BigQuery</p> <p> Database</p> <p>Use Google BigQuery - scalable serverless analytical database on GCP.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>ClickHouse ClickHouse</p> <p> Database</p> <p>Leverage the lightning-fast analytical ClickHouse database for large metadata volume and high-throughput setups.</p> <p>Recommended</p> <p>Ideal for production.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>Delta Lake Delta Lake</p> <p> Storage</p> <p>Store metadata in Delta Lake format in local files or remote object stores (S3, GCS, and others). (1)</p> <p>Recommended</p> <p>Ideal for dev environments.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>DuckDB DuckDB</p> <p> Database \u2022  Storage</p> <p>Use DuckDB - a fast analytical database with support for local and remote compute. DuckLake is available as well.</p> <p>Warning</p> <p>DuckDB is not recommended for production due to parallel writes limitations.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>LanceDB LanceDB</p> <p> Database \u2022  Storage</p> <p>Use the multi-modal LanceDB database or Lance storage format. (2)</p> <p> Integration docs</p> <p> API docs</p> </li> </ul> <ol> <li> <p>uses a local versioning engine implemented in Polars and <code>polars-hash</code></p> </li> <li> <p>uses a local versioning engine implemented in Polars and <code>polars-hash</code></p> </li> </ol>"},{"location":"integrations/#compute","title":"Compute","text":"<ul> <li> <p>Ray Ray</p> <p> Compute \u2022 Distributed</p> <p>Use Metaxy with Ray for distributed computing workloads.</p> <p> Integration docs</p> </li> </ul>"},{"location":"integrations/#plugins","title":"Plugins","text":"<ul> <li> <p>SQLAlchemy SQLAlchemy</p> <p> ORM \u2022 Database</p> <p>Retrieve SQLAlchemy URLs and <code>MetaData</code> for the current Metaxy project from Metaxy <code>MetadataStore</code> objects.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p> SQLModel SQLModel</p> <p> ORM \u2022 Database</p> <p>Adds <code>SQLModel</code> capabilities to <code>metaxy.BaseFeature</code> class.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#ai","title":"AI","text":"<ul> <li> <p> Claude Code</p> <p> AI \u2022 LLM</p> <p>Use Metaxy with Claude Code through the official plugin, providing the <code>/metaxy</code> skill and MCP tools.</p> <p> Integration docs</p> </li> <li> <p> MCP Server</p> <p> AI \u2022 LLM</p> <p>Expose Metaxy's feature graph and metadata store operations to AI assistants via the Model Context Protocol.</p> <p> Integration docs</p> </li> </ul>"},{"location":"integrations/ai/claude/","title":"Claude Code Plugin","text":"<p>A Claude Code plugin that provides additional context for working with Metaxy projects.</p>"},{"location":"integrations/ai/claude/#features","title":"Features","text":"<ul> <li><code>/metaxy</code> skill: Guidance on working with Metaxy, including feature definitions, versioning, and metadata stores</li> <li>MCP tools: Explore feature graphs and query metadata directly from Claude Code via the MCP server</li> </ul>"},{"location":"integrations/ai/claude/#installation","title":"Installation","text":"<pre><code>/plugin marketplace add anam-org/metaxy\n/plugin install metaxy\n</code></pre>"},{"location":"integrations/ai/claude/#requirements","title":"Requirements","text":"<p><code>uv</code> must be installed. The plugin starts the MCP server via <code>uv run</code> to use the project's Python environment.</p>"},{"location":"integrations/ai/mcp/","title":"MCP Server","text":"<p>The Model Context Protocol (MCP) server exposes Metaxy's feature graph and metadata store operations to AI assistants, enabling them to explore your feature definitions and query metadata.</p>"},{"location":"integrations/ai/mcp/#installation","title":"Installation","text":"<p>Install Metaxy with the <code>mcp</code> extra:</p> uvpip <pre><code>uv add metaxy[mcp]\n</code></pre> <pre><code>pip install metaxy[mcp]\n</code></pre>"},{"location":"integrations/ai/mcp/#running-the-server","title":"Running the Server","text":"<p>Run the MCP server from your Metaxy project directory:</p> <pre><code>metaxy mcp # (1)!\n</code></pre> <ol> <li>Use <code>uv run metaxy mcp</code> to run the server within the project's Python environment.</li> </ol> <p>The server uses the standard Metaxy configuration discovery, loading <code>metaxy.toml</code> from the current directory or parent directories.</p>"},{"location":"integrations/ai/mcp/#configuration","title":"Configuration","text":""},{"location":"integrations/ai/mcp/#claude-code","title":"Claude Code","text":"<p>Add the MCP server to your project's <code>.claude/settings.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"metaxy\": {\n      \"command\": \"metaxy\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n</code></pre>"},{"location":"integrations/ai/mcp/#available-tools","title":"Available Tools","text":"<p>The MCP server provides the following tools:</p>"},{"location":"integrations/ai/mcp/#get_config","title":"<code>get_config</code>","text":"<p>Get the current Metaxy configuration as JSON.</p> <p>Returns:</p> <p>The full Metaxy configuration serialized as JSON, including all settings like project, store, entrypoints, stores, migrations_dir, etc.</p>"},{"location":"integrations/ai/mcp/#get_feature","title":"<code>get_feature</code>","text":"<p>Get the complete specification for a feature.</p> <p>Parameters:</p> Name Type Required Default Description <code>feature_key</code> <code>str</code> Yes \u2014 Feature key in slash notation (e.g., \"video/processing\") <p>Returns:</p> <p>Complete feature specification as a dictionary including fields, dependencies, id_columns, and other metadata</p>"},{"location":"integrations/ai/mcp/#get_metadata","title":"<code>get_metadata</code>","text":"<p>Query metadata for a feature from a store.</p> <p>Parameters:</p> Name Type Required Default Description <code>feature_key</code> <code>str</code> Yes \u2014 Feature key in slash notation (e.g., \"video/processing\") <code>store_name</code> <code>str</code> Yes \u2014 Name of the metadata store to read from <code>columns</code> <code>list[str] | None</code> No <code>None</code> List of columns to select (None for all) <code>filters</code> <code>list[str] | None</code> No <code>None</code> List of SQL-like filter expressions (e.g., \"column &gt; 5\", \"name == 'foo'\") <code>with_feature_history</code> <code>bool</code> No <code>False</code> Only return current (non-superseded) rows <code>with_sample_history</code> <code>bool</code> No <code>False</code> Only return latest version of each row <code>include_soft_deleted</code> <code>bool</code> No <code>False</code> Include soft-deleted rows <code>allow_fallback</code> <code>bool</code> No <code>True</code> If True, check fallback stores when feature is not found in the primary store <code>sort_by</code> <code>list[str] | None</code> No <code>None</code> List of column names to sort by <code>descending</code> <code>bool | list[bool]</code> No <code>False</code> Sort descending (bool for all columns, or list per column) <code>limit</code> <code>int</code> No <code>50</code> Maximum number of rows to return <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li>columns: List of column names</li> <li>rows: List of row dictionaries</li> <li>total_rows: Number of rows returned</li> </ul>"},{"location":"integrations/ai/mcp/#get_store","title":"<code>get_store</code>","text":"<p>Get display information for a metadata store.</p> <p>Parameters:</p> Name Type Required Default Description <code>store_name</code> <code>str</code> Yes \u2014 Name of the store to get info for <p>Returns:</p> <p>Human-readable display string for the store (e.g., \"DuckDBMetadataStore(database=/tmp/db.duckdb)\")</p>"},{"location":"integrations/ai/mcp/#list_features","title":"<code>list_features</code>","text":"<p>List all registered features with their metadata.</p> <p>Matches the output format of <code>mx list features --format json</code>.</p> <p>Parameters:</p> Name Type Required Default Description <code>project</code> <code>str | None</code> No <code>None</code> Filter by project name (optional) <code>verbose</code> <code>bool</code> No <code>False</code> Include detailed field information and dependencies <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li>feature_count: Total number of features</li> <li>features: List of feature dictionaries with key, version, is_root, project, import_path, field_count, fields, and optionally deps</li> </ul>"},{"location":"integrations/ai/mcp/#list_stores","title":"<code>list_stores</code>","text":"<p>List all configured metadata stores.</p> <p>Returns:</p> <p>List of dictionaries with 'name' and 'type' for each store </p>"},{"location":"integrations/compute/ray/","title":"Ray integration for Metaxy","text":"<p>Metaxy can integrate with Ray for distributed computing workloads.</p> <p>members: true</p>"},{"location":"integrations/compute/ray/#metaxy.ext.ray","title":"metaxy.ext.ray","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray-classes","title":"Classes","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink","title":"metaxy.ext.ray.MetaxyDatasink","text":"<pre><code>MetaxyDatasink(\n    feature: CoercibleToFeatureKey,\n    store: MetadataStore,\n    config: MetaxyConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>Datasink[_WriteTaskResult]</code></p> <p>A Ray Data Datasink for writing to a Metaxy metadata store.</p> <p>Example</p> <pre><code>import metaxy as mx\nimport ray\n\ncfg = mx.init_metaxy()\ndataset = ...  # a ray.data.Dataset\n\ndatasink = MetaxyDatasink(\n    feature=\"my/feature\",\n    store=cfg.get_store(),\n    config=cfg,\n)\ndataset.write_datasink(datasink)\n\nprint(f\"Wrote {datasink.result.rows_written} rows, {datasink.result.rows_failed} failed\")\n</code></pre> <p>Note</p> <p>In the future this Datasink will support writing multiple features at once.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to write metadata for.</p> </li> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to write to.</p> </li> <li> <code>config</code>               (<code>MetaxyConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration. Will be auto-discovered by the worker if not provided.</p> <p>Warning</p> <p>Ensure the Ray environment is set up properly when not passing <code>config</code> explicitly. This can be achieved by setting <code>METAXY_CONFIG</code> and other <code>METAXY_</code> environment variables. The best practice is to pass <code>config</code> explicitly to avoid surprises.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def __init__(\n    self,\n    feature: mx.CoercibleToFeatureKey,\n    store: mx.MetadataStore,\n    config: mx.MetaxyConfig | None = None,\n):\n    self.config = mx.init_metaxy(config)\n\n    self.store = store\n    self.config = config\n\n    self._feature_key = mx.coerce_to_feature_key(feature)\n\n    # Populated after write completes\n    self._result: MetaxyWriteResult | None = None\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink-attributes","title":"Attributes","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.result","title":"result  <code>property</code>","text":"<pre><code>result: MetaxyWriteResult\n</code></pre> <p>Result of the write operation.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If accessed before the write operation completes.</p> </li> </ul>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink-functions","title":"Functions","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.write","title":"write","text":"<pre><code>write(\n    blocks: Iterable[Block], ctx: TaskContext\n) -&gt; _WriteTaskResult\n</code></pre> <p>Write blocks of metadata to the store.</p> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def write(\n    self,\n    blocks: Iterable[Block],\n    ctx: TaskContext,\n) -&gt; _WriteTaskResult:\n    \"\"\"Write blocks of metadata to the store.\"\"\"\n    # Initialize metaxy on the worker - config and features are needed for write\n    config = mx.init_metaxy(self.config)\n    if config.sync:\n        mx.sync_external_features(self.store)\n\n    rows_written = 0\n    rows_failed = 0\n\n    for i, block in enumerate(blocks):\n        block_accessor = BlockAccessor.for_block(block)\n        num_rows = block_accessor.num_rows()\n\n        try:\n            with self.store.open(\"w\"):\n                self.store.write(self._feature_key, block)\n            rows_written += num_rows\n        except Exception:\n            logger.exception(\n                f\"Failed to write {num_rows} metadata rows for feature {self._feature_key.to_string()} block {i} of task {ctx.task_idx} ({ctx.op_name})\"\n            )\n            rows_failed += num_rows\n\n    return _WriteTaskResult(rows_written=rows_written, rows_failed=rows_failed)\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.on_write_complete","title":"on_write_complete","text":"<pre><code>on_write_complete(\n    write_result: WriteResult[_WriteTaskResult],\n) -&gt; None\n</code></pre> <p>Aggregate write statistics from all tasks.</p> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def on_write_complete(self, write_result: WriteResult[_WriteTaskResult]) -&gt; None:\n    \"\"\"Aggregate write statistics from all tasks.\"\"\"\n    rows_written = 0\n    rows_failed = 0\n\n    for task_result in write_result.write_returns:\n        rows_written += task_result.rows_written\n        rows_failed += task_result.rows_failed\n\n    self._result = MetaxyWriteResult(rows_written=rows_written, rows_failed=rows_failed)\n\n    logger.info(\n        f\"MetaxyDatasink write complete for {self._feature_key.to_string()}: \"\n        f\"{rows_written} rows written, {rows_failed} rows failed\"\n    )\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource","title":"metaxy.ext.ray.MetaxyDatasource","text":"<pre><code>MetaxyDatasource(\n    feature: CoercibleToFeatureKey,\n    store: MetadataStore,\n    config: MetaxyConfig | None = None,\n    *,\n    incremental: bool = False,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n)\n</code></pre> <p>               Bases: <code>Datasource</code></p> <p>A Ray Data Datasource for reading from a Metaxy metadata store.</p> <p>This datasource reads metadata entries from a Metaxy metadata store as Ray Data blocks, associated with a specific feature key.</p> <p>Example</p> <pre><code>import metaxy as mx\nimport ray\n\ncfg = mx.init_metaxy()\n\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n    )\n)\n</code></pre> <p>with filters and column selection</p> <pre><code>import narwhals as nw\n\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n        filters=[nw.col(\"value\") &gt; 10],\n        columns=[\"sample_uid\", \"value\"],\n    )\n)\n</code></pre> <p>incremental mode</p> <pre><code># Read only samples that need processing\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n        incremental=True,\n    )\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for.</p> </li> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to read from.</p> </li> <li> <code>config</code>               (<code>MetaxyConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration. Will be auto-discovered by the worker if not provided.</p> <p>Warning</p> <p>Ensure the Ray environment is set up properly when not passing <code>config</code> explicitly. This can be achieved by setting <code>METAXY_CONFIG</code> and other <code>METAXY_</code> environment variables. The best practice is to pass <code>config</code> explicitly to avoid any surprises.</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return only samples that need processing (new and stale). Adds a <code>metaxy_status</code> column with values:</p> <ul> <li> <p><code>\"new\"</code>: samples that have not been processed yet</p> </li> <li> <p><code>\"stale\"</code>: samples that have been processed but have to be reprocessed</p> </li> </ul> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply.</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on main store miss.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature version.</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature version to filter by (mutually exclusive with <code>with_feature_history=False</code>).</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> <li> <code>include_soft_deleted</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, include soft-deleted rows in the result.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def __init__(\n    self,\n    feature: mx.CoercibleToFeatureKey,\n    store: mx.MetadataStore,\n    config: mx.MetaxyConfig | None = None,\n    *,\n    incremental: bool = False,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n):\n    self.config = mx.init_metaxy(config)\n    self.store = store\n    self.incremental = incremental\n    self.feature_version = feature_version\n    self.filters = list(filters) if filters else None\n    self.columns = list(columns) if columns else None\n    self.allow_fallback = allow_fallback\n    self.with_feature_history = with_feature_history\n    self.with_sample_history = with_sample_history\n    self.include_soft_deleted = include_soft_deleted\n\n    self._feature_key = mx.coerce_to_feature_key(feature)\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource-functions","title":"Functions","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource.get_read_tasks","title":"get_read_tasks","text":"<pre><code>get_read_tasks(\n    parallelism: int, per_task_row_limit: int | None = None\n) -&gt; list[ReadTask]\n</code></pre> <p>Return read tasks for the feature metadata.</p> <p>Parameters:</p> <ul> <li> <code>parallelism</code>               (<code>int</code>)           \u2013            <p>Requested parallelism level (currently ignored, returns single task).</p> </li> <li> <code>per_task_row_limit</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum rows per returned block. If set, the data will be split into multiple blocks of at most this size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[ReadTask]</code>           \u2013            <p>List containing a single ReadTask that may return multiple blocks.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def get_read_tasks(self, parallelism: int, per_task_row_limit: int | None = None) -&gt; list[ReadTask]:\n    \"\"\"Return read tasks for the feature metadata.\n\n    Args:\n        parallelism: Requested parallelism level (currently ignored, returns single task).\n        per_task_row_limit: Maximum rows per returned block. If set, the data will be\n            split into multiple blocks of at most this size.\n\n    Returns:\n        List containing a single ReadTask that may return multiple blocks.\n    \"\"\"\n    num_rows = self._get_row_count()\n\n    # Capture self for the closure\n    datasource = self\n    row_limit = per_task_row_limit\n\n    def read_fn() -&gt; list[pa.Table]:\n        mx.init_metaxy(datasource.config)\n\n        with datasource.store:\n            lf = datasource._read_lazy()\n            table = lf.collect(backend=\"pyarrow\").to_arrow()\n            batches = table.to_batches(max_chunksize=row_limit)\n            return [pa.Table.from_batches([b]) for b in batches]\n\n    metadata = BlockMetadata(\n        num_rows=num_rows,\n        size_bytes=None,\n        input_files=None,\n        exec_stats=None,\n    )\n\n    return [ReadTask(read_fn, metadata)]\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource.estimate_inmemory_data_size","title":"estimate_inmemory_data_size","text":"<pre><code>estimate_inmemory_data_size() -&gt; int | None\n</code></pre> <p>Return an estimate of in-memory data size, or None if unknown.</p> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def estimate_inmemory_data_size(self) -&gt; int | None:\n    \"\"\"Return an estimate of in-memory data size, or None if unknown.\"\"\"\n    return None\n</code></pre>"},{"location":"integrations/metadata-stores/","title":"Metadata Stores","text":"<p>Metadata stores may come in two flavors.</p>"},{"location":"integrations/metadata-stores/#database-backed","title":"Database-Backed","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible. (1)</p> <ol> <li> <p> Typically (1) the entire <code>MetadataStore.resolve_update</code> can be executed externally!</p> </li> <li> <p>Except the cases enumerated in [../../guide/learn/metadata-stores.md]</p> </li> </ol> <p>These metadata stores can be found here.</p> <p>Warning</p> <p>Metaxy does not handle infrastructure setup. Make sure to have large tables partitioned as appropriate for your use case.</p> <p>Example</p> <p>ClickHouse is an excellent choice for a production metadata store.</p> <p>Tip</p> <p>Some of them such as LanceDB or DuckDB can also act as local compute engines.</p>"},{"location":"integrations/metadata-stores/#storage-only","title":"Storage Only","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>The available storage-only stores can be found here.</p> <p>Example</p> <p>DeltaLake is an excellent choice for a storage-only metadata store.</p>"},{"location":"integrations/metadata-stores/#choosing-the-right-metadata-store","title":"Choosing the Right Metadata Store","text":"<p>Compute-backed stores are typically more performant, but require additional infrastructure and maintenance.</p> <p>For production environments that need to handle big metadata volumes, consider database-backed stores.</p> <p>For development, testing, branch deployments, and other scenarios where you want to keep things simple, consider using a storage-only store.</p> <p>Warning</p> <p>Not all metadata stores support parallel writes. For example, DuckDB requires application level work-arounds.</p>"},{"location":"integrations/metadata-stores/#reference","title":"Reference","text":"<ul> <li>Learn more about using metadata stores</li> </ul>"},{"location":"integrations/metadata-stores/databases/","title":"Database-Backed Metadata Stores","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible (typically the entire <code>MetadataStore.resolve_update</code> can be executed externally).</p>"},{"location":"integrations/metadata-stores/databases/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>BigQuery</li> <li>ClickHouse</li> <li>DuckDB</li> <li>Ibis Integration</li> <li>LanceDB</li> </ul>"},{"location":"integrations/metadata-stores/databases/bigquery/","title":"BigQuery","text":"<p>BigQuery is a serverless data warehouse managed by Google Cloud. To use Metaxy with BigQuery, configure <code>BigQueryMetadataStore</code>. Versioning computations run natively in BigQuery.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[bigquery]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.metadata_store.bigquery","title":"metaxy.metadata_store.bigquery","text":"<p>BigQuery metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.metadata_store.bigquery.BigQueryMetadataStore","title":"metaxy.metadata_store.bigquery.BigQueryMetadataStore","text":"<pre><code>BigQueryMetadataStore(\n    project_id: str | None = None,\n    dataset_id: str | None = None,\n    *,\n    credentials_path: str | None = None,\n    credentials: Any | None = None,\n    location: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>BigQuery metadata store using Ibis backend.</p> Warning <p>It's on the user to set up infrastructure for Metaxy correctly. Make sure to have large tables partitioned as appropriate for your use case.</p> Note <p>BigQuery automatically optimizes queries on partitioned tables. When tables are partitioned (e.g., by date or ingestion time with _PARTITIONTIME), BigQuery will automatically prune partitions based on WHERE clauses in queries, without needing explicit configuration in the metadata store. Make sure to use appropriate <code>filters</code> when calling BigQueryMetadataStore.read.</p> Basic Connection <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n)\n</code></pre> With Service Account <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    credentials_path=\"/path/to/service-account.json\",\n)\n</code></pre> With Location Configuration <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    location=\"EU\",  # Specify data location\n)\n</code></pre> With Custom Hash Algorithm <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    hash_algorithm=HashAlgorithm.SHA256,  # Use SHA256 instead of default FARMHASH\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>project_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud project ID containing the dataset. Can also be set via GOOGLE_CLOUD_PROJECT environment variable.</p> </li> <li> <code>dataset_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>BigQuery dataset name for storing metadata tables. If not provided, uses the default dataset for the project.</p> </li> <li> <code>credentials_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to service account JSON file. Alternative to passing credentials object directly.</p> </li> <li> <code>credentials</code>               (<code>Any | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud credentials object. If not provided, uses default credentials from environment.</p> </li> <li> <code>location</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Default location for BigQuery resources (e.g., \"US\", \"EU\"). If not specified, BigQuery determines based on dataset location.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional Ibis BigQuery connection parameters. Overrides individual parameters if provided.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-bigquery not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither project_id nor connection_params provided</p> </li> </ul> Note <p>Authentication priority: 1. Explicit credentials or credentials_path 2. Application Default Credentials (ADC) 3. Google Cloud SDK credentials</p> <p>BigQuery automatically handles partition pruning when querying partitioned tables. If your tables are partitioned (e.g., by date or ingestion time), BigQuery will automatically optimize queries with appropriate WHERE clauses on the partition column.</p> Example <pre><code># Using environment authentication\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n)\n\n# Using service account\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    credentials_path=\"/path/to/key.json\",\n)\n\n# With location specification\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    location=\"EU\",\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/bigquery.py</code> <pre><code>def __init__(\n    self,\n    project_id: str | None = None,\n    dataset_id: str | None = None,\n    *,\n    credentials_path: str | None = None,\n    credentials: Any | None = None,\n    location: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [BigQuery](https://cloud.google.com/bigquery) metadata store.\n\n    Args:\n        project_id: Google Cloud project ID containing the dataset.\n            Can also be set via GOOGLE_CLOUD_PROJECT environment variable.\n        dataset_id: BigQuery dataset name for storing metadata tables.\n            If not provided, uses the default dataset for the project.\n        credentials_path: Path to service account JSON file.\n            Alternative to passing credentials object directly.\n        credentials: Google Cloud credentials object.\n            If not provided, uses default credentials from environment.\n        location: Default location for BigQuery resources (e.g., \"US\", \"EU\").\n            If not specified, BigQuery determines based on dataset location.\n        connection_params: Additional Ibis BigQuery connection parameters.\n            Overrides individual parameters if provided.\n        fallback_stores: Ordered list of read-only fallback stores.\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]\n\n    Raises:\n        ImportError: If ibis-bigquery not installed\n        ValueError: If neither project_id nor connection_params provided\n\n    Note:\n        Authentication priority:\n        1. Explicit credentials or credentials_path\n        2. Application Default Credentials (ADC)\n        3. Google Cloud SDK credentials\n\n        BigQuery automatically handles partition pruning when querying partitioned tables.\n        If your tables are partitioned (e.g., by date or ingestion time), BigQuery will\n        automatically optimize queries with appropriate WHERE clauses on the partition column.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Using environment authentication\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n        )\n\n        # Using service account\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            credentials_path=\"/path/to/key.json\",\n        )\n\n        # With location specification\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            location=\"EU\",\n        )\n        ```\n    \"\"\"\n    # Build connection parameters if not provided\n    if connection_params is None:\n        connection_params = self._build_connection_params(\n            project_id=project_id,\n            dataset_id=dataset_id,\n            credentials_path=credentials_path,\n            credentials=credentials,\n            location=location,\n        )\n\n    # Validate we have minimum required parameters\n    if \"project_id\" not in connection_params and project_id is None:\n        raise ValueError(\n            \"Must provide either project_id or connection_params with project_id. Example: project_id='my-project'\"\n        )\n\n    # Store parameters for display\n    self.project_id = project_id or connection_params.get(\"project_id\")\n    self.dataset_id = dataset_id or connection_params.get(\"dataset_id\", \"\")\n\n    # Initialize Ibis store with BigQuery backend\n    super().__init__(\n        backend=\"bigquery\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#configuration","title":"Configuration","text":""},{"location":"integrations/metadata-stores/databases/bigquery/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#project_id","title":"<code>project_id</code>","text":"<p>Google Cloud project ID containing the dataset.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# project_id = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# project_id = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__PROJECT_ID=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#dataset_id","title":"<code>dataset_id</code>","text":"<p>BigQuery dataset name for storing metadata tables.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# dataset_id = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# dataset_id = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATASET_ID=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#credentials_path","title":"<code>credentials_path</code>","text":"<p>Path to service account JSON file.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# credentials_path = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# credentials_path = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS_PATH=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#credentials","title":"<code>credentials</code>","text":"<p>Google Cloud credentials object.</p> <p>Type: <code>Optional[Any]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# credentials = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# credentials = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#location","title":"<code>location</code>","text":"<p>Default location for BigQuery resources (e.g., 'US', 'EU').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# location = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# location = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LOCATION=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/","title":"ClickHouse","text":"<p>ClickHouse is a (1) column-oriented OLAP database designed for real-time analytics. To use Metaxy with ClickHouse, configure <code>ClickHouseMetadataStore</code>. Versioning computations run natively in ClickHouse, making it well-suited for high-throughput production workloads.</p> <ol> <li>extremely fast</li> </ol>"},{"location":"integrations/metadata-stores/databases/clickhouse/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[clickhouse]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxys-versioning-struct-columns","title":"Metaxy's Versioning Struct Columns","text":"<p>Metaxy uses struct columns (<code>metaxy_provenance_by_field</code>, <code>metaxy_data_version_by_field</code>) to track field-level versioning. In Python world this corresponds to <code>dict[str, str]</code>. In ClickHouse, there are several options to represent these columns.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#how-clickhouse-handles-structs","title":"How ClickHouse Handles Structs","text":"<p>ClickHouse offers multiple approaches to represent Metaxy's structured versioning columns:</p> Type Description Use Case <code>Map(String, String)</code> Native key-value map Recommended for Metaxy because of dynamic keys <code>JSON</code> Native JSON with typed subcolumns Less performant than <code>Map(String, String)</code> but more flexible than <code>Nested</code> <code>Nested(field_1 String, ...)</code> Static struct with named fields More performant than <code>Map(String, String)</code> but keys are static <p>Recommended: <code>Map(String, String)</code></p> <p>For Metaxy's <code>metaxy_provenance_by_field</code> and <code>metaxy_data_version_by_field</code> columns, use <code>Map(String, String)</code>:</p> <ul> <li> <p>No migrations required when feature fields change</p> </li> <li> <p>Good performance for key-value lookups</p> </li> </ul> <p>Special Map columns handling</p> <p>Metaxy transforms its system columns (<code>metaxy_provenance_by_field</code>, <code>metaxy_data_version_by_field</code>):</p> <ul> <li> <p>Reading: System Map columns are converted into Ibis Structs (e.g., <code>Struct[{\"field_a\": str, \"field_b\": str}]</code>)</p> </li> <li> <p>Writing: If the input comes from Polars, then Polars Structs are converted into expected ClickHouse Map format</p> </li> </ul> <p>User-defined Map columns are not transformed. They remain as <code>List[Struct[{\"key\": str, \"value\": str}]]</code> (Arrow's Map representation). Make sure to use the right format when providing a Polars DataFrame for writing.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#sqlalchemy-and-alembic-migrations","title":"SQLAlchemy and Alembic Migrations","text":"<p>For SQLAlchemy and Alembic migrations support, use the <code>clickhouse-sqlalchemy</code> driver with the native protocol:</p> <pre><code>pip install clickhouse-sqlalchemy\n</code></pre> <p>Use Native Clickhouse Protocol</p> <p>The HTTP protocol has limited reflection support. Always use the native protocol (<code>clickhouse+native://</code>) for full SQLAlchemy/Alembic compatibility:</p> <pre><code>connection_string = \"clickhouse+native://user:pass@localhost:9000/default\"\n</code></pre> <p>The <code>ClickHouseMetadataStore.sqlalchemy_url</code> property is tweaked to return the native connection string variant.</p> Alternative: ClickHouse Connect <p>Alternatively, use the official <code>clickhouse-connect</code> driver.</p> <p>Alembic Integration</p> <p>See Alembic setup guide for additional instructions on how to use Alembic with Metaxy.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#performance-optimization","title":"Performance Optimization","text":"<p>Table Design</p> <p>For optimal query performance, create your ClickHouse tables with:</p> <ul> <li>Partitioning: Partition your tables!</li> <li>Ordering: It's probably a good idea to use <code>(metaxy_feature_version, &lt;id_columns&gt;, metaxy_updated_at)</code></li> </ul>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.metadata_store.clickhouse","title":"metaxy.metadata_store.clickhouse","text":"<p>This module implements <code>IbisMetadataStore</code> for ClickHouse.</p> <p>It takes care of some ClickHouse-specific logic such as <code>nw.Struct</code> type conversion against ClickHouse types such as <code>Map(K,V)</code>.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","title":"metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","text":"<pre><code>ClickHouseMetadataStore(\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_cast_struct_for_map: bool = True,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>ClickHouse metadata store using Ibis backend.</p> Connection Parameters <pre><code>store = ClickHouseMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\n        \"host\": \"localhost\",\n        \"port\": 8443,\n        \"database\": \"default\",\n        \"user\": \"default\",\n        \"password\": \"\",\n    },\n    hash_algorithm=HashAlgorithm.XXHASH64,\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ClickHouse connection string.</p> <p>Format: <code>clickhouse://[user[:password]@]host[:port]/database[?param=value]</code></p> <p>Example:     <pre><code>\"clickhouse://localhost:8443/default\"\n</code></pre></p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Alternative to connection_string, specify params as dict:</p> <ul> <li> <p>host: Server host</p> </li> <li> <p>port: Server port (default: <code>8443</code>)</p> </li> <li> <p>database: Database name</p> </li> <li> <p>user: Username</p> </li> <li> <p>password: Password</p> </li> <li> <p>secure: Use secure connection (default: <code>False</code>)</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>auto_cast_struct_for_map</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to auto-convert DataFrame user-defined Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-clickhouse not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor connection_params provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/clickhouse.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    auto_cast_struct_for_map: bool = True,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [ClickHouse](https://clickhouse.com/) metadata store.\n\n    Args:\n        connection_string: ClickHouse connection string.\n\n            Format: `clickhouse://[user[:password]@]host[:port]/database[?param=value]`\n\n            Example:\n                ```\n                \"clickhouse://localhost:8443/default\"\n                ```\n\n        connection_params: Alternative to connection_string, specify params as dict:\n\n            - host: Server host\n\n            - port: Server port (default: `8443`)\n\n            - database: Database name\n\n            - user: Username\n\n            - password: Password\n\n            - secure: Use secure connection (default: `False`)\n\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        auto_cast_struct_for_map: whether to auto-convert DataFrame user-defined Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Raises:\n        ImportError: If ibis-clickhouse not installed\n        ValueError: If neither connection_string nor connection_params provided\n    \"\"\"\n    if connection_string is None and connection_params is None:\n        raise ValueError(\n            \"Must provide either connection_string or connection_params. \"\n            \"Example: connection_string='clickhouse://localhost:8443/default'\"\n        )\n\n    # Cache for ClickHouse table schemas (cleared on close)\n    self._ch_schema_cache: dict[str, IbisSchema] = {}\n\n    # Store auto_cast_struct_for_map setting\n    self.auto_cast_struct_for_map = auto_cast_struct_for_map\n\n    # Initialize Ibis store with ClickHouse backend\n    super().__init__(\n        connection_string=connection_string,\n        backend=\"clickhouse\" if connection_string is None else None,\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#configuration","title":"Configuration","text":""},{"location":"integrations/metadata-stores/databases/clickhouse/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#auto_cast_struct_for_map","title":"<code>auto_cast_struct_for_map</code>","text":"<p>Auto-convert DataFrame Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.</p> <p>Type: <code>bool</code> | Default: <code>True</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nauto_cast_struct_for_map = true\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nauto_cast_struct_for_map = true\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CAST_STRUCT_FOR_MAP=true\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/","title":"DuckDB","text":"<p>DuckDB is an embedded analytical database. To use Metaxy with DuckDB, configure <code>DuckDBMetadataStore</code>. This runs versioning computations natively in DuckDB.</p> <p>Warning</p> <p>File-based DuckDB does not (currently) support concurrent writes. If multiple writers are a requirement (e.g. with distributed data processing), consider either using DuckLake with a <code>PostgreSQL</code> catalog, or refer to DuckDB's documentation to learn about implementing application-side work-arounds.</p> <p>Tip</p> <p>The Delta Lake metadata store might be a better alternative for concurrent writes (with it's Polars-based versioning engine being as fast as DuckDB).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[duckdb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#extensions","title":"Extensions","text":"<p>DuckDB extensions can be loaded automatically:</p> <pre><code>from metaxy.metadata_store.duckdb import DuckDBMetadataStore\n\nstore = DuckDBMetadataStore(\":memory:\", extensions=[\"hashfuncs\"])\n</code></pre> <p><code>hashfuncs</code> is typically used by the versioning engine.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.metadata_store.duckdb","title":"metaxy.metadata_store.duckdb","text":"<p>DuckDB metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore","text":"<pre><code>DuckDBMetadataStore(\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[ExtensionInput] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    ducklake: DuckLakeConfigInput | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>DuckDB metadata store using Ibis backend.</p> Local File <pre><code>store = DuckDBMetadataStore(\"metadata.db\")\n</code></pre> In-memory database <pre><code># In-memory database\nstore = DuckDBMetadataStore(\":memory:\")\n</code></pre> MotherDuck <pre><code># MotherDuck\nstore = DuckDBMetadataStore(\"md:my_database\")\n</code></pre> With extensions <pre><code># With extensions\nstore = DuckDBMetadataStore(\"metadata.db\", hash_algorithm=HashAlgorithm.XXHASH64, extensions=[\"hashfuncs\"])\n</code></pre> <p>Parameters:</p> <ul> <li> <code>database</code>               (<code>str | Path</code>)           \u2013            <p>Database connection string or path. - File path: <code>\"metadata.db\"</code> or <code>Path(\"metadata.db\")</code></p> <ul> <li> <p>In-memory: <code>\":memory:\"</code></p> </li> <li> <p>MotherDuck: <code>\"md:my_database\"</code> or <code>\"md:my_database?motherduck_token=...\"</code></p> </li> <li> <p>S3: <code>\"s3://bucket/path/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>HTTPS: <code>\"https://example.com/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>Any valid DuckDB connection string</p> </li> </ul> </li> <li> <code>config</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})</p> </li> <li> <code>extensions</code>               (<code>Sequence[ExtensionInput] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of DuckDB extensions to install and load on open. Supports strings (community repo), mapping-like objects with <code>name</code>/<code>repository</code> keys, or metaxy.metadata_store.duckdb.ExtensionSpec instances.</p> </li> </ul> Optional DuckLake attachment configuration. Provide either a <p>mapping with 'metadata_backend' and 'storage_backend' entries or a DuckLakeAttachmentConfig instance. When supplied, the DuckDB connection is configured to ATTACH the DuckLake catalog after open(). fallback_stores: Ordered list of read-only fallback stores.</p> <p>**kwargs: Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> Warning <p>Parent directories are NOT created automatically. Ensure paths exist before initializing the store.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def __init__(\n    self,\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[ExtensionInput] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    ducklake: DuckLakeConfigInput | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize [DuckDB](https://duckdb.org/) metadata store.\n\n    Args:\n        database: Database connection string or path.\n            - File path: `\"metadata.db\"` or `Path(\"metadata.db\")`\n\n            - In-memory: `\":memory:\"`\n\n            - MotherDuck: `\"md:my_database\"` or `\"md:my_database?motherduck_token=...\"`\n\n            - S3: `\"s3://bucket/path/database.duckdb\"` (read-only via ATTACH)\n\n            - HTTPS: `\"https://example.com/database.duckdb\"` (read-only via ATTACH)\n\n            - Any valid DuckDB connection string\n\n        config: Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})\n        extensions: List of DuckDB extensions to install and load on open.\n            Supports strings (community repo), mapping-like objects with\n            ``name``/``repository`` keys, or [metaxy.metadata_store.duckdb.ExtensionSpec][] instances.\n\n    ducklake: Optional DuckLake attachment configuration. Provide either a\n        mapping with 'metadata_backend' and 'storage_backend' entries or a\n        DuckLakeAttachmentConfig instance. When supplied, the DuckDB\n        connection is configured to ATTACH the DuckLake catalog after open().\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Warning:\n        Parent directories are NOT created automatically. Ensure paths exist\n        before initializing the store.\n    \"\"\"\n    database_str = str(database)\n\n    # Build connection params for Ibis DuckDB backend\n    # Ibis DuckDB backend accepts config params directly (not nested under 'config')\n    connection_params = {\"database\": database_str}\n    if config:\n        connection_params.update(config)\n\n    self.database = database_str\n    base_extensions: list[NormalisedExtension] = _normalise_extensions(extensions or [])\n\n    self._ducklake_config: DuckLakeAttachmentConfig | None = None\n    self._ducklake_attachment: DuckLakeAttachmentManager | None = None\n    if ducklake is not None:\n        attachment_config, manager = build_ducklake_attachment(ducklake)\n        ensure_extensions_with_plugins(base_extensions, attachment_config.plugins)\n        self._ducklake_config = attachment_config\n        self._ducklake_attachment = manager\n\n    self.extensions = base_extensions\n\n    # Auto-add hashfuncs extension if not present (needed for default XXHASH64)\n    # But we'll fall back to MD5 if hashfuncs is not available\n    extension_names: list[str] = []\n    for ext in self.extensions:\n        if isinstance(ext, str):\n            extension_names.append(ext)\n        elif isinstance(ext, ExtensionSpec):\n            extension_names.append(ext.name)\n        else:\n            # After _normalise_extensions, this should not happen\n            # But keep defensive check for type safety\n            raise TypeError(f\"Extension must be str or ExtensionSpec after normalization; got {type(ext)}\")\n    if \"hashfuncs\" not in extension_names:\n        self.extensions.append(\"hashfuncs\")\n\n    # Initialize Ibis store with DuckDB backend\n    super().__init__(\n        backend=\"duckdb\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.metadata_store.duckdb.ExtensionSpec","title":"metaxy.metadata_store.duckdb.ExtensionSpec  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB extension specification accepted by DuckDBMetadataStore.</p> <p>Supports additional keys for forward compatibility.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\\n\\nSupports additional keys for forward compatibility.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"repository\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Repository\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"ExtensionSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>repository</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.metadata_store.duckdb.DuckLakeConfigInput","title":"metaxy.metadata_store.duckdb.DuckLakeConfigInput  <code>module-attribute</code>","text":"<pre><code>DuckLakeConfigInput = (\n    DuckLakeAttachmentConfig | Mapping[str, Any]\n)\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig","title":"metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration payload used to attach DuckLake to a DuckDB connection.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"Configuration payload used to attach DuckLake to a DuckDB connection.\",\n  \"properties\": {\n    \"metadata_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Metadata Backend\",\n      \"type\": \"object\"\n    },\n    \"storage_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Storage Backend\",\n      \"type\": \"object\"\n    },\n    \"alias\": {\n      \"default\": \"ducklake\",\n      \"title\": \"Alias\",\n      \"type\": \"string\"\n    },\n    \"plugins\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Plugins\",\n      \"type\": \"array\"\n    },\n    \"attach_options\": {\n      \"additionalProperties\": true,\n      \"title\": \"Attach Options\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"metadata_backend\",\n    \"storage_backend\"\n  ],\n  \"title\": \"DuckLakeAttachmentConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metadata_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>storage_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>alias</code>                 (<code>str</code>)             </li> <li> <code>plugins</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>attach_options</code>                 (<code>dict[str, Any]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_coerce_backends</code>                 \u2192                   <code>metadata_backend</code>,                   <code>storage_backend</code> </li> <li> <code>_coerce_alias</code>                 \u2192                   <code>alias</code> </li> <li> <code>_coerce_plugins</code>                 \u2192                   <code>plugins</code> </li> <li> <code>_coerce_attach_options</code>                 \u2192                   <code>attach_options</code> </li> </ul>"},{"location":"integrations/metadata-stores/databases/duckdb/#configuration","title":"Configuration","text":""},{"location":"integrations/metadata-stores/databases/duckdb/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#database","title":"<code>database</code>","text":"<p>Database path (:memory:, file path, or md:database).</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# database = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# database = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATABASE=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#config","title":"<code>config</code>","text":"<p>DuckDB configuration settings (e.g., {'threads': '4'}).</p> <p>Type: <code>dict[str, str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# config = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# config = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONFIG=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#extensions_1","title":"<code>extensions</code>","text":"<p>DuckDB extensions to install and load on open.</p> <p>Type: <code>collections.abc.Sequence[str | metaxy.metadata_store.duckdb.ExtensionSpec | collections.abc.Mapping[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# extensions = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# extensions = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__EXTENSIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#ducklake","title":"<code>ducklake</code>","text":"<p>DuckLake attachment configuration.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metadata_backend","title":"<code>metadata_backend</code>","text":"<p>Type: <code>metaxy.metadata_store._ducklake_support.SupportsDuckLakeParts | dict[str, Any</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# metadata_backend = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# metadata_backend = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__METADATA_BACKEND=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#storage_backend","title":"<code>storage_backend</code>","text":"<p>Type: <code>metaxy.metadata_store._ducklake_support.SupportsDuckLakeParts | dict[str, Any</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# storage_backend = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# storage_backend = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE_BACKEND=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#alias","title":"<code>alias</code>","text":"<p>Type: <code>str</code> | Default: <code>\"ducklake\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ALIAS=ducklake\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#plugins","title":"<code>plugins</code>","text":"<p>Type: <code>tuple[str, ...]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# plugins = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# plugins = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__PLUGINS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#attach_options","title":"<code>attach_options</code>","text":"<p>Type: <code>dict[str, Any]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# attach_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# attach_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ATTACH_OPTIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/","title":"Ibis Integration","text":"<p>Metaxy uses Ibis as a portable dataframe abstraction for SQL-based metadata stores. The <code>IbisMetadataStore</code> is the base class for all SQL-backed stores.</p>"},{"location":"integrations/metadata-stores/databases/ibis/#available-backends","title":"Available Backends","text":"<p>The following metadata stores are built on Ibis:</p> <ul> <li>DuckDB</li> <li>ClickHouse</li> <li>BigQuery</li> </ul>"},{"location":"integrations/metadata-stores/databases/ibis/#metaxy.metadata_store.ibis","title":"metaxy.metadata_store.ibis","text":"<p>Ibis-based metadata store for SQL databases.</p> <p>Supports any SQL database that Ibis supports: - DuckDB, PostgreSQL, MySQL (local/embedded) - ClickHouse, Snowflake, BigQuery (cloud analytical) - And 20+ other backends</p>"},{"location":"integrations/metadata-stores/databases/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore","title":"metaxy.metadata_store.ibis.IbisMetadataStore","text":"<pre><code>IbisMetadataStore(\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code>, <code>ABC</code></p> <p>Generic SQL metadata store using Ibis.</p> <p>Supports any Ibis backend that supports struct types, such as: DuckDB, PostgreSQL, ClickHouse, and others.</p> Warning <p>Backends without native struct support (e.g., SQLite) are NOT supported.</p> <p>Storage layout: - Each feature gets its own table: {feature}__{key} - System tables: metaxy__system__feature_versions, metaxy__system__migrations - Uses Ibis for cross-database compatibility</p> <p>Note: Uses MD5 hash by default for cross-database compatibility. DuckDBMetadataStore overrides this with dynamic algorithm detection. For other backends, override the calculator instance variable with backend-specific implementations.</p> Example <pre><code># ClickHouse\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# PostgreSQL\nstore = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\n\n# DuckDB (use DuckDBMetadataStore instead for better hash support)\nstore = IbisMetadataStore(\"duckdb:///metadata.db\")\n\nwith store:\n    store.write(MyFeature, df)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use. - \"auto\": Prefer the store's native engine, fall back to Polars if needed - \"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible - \"polars\": Always use the Polars engine</p> </li> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis connection string (e.g., \"clickhouse://host:9000/db\") If provided, backend and connection_params are ignored.</p> </li> <li> <code>backend</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\") Used with connection_params for more control.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Backend-specific connection parameters e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}</p> </li> <li> <code>table_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional prefix applied to all feature and system table names. Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL identifier when combined with the generated table name.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor backend is provided</p> </li> <li> <code>ImportError</code>             \u2013            <p>If Ibis or required backend driver not installed</p> </li> </ul> Example <pre><code># Using connection string\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# Using backend + params\nstore = IbisMetadataStore(backend=\"clickhouse\", connection_params={\"host\": \"localhost\", \"port\": 9000})\n</code></pre> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def __init__(\n    self,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize Ibis metadata store.\n\n    Args:\n        versioning_engine: Which versioning engine to use.\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n            - \"polars\": Always use the Polars engine\n        connection_string: Ibis connection string (e.g., \"clickhouse://host:9000/db\")\n            If provided, backend and connection_params are ignored.\n        backend: Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\")\n            Used with connection_params for more control.\n        connection_params: Backend-specific connection parameters\n            e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}\n        table_prefix: Optional prefix applied to all feature and system table names.\n            Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL\n            identifier when combined with the generated table name.\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n\n    Raises:\n        ValueError: If neither connection_string nor backend is provided\n        ImportError: If Ibis or required backend driver not installed\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Using connection string\n        store = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n        # Using backend + params\n        store = IbisMetadataStore(backend=\"clickhouse\", connection_params={\"host\": \"localhost\", \"port\": 9000})\n        ```\n    \"\"\"\n    from ibis.backends.sql import SQLBackend\n\n    self.connection_string = connection_string\n    self.backend = backend\n    self.connection_params = connection_params or {}\n    self._conn: SQLBackend | None = None\n    self._table_prefix = table_prefix or \"\"\n\n    super().__init__(\n        **kwargs,\n        versioning_engine=versioning_engine,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/","title":"LanceDB","text":"<p>LanceDB is an vector database built on the Lance columnar format. To use Metaxy with LanceDB, configure <code>LanceDBMetadataStore</code>. It uses the in-memory Polars engine for versioning computations. LanceDB handles schema evolution, transactions, and compaction automatically.</p> <p>It runs embedded (local directory) or against external storage (object stores, HTTP endpoints, LanceDB Cloud), so you can use the same store type for local development and cloud workloads.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#installation","title":"Installation","text":"<p>The backend relies on <code>lancedb</code>, which is shipped with Metaxy's <code>lancedb</code> extras.</p> <pre><code>pip install 'metaxy[lancedb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-targets","title":"Storage Targets","text":"<p>Point <code>uri</code> at any supported URI (<code>s3://</code>, <code>gs://</code>, <code>az://</code>, <code>db://</code>, ...) and forward credentials with the platform's native mechanism (environment variables, IAM roles, workload identity, etc.). LanceDB supports local filesystem, S3, GCS, Azure, LanceDB Cloud, and remote HTTP/HTTPS endpoints.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-layout","title":"Storage Layout","text":"<p>All tables are stored within a single LanceDB database at the configured URI location. Each feature gets its own Lance table.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.metadata_store.lancedb","title":"metaxy.metadata_store.lancedb","text":"<p>LanceDB metadata store implementation.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.metadata_store.lancedb.LanceDBMetadataStore","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore","text":"<pre><code>LanceDBMetadataStore(\n    uri: str | Path,\n    *,\n    fallback_stores: list[MetadataStore] | None = None,\n    connect_kwargs: dict[str, Any] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>LanceDB metadata store for vector and structured data.</p> <p>LanceDB is a columnar database optimized for vector search and multimodal data. Each feature is stored in its own Lance table within the database directory. Uses Polars components for data processing (no native SQL execution).</p> <p>Storage layout:</p> <ul> <li> <p>Each feature gets its own table: <code>{namespace}__{feature_name}</code></p> </li> <li> <p>Tables are stored as Lance format in the directory specified by the URI</p> </li> <li> <p>LanceDB handles schema evolution, transactions, and compaction automatically</p> </li> </ul> Local Directory <pre><code>from pathlib import Path\nfrom metaxy.metadata_store.lancedb import LanceDBMetadataStore\n\n# Local filesystem\nstore = LanceDBMetadataStore(Path(\"/path/to/featuregraph\"))\n</code></pre> Object Storage (S3, GCS, Azure) <pre><code># object store (requires credentials)\nstore = LanceDBMetadataStore(\"s3:///path/to/featuregraph\")\n</code></pre> LanceDB Cloud <pre><code>import os\n\n# Option 1: Environment variable\nos.environ[\"LANCEDB_API_KEY\"] = \"your-api-key\"\nstore = LanceDBMetadataStore(\"db://my-database\")\n\n# Option 2: Explicit credentials\nstore = LanceDBMetadataStore(\n    \"db://my-database\", connect_kwargs={\"api_key\": \"your-api-key\", \"region\": \"us-east-1\"}\n)\n</code></pre> <p>The database directory is created automatically if it doesn't exist (local paths only). Tables are created on-demand when features are first written.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str | Path</code>)           \u2013            <p>Directory path or URI for LanceDB tables. Supports:</p> <ul> <li> <p>Local path: <code>\"./metadata\"</code> or <code>Path(\"/data/metaxy/lancedb\")</code></p> </li> <li> <p>Object stores: <code>s3://</code>, <code>gs://</code>, <code>az://</code> (requires cloud credentials)</p> </li> <li> <p>LanceDB Cloud: <code>\"db://database-name\"</code> (requires API key)</p> </li> <li> <p>Remote HTTP/HTTPS: Any URI supported by LanceDB</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. When reading features not found in this store, Metaxy searches fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.</p> </li> <li> <code>connect_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed directly to lancedb.connect(). Useful for LanceDB Cloud credentials (api_key, region) when you cannot rely on environment variables.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.base.MetadataStore (e.g., hash_algorithm, hash_truncation_length, prefer_native)</p> </li> </ul> Note <p>Unlike SQL stores, LanceDB doesn't require explicit table creation. Tables are created automatically when writing metadata.</p> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>def __init__(\n    self,\n    uri: str | Path,\n    *,\n    fallback_stores: list[MetadataStore] | None = None,\n    connect_kwargs: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [LanceDB](https://lancedb.com/docs/) metadata store.\n\n    The database directory is created automatically if it doesn't exist (local paths only).\n    Tables are created on-demand when features are first written.\n\n    Args:\n        uri: Directory path or URI for LanceDB tables. Supports:\n\n            - **Local path**: `\"./metadata\"` or `Path(\"/data/metaxy/lancedb\")`\n\n            - **Object stores**: `s3://`, `gs://`, `az://` (requires cloud credentials)\n\n            - **LanceDB Cloud**: `\"db://database-name\"` (requires API key)\n\n            - **Remote HTTP/HTTPS**: Any URI supported by LanceDB\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            When reading features not found in this store, Metaxy searches\n            fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.\n        connect_kwargs: Extra keyword arguments passed directly to\n            [lancedb.connect()](https://lancedb.github.io/lancedb/python/python/#lancedb.connect).\n            Useful for LanceDB Cloud credentials (api_key, region) when you cannot\n            rely on environment variables.\n        **kwargs: Passed to [metaxy.metadata_store.base.MetadataStore][]\n            (e.g., hash_algorithm, hash_truncation_length, prefer_native)\n\n    Note:\n        Unlike SQL stores, LanceDB doesn't require explicit table creation.\n        Tables are created automatically when writing metadata.\n    \"\"\"\n    self.uri: str = str(uri)\n    self._conn: Any | None = None\n    self._connect_kwargs = connect_kwargs or {}\n    super().__init__(\n        fallback_stores=fallback_stores,\n        auto_create_tables=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#configuration","title":"Configuration","text":""},{"location":"integrations/metadata-stores/databases/lancedb/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#uri","title":"<code>uri</code>","text":"<p>Directory path or URI for LanceDB tables.</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# uri = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# uri = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__URI=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#connect_kwargs","title":"<code>connect_kwargs</code>","text":"<p>Extra keyword arguments passed to lancedb.connect().</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connect_kwargs = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connect_kwargs = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECT_KWARGS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/","title":"Storage-Only Metadata Stores","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>Recommended</p> <p>DeltaLake is an excellent choice for a storage-only metadata store.</p>"},{"location":"integrations/metadata-stores/storage/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>Delta Lake</li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/","title":"Delta Lake","text":"<p>Delta Lake is an open-source lakehouse storage format with ACID transactions and schema enforcement. To use Metaxy with Delta Lake, configure <code>DeltaMetadataStore</code>. It persists metadata as Delta tables and uses an in-memory Polars engine for versioning computations.</p> <p>It supports the local filesystem and remote object stores.</p> <p>Tip</p> <p>If Polars 1.37 or greater is installed, lazy Polars frames are sinked via <code>LazyFrame.sink_delta</code>, avoiding unnecessary materialization.</p>"},{"location":"integrations/metadata-stores/storage/delta/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[delta]'\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#using-object-stores","title":"Using Object Stores","text":"<p>Point <code>root_path</code> at any supported URI (<code>s3://</code>, <code>abfss://</code>, <code>gs://</code>, ...) and forward credentials with <code>storage_options</code>. The dict is passed verbatim to <code>deltalake</code>.</p> <p>Learn more in the API docs.</p>"},{"location":"integrations/metadata-stores/storage/delta/#storage-layout","title":"Storage Layout","text":"<p>It's possible to control how feature keys map to DeltaLake table locations with the <code>layout</code> parameter:</p> <ul> <li><code>nested</code> (default) places every feature in its own directory: <code>your/feature/key.delta</code></li> <li><code>flat</code> stores all of them in the same directory: <code>your__feature_key.delta</code></li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.metadata_store.delta","title":"metaxy.metadata_store.delta","text":"<p>Delta Lake metadata store implemented with delta-rs.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.metadata_store.delta.DeltaMetadataStore","title":"metaxy.metadata_store.delta.DeltaMetadataStore","text":"<pre><code>DeltaMetadataStore(\n    root_path: str | Path,\n    *,\n    storage_options: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    layout: Literal[\"flat\", \"nested\"] = \"nested\",\n    delta_write_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>Delta Lake metadata store backed by delta-rs.</p> <p>It stores feature metadata in Delta Lake tables located under <code>root_path</code>. It uses the Polars versioning engine for provenance calculations.</p> <p>Tip</p> <p>If Polars 1.37 or greater is installed, lazy Polars frames are sinked via <code>LazyFrame.sink_delta</code>, avoiding unnecessary materialization.</p> <p>Example:</p> <pre><code>```py\nfrom metaxy.metadata_store.delta import DeltaMetadataStore\n\nstore = DeltaMetadataStore(\n    root_path=\"s3://my-bucket/metaxy\",\n    storage_options={\"AWS_REGION\": \"us-west-2\"},\n)\n```\n</code></pre> <p>Parameters:</p> <ul> <li> <code>root_path</code>               (<code>str | Path</code>)           \u2013            <p>Base directory or URI where feature tables are stored. Supports local paths (<code>/path/to/dir</code>), <code>s3://</code> URLs, and other object store URIs.</p> </li> <li> <code>storage_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Storage backend options passed to delta-rs. Example: <code>{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}</code> See https://delta-io.github.io/delta-rs/ for details on supported options.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>layout</code>               (<code>Literal['flat', 'nested']</code>, default:                   <code>'nested'</code> )           \u2013            <p>Directory layout for feature tables. Options:</p> <ul> <li> <p><code>\"nested\"</code>: Feature tables stored in nested directories <code>{part1}/{part2}.delta</code></p> </li> <li> <p><code>\"flat\"</code>: Feature tables stored as <code>{part1}__{part2}.delta</code></p> </li> </ul> </li> <li> <code>delta_write_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional options passed to deltalake.write_deltalake() - see https://delta-io.github.io/delta-rs/upgrade-guides/guide-1.0.0/#write_deltalake-api. Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Forwarded to metaxy.metadata_store.base.MetadataStore.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def __init__(\n    self,\n    root_path: str | Path,\n    *,\n    storage_options: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    layout: Literal[\"flat\", \"nested\"] = \"nested\",\n    delta_write_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize Delta Lake metadata store.\n\n    Args:\n        root_path: Base directory or URI where feature tables are stored.\n            Supports local paths (`/path/to/dir`), `s3://` URLs, and other object store URIs.\n        storage_options: Storage backend options passed to delta-rs.\n            Example: `{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}`\n            See https://delta-io.github.io/delta-rs/ for details on supported options.\n        fallback_stores: Ordered list of read-only fallback stores.\n        layout: Directory layout for feature tables. Options:\n\n            - `\"nested\"`: Feature tables stored in nested directories `{part1}/{part2}.delta`\n\n            - `\"flat\"`: Feature tables stored as `{part1}__{part2}.delta`\n\n        delta_write_options: Additional options passed to deltalake.write_deltalake() - see https://delta-io.github.io/delta-rs/upgrade-guides/guide-1.0.0/#write_deltalake-api.\n            Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}\n        **kwargs: Forwarded to [metaxy.metadata_store.base.MetadataStore][metaxy.metadata_store.base.MetadataStore].\n    \"\"\"\n    self.storage_options = storage_options or {}\n    if layout not in (\"flat\", \"nested\"):\n        raise ValueError(f\"Invalid layout: {layout}. Must be 'flat' or 'nested'.\")\n    self.layout = layout\n    self.delta_write_options = delta_write_options or {}\n\n    root_str = str(root_path)\n    self._is_remote = not is_local_path(root_str)\n\n    if self._is_remote:\n        # Remote path (S3, Azure, GCS, etc.)\n        self._root_uri = root_str.rstrip(\"/\")\n    else:\n        # Local path (including file:// and local:// URLs)\n        if root_str.startswith(\"file://\"):\n            # Strip file:// prefix\n            root_str = root_str[7:]\n        elif root_str.startswith(\"local://\"):\n            # Strip local:// prefix\n            root_str = root_str[8:]\n        local_path = Path(root_str).expanduser().resolve()\n        self._root_uri = str(local_path)\n\n    super().__init__(\n        fallback_stores=fallback_stores,\n        versioning_engine=\"polars\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#configuration","title":"Configuration","text":""},{"location":"integrations/metadata-stores/storage/delta/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#root_path","title":"<code>root_path</code>","text":"<p>Base directory or URI where feature tables are stored.</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# root_path = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# root_path = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__ROOT_PATH=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#storage_options","title":"<code>storage_options</code>","text":"<p>Storage backend options passed to delta-rs.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# storage_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# storage_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__STORAGE_OPTIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#layout","title":"<code>layout</code>","text":"<p>Directory layout for feature tables ('nested' or 'flat').</p> <p>Type: <code>Literal['flat', 'nested']</code> | Default: <code>\"nested\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LAYOUT=nested\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#delta_write_options","title":"<code>delta_write_options</code>","text":"<p>Options passed to deltalake.write_deltalake().</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# delta_write_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# delta_write_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DELTA_WRITE_OPTIONS=...\n</code></pre>"},{"location":"integrations/orchestration/","title":"Metaxy + Orchestrators","text":"<p>Metaxy can manage feature metadata, but it's not opinionated about how and where to materialize the data itself.</p> <p>This task is typically handled by an orchestrator.</p>"},{"location":"integrations/orchestration/#dagster","title":"Dagster","text":"<p>Metaxy has been built with the Dagster integration in mind from the very beginning. Dagster is an excellent choice for managing Metaxy jobs.</p>"},{"location":"integrations/orchestration/dagster/","title":"Metaxy + Dagster","text":"<p>Metaxy's dependency system has been originally inspired by Dagster.</p> <p>Because of this, Metaxy code can be naturally composed with Dagster code, Metaxy concepts map directly into Dagster concepts, and the provided <code>@metaxify</code> decorator makes this process effortless.</p> <p>The only step that has to be taken in order to inject Metaxy into Dagster assets is to associate the Dagster asset with the Metaxy feature.</p> <p>Unleash the full power of <code>@metaxify</code> on Dagster!</p> <p>Example</p> <p>Set the well-known <code>\"metaxy/feature\"</code> key (1):</p> <ol> <li> point it to... the Metaxy feature key!</li> </ol> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\nclass MyMetaxyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/metaxy/feature\", id_columns=[\"id\"])):\n    id: str\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/metaxy/feature\"})\ndef my_asset():\n    pass\n</code></pre> <p><code>@metaxify</code> will take care of injecting information (such as asset dependencies or metadata) from the Metaxy feature to the Dagster asset. Learn more about <code>@metaxify</code> (with example screenshots) here.</p>"},{"location":"integrations/orchestration/dagster/#whats-in-the-box","title":"What's in the box","text":"<p>This integration provides:</p> <ul> <li> <p><code>metaxify</code> - a decorator that enriches Dagster asset definitions with Metaxy information such as upstream dependencies, description, metadata, code version, table schema, column lineage, and so on. More info and some screenshots here.</p> </li> <li> <p><code>MetaxyIOManager</code> - an IO manager that reads and writes Dagster assets that are Metaxy features and logs useful runtime metadata.</p> </li> <li> <p><code>MetaxyStoreFromConfigResource</code> - a resource that provides access to <code>MetadataStore</code></p> </li> <li> <p><code>generate_materialize_results</code> / <code>generate_observe_results</code> - generators for yielding <code>dagster.MaterializeResult</code> or <code>dagster.ObserveResult</code> events from Dagster assets (and multi-assets), with automatic topological ordering, partition filtering, logging row counts, and setting Dagster data versions.</p> </li> <li> <p><code>observable_metaxy_asset</code> - a decorator that creates observable source assets for monitoring external Metaxy features.</p> </li> </ul>"},{"location":"integrations/orchestration/dagster/#quick-start","title":"Quick Start","text":""},{"location":"integrations/orchestration/dagster/#1-define-metaxy-features","title":"1. Define Metaxy Features","text":"defs.py<pre><code># Upstream feature\nupstream_spec = mx.FeatureSpec(\n    key=\"audio/embeddings\",\n    id_columns=[\"audio_id\"],\n    fields=[\"embedding\"],\n)\n\n\nclass AudioEmbeddings(mx.BaseFeature, spec=upstream_spec):\n    audio_id: str\n\n\n# Downstream feature that depends on upstream\ndownstream_spec = mx.FeatureSpec(\n    key=\"audio/clusters\",\n    id_columns=[\"audio_id\"],\n    deps=[AudioEmbeddings],\n)\n\n\nclass AudioClusters(mx.BaseFeature, spec=downstream_spec):\n    audio_id: str\n    mean: float\n    std: float\n</code></pre>"},{"location":"integrations/orchestration/dagster/#2-define-dagster-assets","title":"2. Define Dagster Assets","text":"<p>Root Asset</p> <p>Let's define an asset that doesn't have any upstream Metaxy features.</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/embeddings\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_embeddings(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    # somehow, acquire root source data\n    samples = pl.DataFrame(\n        {\n            \"audio_id\": [\"a1\", \"a2\", \"a3\"],\n            \"metaxy_provenance_by_field\": [\n                {\"embedding\": \"hash1\"},\n                {\"embedding\": \"hash2\"},\n                {\"embedding\": \"hash3\"},\n            ],\n        }\n    )\n\n    # resolve the increment with Metaxy\n\n    with store:\n        increment = store.resolve_update(\"audio/embeddings\", samples=samples)  # noqa: F841\n\n    # Compute embeddings...\n\n    df = ...  # at this point this dataframe should have `mean` and `std` columns set\n\n    # either write embeddings metadata via Metaxy\n    # or return a dataframe to write it via MetaxyIOManager\n\n    return df\n</code></pre> <p>Downstream Asset</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/clusters\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_clusters(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    with store:\n        # Get IDs that need recomputation\n        update = store.resolve_update(AudioClusters)  # noqa: F841\n    ...\n</code></pre> <p>Non-Metaxy Downstream Asset</p> <pre><code>@dg.asset(\n    ins={\n        \"clusters\": dg.AssetIn(\n            key=[\"audio\", \"clusters\"],\n        )\n    },\n)\ndef cluster_report(clusters: nw.LazyFrame):\n    # clusters is a narwhals LazyFrame loaded via MetaxyIOManager\n    df = clusters.collect().to_polars()\n    # Generate a report...\n    return {\"total_clusters\": df.select(\"cluster_id\").n_unique()}\n</code></pre>"},{"location":"integrations/orchestration/dagster/#3-create-dagster-definitions","title":"3. Create Dagster Definitions","text":"defs.py<pre><code>store = mxd.MetaxyStoreFromConfigResource(name=\"dev\")\nmetaxy_io_manager = mxd.MetaxyIOManager(store=store)\n\n\n@dg.definitions\ndef definitions():\n    mx.init_metaxy()  # (1)!\n\n    return dg.Definitions(\n        assets=[  # ty: ignore[invalid-argument-type]\n            audio_embeddings,\n            audio_clusters,\n            cluster_report,\n        ],\n        resources={\n            \"store\": store,\n            \"metaxy_io_manager\": metaxy_io_manager,\n        },\n    )\n</code></pre> <ol> <li>This loads Metaxy configuration and feature definitions</li> </ol>"},{"location":"integrations/orchestration/dagster/#4-start-dagster","title":"4. Start Dagster","text":"<pre><code>dg dev -f defs.py\n</code></pre> <p>Materialize your assets and let Metaxy take care of state and versioning!</p>"},{"location":"integrations/orchestration/dagster/#observable-source-assets","title":"Observable Source Assets","text":"<p>Use <code>observable_metaxy_asset</code> to create observable source assets that monitor external Metaxy features. This is useful when Metaxy features are populated outside of Dagster (e.g., by external pipelines) and you want Dagster to track their data versions.</p> <p>Basic Observable Asset</p> <pre><code>import dagster as dg\nimport narwhals as nw\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\nclass ExternalFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"external/feature\", id_columns=[\"id\"])):\n    id: str\n\n@mxd.observable_metaxy_asset(key=\"dagster/asset/key\", feature=\"external/feature\")\ndef external_data(context, store: dg.ResourceParam[mx.MetadataStore], lazy_df: nw.LazyFrame):\n    # build a custom metadata dict\n    metadata = {\"row_count\": lazy_df.collect().shape[0]}\n    return metadata\n</code></pre> <p>The observation automatically tracks:</p> <ul> <li>Data version: Uses <code>mean(metaxy_created_at)</code> to detect both additions and deletions</li> <li>Row count: Logged as <code>dagster/row_count</code> metadata</li> </ul>"},{"location":"integrations/orchestration/dagster/#deletion-workflows-with-dagster-ops","title":"Deletion workflows with Dagster ops","text":"<p>The Dagster integration provides a <code>delete</code> op:</p> <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n# Define a job with the delete op\n@dg.job(resource_defs={\"metaxy_store\": mxd.MetaxyStoreFromConfigResource(name=\"default\")})\ndef cleanup_job():\n    mxd.delete()\n</code></pre> <p><code>filters</code> is a list of SQL WHERE clause strings (e.g., <code>[\"status = 'inactive'\", \"age &gt; 18\"]</code>) that are parsed into Narwhals expressions. Multiple filters are combined with AND logic. See the filter expressions guide for supported syntax. Set <code>soft=False</code> to physically remove rows.</p>"},{"location":"integrations/orchestration/dagster/#reference","title":"Reference","text":"<ul> <li>API</li> <li>Dagster docs</li> </ul>"},{"location":"integrations/orchestration/dagster/api/","title":"Dagster Integration API","text":"<p>Integration docs</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster","title":"metaxy.ext.dagster","text":""},{"location":"integrations/orchestration/dagster/api/#decorators","title":"Decorators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.metaxify.metaxify","title":"metaxy.ext.dagster.metaxify.metaxify","text":"<pre><code>metaxify(\n    _asset: _T | None = None,\n    *,\n    key: CoercibleToAssetKey | None = None,\n    key_prefix: CoercibleToAssetKeyPrefix | None = None,\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n)\n</code></pre> <p>Inject Metaxy metadata into a Dagster <code>AssetsDefinition</code> or <code>AssetSpec</code>.</p> <p>Affects assets with <code>metaxy/feature</code> metadata set.</p> <p>Learn more about <code>@metaxify</code> and see example screenshots here.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToAssetKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit asset key that overrides all other key resolution logic. Cannot be used with <code>key_prefix</code> or with multi-asset definitions that produce multiple outputs.</p> </li> <li> <code>key_prefix</code>               (<code>CoercibleToAssetKeyPrefix | None</code>, default:                   <code>None</code> )           \u2013            <p>Prefix to prepend to the resolved asset key. Also applied to upstream dependency keys. Cannot be used with <code>key</code>.</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version into the asset's code version. The version is appended in the format <code>metaxy:&lt;version&gt;</code>.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set the asset description from the feature class docstring if the asset doesn't already have a description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject Pydantic field definitions as Dagster column schema. Field types are converted to strings, and field descriptions are used as column descriptions.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column-level lineage into the asset metadata under <code>dagster/column_lineage</code>. Uses Pydantic model fields to track column provenance via <code>FeatureDep.rename</code>, <code>FeatureDep.lineage</code>, and direct pass-through.</p> </li> </ul> <p>Tip</p> <p>Multiple Dagster assets can contribute to the same Metaxy feature. This is a perfectly valid setup since Metaxy writes are append-only. In order to do this, set the following metadata keys:</p> <pre><code>- `\"metaxy/feature\"` pointing to the same Metaxy feature key\n- `\"metaxy/partition\"` should be set to a dictionary mapping column names to values produced by the specific Dagster asset\n</code></pre> <p>Example</p> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(\n    metadata={\"metaxy/feature\": \"my/feature/key\"},\n)\ndef my_asset(store: mx.MetadataStore):\n    with store:\n        increment = store.resolve_update(\"my/feature/key\")\n    ...\n</code></pre> With <code>@multi_asset</code> <p>Multiple Metaxy features can be produced by the same <code>@multi_asset</code>. (1)</p> <ol> <li>Typically, they are produced independently of each other</li> </ol> <pre><code>@mxd.metaxify()\n@dg.multi_asset(\n    specs=[\n        dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"feature/a\"}),\n        dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"feature/b\"}),\n    ]\n)\ndef my_multi_asset(): ...\n</code></pre> With <code>dagster.AssetSpec</code> <pre><code>asset_spec = dg.AssetSpec(\n    key=\"my_asset\",\n    metadata={\"metaxy/feature\": \"my/feature/key\"},\n)\nasset_spec = mxd.metaxify()(asset_spec)\n</code></pre> Multiple Dagster assets contributing to the same Metaxy feature <pre><code>@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"metaxy/partition\": {\"dataset\": \"a\"},\n    },\n)\ndef my_feature_dataset_a(): ...\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"metaxy/partition\": {\"dataset\": \"b\"},\n    },\n)\ndef my_feature_dataset_b(): ...\n</code></pre> Source code in <code>src/metaxy/ext/dagster/metaxify.py</code> <pre><code>def __init__(\n    self,\n    _asset: \"_T | None\" = None,\n    *,\n    key: CoercibleToAssetKey | None = None,\n    key_prefix: CoercibleToAssetKeyPrefix | None = None,\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n) -&gt; None:\n    # Actual initialization happens in __new__, but we set defaults here for type checkers\n    self.key = dg.AssetKey.from_coercible(key) if key is not None else None\n    self.key_prefix = dg.AssetKey.from_coercible(key_prefix) if key_prefix is not None else None\n    self.inject_metaxy_kind = inject_metaxy_kind\n    self.inject_code_version = inject_code_version\n    self.set_description = set_description\n    self.inject_column_schema = inject_column_schema\n    self.inject_column_lineage = inject_column_lineage\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observable.observable_metaxy_asset","title":"metaxy.ext.dagster.observable.observable_metaxy_asset","text":"<pre><code>observable_metaxy_asset(\n    feature: CoercibleToFeatureKey,\n    *,\n    store_resource_key: str = \"store\",\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    **observable_kwargs: Any,\n)\n</code></pre> <p>Decorator to create an observable source asset for a Metaxy feature.</p> <p>The observation reads the feature's metadata from the store, counts rows, and uses <code>mean(metaxy_created_at)</code> as the data version to track changes. Using mean ensures that both additions and deletions are detected.</p> <p>The decorated function receives <code>(context, store, lazy_df)</code> and can return a dict of additional metadata to include in the observation.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to observe.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set description from feature class docstring.</p> </li> <li> <code>**observable_kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to <code>@observable_source_asset</code> (key, group_name, tags, metadata, description, partitions_def, etc.)</p> </li> </ul> Example <pre><code>import metaxy.ext.dagster as mxd\nfrom myproject.features import ExternalFeature\n\n\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data(context, store, lazy_df):\n    pass\n\n\n# With custom metadata - return a dict\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data_with_metrics(context, store, lazy_df):\n    # Run aggregations in the database\n    total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n    return {\"custom/total\": total}\n</code></pre> Note <p><code>observable_source_asset</code> does not support <code>deps</code>. Upstream Metaxy feature dependencies from the feature spec are not propagated to the SourceAsset.</p> Source code in <code>src/metaxy/ext/dagster/observable.py</code> <pre><code>@public\ndef observable_metaxy_asset(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    store_resource_key: str = \"store\",\n    # metaxify kwargs\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    # observable_source_asset kwargs\n    **observable_kwargs: Any,\n):\n    \"\"\"Decorator to create an observable source asset for a Metaxy feature.\n\n    The observation reads the feature's metadata from the store, counts rows,\n    and uses `mean(metaxy_created_at)` as the data version to track changes.\n    Using mean ensures that both additions and deletions are detected.\n\n    The decorated function receives `(context, store, lazy_df)` and can return\n    a dict of additional metadata to include in the observation.\n\n    Args:\n        feature: The Metaxy feature to observe.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        inject_metaxy_kind: Whether to inject `\"metaxy\"` kind into asset kinds.\n        inject_code_version: Whether to inject the Metaxy feature code version.\n        set_description: Whether to set description from feature class docstring.\n        **observable_kwargs: Passed to `@observable_source_asset`\n            (key, group_name, tags, metadata, description, partitions_def, etc.)\n\n    Example:\n        ```python\n        import metaxy.ext.dagster as mxd\n        from myproject.features import ExternalFeature\n\n\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data(context, store, lazy_df):\n            pass\n\n\n        # With custom metadata - return a dict\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data_with_metrics(context, store, lazy_df):\n            # Run aggregations in the database\n            total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n            return {\"custom/total\": total}\n        ```\n\n    Note:\n        `observable_source_asset` does not support `deps`. Upstream Metaxy feature\n        dependencies from the feature spec are not propagated to the SourceAsset.\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n\n    def decorator(fn: Callable[..., Any]) -&gt; dg.SourceAsset:\n        # Build an AssetSpec from kwargs and enrich with metaxify\n        # Merge user metadata with metaxy/feature\n        user_metadata = observable_kwargs.pop(\"metadata\", None) or {}\n        spec = dg.AssetSpec(\n            key=observable_kwargs.pop(\"key\", None) or fn.__name__,  # ty: ignore[unresolved-attribute]\n            group_name=observable_kwargs.pop(\"group_name\", None),\n            tags=observable_kwargs.pop(\"tags\", None),\n            metadata={\n                **user_metadata,\n                DAGSTER_METAXY_FEATURE_METADATA_KEY: feature_key.to_string(),\n            },\n            description=observable_kwargs.pop(\"description\", None),\n        )\n        enriched = metaxify(\n            inject_metaxy_kind=inject_metaxy_kind,\n            inject_code_version=inject_code_version,\n            set_description=set_description,\n        )(spec)\n\n        def _observe(context: dg.AssetExecutionContext) -&gt; dg.ObserveResult:\n            store: mx.MetadataStore = getattr(context.resources, store_resource_key)\n\n            # Check for metaxy/partition metadata to apply filtering\n            metaxy_partition = enriched.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n            filters = build_metaxy_partition_filter(metaxy_partition)\n\n            with store:\n                lazy_df = store.read(feature_key, filters=filters)\n                stats = compute_stats_from_lazy_frame(lazy_df)\n\n                # Call the user's function - it can return additional metadata\n                extra_metadata = fn(context, store, lazy_df) or {}\n\n            metadata: dict[str, Any] = {\"dagster/row_count\": stats.row_count}\n            metadata.update(extra_metadata)\n\n            return dg.ObserveResult(\n                data_version=stats.data_version,\n                metadata=metadata,\n                tags=build_feature_event_tags(feature_key),\n            )\n\n        # Apply observable_source_asset decorator\n        return dg.observable_source_asset(\n            key=enriched.key,\n            description=enriched.description,\n            group_name=enriched.group_name,\n            tags=dict(enriched.tags) if enriched.tags else None,\n            metadata=dict(enriched.metadata) if enriched.metadata else None,\n            required_resource_keys={store_resource_key},\n            **observable_kwargs,\n        )(_observe)\n\n    return decorator\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#io-manager","title":"IO Manager","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager","title":"metaxy.ext.dagster.MetaxyIOManager","text":"<p>               Bases: <code>ConfigurableIOManager</code></p> <p>MetaxyIOManager is a Dagster IOManager that reads and writes data to/from Metaxy's <code>MetadataStore</code>.</p> <p>It automatically attaches Metaxy feature and store metadata to Dagster materialization events and handles partitioned assets.</p> <p>Always set <code>\"metaxy/feature\"</code> Dagster metadata</p> <p>This IOManager is using <code>\"metaxy/feature\"</code> Dagster metadata key to map Dagster assets into Metaxy features. It expects it to be set on the assets being loaded or materialized.</p> Example <pre><code>import dagster as dg\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n    }\n)\ndef my_asset(): ...\n</code></pre> <p>Defining Partitioned Assets</p> <p>To tell Metaxy which column to use when filtering partitioned assets, set <code>\"partition_by\"</code> Dagster metadata key.</p> Example <pre><code>import dagster as dg\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"partition_by\": \"date\",\n    }\n)\ndef my_partitioned_asset(): ...\n</code></pre> <p>This key is commonly used to configure partitioning behavior by various Dagster IO managers.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.load_input","title":"metaxy.ext.dagster.MetaxyIOManager.load_input","text":"<pre><code>load_input(context: InputContext) -&gt; LazyFrame[Any]\n</code></pre> <p>Load feature metadata from <code>MetadataStore</code>.</p> <p>Reads metadata for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. For partitioned assets, filters to the current partition using the column specified in <code>\"partition_by\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InputContext</code>)           \u2013            <p>Dagster input context containing asset metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>A narwhals LazyFrame with the feature metadata.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def load_input(self, context: \"dg.InputContext\") -&gt; nw.LazyFrame[Any]:\n    \"\"\"Load feature metadata from [`MetadataStore`][metaxy.MetadataStore].\n\n    Reads metadata for the feature specified in the asset's `\"metaxy/feature\"` metadata.\n    For partitioned assets, filters to the current partition using the column specified\n    in `\"partition_by\"` metadata.\n\n    Args:\n        context: Dagster input context containing asset metadata.\n\n    Returns:\n        A narwhals LazyFrame with the feature metadata.\n    \"\"\"\n    with self.metadata_store:\n        feature_key = self._feature_key_from_context(context)\n\n        # Build partition filters from context (handles partition_by and metaxy/partition)\n        filters = build_partition_filter_from_input_context(context)\n\n        # Read metadata with store info in a single call (avoids extra network round-trip)\n        lazy_frame, resolved_store = self.metadata_store.read(\n            feature=feature_key,\n            filters=filters,\n            with_store_info=True,\n        )\n\n        # Build input metadata from resolved store\n        # metaxy/store shows where data was actually found (may be a fallback store)\n        resolved_from = resolved_store.get_store_info(feature_key)\n        input_metadata: dict[str, Any] = {\n            \"name\": self.metadata_store.name,\n            \"metaxy/store\": resolved_store.display(),\n            \"resolved_from\": resolved_from,\n        }\n\n        # Map resolved store metadata to dagster standard keys\n        if \"table_name\" in resolved_from:\n            input_metadata[\"dagster/table_name\"] = resolved_from[\"table_name\"]\n        if \"uri\" in resolved_from:\n            input_metadata[\"dagster/uri\"] = dg.MetadataValue.path(resolved_from[\"uri\"])\n\n        # Only add input metadata if we have exactly one partition key\n        # (add_input_metadata internally uses asset_partition_key which fails with multiple)\n        has_single_partition = context.has_asset_partitions and len(list(context.asset_partition_keys)) == 1\n        if input_metadata and (not context.has_asset_partitions or has_single_partition):\n            context.add_input_metadata(input_metadata, description=\"Metadata Store Info\")\n\n        return lazy_frame\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.handle_output","title":"metaxy.ext.dagster.MetaxyIOManager.handle_output","text":"<pre><code>handle_output(\n    context: OutputContext, obj: MetaxyOutput\n) -&gt; None\n</code></pre> <p>Write feature metadata to <code>MetadataStore</code>.</p> <p>Writes the output dataframe to the metadata store for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. Also logs metadata about the feature and store to Dagster's materialization events.</p> <p>If <code>obj</code> is <code>None</code>, only metadata logging is performed (no data is written).</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>OutputContext</code>)           \u2013            <p>Dagster output context containing asset metadata.</p> </li> <li> <code>obj</code>               (<code>MetaxyOutput</code>)           \u2013            <p>A narwhals-compatible dataframe to write, or None to skip writing.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def handle_output(self, context: \"dg.OutputContext\", obj: MetaxyOutput) -&gt; None:\n    \"\"\"Write feature metadata to [`MetadataStore`][metaxy.MetadataStore].\n\n    Writes the output dataframe to the metadata store for the feature specified\n    in the asset's `\"metaxy/feature\"` metadata. Also logs metadata about the\n    feature and store to Dagster's materialization events.\n\n    If `obj` is `None`, only metadata logging is performed (no data is written).\n\n    Args:\n        context: Dagster output context containing asset metadata.\n        obj: A narwhals-compatible dataframe to write, or None to skip writing.\n    \"\"\"\n    assert DAGSTER_METAXY_FEATURE_METADATA_KEY in context.definition_metadata, (\n        f'Missing `\"{DAGSTER_METAXY_FEATURE_METADATA_KEY}\"` key in asset metadata'\n    )\n    key = self._feature_key_from_context(context)\n    feature = mx.get_feature_by_key(key)\n\n    if obj is not None:\n        context.log.debug(f'Writing metadata for Metaxy feature \"{key.to_string()}\" into {self.metadata_store}')\n        with self.metadata_store.open(\"w\"):\n            self.metadata_store.write(feature=feature, df=obj)\n        context.log.debug(f'Metadata written for Metaxy feature \"{key.to_string()}\" into {self.metadata_store}')\n    else:\n        context.log.debug(\n            f'The output corresponds to Metaxy feature \"{key.to_string()}\" stored in {self.metadata_store}'\n        )\n\n    self._log_output_metadata(context)\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-types","title":"Dagster Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.dagster_type.feature_to_dagster_type","title":"metaxy.ext.dagster.dagster_type.feature_to_dagster_type","text":"<pre><code>feature_to_dagster_type(\n    feature: CoercibleToFeatureKey,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n    metadata: Mapping[str, Any] | None = None,\n) -&gt; DagsterType\n</code></pre> <p>Build a Dagster type from a Metaxy feature.</p> <p>Creates a <code>dagster.DagsterType</code> that validates outputs are <code>MetaxyOutput</code> (i.e., narwhals-compatible dataframes or <code>None</code>) and includes metadata derived from the feature's Pydantic model fields.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to create a type for. Can be a feature class, feature key, or string that can be coerced to a feature key.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom name for the DagsterType. Defaults to the feature's table name (e.g., \"project__feature_name\").</p> </li> <li> <code>description</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom description. Defaults to the feature class docstring or a generated description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the column schema as metadata. The schema is derived from Pydantic model fields.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column lineage as metadata. The lineage is derived from feature dependencies.</p> </li> <li> <code>metadata</code>               (<code>Mapping[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom metadata to inject into the DagsterType.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DagsterType</code>           \u2013            <p>A DagsterType configured for the Metaxy feature with appropriate</p> </li> <li> <code>DagsterType</code>           \u2013            <p>type checking and metadata.</p> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>import dagster as dg\nimport polars as pl\nimport metaxy.ext.dagster as mxd\nfrom myproject.features import MyFeature  # Your Metaxy feature class\n\n\n@mxd.metaxify(feature=MyFeature)\n@dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\ndef my_asset():\n    return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n</code></pre> <p>See also</p> <ul> <li><code>metaxify</code>: Decorator for injecting   Metaxy metadata into Dagster assets.</li> <li><code>MetaxyOutput</code>: The type alias for valid   Metaxy outputs.</li> </ul> Source code in <code>src/metaxy/ext/dagster/dagster_type.py</code> <pre><code>@public\ndef feature_to_dagster_type(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n    metadata: Mapping[str, Any] | None = None,\n) -&gt; dg.DagsterType:\n    \"\"\"Build a Dagster type from a Metaxy feature.\n\n    Creates a `dagster.DagsterType` that validates outputs are\n    [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput] (i.e., narwhals-compatible\n    dataframes or `None`) and includes metadata derived from the feature's Pydantic\n    model fields.\n\n    Args:\n        feature: The Metaxy feature to create a type for. Can be a feature class,\n            feature key, or string that can be coerced to a feature key.\n        name: Optional custom name for the DagsterType. Defaults to the feature's\n            table name (e.g., \"project__feature_name\").\n        description: Optional custom description. Defaults to the feature class\n            docstring or a generated description.\n        inject_column_schema: Whether to inject the column schema as metadata.\n            The schema is derived from Pydantic model fields.\n        inject_column_lineage: Whether to inject column lineage as metadata.\n            The lineage is derived from feature dependencies.\n        metadata: Optional custom metadata to inject into the DagsterType.\n\n    Returns:\n        A DagsterType configured for the Metaxy feature with appropriate\n        type checking and metadata.\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        import dagster as dg\n        import polars as pl\n        import metaxy.ext.dagster as mxd\n        from myproject.features import MyFeature  # Your Metaxy feature class\n\n\n        @mxd.metaxify(feature=MyFeature)\n        @dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\n        def my_asset():\n            return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n        ```\n\n    !!! info \"See also\"\n        - [`metaxify`][metaxy.ext.dagster.metaxify.metaxify]: Decorator for injecting\n          Metaxy metadata into Dagster assets.\n        - [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput]: The type alias for valid\n          Metaxy outputs.\n    \"\"\"\n    from metaxy.ext.dagster.io_manager import MetaxyOutput\n\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_def = mx.get_feature_by_key(feature_key)\n\n    # For build_column_schema, prefer the original class if provided\n    # (handles cases where class is defined inside a function and can't be imported)\n    feature_for_schema: mx.FeatureDefinition | type[mx.BaseFeature]\n    if isinstance(feature, type) and issubclass(feature, mx.BaseFeature):\n        feature_for_schema = feature\n    else:\n        feature_for_schema = feature_def\n\n    # Determine name\n    type_name = name or feature_key.table_name\n\n    # Determine description - use schema description if available, else default\n    if description is None:\n        schema_desc = feature_def.feature_schema.get(\"description\")\n        if schema_desc:\n            description = schema_desc\n        else:\n            description = f\"Metaxy feature '{feature_key.to_string()}'.\"\n\n    # Build metadata - start with custom metadata if provided\n    final_metadata: dict[str, Any] = dict(metadata) if metadata else {}\n    final_metadata[DAGSTER_METAXY_INFO_METADATA_KEY] = build_feature_info_metadata(feature_key)\n    # Skip column schema for external features (no Python class to extract schema from)\n    if inject_column_schema and not feature_def.is_external:\n        column_schema = build_column_schema(feature_for_schema)\n        if column_schema is not None:\n            final_metadata[DAGSTER_COLUMN_SCHEMA_METADATA_KEY] = column_schema\n\n    # Skip column lineage for external features (no Python class to extract columns from)\n    if inject_column_lineage and not feature_def.is_external:\n        column_lineage = build_column_lineage(feature_for_schema)\n        if column_lineage is not None:\n            final_metadata[DAGSTER_COLUMN_LINEAGE_METADATA_KEY] = column_lineage\n\n    dagster_type = dg.DagsterType(\n        type_check_fn=_create_type_check_fn(feature_key),\n        name=type_name,\n        description=description,\n        typing_type=MetaxyOutput,\n        metadata=final_metadata,\n    )\n\n    return dagster_type\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-event-generators","title":"Dagster Event Generators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_materialize_results","title":"metaxy.ext.dagster.utils.generate_materialize_results","text":"<pre><code>generate_materialize_results(\n    context: AssetExecutionContext | OpExecutionContext,\n    store: MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[AssetSpec] | None = None,\n) -&gt; Iterator[MaterializeResult[None]]\n</code></pre> <p>Generate <code>dagster.MaterializeResult</code> events for assets in topological order.</p> <p>Yields a <code>MaterializeResult</code> for each asset spec, sorted by their associated Metaxy features in topological order (dependencies before dependents). Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext | OpExecutionContext</code>)           \u2013            <p>The Dagster execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete Dagster asset specs. Required when using <code>OpExecutionContext</code>. Optional for <code>AssetExecutionContext</code> (defaults to <code>context.assets_def.specs</code>).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>MaterializeResult[None]</code>           \u2013            <p>Materialization result for each asset in topological order.</p> </li> </ul> Example <p>Using with <code>@multi_asset</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n\n@metaxify\n@dg.multi_asset(specs=specs)\ndef my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    # ... compute and write data ...\n    yield from generate_materialize_results(context, store)\n</code></pre></p> <p>Using with <code>@op</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n]\n\n\n@dg.op\ndef my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n    # ... compute and write data ...\n    yield from generate_materialize_results(context, store, specs=specs)\n</code></pre></p> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef generate_materialize_results(\n    context: dg.AssetExecutionContext | dg.OpExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.MaterializeResult[None]]:\n    \"\"\"Generate `dagster.MaterializeResult` events for assets in topological order.\n\n    Yields a `MaterializeResult` for each asset spec, sorted by their associated\n    Metaxy features in topological order (dependencies before dependents).\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Concrete Dagster asset specs. Required when using `OpExecutionContext`.\n            Optional for `AssetExecutionContext` (defaults to `context.assets_def.specs`).\n\n    Yields:\n        Materialization result for each asset in topological order.\n\n    Example:\n        Using with `@multi_asset`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n\n        @metaxify\n        @dg.multi_asset(specs=specs)\n        def my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            # ... compute and write data ...\n            yield from generate_materialize_results(context, store)\n        ```\n\n        Using with `@op`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n        ]\n\n\n        @dg.op\n        def my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n            # ... compute and write data ...\n            yield from generate_materialize_results(context, store, specs=specs)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    if specs is None:\n        if not isinstance(context, dg.AssetExecutionContext):\n            raise ValueError(\"specs must be provided when using OpExecutionContext\")\n        specs = context.assets_def.specs\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_col = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_KEY)\n        metaxy_partition = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n\n        with store:  # ty: ignore[invalid-context-manager]\n            try:\n                # Build runtime metadata (handles reading, filtering, and stats internally)\n                metadata, stats = build_runtime_feature_metadata(\n                    key,\n                    store,\n                    context,\n                    partition_col=partition_col,\n                    metaxy_partition=metaxy_partition,\n                )\n            except FeatureNotFoundError:\n                context.log.exception(f\"Feature {key.to_string()} not found in store, skipping materialization result\")\n                continue\n\n            # Get materialized-in-run count if materialization_id is set\n            if store.materialization_id is not None:  # ty: ignore[possibly-missing-attribute]\n                mat_df = store.read(  # ty: ignore[possibly-missing-attribute]\n                    key,\n                    filters=[\n                        nw.col(METAXY_MATERIALIZATION_ID) == store.materialization_id  # ty: ignore[possibly-missing-attribute]\n                    ],\n                )\n                metadata[\"metaxy/materialized_in_run\"] = mat_df.select(nw.len()).collect().item(0, 0)\n\n        yield dg.MaterializeResult(\n            value=None,\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n            tags=build_feature_event_tags(key),\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_observe_results","title":"metaxy.ext.dagster.utils.generate_observe_results","text":"<pre><code>generate_observe_results(\n    context: AssetExecutionContext | OpExecutionContext,\n    store: MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[AssetSpec] | None = None,\n) -&gt; Iterator[ObserveResult]\n</code></pre> <p>Generate <code>dagster.ObserveResult</code> events for assets in topological order.</p> <p>Yields an <code>ObserveResult</code> for each asset spec that has <code>\"metaxy/feature\"</code> metadata key set, sorted by their associated Metaxy features in topological order. Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext | OpExecutionContext</code>)           \u2013            <p>The Dagster execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete Dagster asset specs. Required when using <code>OpExecutionContext</code>. Optional for <code>AssetExecutionContext</code> (defaults to <code>context.assets_def.specs</code>).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ObserveResult</code>           \u2013            <p>Observation result for each asset in topological order.</p> </li> </ul> Example <p>Using with <code>@multi_observable_source_asset</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n\n@metaxify\n@dg.multi_observable_source_asset(specs=specs)\ndef my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    yield from generate_observe_results(context, store)\n</code></pre></p> <p>Using with <code>@op</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n]\n\n\n@dg.op\ndef my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n    yield from generate_observe_results(context, store, specs=specs)\n</code></pre></p> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef generate_observe_results(\n    context: dg.AssetExecutionContext | dg.OpExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.ObserveResult]:\n    \"\"\"Generate `dagster.ObserveResult` events for assets in topological order.\n\n    Yields an `ObserveResult` for each asset spec that has `\"metaxy/feature\"` metadata key set, sorted by their associated\n    Metaxy features in topological order.\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Concrete Dagster asset specs. Required when using `OpExecutionContext`.\n            Optional for `AssetExecutionContext` (defaults to `context.assets_def.specs`).\n\n    Yields:\n        Observation result for each asset in topological order.\n\n    Example:\n        Using with `@multi_observable_source_asset`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n\n        @metaxify\n        @dg.multi_observable_source_asset(specs=specs)\n        def my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            yield from generate_observe_results(context, store)\n        ```\n\n        Using with `@op`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n        ]\n\n\n        @dg.op\n        def my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n            yield from generate_observe_results(context, store, specs=specs)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    if specs is None:\n        if not isinstance(context, dg.AssetExecutionContext):\n            raise ValueError(\"specs must be provided when using OpExecutionContext\")\n        specs = context.assets_def.specs\n\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_col = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_KEY)\n        metaxy_partition = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n\n        with store:  # ty: ignore[invalid-context-manager]\n            try:\n                # Build runtime metadata (handles reading, filtering, and stats internally)\n                # For observers with no metaxy_partition, this reads all data\n                metadata, stats = build_runtime_feature_metadata(\n                    key,\n                    store,\n                    context,\n                    partition_col=partition_col,\n                    metaxy_partition=metaxy_partition,\n                )\n            except FeatureNotFoundError:\n                context.log.exception(f\"Feature {key.to_string()} not found in store, skipping observation result\")\n                continue\n\n        yield dg.ObserveResult(\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n            tags=build_feature_event_tags(key),\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.build_feature_info_metadata","title":"metaxy.ext.dagster.utils.build_feature_info_metadata","text":"<pre><code>build_feature_info_metadata(\n    feature: CoercibleToFeatureKey,\n) -&gt; dict[str, Any]\n</code></pre> <p>Build feature info metadata dict for Dagster assets.</p> <p>Creates a dictionary with information about the Metaxy feature that can be used as Dagster asset metadata under the <code>\"metaxy/feature_info\"</code> key.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature (class, key, or string).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A nested dictionary containing:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>feature</code>: Feature information</li> <li><code>project</code>: The project name</li> <li><code>spec</code>: The full feature spec as a dict (via <code>model_dump()</code>)</li> <li><code>version</code>: The feature version string</li> <li><code>type</code>: The feature class module path</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>metaxy</code>: Metaxy library information</li> <li><code>version</code>: The metaxy library version</li> </ul> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>from metaxy.ext.dagster.utils import build_feature_info_metadata\n\ninfo = build_feature_info_metadata(MyFeature)\n# {\n#     \"feature\": {\n#         \"project\": \"my_project\",\n#         \"spec\": {...},  # Full FeatureSpec model_dump()\n#         \"version\": \"my__feature@abc123\",\n#         \"type\": \"myproject.features\",\n#     },\n#     \"metaxy\": {\n#         \"version\": \"0.1.0\",\n#     },\n# }\n</code></pre> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef build_feature_info_metadata(\n    feature: mx.CoercibleToFeatureKey,\n) -&gt; dict[str, Any]:\n    \"\"\"Build feature info metadata dict for Dagster assets.\n\n    Creates a dictionary with information about the Metaxy feature that can be\n    used as Dagster asset metadata under the `\"metaxy/feature_info\"` key.\n\n    Args:\n        feature: The Metaxy feature (class, key, or string).\n\n    Returns:\n        A nested dictionary containing:\n\n        - `feature`: Feature information\n            - `project`: The project name\n            - `spec`: The full feature spec as a dict (via `model_dump()`)\n            - `version`: The feature version string\n            - `type`: The feature class module path\n        - `metaxy`: Metaxy library information\n            - `version`: The metaxy library version\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        from metaxy.ext.dagster.utils import build_feature_info_metadata\n\n        info = build_feature_info_metadata(MyFeature)\n        # {\n        #     \"feature\": {\n        #         \"project\": \"my_project\",\n        #         \"spec\": {...},  # Full FeatureSpec model_dump()\n        #         \"version\": \"my__feature@abc123\",\n        #         \"type\": \"myproject.features\",\n        #     },\n        #     \"metaxy\": {\n        #         \"version\": \"0.1.0\",\n        #     },\n        # }\n        ```\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_def = mx.get_feature_by_key(feature_key)\n    feature_version = mx.current_graph().get_feature_version(feature_key)\n\n    return {\n        \"feature\": {\n            \"project\": feature_def.project,\n            \"spec\": feature_def.spec.model_dump(mode=\"json\"),\n            \"version\": feature_version,\n            \"type\": feature_def.feature_class_path,\n        },\n        \"metaxy\": {\n            \"version\": mx.__version__,\n            \"plugins\": mx.MetaxyConfig.get().plugins,\n        },\n    }\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#observation-jobs","title":"Observation Jobs","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observation_job.build_metaxy_multi_observation_job","title":"metaxy.ext.dagster.observation_job.build_metaxy_multi_observation_job","text":"<pre><code>build_metaxy_multi_observation_job(\n    name: str,\n    *,\n    asset_selection: AssetSelection | None = None,\n    defs: Definitions | None = None,\n    assets: Sequence[\n        AssetSpec | AssetsDefinition | SourceAsset\n    ]\n    | None = None,\n    store_resource_key: str = \"store\",\n    tags: Mapping[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; JobDefinition\n</code></pre> <p>Build a dynamic Dagster job that observes multiple Metaxy feature assets.</p> <p>Creates a job that dynamically spawns one op per asset, yielding <code>AssetObservation</code> events. Uses Dagster's dynamic orchestration to process multiple assets in parallel.</p> <p>Tip</p> <p>This is a very powerful way to observe all your Metaxy features at once. Use it in combination with a Dagster schedule to run it periodically.</p> <p>Provide either: - <code>asset_selection</code> and <code>defs</code>: Select assets from a   <code>Definitions</code> object</p> <ul> <li><code>assets</code>: Direct list of assets to observe</li> </ul> <p>Note</p> <p>All selected assets must share the same partitioning (if any).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name for the job.</p> </li> <li> <code>asset_selection</code>               (<code>AssetSelection | None</code>, default:                   <code>None</code> )           \u2013            <p>An <code>AssetSelection</code> specifying which assets to observe. Must be used together with <code>defs</code>.</p> </li> <li> <code>defs</code>               (<code>Definitions | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>Definitions</code> object to resolve the selection against. Must be used together with <code>asset_selection</code>.</p> </li> <li> <code>assets</code>               (<code>Sequence[AssetSpec | AssetsDefinition | SourceAsset] | None</code>, default:                   <code>None</code> )           \u2013            <p>Direct sequence of assets to observe. Each item can be an <code>AssetSpec</code>, <code>AssetsDefinition</code>, or <code>SourceAsset</code>. Cannot be used together with <code>asset_selection</code>/<code>defs</code>.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>tags</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional tags to apply to the job.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to the <code>@job</code> decorator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>JobDefinition</code>           \u2013            <p>A Dagster job definition that observes all matching Metaxy assets.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no specs have <code>metaxy/feature</code> metadata, if assets have inconsistent <code>partitions_def</code>, or if invalid argument combinations are provided.</p> </li> </ul> Example <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature_a\"})\ndef feature_a(): ...\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature_b\"})\ndef feature_b(): ...\n\n\n# Option 1: Using asset_selection + defs\nmy_defs = dg.Definitions(assets=[feature_a, feature_b])\nobservation_job = mxd.build_metaxy_multi_observation_job(\n    name=\"observe_my_features\",\n    asset_selection=dg.AssetSelection.kind(\"metaxy\"),\n    defs=my_defs,\n)\n\n# Option 2: Using direct assets list\nobservation_job = mxd.build_metaxy_multi_observation_job(\n    name=\"observe_my_features\",\n    assets=[feature_a, feature_b],\n)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/observation_job.py</code> <pre><code>@public\ndef build_metaxy_multi_observation_job(\n    name: str,\n    *,\n    asset_selection: dg.AssetSelection | None = None,\n    defs: dg.Definitions | None = None,\n    assets: Sequence[dg.AssetSpec | dg.AssetsDefinition | dg.SourceAsset] | None = None,\n    store_resource_key: str = \"store\",\n    tags: Mapping[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; dg.JobDefinition:\n    \"\"\"Build a dynamic Dagster job that observes multiple Metaxy feature assets.\n\n    Creates a job that dynamically spawns one op per asset, yielding\n    [`AssetObservation`](https://docs.dagster.io/api/python-api/ops#dagster.AssetObservation) events.\n    Uses Dagster's dynamic orchestration to process multiple assets in parallel.\n\n    !!! tip\n        This is a very powerful way to observe all your Metaxy features at once.\n        Use it in combination with a [Dagster schedule](https://docs.dagster.io/concepts/schedules)\n        to run it periodically.\n\n    Provide either:\n    - `asset_selection` and `defs`: Select assets from a\n      [`Definitions`](https://docs.dagster.io/api/python-api/definitions#dagster.Definitions) object\n\n    - `assets`: Direct list of assets to observe\n\n    !!! note\n        All selected assets must share the same partitioning (if any).\n\n    Args:\n        name: Name for the job.\n        asset_selection: An `AssetSelection` specifying which assets to observe.\n            Must be used together with `defs`.\n        defs: The `Definitions` object to resolve the selection against.\n            Must be used together with `asset_selection`.\n        assets: Direct sequence of assets to observe. Each item can be an\n            `AssetSpec`, `AssetsDefinition`, or `SourceAsset`.\n            Cannot be used together with `asset_selection`/`defs`.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        tags: Optional tags to apply to the job.\n        **kwargs: Additional keyword arguments passed to the\n            [`@job`](https://docs.dagster.io/api/python-api/jobs#dagster.job) decorator.\n\n    Returns:\n        A Dagster job definition that observes all matching Metaxy assets.\n\n    Raises:\n        ValueError: If no specs have `metaxy/feature` metadata, if assets have\n            inconsistent `partitions_def`, or if invalid argument combinations\n            are provided.\n\n    Example:\n        ```python\n        import dagster as dg\n        import metaxy.ext.dagster as mxd\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature_a\"})\n        def feature_a(): ...\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature_b\"})\n        def feature_b(): ...\n\n\n        # Option 1: Using asset_selection + defs\n        my_defs = dg.Definitions(assets=[feature_a, feature_b])\n        observation_job = mxd.build_metaxy_multi_observation_job(\n            name=\"observe_my_features\",\n            asset_selection=dg.AssetSelection.kind(\"metaxy\"),\n            defs=my_defs,\n        )\n\n        # Option 2: Using direct assets list\n        observation_job = mxd.build_metaxy_multi_observation_job(\n            name=\"observe_my_features\",\n            assets=[feature_a, feature_b],\n        )\n        ```\n    \"\"\"\n    tags = tags or {}\n\n    # Validate argument combinations\n    has_selection = asset_selection is not None or defs is not None\n    has_assets = assets is not None\n\n    if has_selection and has_assets:\n        raise ValueError(\n            \"Cannot provide both 'assets' and 'asset_selection'/'defs'. \"\n            \"Use either asset_selection + defs, or assets alone.\"\n        )\n\n    if not has_selection and not has_assets:\n        raise ValueError(\"Must provide either 'asset_selection' + 'defs', or 'assets'.\")\n\n    if has_selection:\n        if asset_selection is None:\n            raise ValueError(\"'defs' requires 'asset_selection' to be provided.\")\n        if defs is None:\n            raise ValueError(\"'asset_selection' requires 'defs' to be provided.\")\n\n        # Resolve selection using defs\n        all_assets_defs = list(defs.resolve_asset_graph().assets_defs)\n        selected_keys = asset_selection.resolve(all_assets_defs)\n\n        # Get specs for selected keys, with partitions_def\n        metaxy_specs: list[dg.AssetSpec] = []\n        partitions_defs: list[dg.PartitionsDefinition | None] = []\n\n        for asset_def in all_assets_defs:\n            for spec in asset_def.specs:\n                if spec.key in selected_keys:\n                    if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                        metaxy_specs.append(spec)\n                        partitions_defs.append(asset_def.partitions_def)\n    else:\n        # Direct assets list\n        assert assets is not None\n        metaxy_specs = []\n        partitions_defs = []\n\n        for asset in assets:\n            if isinstance(asset, dg.AssetSpec):\n                if asset.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                    metaxy_specs.append(asset)\n                    partitions_defs.append(asset.partitions_def)\n            elif isinstance(asset, dg.AssetsDefinition):\n                for spec in asset.specs:\n                    if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                        metaxy_specs.append(spec)\n                        partitions_defs.append(asset.partitions_def)\n            elif isinstance(asset, dg.SourceAsset):\n                # SourceAsset doesn't have metaxy/feature metadata typically\n                pass\n            else:\n                raise TypeError(f\"Expected AssetSpec, AssetsDefinition, or SourceAsset, got {type(asset).__name__}\")\n\n    if not metaxy_specs:\n        raise ValueError(\n            \"No assets have specs with 'metaxy/feature' metadata. \"\n            \"Ensure your assets have metadata={'metaxy/feature': 'feature/key'}.\"\n        )\n\n    # Validate all specs have the same partitions_def\n    first_partitions_def = partitions_defs[0]\n    for i, pdef in enumerate(partitions_defs[1:], start=1):\n        if pdef != first_partitions_def:\n            raise ValueError(\n                f\"All assets must have the same partitions_def. \"\n                f\"Asset 0 has {first_partitions_def}, but asset {i} has {pdef}.\"\n            )\n    partitions_def = first_partitions_def\n\n    # Build feature keys for description (may have duplicates when multiple assets share a feature)\n    feature_keys = [\n        mx.coerce_to_feature_key(spec.metadata[DAGSTER_METAXY_FEATURE_METADATA_KEY]) for spec in metaxy_specs\n    ]\n\n    # Build a mapping of asset key -&gt; spec for the dynamic op\n    # This ensures each asset gets its own op, even if multiple assets share the same feature\n    spec_by_asset_key = {spec.key.to_user_string(): spec for spec in metaxy_specs}\n    all_asset_keys = list(spec_by_asset_key.keys())\n\n    # Config class for runtime filtering of assets to observe\n    class _ObserveAssetsConfig(dg.Config):\n        asset_keys: list[str] = all_asset_keys\n\n    # Op that emits dynamic outputs for each asset, optionally filtered by config\n    @dg.op(\n        name=f\"{name}_fanout\",\n        out=dg.DynamicOut(str),\n        config_schema=_ObserveAssetsConfig.to_config_schema(),\n    )\n    def fanout_assets(context: dg.OpExecutionContext) -&gt; Any:\n        config = _ObserveAssetsConfig.model_validate(context.op_config)\n\n        # Validate that requested asset keys exist\n        requested_keys = set(config.asset_keys)\n        available_keys = set(spec_by_asset_key.keys())\n        invalid_keys = requested_keys - available_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Requested asset keys not found in job: {sorted(invalid_keys)}. \"\n                f\"Available keys: {sorted(available_keys)}\"\n            )\n        asset_keys_to_observe = [k for k in spec_by_asset_key if k in requested_keys]\n\n        for asset_key_str in asset_keys_to_observe:\n            # Use asset key (with / replaced by __) as mapping key for Dagster identifiers\n            safe_mapping_key = asset_key_str.replace(\"/\", \"__\")\n            yield dg.DynamicOutput(asset_key_str, mapping_key=safe_mapping_key)\n\n    # Build the shared observation op\n    observe_op = _build_observation_op_for_specs(\n        name=f\"{name}_observe\",\n        spec_by_asset_key=spec_by_asset_key,\n        store_resource_key=store_resource_key,\n    )\n\n    # Build job metadata with asset references\n    job_metadata: dict[str, Any] = {\n        \"metaxy/features\": [fk.to_string() for fk in feature_keys],\n    }\n    for spec in metaxy_specs:\n        job_metadata[f\"metaxy/asset/{spec.key.to_user_string()}\"] = dg.MetadataValue.asset(spec.key)\n\n    # Build description as markdown list showing both assets and features\n    asset_list = \"\\n\".join(\n        f\"- `{spec.key.to_user_string()}` \u2192 `{spec.metadata[DAGSTER_METAXY_FEATURE_METADATA_KEY]}`\"\n        for spec in metaxy_specs\n    )\n    description = f\"Observe {len(metaxy_specs)} Metaxy assets:\\n\\n{asset_list}\"\n\n    @dg.job(\n        name=name,\n        partitions_def=partitions_def,\n        tags=tags,\n        description=description,\n        metadata=job_metadata,\n        **kwargs,\n    )\n    def observation_job() -&gt; None:\n        asset_keys_dynamic = fanout_assets()\n        asset_keys_dynamic.map(observe_op)\n\n    return observation_job\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observation_job.build_metaxy_observation_job","title":"metaxy.ext.dagster.observation_job.build_metaxy_observation_job","text":"<pre><code>build_metaxy_observation_job(\n    asset: AssetSpec | AssetsDefinition,\n    *,\n    store_resource_key: str = \"store\",\n    tags: dict[str, str] | None = None,\n) -&gt; list[JobDefinition]\n</code></pre> <p>Build Dagster job(s) that observe Metaxy feature asset(s).</p> <p>Creates job(s) that yield <code>AssetObservation</code> events for the given asset. The job can be run independently from asset materialization, e.g., on a schedule.</p> <p>Returns one job per <code>metaxy/feature</code> spec found in the asset.</p> <p>Jobs are constructed with matching partitions definitions. Job names are always derived as <code>observe_&lt;FeatureKey.table_name()&gt;</code>.</p> <p>Parameters:</p> <ul> <li> <code>asset</code>               (<code>AssetSpec | AssetsDefinition</code>)           \u2013            <p>Asset spec or asset definition to observe. Must have <code>metaxy/feature</code> metadata on at least one spec.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>tags</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional tags to apply to the job(s).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[JobDefinition]</code>           \u2013            <p>List of Dagster job definitions, one per <code>metaxy/feature</code> spec.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no specs have <code>metaxy/feature</code> metadata.</p> </li> </ul> Example <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature\"})\ndef my_asset(): ...\n\n\n# Build the observation job - partitions_def is extracted automatically\nobservation_job = mxd.build_metaxy_observation_job(my_asset)\n\n# Include in your Definitions\ndefs = dg.Definitions(\n    jobs=[observation_job],\n    resources={\"store\": my_store_resource},\n)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/observation_job.py</code> <pre><code>@public\ndef build_metaxy_observation_job(\n    asset: dg.AssetSpec | dg.AssetsDefinition,\n    *,\n    store_resource_key: str = \"store\",\n    tags: dict[str, str] | None = None,\n) -&gt; list[dg.JobDefinition]:\n    \"\"\"Build Dagster job(s) that observe Metaxy feature asset(s).\n\n    Creates job(s) that yield `AssetObservation` events for the given asset.\n    The job can be run independently from asset materialization, e.g., on a schedule.\n\n    Returns one job per `metaxy/feature` spec found in the asset.\n\n    Jobs are constructed with matching partitions definitions.\n    Job names are always derived as `observe_&lt;FeatureKey.table_name()&gt;`.\n\n    Args:\n        asset: Asset spec or asset definition to observe. Must have `metaxy/feature`\n            metadata on at least one spec.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        tags: Optional tags to apply to the job(s).\n\n    Returns:\n        List of Dagster job definitions, one per `metaxy/feature` spec.\n\n    Raises:\n        ValueError: If no specs have `metaxy/feature` metadata.\n\n    Example:\n        ```python\n        import dagster as dg\n        import metaxy.ext.dagster as mxd\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature\"})\n        def my_asset(): ...\n\n\n        # Build the observation job - partitions_def is extracted automatically\n        observation_job = mxd.build_metaxy_observation_job(my_asset)\n\n        # Include in your Definitions\n        defs = dg.Definitions(\n            jobs=[observation_job],\n            resources={\"store\": my_store_resource},\n        )\n        ```\n    \"\"\"\n    # Extract specs and partitions_def from asset\n    if isinstance(asset, dg.AssetSpec):\n        specs = [asset]\n        partitions_def = None\n    elif isinstance(asset, dg.AssetsDefinition):\n        specs = list(asset.specs)\n        partitions_def = asset.partitions_def\n    else:\n        raise TypeError(f\"Expected AssetSpec or AssetsDefinition, got {type(asset).__name__}\")\n\n    # Filter to specs with metaxy/feature metadata\n    metaxy_specs = [spec for spec in specs if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None]\n\n    if not metaxy_specs:\n        raise ValueError(\n            \"Asset has no specs with 'metaxy/feature' metadata. \"\n            \"Ensure your asset has metadata={'metaxy/feature': 'feature/key'}.\"\n        )\n\n    # Build jobs for each metaxy spec\n    jobs = [\n        _build_observation_job_for_spec(\n            spec,\n            partitions_def=partitions_def,\n            store_resource_key=store_resource_key,\n            tags=tags,\n        )\n        for spec in metaxy_specs\n    ]\n\n    return jobs\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#resources","title":"Resources","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource","text":"<p>               Bases: <code>ConfigurableResource[MetadataStore]</code></p> <p>This resource creates a <code>metaxy.MetadataStore</code> based on the current Metaxy configuration (<code>metaxy.toml</code>).</p> <p>If <code>name</code> is not provided, the default store will be used. The default store name can be set with <code>store = \"my_name\"</code> in <code>metaxy.toml</code> or with<code>$METAXY_STORE</code> environment variable.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","text":"<pre><code>create_resource(\n    context: InitResourceContext,\n) -&gt; MetadataStore\n</code></pre> <p>Create a MetadataStore from the Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InitResourceContext</code>)           \u2013            <p>Dagster resource initialization context.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore</code>           \u2013            <p>A MetadataStore configured with the Dagster run ID as the materialization ID.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/resources.py</code> <pre><code>def create_resource(self, context: dg.InitResourceContext) -&gt; mx.MetadataStore:\n    \"\"\"Create a MetadataStore from the Metaxy configuration.\n\n    Args:\n        context: Dagster resource initialization context.\n\n    Returns:\n        A MetadataStore configured with the Dagster run ID as the materialization ID.\n    \"\"\"\n    assert context.run is not None\n    return mx.MetaxyConfig.get().get_store(self.name, materialization_id=context.run.run_id)\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#helpers","title":"Helpers","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.FeatureStats","title":"metaxy.ext.dagster.utils.FeatureStats","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Statistics about a feature's metadata for Dagster events.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.selection.select_metaxy_assets","title":"metaxy.ext.dagster.selection.select_metaxy_assets","text":"<pre><code>select_metaxy_assets(\n    *,\n    project: str | None = None,\n    feature: CoercibleToFeatureKey | None = None,\n) -&gt; AssetSelection\n</code></pre> <p>Select Metaxy assets by project and/or feature.</p> <p>This helper creates an <code>AssetSelection</code> that filters assets tagged by <code>@metaxify</code>.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by project name. If None, uses <code>MetaxyConfig.get().project</code>.</p> </li> <li> <code>feature</code>               (<code>CoercibleToFeatureKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature key. If provided, further narrows the selection.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AssetSelection</code>           \u2013            <p>An <code>AssetSelection</code> that can be used with <code>dg.define_asset_job</code>,</p> </li> <li> <code>AssetSelection</code>           \u2013            <p><code>dg.materialize</code>, or <code>AssetSelection</code> operations like <code>|</code> and <code>&amp;</code>.</p> </li> </ul> Select all Metaxy assets in current project <pre><code>import metaxy.ext.dagster as mxd\n\nall_metaxy = mxd.select_metaxy_assets()\n</code></pre> Select assets for a specific project <pre><code>prod_assets = mxd.select_metaxy_assets(project=\"production\")\n</code></pre> Select a specific feature's assets <pre><code>feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n</code></pre> Use with asset jobs <pre><code>metaxy_job = dg.define_asset_job(\n    name=\"materialize_metaxy\",\n    selection=mxd.select_metaxy_assets(),\n)\n</code></pre> Combine with other selections <pre><code># All metaxy assets plus some other assets\ncombined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n# Metaxy assets that are also in a specific group\nfiltered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n</code></pre> Source code in <code>src/metaxy/ext/dagster/selection.py</code> <pre><code>@public\ndef select_metaxy_assets(\n    *,\n    project: str | None = None,\n    feature: mx.CoercibleToFeatureKey | None = None,\n) -&gt; dg.AssetSelection:\n    \"\"\"Select Metaxy assets by project and/or feature.\n\n    This helper creates an `AssetSelection` that filters assets tagged by `@metaxify`.\n\n    Args:\n        project: Filter by project name. If None, uses `MetaxyConfig.get().project`.\n        feature: Filter by specific feature key. If provided, further narrows the selection.\n\n    Returns:\n        An `AssetSelection` that can be used with `dg.define_asset_job`,\n        `dg.materialize`, or `AssetSelection` operations like `|` and `&amp;`.\n\n    Example: Select all Metaxy assets in current project\n        ```python\n        import metaxy.ext.dagster as mxd\n\n        all_metaxy = mxd.select_metaxy_assets()\n        ```\n\n    Example: Select assets for a specific project\n        ```python\n        prod_assets = mxd.select_metaxy_assets(project=\"production\")\n        ```\n\n    Example: Select a specific feature's assets\n        ```python\n        feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n        ```\n\n    Example: Use with asset jobs\n        ```python\n        metaxy_job = dg.define_asset_job(\n            name=\"materialize_metaxy\",\n            selection=mxd.select_metaxy_assets(),\n        )\n        ```\n\n    Example: Combine with other selections\n        ```python\n        # All metaxy assets plus some other assets\n        combined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n        # Metaxy assets that are also in a specific group\n        filtered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n        ```\n    \"\"\"\n    resolved_project = project if project is not None else mx.MetaxyConfig.get().project\n\n    selection = dg.AssetSelection.tag(DAGSTER_METAXY_PROJECT_TAG_KEY, resolved_project)\n\n    if feature is not None:\n        feature_key = mx.coerce_to_feature_key(feature)\n        selection = selection &amp; dg.AssetSelection.tag(DAGSTER_METAXY_FEATURE_METADATA_KEY, str(feature_key))\n\n    return selection\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#types","title":"Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyOutput","title":"metaxy.ext.dagster.MetaxyOutput  <code>module-attribute</code>","text":"<pre><code>MetaxyOutput = IntoFrame | None\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#constants","title":"Constants","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_FEATURE_METADATA_KEY = 'metaxy/feature'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_KIND","title":"metaxy.ext.dagster.DAGSTER_METAXY_KIND  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_KIND = 'metaxy'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_INFO_METADATA_KEY = 'metaxy/info'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_PARTITION_KEY = 'partition_by'\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/","title":"Metaxify","text":"<p>The <code>@metaxify</code> decorator can be used to automatically enrich Dagster assets definitions with information taken from Metaxy features.</p> <p>It's highly recommended to <code>@metaxify</code> all your Dagster assets that produce Metaxy features. It's also recommended to use it in combination with <code>MetaxyIOManager</code>.</p> <p><code>@metaxify</code> modifies most of the attributes available on the asset spec.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#deps","title":"Deps","text":"<p>Upstream Metaxy features are injected into <code>deps</code>.</p> Dagster UICode <p></p> <p> <code>models/landmarker_v1</code> is an upstream non-metaxy Dagster asset.</p> <pre><code>import metaxy as mx\n\nclass Chunk(mx.BaseFeature, spec=mx.FeatureSpec(key=\"chunk\", id_columns=[\"id\"])):\n    id: str\n\nclass BodyPose(mx.BaseFeature, spec=mx.FeatureSpec(key=\"body/pose\", id_columns=[\"id\"])):\n    id: str\n\nspec = mx.FeatureSpec(key=\"downstream\", id_columns=[\"id\"], deps=[Chunk, BodyPose])\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#code-version","title":"Code Version","text":"<p>Metaxy's feature spec code version is appended to the asset's code version in the format <code>metaxy:&lt;version&gt;</code>.</p> <p></p>"},{"location":"integrations/orchestration/dagster/metaxify/#description","title":"Description","text":"<p>The Metaxy feature class docstring is used if the asset spec doesn't have a description set.</p> Dagster UICode <p></p> <pre><code>import metaxy as mx\n\nclass AudioFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/feature\", id_columns=[\"id\"])):\n    \"\"\"Scene chunk audio with optional waveform visualization.\"\"\"\n    id: str\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#metadata","title":"Metadata","text":"<p><code>@metaxify</code> injects static metadata into the asset spec.</p> <p>All standard metadata types are supported. Additionally, <code>metaxy/info</code> is added. It contains the Metaxy feature spec, Metaxy project, Metaxy version and enabled Metaxy plugins.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#column-schema","title":"Column Schema","text":"<p>Pydantic fields schema is injected into the asset metadata under <code>dagster/column_schema</code>. Field types are converted to strings, and field descriptions are used as column descriptions. If the asset already has a column schema defined, Metaxy columns are appended (user-defined columns take precedence for columns with the same name).</p> <p>Warning</p> <p>Pydantic feature schema may not match the corresponding table schema in the metadata store. This will be improved in the future.</p> Dagster UICode <p></p> <pre><code>import metaxy as mx\nfrom pydantic import Field\n\nclass AudioFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/feature2\", id_columns=[\"id\"])):\n    id: str\n    duration: float = Field(description=\"duration in seconds\")\n    sample_rate: int = Field(description=\"sample rate in Hz\")\n    channels: int = Field(description=\"number of audio channels\")\n    codec: str = Field(description=\"audio codec\")\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#column-lineage","title":"Column Lineage","text":"<p>Column lineage is injected into the asset metadata under <code>dagster/column_lineage</code>.</p> <p>Tracks which upstream columns each downstream column depends on by analyzing:</p> <ul> <li> <p>Direct pass-through: Columns with the same name in both upstream and downstream features.</p> </li> <li> <p><code>FeatureDep.rename</code>: Renamed columns trace back to their original upstream column names.</p> </li> <li> <p><code>FeatureDep.lineage</code>: ID column relationships based on lineage type (identity, aggregation, expansion).</p> </li> </ul> <p>Column lineage is derived from Pydantic model fields on the feature class. If the asset already has column lineage defined, Metaxy lineage is merged with user-defined lineage (user-defined dependencies are appended to Metaxy-detected dependencies for each column).</p>"},{"location":"integrations/orchestration/dagster/metaxify/#kinds","title":"Kinds","text":"<p><code>\"metaxy\"</code> kind is injected into asset kinds if <code>inject_metaxy_kind</code> is <code>True</code> and there are less than 3 kinds currently.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#tags","title":"Tags","text":"<p><code>metaxy/feature</code> and <code>metaxy/project</code> are injected into the asset tags.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#arbitrary-asset-attributes","title":"Arbitrary Asset Attributes","text":"<p>All keys from <code>\"dagster/attributes\"</code> in the feature spec metadata (such as <code>group_name</code>, <code>owners</code>, <code>tags</code>) are applied to the Dagster asset spec (with replacement).</p>"},{"location":"integrations/plugins/","title":"Metaxy Plugins","text":"<p>These integrations typically are Python libraries or have to be enabled in <code>metaxy.toml</code>.</p>"},{"location":"integrations/plugins/#available-plugins","title":"Available Plugins","text":"<ul> <li>SQLAlchemy</li> <li>SQLModel</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/","title":"SQLAlchemy","text":"<p>Metaxy provides helpers for integrating with SQLAlchemy. These helpers allow to construct <code>sqlalchemy.MetaData</code> objects for user-defined feature tables and for Metaxy system tables.</p> <p>This integration is convenient for setting up Alembic migrations.</p> <p>SQLModel Features</p> <p>Check out our SQLModel integration for metaclass features that combine Metaxy features with SQLModel ORM models. This is the recommended way to use Metaxy with SQLAlchemy.</p>"},{"location":"integrations/plugins/sqlalchemy/#alembic-integration","title":"Alembic Integration","text":"<p>Alembic is a database migration toolkit for SQLAlchemy.</p> <p>The two helper functions: <code>filter_feature_sqla_metadata</code> and <code>get_system_slqa_metadata</code> can be used to retrieve an SQLAlchemy URL and <code>MetaData</code> object for a given <code>IbisMetadataStore</code>.</p> <p><code>filter_feature_sqla_metadata</code> returns table metadata for the user-defined tables, while <code>get_system_slqa_metadata</code> returns metadata for Metaxy's system tables.</p> <p>Call <code>init_metaxy</code> first</p> <p>You must call <code>init_metaxy</code> before using <code>filter_feature_sqla_metadata</code> to ensure all features are loaded into the feature graph.</p> <p>Here is an example Alembic <code>env.py</code> that uses the Metaxy SQLAlchemy integration:</p> env.py<pre><code>from alembic import context\nfrom metaxy import init_metaxy\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n\n# Alembic Config object\nconfig = context.config\n\nmetaxy_cfg = init_metaxy()\nstore = metaxy_cfg.get_store(\"my_store\")\n\n# import your SQLAlchemy metadata from somewhere\nmy_metadata = ...\n\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n\n# Configure Alembic\nconfig.set_main_option(\"sqlalchemy.url\", url)\n\n\n# continue with the standard Alembic workflow\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#multi-store-setup","title":"Multi-Store Setup","text":"<p>You can configure separate metadata stores for different environments:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"dev_metadata.duckdb\" }\n\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"prod_metadata.duckdb\" }\n</code></pre> <p>Then create multiple Alembic directories and register them with Alembic:</p> alembic.ini<pre><code>[dev]\nscript_location = alembic/dev\n\n[prod]\nscript_location = alembic/prod\n</code></pre> <p>Separate Alembic Version Tables</p> <p>When using multiple Alembic environments (e.g., system tables vs feature tables), configure separate version tables to avoid conflicts. Set up separate script locations in <code>alembic.ini</code>:</p> alembic.ini<pre><code>[dev:metaxy_system]\nscript_location = alembic/dev/system\n\n[dev:metaxy_features]\nscript_location = alembic/dev/features\n</code></pre> <p>Then pass <code>version_table</code> to <code>context.configure()</code> in each env.py:</p> alembic/dev/system/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_system\",\n)\n</code></pre> alembic/dev/features/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_features\",\n)\n</code></pre> <p>Each environment now tracks migrations independently:</p> <ul> <li><code>alembic_version_metaxy_system</code> for system tables</li> <li><code>alembic_version_metaxy_features</code> for feature tables</li> </ul> <p>Create and run migrations separately:</p> <pre><code>alembic -n dev:metaxy_system revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_features revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_system upgrade head\nalembic -n dev:metaxy_features upgrade head\n</code></pre> <p>The two environments now can be managed independently:</p> devprod alembic/dev/env.py<pre><code>from metaxy import init_metaxy\nconfig = init_metaxy()\nstore = config.get_store(\"dev\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n dev upgrade head\n</code></pre> alembic/prod/env.py<pre><code>from metaxy import init_metaxy\nconfig = init_metaxy()\nstore = config.get_store(\"prod\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n prod upgrade head\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#alembic-sqlmodel","title":"Alembic + SQLModel","text":"<p>To add <code>SQLModel</code> into the mix, make sure to use the SQLModel integration and pass <code>sqlmodel.SQLModel.metadata</code> into <code>filter_feature_sqla_metadata</code>.</p>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy","title":"metaxy.ext.sqlalchemy","text":"<p>SQLAlchemy integration for metaxy.</p> <p>This module provides SQLAlchemy table definitions and helpers for metaxy. These can be used with migration tools like Alembic.</p> <p>The main functions return tuples of (sqlalchemy_url, metadata) for easy integration with migration tools:</p> <ul> <li><code>get_system_slqa_metadata</code>: Get URL and system table metadata for a store</li> <li><code>filter_feature_sqla_metadata</code>: Get URL and feature table metadata for a store</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","title":"metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","text":"<pre><code>filter_feature_sqla_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and feature table metadata for a metadata store.</p> <p>This function filters the source metadata to include only feature tables belonging to the specified project, and returns the connection URL for the store.</p> <p>This function must be called after init_metaxy() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.               If False, include all features.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> <li> <code>ImportError</code>             \u2013            <p>If source_metadata is None and SQLModel is not installed</p> </li> </ul> Note <p>Metadata stores do their best at providing the correct <code>sqlalchemy_url</code>, so you typically don't need to modify the output of this function.</p> <p>Example: Basic Usage</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\nfrom sqlalchemy import MetaData\n\n# Load features first\nmx.init_metaxy()\n\n# Get store instance\nconfig = mx.MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\nmy_metadata = MetaData()\n# ... define tables in my_metadata ...\n\n# apply the filter function\nurl, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n```\n</code></pre> <p>Example: With SQLModel</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom sqlmodel import SQLModel\n\nurl, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>@public\ndef filter_feature_sqla_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and feature table metadata for a metadata store.\n\n    This function filters the source metadata to include only feature tables\n    belonging to the specified project, and returns the connection URL for the store.\n\n    This function must be called after init_metaxy() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance\n        source_metadata: Source SQLAlchemy MetaData to filter.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n                          If False, include all features.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n        protocol: Optional protocol to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n        ImportError: If source_metadata is None and SQLModel is not installed\n\n    Note:\n        Metadata stores do their best at providing the correct `sqlalchemy_url`, so you typically don't need to modify the output of this function.\n\n    Example: Basic Usage\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n        from sqlalchemy import MetaData\n\n        # Load features first\n        mx.init_metaxy()\n\n        # Get store instance\n        config = mx.MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        my_metadata = MetaData()\n        # ... define tables in my_metadata ...\n\n        # apply the filter function\n        url, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n        ```\n\n    Example: With SQLModel\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from sqlmodel import SQLModel\n\n        url, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n        ```\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n    metadata = _get_features_metadata(\n        source_metadata=source_metadata,\n        store=store,\n        project=project,\n        filter_by_project=filter_by_project,\n        inject_primary_key=inject_primary_key,\n        inject_index=inject_index,\n    )\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.get_system_slqa_metadata","title":"metaxy.ext.sqlalchemy.get_system_slqa_metadata","text":"<pre><code>get_system_slqa_metadata(\n    store: IbisMetadataStore,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.</p> <p>This function retrieves both the connection URL and system table metadata for a store, with the store's <code>table_prefix</code> automatically applied to table names.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol (drivername) to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, system_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> Note <p>Metadata stores do their best at providing the correct <code>sqlalchemy_url</code>, so you typically don't need to modify the output of this function.</p> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>@public\ndef get_system_slqa_metadata(\n    store: IbisMetadataStore,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.\n\n    This function retrieves both the connection URL and system table metadata\n    for a store, with the store's `table_prefix` automatically applied to table names.\n\n    Args:\n        store: IbisMetadataStore instance\n        protocol: Optional protocol (drivername) to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, system_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n\n    Note:\n        Metadata stores do their best at providing the correct `sqlalchemy_url`, so you typically don't need to modify the output of this function.\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n    metadata = _get_system_metadata(table_prefix=store._table_prefix)\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#configuration","title":"Configuration","text":""},{"location":"integrations/plugins/sqlalchemy/#enable","title":"<code>enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__ENABLE=false\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#inject_primary_key","title":"<code>inject_primary_key</code>","text":"<p>Automatically inject composite primary key constraints on user-defined feature tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_PRIMARY_KEY=false\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#inject_index","title":"<code>inject_index</code>","text":"<p>Automatically inject composite index on user-defined feature tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_INDEX=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/","title":"SQLModel","text":"<p>SQLModel combines SQLAlchemy and Pydantic into a single ORM. If you want your Metaxy feature definitions to double as ORM models, enable the SQLModel integration. This exposes user-defined feature tables directly to SQLAlchemy.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores.</p> <p>Database Migrations</p> <p>For database migration management with Alembic, see the SQLAlchemy integration guide.</p>"},{"location":"integrations/plugins/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install 'metaxy[sqlmodel]'\n</code></pre> <p>and has to be enabled explicitly:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_ENABLE=true\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#usage","title":"Usage","text":"<p>The SQLModel integration provides <code>BaseSQLModelFeature</code> which combines the functionality of a Metaxy feature and an SQLModel table.</p> <pre><code>import metaxy as mx\nimport metaxy.ext.sqlmodel as mxsql\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    mxsql.BaseSQLModelFeature,\n    table=True,\n    spec=mx.FeatureSpec(\n        key=FeatureKey([\"video\"]),\n        id_columns=[\"video_id\"],\n        fields=[\n            \"frames\",\n            \"duration\",\n        ],\n    ),\n):\n    # User-defined metadata columns\n    video_id: str\n    path: str\n    duration: float\n</code></pre> <p>Do Not Use Server-Generated IDs</p> <p>ID columns should not be server-generated because they are typically used to determine data locations such as object storage keys, so they have to be defined before metadata is inserted into the database</p> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in the plugin configuration.</p>"},{"location":"integrations/plugins/sqlmodel/#database-migrations","title":"Database Migrations","text":"<p>When using SQLModel features with Alembic or other migration tools, use <code>filter_feature_sqlmodel_metadata()</code> to transform table names and filter metadata.</p> <p>Table Name Transformation</p> <p>Pass <code>SQLModel.metadata</code> to <code>filter_feature_sqlmodel_metadata()</code> and it will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <pre><code>from sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom metaxy.config import MetaxyConfig\nfrom metaxy import init_metaxy\n\nconfig = init_metaxy()\nstore = config.get_store()\n\n# Transform SQLModel metadata with table_prefix\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nfrom alembic import context\n\ncontext.configure(url=url, target_metadata=target_metadata)\n</code></pre> <p>The <code>filter_feature_sqlmodel_metadata()</code> function:</p> <ul> <li>Transforms table names by adding the store's <code>table_prefix</code></li> <li>Filters tables by project (configurable)</li> <li>Returns the SQLAlchemy URL for the store</li> <li>Optionally injects primary key and index constraints</li> </ul> <p>See the SQLAlchemy integration guide for complete Alembic setup examples.</p> <p>Separate Alembic Version Tables</p> <p>When managing both system tables and feature tables with Alembic, use separate version tables to avoid conflicts. See the Multi-Store Setup section for configuration details.</p>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel","title":"metaxy.ext.sqlmodel","text":""},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.BaseSQLModelFeature","title":"metaxy.ext.sqlmodel.BaseSQLModelFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>SQLModel</code>, <code>BaseFeature</code></p> <p>Base class for <code>Metaxy</code> features that are also <code>SQLModel</code> tables.</p> <p>Example</p> <p> <pre><code>from metaxy.integrations.sqlmodel import BaseSQLModelFeature\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    BaseSQLModelFeature,\n    table=True,\n    spec=mx.FeatureSpec(\n        key=mx.FeatureKey([\"video\"]),\n        id_columns=[\"uid\"],\n        fields=[\n            mx.FieldSpec(\n                key=mx.FieldKey([\"video_file\"]),\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n    uid: str = Field(primary_key=True)\n    path: str\n    duration: float\n\n    # Now you can use both Metaxy and SQLModel features:\n    # - VideoFeature.feature_version() -&gt; Metaxy versioning\n    # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\n</code></pre></p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Base class for `Metaxy` features that are also `SQLModel` tables.\\n\\n!!! example\\n\\n    &lt;!-- skip next --&gt;\\n    ```py\\n    from metaxy.integrations.sqlmodel import BaseSQLModelFeature\\n    from sqlmodel import Field\\n\\n\\n    class VideoFeature(\\n        BaseSQLModelFeature,\\n        table=True,\\n        spec=mx.FeatureSpec(\\n            key=mx.FeatureKey([\\\"video\\\"]),\\n            id_columns=[\\\"uid\\\"],\\n            fields=[\\n                mx.FieldSpec(\\n                    key=mx.FieldKey([\\\"video_file\\\"]),\\n                    code_version=\\\"1\\\",\\n                ),\\n            ],\\n        ),\\n    ):\\n        uid: str = Field(primary_key=True)\\n        path: str\\n        duration: float\\n\\n        # Now you can use both Metaxy and SQLModel features:\\n        # - VideoFeature.feature_version() -&gt; Metaxy versioning\\n        # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\\n    ```\",\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"default\": null,\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_snapshot_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph snapshot\",\n      \"title\": \"Metaxy Snapshot Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    },\n    \"metaxy_updated_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was last updated (UTC)\",\n      \"title\": \"Metaxy Updated At\"\n    },\n    \"metaxy_deleted_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Soft delete timestamp (UTC); null means active row\",\n      \"title\": \"Metaxy Deleted At\"\n    }\n  },\n  \"title\": \"BaseSQLModelFeature\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'frozen': False}</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metaxy_provenance</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_provenance_by_field</code>                 (<code>dict[str, str]</code>)             </li> <li> <code>metaxy_feature_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_snapshot_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version_by_field</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>metaxy_created_at</code>                 (<code>AwareDatetime | None</code>)             </li> <li> <code>metaxy_updated_at</code>                 (<code>AwareDatetime | None</code>)             </li> <li> <code>metaxy_materialization_id</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_deleted_at</code>                 (<code>AwareDatetime | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_id_columns_exist</code> </li> </ul>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","title":"metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","text":"<pre><code>filter_feature_sqlmodel_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.</p> <p>This function transforms SQLModel table names to include the store's table_prefix, ensuring that table names in the metadata match what's expected in the database.</p> <p>You can pass <code>SQLModel.metadata</code> directly - this function will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <p>This function must be called after init_metaxy() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).             Tables are looked up in this metadata by their unprefixed names.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> Note <p>For ClickHouse, the <code>sqlalchemy_url</code> property already returns the native protocol with port 9000, so you typically don't need to override these.</p> <p>Example: Basic Usage</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom alembic import context\n\n# Load features first\nmx.init_metaxy()\n\n# Get store instance\nconfig = mx.MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\n# Filter SQLModel metadata with prefix transformation\nurl, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\ncontext.configure(url=url, target_metadata=target_metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlmodel/plugin.py</code> <pre><code>@public\ndef filter_feature_sqlmodel_metadata(\n    store: \"IbisMetadataStore\",\n    source_metadata: \"MetaData\",\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, \"MetaData\"]:\n    \"\"\"Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.\n\n    This function transforms SQLModel table names to include the store's table_prefix,\n    ensuring that table names in the metadata match what's expected in the database.\n\n    You can pass `SQLModel.metadata` directly - this function will transform table names\n    by adding the store's `table_prefix`. The returned metadata will have prefixed table\n    names that match the actual database tables.\n\n    This function must be called after init_metaxy() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)\n        source_metadata: Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).\n                        Tables are looked up in this metadata by their unprefixed names.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n        protocol: Optional protocol to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n\n    Note:\n        For ClickHouse, the `sqlalchemy_url` property already returns the native\n        protocol with port 9000, so you typically don't need to override these.\n\n    Example: Basic Usage\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from sqlmodel import SQLModel\n        from metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\n        from alembic import context\n\n        # Load features first\n        mx.init_metaxy()\n\n        # Get store instance\n        config = mx.MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        # Filter SQLModel metadata with prefix transformation\n        url, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n        # Use with Alembic env.py\n        url, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n        context.configure(url=url, target_metadata=target_metadata)\n        ```\n    \"\"\"\n    from sqlalchemy import MetaData\n\n    from metaxy.ext.sqlalchemy.plugin import _get_store_sqlalchemy_url\n\n    config = MetaxyConfig.get(load=True)\n\n    if project is None:\n        project = config.project\n\n    # Check plugin config for defaults\n    sqlmodel_config = config.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    if inject_primary_key is None:\n        inject_primary_key = sqlmodel_config.inject_primary_key\n    if inject_index is None:\n        inject_index = sqlmodel_config.inject_index\n\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n\n    # Create new metadata with transformed table names\n    filtered_metadata = MetaData()\n\n    # Get the FeatureGraph to look up feature classes by key\n    from metaxy.models.feature import FeatureGraph\n\n    feature_graph = FeatureGraph.get_active()\n\n    # Iterate over tables in source metadata\n    for table_name, original_table in source_metadata.tables.items():\n        # Check if this table has Metaxy feature metadata\n        if metaxy_system_info := original_table.info.get(\"metaxy-system\"):\n            metaxy_info = MetaxyTableInfo.model_validate(metaxy_system_info)\n            feature_key = metaxy_info.feature_key\n        else:\n            continue\n        # Look up the feature definition from the FeatureGraph\n        definition = feature_graph.feature_definitions_by_key.get(feature_key)\n        if definition is None:\n            # Skip tables for features that aren't registered\n            continue\n\n        # Filter by project if requested\n        if filter_by_project:\n            if definition.project != project:\n                continue\n\n        # Compute prefixed name using store's table_prefix\n        prefixed_name = store.get_table_name(feature_key)\n\n        # Copy table to new metadata with prefixed name\n        new_table = original_table.to_metadata(filtered_metadata, name=prefixed_name)\n\n        # Inject constraints if requested\n        if inject_primary_key or inject_index:\n            from metaxy.ext.sqlalchemy.plugin import _inject_constraints\n\n            _inject_constraints(\n                table=new_table,\n                spec=definition.spec,\n                inject_primary_key=inject_primary_key,\n                inject_index=inject_index,\n            )\n\n    return url, filtered_metadata\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#configuration","title":"Configuration","text":""},{"location":"integrations/plugins/sqlmodel/#enable","title":"<code>enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__ENABLE=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#inject_primary_key","title":"<code>inject_primary_key</code>","text":"<p>Automatically inject composite primary key constraints on SQLModel tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_PRIMARY_KEY=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#inject_index","title":"<code>inject_index</code>","text":"<p>Automatically inject composite index on SQLModel tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_INDEX=false\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>Technical documentation for Metaxy.</p> <ul> <li> <p>CLI - Command-line interface</p> </li> <li> <p>API - Python interface</p> </li> <li> <p>Configuration - Configuration options</p> </li> </ul>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Warning</p> <p>The CLI interface is not stable yet.</p> <pre><code>metaxy COMMAND\n</code></pre> <p>Metaxy CLI.</p> <p>Auto-discovers configuration (<code>metaxy.toml</code> or <code>pyproject.toml</code>) in current or parent directories. Feature definitions are collected via feature discovery. Supports loading environment variables from a <code>.env</code> file in the current directory.</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>config</code><ul> <li><code>print</code></li> </ul> </li> <li><code>describe</code><ul> <li><code>graph</code></li> <li><code>features</code></li> </ul> </li> <li><code>graph</code><ul> <li><code>history</code></li> <li><code>render</code></li> </ul> </li> <li><code>list</code><ul> <li><code>features</code></li> <li><code>stores</code></li> </ul> </li> <li><code>metadata</code><ul> <li><code>status</code></li> <li><code>delete</code></li> <li><code>copy</code></li> </ul> </li> <li><code>mcp</code></li> <li><code>push</code></li> <li><code>lock</code></li> </ul> <p>Commands:</p> <ul> <li><code>config</code>: Manage Metaxy configuration</li> <li><code>describe</code>: Describe Metaxy entities in detail</li> <li><code>graph</code>: Manage feature graphs</li> <li><code>list</code>: List Metaxy entities</li> <li><code>lock</code>: Generate <code>metaxy.lock</code> file with external feature definitions fetched from the metadata store.</li> <li><code>mcp</code>: MCP server commands.</li> <li><code>metadata</code>: Manage Metaxy metadata in metadata stores.</li> <li><code>push</code>: Push feature definitions to the metadata store</li> <li><code>shell</code>: Start interactive shell.</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-shell","title":"metaxy shell","text":"<pre><code>metaxy shell\n</code></pre> <p>Start interactive shell.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-config","title":"metaxy config","text":"<p>Manage Metaxy configuration</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-config-print","title":"metaxy config print","text":"<pre><code>metaxy config print [OPTIONS]\n</code></pre> <p>Print the current Metaxy configuration.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'toml' (with syntax highlighting) or 'json'.  [choices: toml, json] [default: toml]</li> </ul>"},{"location":"reference/cli/#metaxy-describe","title":"metaxy describe","text":"<p>Describe Metaxy entities in detail</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-describe-graph","title":"metaxy describe graph","text":"<pre><code>metaxy describe graph [ARGS]\n</code></pre> <p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no dependents) - Project breakdown (if there some features are defined in different projects)</p> <p>Parameters:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-describe-features","title":"metaxy describe features","text":"<pre><code>metaxy describe features [OPTIONS] [ARGS]\n</code></pre> <p>Describe one or more features in detail.</p> <p>Shows comprehensive information about features including project, key, version, description, fields with their versions and dependencies.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-graph","title":"metaxy graph","text":"<p>Manage feature graphs</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"metaxy graph history","text":"<pre><code>metaxy graph history [ARGS]\n</code></pre> <p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing snapshot versions, when they were recorded, and feature counts.</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"metaxy graph render","text":"<pre><code>metaxy graph render [OPTIONS] [ARGS]\n</code></pre> <p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:   - graph (default): Hierarchical tree view   - cards: Panel/card-based view with dependency edges - mermaid: Mermaid flowchart markup - graphviz: Graphviz DOT format</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>--type, -t</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>--output, -o</code>: Output file path (default: stdout)</li> <li><code>--snapshot</code>: Snapshot version to render (default: current graph from code)</li> <li><code>--store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: False]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: False]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: True]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: True]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: True]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: False]</li> <li><code>--show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: True]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: True]</li> <li><code>--title</code>: Custom title for the graph. Defaults to 'Feature Graph' or 'Feature Graph Changes' for diffs.</li> </ul>"},{"location":"reference/cli/#metaxy-list","title":"metaxy list","text":"<p>List Metaxy entities</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-list-features","title":"metaxy list features","text":"<pre><code>metaxy list features [OPTIONS]\n</code></pre> <p>List Metaxy features in the current project.</p> <p>Parameters:</p> <ul> <li><code>--verbose, --no-verbose, -v</code>: Show detailed information including field dependencies and versions.  [default: False]</li> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/cli/#metaxy-list-stores","title":"metaxy list stores","text":"<pre><code>metaxy list stores [OPTIONS]\n</code></pre> <p>List configured metadata stores.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata","title":"metaxy metadata","text":"<p>Manage Metaxy metadata in metadata stores.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-status","title":"metaxy metadata status","text":"<pre><code>metaxy metadata status [OPTIONS] [ARGS]\n</code></pre> <p>Check metadata completeness and freshness for specified features.</p> <p>Parameters:</p> <ul> <li><code>--store</code>: Metadata store name (defaults to configured default store).</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--global-filter, --empty-global-filter</code>: SQL WHERE clause filter applied to all features being selected (including upstream). Can be repeated.</li> <li><code>--snapshot-id</code>: Check metadata against a specific snapshot version.</li> <li><code>--assert-in-sync, --no-assert-in-sync</code>: Exit with error if any feature needs updates or metadata is missing.  [default: False]</li> <li><code>--verbose, --no-verbose</code>: Whether to display sample slices of dataframes.  [default: False]</li> <li><code>--progress, --no-progress</code>: Display progress percentage showing how many input units have been processed at least once. Stale samples are counted as processed.  [default: False]</li> <li><code>--allow-fallback-stores, --no-allow-fallback-stores</code>: Whether to read metadata from fallback stores.  [default: True]</li> <li><code>--format</code>:   [choices: plain, json] [default: plain]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-delete","title":"metaxy metadata delete","text":"<pre><code>metaxy metadata delete [OPTIONS] [ARGS]\n</code></pre> <p>Delete metadata rows matching filters.</p> <p>Parameters:</p> <ul> <li><code>--store</code>: Metadata store name (defaults to configured default store).</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--soft, --hard</code>: Whether to mark records with deletion timestamps vs physically remove them.  [default: True]</li> <li><code>--with-feature-history, --no-with-feature-history</code>: Include rows from all historical feature versions (by default, only current version is affected).  [default: False]</li> <li><code>--yes, --no-yes</code>: Confirm deletion without prompting (required for hard deletes without filters).  [default: False]</li> <li><code>--dry-run, --no-dry-run</code>: Preview deletion: show features, filters, and row counts without executing.  [default: False]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-copy","title":"metaxy metadata copy","text":"<pre><code>metaxy metadata copy --from STR --to STR [OPTIONS] [ARGS]\n</code></pre> <p>Copy metadata from one store to another.</p> <p>Copies metadata for specified features from source to destination store. By default, copies all versions (--with-feature-history) and deduplicates by keeping only the latest row per sample (--no-with-sample-history).</p> <p>Parameters:</p> <ul> <li><code>--from</code>: Source store name to copy metadata from.  [required]</li> <li><code>--to</code>: Destination store name to copy metadata to.  [required]</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--with-feature-history, --no-with-feature-history</code>: Include rows from all historical feature versions (by default, only current version is copied).  [default: True]</li> <li><code>--with-sample-history, --no-with-sample-history</code>: Include all historical materializations per sample (by default, deduplicates by id_columns).  [default: False]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-mcp","title":"metaxy mcp","text":"<pre><code>metaxy mcp\n</code></pre> <p>MCP server commands.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-push","title":"metaxy push","text":"<pre><code>metaxy push [OPTIONS] [ARGS]\n</code></pre> <p>Push feature definitions to the metadata store</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)  [env: METAXY_STORE]</li> <li><code>--tags, -t</code>: Arbitrary key-value pairs to attach to the pushed snapshot. Example: <code>--tags.git_commit abc123def</code>.  [env: METAXY_TAGS]</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-lock","title":"metaxy lock","text":"<pre><code>metaxy lock [OPTIONS] [ARGS]\n</code></pre> <p>Generate <code>metaxy.lock</code> file with external feature definitions fetched from the metadata store.</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)  [env: METAXY_STORE]</li> <li><code>--output, -o</code>: Output file path (defaults to metaxy.lock in config directory)  [env: METAXY_OUTPUT] [default: \"\"]</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Metaxy can be configured using TOML configuration files, environment variables, or programmatically.</p>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol> <p>Configuration files are discovered automatically by searching in the current or parent directories.</p>"},{"location":"reference/configuration/#templating-environment-variables","title":"Templating Environment Variables","text":"<p>Metaxy supports templating environment variables in configuration files using the <code>${VARIABLE_NAME}</code> syntax.</p> <p>Example</p> metaxy.toml<pre><code>[stores.branch.config]\nroot_path = \"s3://my-bucket/${BRANCH_NAME}\"\n</code></pre>"},{"location":"reference/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/configuration/#store","title":"<code>store</code>","text":"<p>Default metadata store to use</p> <p>Type: <code>str</code> | Default: <code>\"dev\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <pre><code>export METAXY_STORE=dev\n</code></pre>"},{"location":"reference/configuration/#stores","title":"<code>stores</code>","text":"<p>Named store configurations</p> <p>Type: dict[str, metaxy.config.StoreConfig]</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# stores = {}\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# stores = {}\n</code></pre> <pre><code>export METAXY_STORES=...\n</code></pre>"},{"location":"reference/configuration/#migrations_dir","title":"<code>migrations_dir</code>","text":"<p>Directory where migration files are stored</p> <p>Type: <code>str</code> | Default: <code>\".metaxy/migrations\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>migrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>[tool.metaxy]\nmigrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>export METAXY_MIGRATIONS_DIR=.metaxy/migrations\n</code></pre>"},{"location":"reference/configuration/#entrypoints","title":"<code>entrypoints</code>","text":"<p>List of Python module paths to load for feature discovery</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# entrypoints = []\n</code></pre> <pre><code>export METAXY_ENTRYPOINTS=...\n</code></pre>"},{"location":"reference/configuration/#theme","title":"<code>theme</code>","text":"<p>Graph rendering theme for CLI visualization</p> <p>Type: <code>str</code> | Default: <code>\"default\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>theme = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\ntheme = \"default\"\n</code></pre> <pre><code>export METAXY_THEME=default\n</code></pre>"},{"location":"reference/configuration/#hash_truncation_length","title":"<code>hash_truncation_length</code>","text":"<p>Truncate hash values to this length.</p> <p>Type: <code>int</code> | Default: <code>8</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>hash_truncation_length = 8\n</code></pre> <pre><code>[tool.metaxy]\nhash_truncation_length = 8\n</code></pre> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=8\n</code></pre>"},{"location":"reference/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>Auto-create tables when opening stores. It is not advised to enable this setting in production.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre>"},{"location":"reference/configuration/#project","title":"<code>project</code>","text":"<p>Project name for metadata isolation. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (<code>/</code>) or double underscores (<code>__</code>)</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# project = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# project = null\n</code></pre> <pre><code>export METAXY_PROJECT=...\n</code></pre>"},{"location":"reference/configuration/#locked","title":"<code>locked</code>","text":"<p>Whether to raise an error if an external feature doesn't have a matching feature version when syncing external features from the metadata store.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# locked = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# locked = null\n</code></pre> <pre><code>export METAXY_LOCKED=...\n</code></pre>"},{"location":"reference/configuration/#sync","title":"<code>sync</code>","text":"<p>Whether to automatically sync external feature definitions from the metadata during some operations. It's recommended to keep this enabled as it ensures versioning correctness for external feature definitions with a negligible performance impact.</p> <p>Type: <code>bool</code> | Default: <code>True</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>sync = true\n</code></pre> <pre><code>[tool.metaxy]\nsync = true\n</code></pre> <pre><code>export METAXY_SYNC=true\n</code></pre>"},{"location":"reference/configuration/#metaxy_lock_path","title":"<code>metaxy_lock_path</code>","text":"<p>Relative or absolute path to the lock file, resolved from the config file's location.</p> <p>Type: <code>str</code> | Default: <code>\"metaxy.lock\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>metaxy_lock_path = \"metaxy.lock\"\n</code></pre> <pre><code>[tool.metaxy]\nmetaxy_lock_path = \"metaxy.lock\"\n</code></pre> <pre><code>export METAXY_METAXY_LOCK_PATH=metaxy.lock\n</code></pre>"},{"location":"reference/configuration/#store-configuration","title":"Store Configuration","text":"<p>The <code>stores</code> field configures metadata store backends. Each store is defined by:</p> <ul> <li><code>type</code>: Full import path to the store class (e.g., <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code>)</li> <li><code>config</code>: Dictionary of store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#example-multiple-stores-with-fallback-stores","title":"Example: Multiple Stores with Fallback Stores","text":"metaxy.tomlpyproject.toml <pre><code># Default store to use\nstore = \"dev\"\n\n# Development store (in-memory) with fallback to production\n[stores.dev]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\n[stores.dev.config]\nroot_path = \"${HOME}/.metaxy/metadata\"\nfallback_stores = [\"prod\"]\n\n# Production store\n[stores.prod]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\nconfig = { root_path = \"s3://my-bucket/metadata\" }\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n\n[tool.metaxy.stores.dev]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\n[tool.metaxy.stores.dev.config]\nroot_path = \"${HOME}/.metaxy/metadata\"\nfallback_stores = [\"prod\"]\n\n[tool.metaxy.stores.prod]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\nconfig = { root_path = \"s3://my-bucket/metadata\" }\n</code></pre>"},{"location":"reference/configuration/#configuring-metadata-stores","title":"Configuring Metadata Stores","text":"<p>Configuration options for metadata stores can be found at the respective store documentation page.</p>"},{"location":"reference/configuration/#configuring-metaxy-plugins","title":"Configuring Metaxy Plugins","text":"<p>Configuration options for Metaxy plugins can be found at the respective plugin documentation page.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#metaxy","title":"<code>metaxy</code>","text":"<p>The top-level <code>metaxy</code> module provides the main public API for Metaxy. It is typically referenced as <code>mx</code>:</p> <pre><code>import metaxy as mx\n</code></pre>"},{"location":"reference/api/#initialization","title":"Initialization","text":""},{"location":"reference/api/#metaxy.init_metaxy","title":"metaxy.init_metaxy","text":"<pre><code>init_metaxy(\n    config: MetaxyConfig | Path | str | None = None,\n    search_parents: bool = True,\n) -&gt; MetaxyConfig\n</code></pre> <p>Main user-facing initialization function for Metaxy. It loads feature definitions and Metaxy configuration.</p> <p>Features are discovered from installed Python packages metadata. External features are loaded from <code>metaxy.lock</code> if present.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetaxyConfig | Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration to use for initialization. Will be auto-discovered if not provided.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set the config file path.</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to search parent directories for configuration files during config auto-discovery.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>The activated Metaxy configuration.</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>@public\ndef init_metaxy(\n    config: MetaxyConfig | Path | str | None = None,\n    search_parents: bool = True,\n) -&gt; MetaxyConfig:\n    \"\"\"Main user-facing initialization function for Metaxy. It loads feature definitions and Metaxy configuration.\n\n    Features are [discovered](../../guide/learn/feature-discovery.md) from installed Python packages metadata.\n    External features are loaded from `metaxy.lock` if present.\n\n    Args:\n        config: Metaxy configuration to use for initialization. Will be auto-discovered if not provided.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set the config file path.\n\n        search_parents: Whether to search parent directories for configuration files during config auto-discovery.\n\n    Returns:\n        The activated Metaxy configuration.\n    \"\"\"\n    from metaxy.utils.lock_file import load_lock_file\n\n    if isinstance(config, MetaxyConfig):\n        MetaxyConfig.set(config)\n    else:\n        config = MetaxyConfig.load(\n            config_file=config,\n            search_parents=search_parents,\n        )\n    load_lock_file(config)\n    load_features(config.entrypoints)\n    return config\n</code></pre>"},{"location":"reference/api/#metaxy.sync_external_features","title":"metaxy.sync_external_features","text":"<pre><code>sync_external_features(\n    store: MetadataStore,\n    *,\n    on_version_mismatch: Literal[\"warn\", \"error\"]\n    | None = None,\n) -&gt; list[FeatureDefinition]\n</code></pre> <p>Sync external feature definitions from a metadata store if the graph has any.</p> <p>This function loads feature definitions from the metadata store to replace external feature placeholders in the active graph. It also validates that the versions match and warns or errors on mismatches.</p> <p>Additionally, this function loads any feature keys specified in the <code>features</code> config field, warning if any of them are not found in the metadata store.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to load from. Will be opened automatically if not already open.</p> </li> <li> <code>on_version_mismatch</code>               (<code>Literal['warn', 'error'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional override for the <code>on_version_mismatch</code> setting on external feature definitions.</p> <p>Info</p> <p>Setting <code>MetaxyConfig.locked</code> to <code>True</code> takes precedence over this argument.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureDefinition]</code>           \u2013            <p>List of loaded FeatureDefinition objects.</p> </li> </ul> Example <pre><code>import metaxy as mx\n\n# Sync external features before running a pipeline\nmx.sync_external_features(store)\n\n# Or with explicit error handling\nmx.sync_external_features(store, on_version_mismatch=\"error\")\n</code></pre> Source code in <code>src/metaxy/utils/external_features.py</code> <pre><code>@public\ndef sync_external_features(\n    store: MetadataStore,\n    *,\n    on_version_mismatch: Literal[\"warn\", \"error\"] | None = None,\n) -&gt; list[FeatureDefinition]:\n    \"\"\"Sync external feature definitions from a metadata store if the graph has any.\n\n    This function loads feature definitions from the metadata store to replace\n    external feature placeholders in the active graph. It also validates that\n    the versions match and warns or errors on mismatches.\n\n    Additionally, this function loads any feature keys specified in the\n    `features` config field, warning if any of them are not found in the metadata store.\n\n    Args:\n        store: Metadata store to load from. Will be opened automatically if not already open.\n        on_version_mismatch: Optional override for the `on_version_mismatch` setting on external feature definitions.\n\n            !!! info\n                Setting [`MetaxyConfig.locked`][metaxy.MetaxyConfig] to `True` takes precedence over this argument.\n\n    Returns:\n        List of loaded FeatureDefinition objects.\n\n    Example:\n        ```python\n        import metaxy as mx\n\n        # Sync external features before running a pipeline\n        mx.sync_external_features(store)\n\n        # Or with explicit error handling\n        mx.sync_external_features(store, on_version_mismatch=\"error\")\n        ```\n    \"\"\"\n    from metaxy.config import MetaxyConfig\n    from metaxy.metadata_store.system import SystemTableStorage\n\n    graph = FeatureGraph.get_active()\n    config = MetaxyConfig.get(_allow_default_config=True)\n\n    if not graph.has_external_features:\n        return []\n\n    # Check if locked mode is enabled\n    if config.locked:\n        on_version_mismatch = \"error\"\n\n    # Record versions of external features BEFORE loading\n    external_versions_before: dict[FeatureKey, tuple[str, dict[str, str], FeatureDefinition]] = {}\n    external_keys: list[str] = []\n    for key, defn in graph.feature_definitions_by_key.items():\n        if defn.is_external:\n            external_versions_before[key] = (\n                graph.get_feature_version(key),\n                graph.get_feature_version_by_field(key),\n                defn,\n            )\n            external_keys.append(key.to_string())\n\n    # Use nullcontext if store is already open, otherwise open it\n    cm = nullcontext(store) if store._is_open else store\n    with cm:\n        storage = SystemTableStorage(store)\n        # Load features by key, not by project - external features may have placeholder projects\n        result = storage._load_feature_definitions_raw(\n            filters=[nw.col(\"feature_key\").is_in(external_keys)],\n        )\n\n    # Check for version mismatches\n    _check_version_mismatches(graph, external_versions_before, on_version_mismatch)\n\n    # Warn if there are still unresolved external features after sync\n    remaining_external = list(sorted(d.spec.key for d in graph.feature_definitions_by_key.values() if d.is_external))\n    if remaining_external:\n        keys_str = \", \".join(str(k) for k in remaining_external)\n        warnings.warn(\n            f\"After syncing, {len(remaining_external)} external feature(s) could not be resolved \"\n            f\"from the metadata store: {keys_str}. \"\n            f\"These features may not exist in the store.\",\n            UnresolvedExternalFeatureWarning,\n            stacklevel=2,\n        )\n\n    return result\n</code></pre>"},{"location":"reference/api/#metadata-stores","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata through the MetadaStore interface.</p>"},{"location":"reference/api/#dependency-specification","title":"Dependency Specification","text":"<p>Metaxy has a declarative feature specification system that allows users to express dependencies on versioned fields of other upstream features.</p>"},{"location":"reference/api/config/","title":"Configuration","text":"<p>This is the Python SDK for Metaxy's configuration. See config file reference to learn how to configure Metaxy via <code>TOML</code> files.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig","title":"metaxy.MetaxyConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> <p>Environment variables can be templated with <code>${MY_VAR:-default}</code> syntax.</p> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> Templating environment variables metaxy.toml<pre><code>[stores.branch.config]\nroot_path = \"s3://my-bucket/${BRANCH_NAME}\"\n</code></pre> <p>The default store is <code>\"dev\"</code>; <code>METAXY_STORE</code> can be used to override it.</p> <p>Incomplete store configurations are filtered out if the store type is not set.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.config_file","title":"metaxy.MetaxyConfig.config_file  <code>property</code>","text":"<pre><code>config_file: Path | None\n</code></pre> <p>The config file path used to load this configuration.</p> <p>Returns None if the config was created directly (not via load()).</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.lock_file","title":"metaxy.MetaxyConfig.lock_file  <code>property</code>","text":"<pre><code>lock_file: Path | None\n</code></pre> <p>The resolved lock file path.</p> <p>Returns the absolute path if <code>metaxy_lock_path</code> is absolute, otherwise resolves it relative to the config file's directory.</p> <p>Returns None if the path is relative and no config file is set.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.plugins","title":"metaxy.MetaxyConfig.plugins  <code>property</code>","text":"<pre><code>plugins: list[str]\n</code></pre> <p>Returns all enabled plugin names from ext configuration.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_project","title":"metaxy.MetaxyConfig.validate_project  <code>classmethod</code>","text":"<pre><code>validate_project(v: str | None) -&gt; str | None\n</code></pre> <p>Validate project name follows naming rules.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"project\")\n@classmethod\ndef validate_project(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate project name follows naming rules.\"\"\"\n    if v is None:\n        return None\n    if not v:\n        raise ValueError(\"project name cannot be empty\")\n    if \"/\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain forward slashes (/). \"\n            f\"Forward slashes are reserved for FeatureKey separation\"\n        )\n    if \"__\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain double underscores (__). \"\n            f\"Double underscores are reserved for table name generation\"\n        )\n    import re\n\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", v):\n        raise ValueError(f\"project name '{v}' must contain only alphanumeric characters, underscores, and hyphens\")\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_plugin","title":"metaxy.MetaxyConfig.get_plugin  <code>classmethod</code>","text":"<pre><code>get_plugin(\n    name: str, plugin_cls: type[PluginConfigT]\n) -&gt; PluginConfigT\n</code></pre> <p>Get the plugin config from the global Metaxy config.</p> <p>Unlike <code>get()</code>, this method does not warn when the global config is not initialized. This is intentional because plugins may call this at import time to read their configuration, and returning default plugin config is always safe.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get_plugin(cls, name: str, plugin_cls: type[PluginConfigT]) -&gt; PluginConfigT:\n    \"\"\"Get the plugin config from the global Metaxy config.\n\n    Unlike `get()`, this method does not warn when the global config is not\n    initialized. This is intentional because plugins may call this at import\n    time to read their configuration, and returning default plugin config\n    is always safe.\n    \"\"\"\n    ext = cls.get(_allow_default_config=True).ext\n    if name in ext:\n        existing = ext[name]\n        if isinstance(existing, plugin_cls):\n            # Already the correct type\n            plugin = existing\n        else:\n            # Convert from generic PluginConfig or dict to specific plugin class\n            plugin = plugin_cls.model_validate(existing.model_dump())\n    else:\n        # Return default config if plugin not configured\n        plugin = plugin_cls()\n    return plugin\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.settings_customise_sources","title":"metaxy.MetaxyConfig.settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources: init \u2192 env \u2192 TOML.</p> <p>Priority (first wins): 1. Init arguments 2. Environment variables 3. TOML file</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources: init \u2192 env \u2192 TOML.\n\n    Priority (first wins):\n    1. Init arguments\n    2. Environment variables\n    3. TOML file\n    \"\"\"\n    toml_settings = TomlConfigSettingsSource(settings_cls)\n    return (init_settings, env_settings, toml_settings)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get","title":"metaxy.MetaxyConfig.get  <code>classmethod</code>","text":"<pre><code>get(\n    *,\n    load: bool = False,\n    _allow_default_config: bool = False,\n) -&gt; MetaxyConfig\n</code></pre> <p>Get the current Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>load</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and config is not set, calls <code>MetaxyConfig.load()</code> to load configuration from file. Useful for plugins that need config but don't want to require manual initialization.</p> </li> <li> <code>_allow_default_config</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Internal parameter. When True, returns default config without warning if global config is not set. Used by methods like <code>get_plugin</code> that may be called at import time.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get(cls, *, load: bool = False, _allow_default_config: bool = False) -&gt; \"MetaxyConfig\":\n    \"\"\"Get the current Metaxy configuration.\n\n    Args:\n        load: If True and config is not set, calls `MetaxyConfig.load()` to\n            load configuration from file. Useful for plugins that need config\n            but don't want to require manual initialization.\n        _allow_default_config: Internal parameter. When True, returns default\n            config without warning if global config is not set. Used by methods\n            like `get_plugin` that may be called at import time.\n    \"\"\"\n    cfg = _metaxy_config.get()\n    if cfg is None:\n        if load:\n            return cls.load()\n        if not _allow_default_config:\n            warnings.warn(\n                UserWarning(\n                    \"Global Metaxy configuration not initialized. It can be set with MetaxyConfig.set(config) typically after loading it from a toml file. Returning default configuration (with environment variables and other pydantic settings sources resolved).\"\n                ),\n                stacklevel=2,\n            )\n        return cls()\n    else:\n        return cfg\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.set","title":"metaxy.MetaxyConfig.set  <code>classmethod</code>","text":"<pre><code>set(config: Self | None) -&gt; None\n</code></pre> <p>Set the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef set(cls, config: Self | None) -&gt; None:\n    \"\"\"Set the current Metaxy configuration.\"\"\"\n    _metaxy_config.set(config)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.is_set","title":"metaxy.MetaxyConfig.is_set  <code>classmethod</code>","text":"<pre><code>is_set() -&gt; bool\n</code></pre> <p>Check if the current Metaxy configuration is set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef is_set(cls) -&gt; bool:\n    \"\"\"Check if the current Metaxy configuration is set.\"\"\"\n    return _metaxy_config.get() is not None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.reset","title":"metaxy.MetaxyConfig.reset  <code>classmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the current Metaxy configuration to None.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset the current Metaxy configuration to None.\"\"\"\n    _metaxy_config.set(None)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.use","title":"metaxy.MetaxyConfig.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Use this configuration temporarily, restoring previous config on exit.</p> Example <pre><code>test_config = MetaxyConfig(project=\"test\")\nwith test_config.use():\n    # Code here uses test config\n    assert MetaxyConfig.get().project == \"test\"\n# Previous config restored\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Use this configuration temporarily, restoring previous config on exit.\n\n    Example:\n        ```py\n        test_config = MetaxyConfig(project=\"test\")\n        with test_config.use():\n            # Code here uses test config\n            assert MetaxyConfig.get().project == \"test\"\n        # Previous config restored\n        ```\n    \"\"\"\n    previous = _metaxy_config.get()\n    _metaxy_config.set(self)\n    try:\n        yield self\n    finally:\n        _metaxy_config.set(previous)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.load","title":"metaxy.MetaxyConfig.load  <code>classmethod</code>","text":"<pre><code>load(\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; MetaxyConfig\n</code></pre> <p>Load config with auto-discovery and parent directory search.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional config file path.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set this parameter</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Search parent directories for config file</p> </li> <li> <code>auto_discovery_start</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to start search from. Defaults to current working directory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>Loaded config (TOML + env vars merged)</p> </li> </ul> Example <pre><code># Auto-discover with parent search\nconfig = MetaxyConfig.load()\n\n# Explicit file\nconfig = MetaxyConfig.load(\"custom.toml\")\n\n# Auto-discover without parent search\nconfig = MetaxyConfig.load(search_parents=False)\n\n# Auto-discover from a specific directory\nconfig = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; \"MetaxyConfig\":\n    \"\"\"Load config with auto-discovery and parent directory search.\n\n    Args:\n        config_file: Optional config file path.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set this parameter\n\n        search_parents: Search parent directories for config file\n        auto_discovery_start: Directory to start search from.\n            Defaults to current working directory.\n\n    Returns:\n        Loaded config (TOML + env vars merged)\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Auto-discover with parent search\n        config = MetaxyConfig.load()\n\n        # Explicit file\n        config = MetaxyConfig.load(\"custom.toml\")\n\n        # Auto-discover without parent search\n        config = MetaxyConfig.load(search_parents=False)\n\n        # Auto-discover from a specific directory\n        config = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n        ```\n    \"\"\"\n    # Search for config file if not explicitly provided\n\n    if config_from_env := os.getenv(\"METAXY_CONFIG\"):\n        config_file = Path(config_from_env)\n\n    if config_file is None and search_parents:\n        config_file = cls._discover_config_with_parents(auto_discovery_start)\n\n    # For explicit file, temporarily patch the TomlConfigSettingsSource\n    # to use that file, then use normal instantiation\n    # This ensures env vars still work\n\n    if config_file:\n        # Create a custom settings source class for this file\n        toml_path = Path(config_file)\n\n        class CustomTomlSource(TomlConfigSettingsSource):\n            def __init__(self, settings_cls: type[BaseSettings]):\n                # Skip auto-discovery, use explicit file\n                super(TomlConfigSettingsSource, self).__init__(settings_cls)\n                self.toml_file = toml_path\n                self.toml_data = self._load_toml()\n\n        # Customize sources to use custom TOML file\n        original_method = cls.settings_customise_sources\n\n        @classmethod\n        def custom_sources(\n            cls_inner,\n            settings_cls,\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n        ):\n            toml_settings = CustomTomlSource(settings_cls)\n            return (init_settings, env_settings, toml_settings)\n\n        # Temporarily replace method\n        cls.settings_customise_sources = custom_sources  # ty: ignore[invalid-assignment]\n        config = cls()\n        cls.settings_customise_sources = original_method  # ty: ignore[invalid-assignment]\n        # Store the resolved config file path\n        config._config_file = toml_path.resolve()\n    else:\n        # Use default sources (auto-discovery + env vars)\n        config = cls()\n        # No config file used\n        config._config_file = None\n\n    cls.set(config)\n\n    # Load plugins after config is set (plugins may access MetaxyConfig.get())\n    config._load_plugins()\n\n    return config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_store","title":"metaxy.MetaxyConfig.get_store","text":"<pre><code>get_store(\n    name: str | None = None,\n    *,\n    expected_type: Literal[None] = None,\n    **kwargs: Any,\n) -&gt; MetadataStore\n</code></pre><pre><code>get_store(\n    name: str | None = None,\n    *,\n    expected_type: type[StoreTypeT],\n    **kwargs: Any,\n) -&gt; StoreTypeT\n</code></pre> <p>Instantiate metadata store by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Store name (uses config.store if None)</p> </li> <li> <code>expected_type</code>               (<code>type[StoreTypeT] | None</code>, default:                   <code>None</code> )           \u2013            <p>Expected type of the store. If the actual store type does not match the expected type, a <code>TypeError</code> is raised.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the store constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore | StoreTypeT</code>           \u2013            <p>Instantiated metadata store</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store name not found in config, or if fallback stores have different hash algorithms than the parent store</p> </li> <li> <code>ImportError</code>             \u2013            <p>If store class cannot be imported</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the actual store type does not match the expected type</p> </li> </ul> Example <pre><code>store = config.get_store(\"prod\")\n\n# Use default store\nstore = config.get_store()\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>def get_store(\n    self,\n    name: str | None = None,\n    *,\n    expected_type: type[StoreTypeT] | None = None,\n    **kwargs: Any,\n) -&gt; \"MetadataStore | StoreTypeT\":\n    \"\"\"Instantiate metadata store by name.\n\n    Args:\n        name: Store name (uses config.store if None)\n        expected_type: Expected type of the store.\n            If the actual store type does not match the expected type, a `TypeError` is raised.\n        **kwargs: Additional keyword arguments to pass to the store constructor.\n\n    Returns:\n        Instantiated metadata store\n\n    Raises:\n        ValueError: If store name not found in config, or if fallback stores\n            have different hash algorithms than the parent store\n        ImportError: If store class cannot be imported\n        TypeError: If the actual store type does not match the expected type\n\n    Example:\n        ```py\n        store = config.get_store(\"prod\")\n\n        # Use default store\n        store = config.get_store()\n        ```\n    \"\"\"\n    from metaxy.versioning.types import HashAlgorithm\n\n    if len(self.stores) == 0:\n        raise InvalidConfigError.from_config(\n            self,\n            \"No Metaxy stores available. They should be configured in metaxy.toml|pyproject.toml or via environment variables.\",\n        )\n\n    name = name or self.store\n\n    if name not in self.stores:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' not found in config. Available stores: {list(self.stores.keys())}\",\n        )\n\n    store_config = self.stores[name]\n\n    # Get store class (lazily imported on first access)\n    try:\n        store_class = store_config.type\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to import store class '{store_config.type_path}' for store '{name}': {e}\",\n        ) from e\n\n    if expected_type is not None and not issubclass(store_class, expected_type):\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' is not of type '{expected_type.__name__}'\",\n        )\n\n    # Extract configuration and prepare for typed config model\n    config_copy = store_config.config.copy()\n\n    # Get hash_algorithm from config (if specified) and convert to enum\n    configured_hash_algorithm = config_copy.get(\"hash_algorithm\")\n    if configured_hash_algorithm is not None:\n        # Convert string to enum if needed\n        if isinstance(configured_hash_algorithm, str):\n            configured_hash_algorithm = HashAlgorithm(configured_hash_algorithm)\n            config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n    else:\n        # Don't set a default here - let the store choose its own default\n        configured_hash_algorithm = None\n\n    # Get the store's config model class and create typed config\n    config_model_cls = store_class.config_model()\n\n    # Get auto_create_tables from global config only if the config model supports it\n    if (\n        \"auto_create_tables\" not in config_copy\n        and self.auto_create_tables is not None\n        and \"auto_create_tables\" in config_model_cls.model_fields\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"auto_create_tables\"] = self.auto_create_tables\n\n    # Separate kwargs into config fields and extra constructor args\n    config_fields = set(config_model_cls.model_fields.keys())\n    extra_kwargs = {}\n    for key, value in kwargs.items():\n        if key in config_fields:\n            config_copy[key] = value\n        else:\n            extra_kwargs[key] = value\n\n    try:\n        typed_config = config_model_cls.model_validate(config_copy)\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to validate config for store '{name}': {e}\",\n        ) from e\n\n    # Instantiate using from_config() - fallback stores are resolved via MetaxyConfig.get()\n    # Use self.use() to ensure this config is available for fallback resolution\n    try:\n        with self.use():\n            store = store_class.from_config(typed_config, name=name, **extra_kwargs)\n    except InvalidConfigError:\n        # Don't re-wrap InvalidConfigError (e.g., from nested fallback store resolution)\n        raise\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to instantiate store '{name}' ({store_class.__name__}): {e}\",\n        ) from e\n\n    # Verify the store actually uses the hash algorithm we configured\n    # (in case a store subclass overrides the default or ignores the parameter)\n    # Only check if we explicitly configured a hash algorithm\n    if configured_hash_algorithm is not None and store.hash_algorithm != configured_hash_algorithm:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' ({store_class.__name__}) was configured with \"\n            f\"hash_algorithm='{configured_hash_algorithm.value}' but is using \"\n            f\"'{store.hash_algorithm.value}'. The store class may have overridden \"\n            f\"the hash algorithm. All stores must use the same hash algorithm.\",\n        )\n\n    if expected_type is not None and not isinstance(store, expected_type):\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' is not of type '{expected_type.__name__}'\",\n        )\n\n    return store\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.to_toml","title":"metaxy.MetaxyConfig.to_toml","text":"<pre><code>to_toml() -&gt; str\n</code></pre> <p>Serialize to TOML string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>TOML representation of this configuration.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>def to_toml(self) -&gt; str:\n    \"\"\"Serialize to TOML string.\n\n    Returns:\n        TOML representation of this configuration.\n    \"\"\"\n    data = self.model_dump(mode=\"json\", by_alias=True)\n    # Remove None values (TOML doesn't support them)\n    data = _remove_none_values(data)\n    return tomli_w.dumps(data)\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig","title":"metaxy.StoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for a single metadata store.</p> Example <pre><code>store_config = StoreConfig(\n    type=\"metaxy_delta.DeltaMetadataStore\",\n    config={\n        \"root_path\": \"s3://bucket/metadata\",\n        \"region\": \"us-west-2\",\n        \"fallback_stores\": [\"prod\"],\n    },\n)\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.StoreConfig.type","title":"metaxy.StoreConfig.type  <code>cached</code> <code>property</code>","text":"<pre><code>type: type[Any]\n</code></pre> <p>Get the store class, importing lazily on first access.</p> <p>Returns:</p> <ul> <li> <code>type[Any]</code>           \u2013            <p>The metadata store class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the store class cannot be imported</p> </li> </ul>"},{"location":"reference/api/config/#metaxy.StoreConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.StoreConfig.to_toml","title":"metaxy.StoreConfig.to_toml","text":"<pre><code>to_toml() -&gt; str\n</code></pre> <p>Serialize to TOML string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>TOML representation of this store configuration.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>def to_toml(self) -&gt; str:\n    \"\"\"Serialize to TOML string.\n\n    Returns:\n        TOML representation of this store configuration.\n    \"\"\"\n    data = self.model_dump(mode=\"json\", by_alias=True)\n    return tomli_w.dumps(data)\n</code></pre>"},{"location":"reference/api/config/#metaxy.config.InvalidConfigError","title":"metaxy.config.InvalidConfigError","text":"<pre><code>InvalidConfigError(\n    message: str, *, config_file: Path | None = None\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Raised when Metaxy configuration is invalid.</p> <p>This error includes helpful context about where the configuration was loaded from and how environment variables can affect configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    config_file: Path | None = None,\n):\n    self.config_file = config_file\n    self.base_message = message\n\n    # Build the full error message with context\n    parts = [message]\n\n    if config_file:\n        parts.append(f\"Config file: {config_file}\")\n\n    parts.append(\"Note: METAXY_* environment variables can override config file settings \")\n\n    super().__init__(\"\\n\".join(parts))\n</code></pre>"},{"location":"reference/api/config/#metaxy.config.InvalidConfigError-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.config.InvalidConfigError.from_config","title":"metaxy.config.InvalidConfigError.from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: MetaxyConfig, message: str\n) -&gt; InvalidConfigError\n</code></pre> <p>Create an InvalidConfigError from a MetaxyConfig instance.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetaxyConfig</code>)           \u2013            <p>The MetaxyConfig instance that has the invalid configuration.</p> </li> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>The error message describing what's wrong.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InvalidConfigError</code>           \u2013            <p>An InvalidConfigError with context from the config.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef from_config(cls, config: \"MetaxyConfig\", message: str) -&gt; \"InvalidConfigError\":\n    \"\"\"Create an InvalidConfigError from a MetaxyConfig instance.\n\n    Args:\n        config: The MetaxyConfig instance that has the invalid configuration.\n        message: The error message describing what's wrong.\n\n    Returns:\n        An InvalidConfigError with context from the config.\n    \"\"\"\n    return cls(message, config_file=config._config_file)\n</code></pre>"},{"location":"reference/api/constants/","title":"Constants","text":""},{"location":"reference/api/constants/#metaxy.models.constants","title":"Constants","text":"<p>Shared constants for system-managed column names.</p> <p>All system columns use the metaxy_ prefix to avoid conflicts with user columns.</p>"},{"location":"reference/api/constants/#metaxy.models.constants-attributes","title":"Attributes","text":""},{"location":"reference/api/constants/#metaxy.models.constants.DEFAULT_CODE_VERSION","title":"metaxy.models.constants.DEFAULT_CODE_VERSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CODE_VERSION = '__metaxy_initial__'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMN_PREFIX","title":"metaxy.models.constants.SYSTEM_COLUMN_PREFIX  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMN_PREFIX = 'metaxy_'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD","title":"metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE_BY_FIELD = (\n    f\"{SYSTEM_COLUMN_PREFIX}provenance_by_field\"\n)\n</code></pre> <p>Field-level provenance hashes (struct column mapping field names to hashes).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE","title":"metaxy.models.constants.METAXY_PROVENANCE  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE = f'{SYSTEM_COLUMN_PREFIX}provenance'\n</code></pre> <p>Hash of<code>metaxy_provenance_by_field</code> -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_VERSION","title":"metaxy.models.constants.METAXY_FEATURE_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}feature_version\"\n)\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_SNAPSHOT_VERSION","title":"metaxy.models.constants.METAXY_SNAPSHOT_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_SNAPSHOT_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}snapshot_version\"\n)\n</code></pre> <p>Hash of the entire feature graph snapshot (recorded during deployment).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DEFINITION_VERSION","title":"metaxy.models.constants.METAXY_DEFINITION_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DEFINITION_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}definition_version\"\n)\n</code></pre> <p>Hash of the complete feature definition including Pydantic schema and feature spec.</p> <p>This comprehensive hash captures the feature definition (excluding project): - Pydantic model schema (field types, descriptions, validators, serializers, etc.) - Feature specification (dependencies, fields, code_versions, metadata)</p> <p>Project is stored separately. Used in system tables to detect when ANY part of a feature changes.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD","title":"metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION_BY_FIELD = (\n    f\"{SYSTEM_COLUMN_PREFIX}data_version_by_field\"\n)\n</code></pre> <p>Field-level data version hashes (struct column mapping field names to version hashes).</p> <p>Similar to provenance_by_field, but can be user-overridden to implement custom versioning (e.g., content hashes, timestamps, semantic versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION","title":"metaxy.models.constants.METAXY_DATA_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION = f'{SYSTEM_COLUMN_PREFIX}data_version'\n</code></pre> <p>Hash of metaxy_data_version_by_field -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_CREATED_AT","title":"metaxy.models.constants.METAXY_CREATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_CREATED_AT = f'{SYSTEM_COLUMN_PREFIX}created_at'\n</code></pre> <p>Timestamp when the metadata row was created.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_UPDATED_AT","title":"metaxy.models.constants.METAXY_UPDATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_UPDATED_AT = f'{SYSTEM_COLUMN_PREFIX}updated_at'\n</code></pre> <p>Timestamp when the metadata row was last updated (written to the store).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DELETED_AT","title":"metaxy.models.constants.METAXY_DELETED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_DELETED_AT = f'{SYSTEM_COLUMN_PREFIX}deleted_at'\n</code></pre> <p>Timestamp when the metadata row was soft-deleted.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_MATERIALIZATION_ID","title":"metaxy.models.constants.METAXY_MATERIALIZATION_ID  <code>module-attribute</code>","text":"<pre><code>METAXY_MATERIALIZATION_ID = (\n    f\"{SYSTEM_COLUMN_PREFIX}materialization_id\"\n)\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID, Airflow Run ID) for tracking pipeline executions.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.ALL_SYSTEM_COLUMNS","title":"metaxy.models.constants.ALL_SYSTEM_COLUMNS  <code>module-attribute</code>","text":"<pre><code>ALL_SYSTEM_COLUMNS = frozenset(\n    {\n        METAXY_PROVENANCE_BY_FIELD,\n        METAXY_PROVENANCE,\n        METAXY_FEATURE_VERSION,\n        METAXY_SNAPSHOT_VERSION,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_DELETED_AT,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre> <p>All Metaxy-managed column names that are injected into feature tables.</p>"},{"location":"reference/api/constants/#metaxy.models.constants._DROPPABLE_COLUMNS","title":"metaxy.models.constants._DROPPABLE_COLUMNS  <code>module-attribute</code>","text":"<pre><code>_DROPPABLE_COLUMNS = frozenset(\n    {\n        METAXY_FEATURE_VERSION,\n        METAXY_SNAPSHOT_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_DELETED_AT,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants._COLUMNS_TO_DROP_BEFORE_JOIN","title":"metaxy.models.constants._COLUMNS_TO_DROP_BEFORE_JOIN  <code>module-attribute</code>","text":"<pre><code>_COLUMNS_TO_DROP_BEFORE_JOIN = frozenset(\n    {\n        METAXY_FEATURE_VERSION,\n        METAXY_SNAPSHOT_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE","title":"metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMNS_WITH_LINEAGE: frozenset[str] = frozenset(\n    {\n        METAXY_PROVENANCE_BY_FIELD,\n        METAXY_PROVENANCE,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n    }\n)\n</code></pre>"},{"location":"reference/api/types/","title":"Types","text":""},{"location":"reference/api/types/#versioning-engine","title":"Versioning Engine","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement","title":"metaxy.versioning.types.LazyIncrement  <code>dataclass</code>","text":"<pre><code>LazyIncrement(\n    *,\n    added: LazyFrame[Any],\n    changed: LazyFrame[Any],\n    removed: LazyFrame[Any],\n    input: LazyFrame[Any] | None = None,\n)\n</code></pre> <p>Result of an incremental update containing lazy dataframes.</p> <p>Attributes:</p> <ul> <li> <code>added</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>New samples from upstream not present in current metadata.</p> </li> <li> <code>changed</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples with different provenance.</p> </li> <li> <code>removed</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples in current metadata but not in upstream state.</p> </li> <li> <code>input</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Joined upstream metadata with <code>FeatureDep</code> rules applied.</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.collect","title":"metaxy.versioning.types.LazyIncrement.collect","text":"<pre><code>collect(**kwargs: Any) -&gt; Increment\n</code></pre> <p>Collect all lazy frames to eager DataFrames.</p> <p>Tip</p> <p>If all lazy frames are Polars frames, leverages <code>polars.collect_all</code> to optimize the collection process and take advantage of common subplan elimination.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>backend-specific keyword arguments to pass to the collect method of the lazy frames.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment</code> (              <code>Increment</code> )          \u2013            <p>The collected increment.</p> </li> </ul> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self, **kwargs: Any) -&gt; Increment:\n    \"\"\"Collect all lazy frames to eager DataFrames.\n\n    !!! tip\n        If all lazy frames are Polars frames, leverages\n        [`polars.collect_all`](https://docs.pola.rs/api/python/stable/reference/api/polars.collect_all.html)\n        to optimize the collection process and take advantage of common subplan elimination.\n\n    Args:\n        **kwargs: backend-specific keyword arguments to pass to the collect method of the lazy frames.\n\n    Returns:\n        Increment: The collected increment.\n    \"\"\"\n    if (\n        self.added.implementation\n        == self.changed.implementation\n        == self.removed.implementation\n        == nw.Implementation.POLARS\n    ):\n        polars_eager_increment = PolarsLazyIncrement(\n            added=self.added.to_native(),\n            changed=self.changed.to_native(),\n            removed=self.removed.to_native(),\n        ).collect(**kwargs)\n        return Increment(\n            added=nw.from_native(polars_eager_increment.added),\n            changed=nw.from_native(polars_eager_increment.changed),\n            removed=nw.from_native(polars_eager_increment.removed),\n        )\n    else:\n        return Increment(\n            added=self.added.collect(**kwargs),\n            changed=self.changed.collect(**kwargs),\n            removed=self.removed.collect(**kwargs),\n        )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.to_polars","title":"metaxy.versioning.types.LazyIncrement.to_polars","text":"<pre><code>to_polars() -&gt; PolarsLazyIncrement\n</code></pre> <p>Convert to Polars.</p> <p>Tip</p> <p>If the Narwhals lazy frames are already backed by Polars, this is a no-op.</p> <p>Warning</p> <p>If the Narwhals lazy frames are not backed by Polars, this will trigger a full materialization for them.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def to_polars(self) -&gt; PolarsLazyIncrement:\n    \"\"\"Convert to Polars.\n\n    !!! tip\n        If the Narwhals lazy frames are already backed by Polars, this is a no-op.\n\n    !!! warning\n        If the Narwhals lazy frames are **not** backed by Polars, this will\n        trigger a full materialization for them.\n    \"\"\"\n    return PolarsLazyIncrement(\n        added=lazy_frame_to_polars(self.added),\n        changed=lazy_frame_to_polars(self.changed),\n        removed=lazy_frame_to_polars(self.removed),\n        input=lazy_frame_to_polars(self.input) if self.input is not None else None,\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment","title":"metaxy.versioning.types.Increment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing eager dataframes.</p> <p>Contains three sets of samples:</p> <ul> <li> <p>added: New samples from upstream not present in current metadata</p> </li> <li> <p>changed: Samples with different provenance</p> </li> <li> <p>removed: Samples in current metadata but not in upstream state</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.Increment.collect","title":"metaxy.versioning.types.Increment.collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>Convenience method that's a no-op.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; \"Increment\":\n    \"\"\"Convenience method that's a no-op.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment.to_polars","title":"metaxy.versioning.types.Increment.to_polars","text":"<pre><code>to_polars() -&gt; PolarsIncrement\n</code></pre> <p>Convert to Polars.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def to_polars(self) -&gt; PolarsIncrement:\n    \"\"\"Convert to Polars.\"\"\"\n    return PolarsIncrement(\n        added=self.added.to_polars(),\n        changed=self.changed.to_polars(),\n        removed=self.removed.to_polars(),\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement","title":"metaxy.versioning.types.PolarsIncrement","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Like <code>Increment</code>, but converted to Polars frames.</p>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.added","title":"metaxy.versioning.types.PolarsIncrement.added  <code>instance-attribute</code>","text":"<pre><code>added: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.changed","title":"metaxy.versioning.types.PolarsIncrement.changed  <code>instance-attribute</code>","text":"<pre><code>changed: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.removed","title":"metaxy.versioning.types.PolarsIncrement.removed  <code>instance-attribute</code>","text":"<pre><code>removed: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement","title":"metaxy.versioning.types.PolarsLazyIncrement  <code>dataclass</code>","text":"<pre><code>PolarsLazyIncrement(\n    *,\n    added: LazyFrame,\n    changed: LazyFrame,\n    removed: LazyFrame,\n    input: LazyFrame | None = None,\n)\n</code></pre> <p>Like <code>LazyIncrement</code>, but converted to Polars lazy frames.</p> <p>Attributes:</p> <ul> <li> <code>added</code>               (<code>LazyFrame</code>)           \u2013            <p>New samples from upstream not present in current metadata.</p> </li> <li> <code>changed</code>               (<code>LazyFrame</code>)           \u2013            <p>Samples with different provenance.</p> </li> <li> <code>removed</code>               (<code>LazyFrame</code>)           \u2013            <p>Samples in current metadata but not in upstream state.</p> </li> <li> <code>input</code>               (<code>LazyFrame | None</code>)           \u2013            <p>Joined upstream metadata with <code>FeatureDep</code> rules applied.</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.added","title":"metaxy.versioning.types.PolarsLazyIncrement.added  <code>instance-attribute</code>","text":"<pre><code>added: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.changed","title":"metaxy.versioning.types.PolarsLazyIncrement.changed  <code>instance-attribute</code>","text":"<pre><code>changed: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.removed","title":"metaxy.versioning.types.PolarsLazyIncrement.removed  <code>instance-attribute</code>","text":"<pre><code>removed: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.input","title":"metaxy.versioning.types.PolarsLazyIncrement.input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input: LazyFrame | None = None\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.collect","title":"metaxy.versioning.types.PolarsLazyIncrement.collect","text":"<pre><code>collect(**kwargs: Any) -&gt; PolarsIncrement\n</code></pre> <p>Collect into a <code>PolarsIncrement</code>.</p> <p>Tip</p> <p>Leverages <code>polars.collect_all</code> to optimize the collection process and take advantage of common subplan elimination.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>backend-specific keyword arguments to pass to the collect method of the lazy frames.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PolarsIncrement</code> (              <code>PolarsIncrement</code> )          \u2013            <p>The collected increment.</p> </li> </ul> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self, **kwargs: Any) -&gt; PolarsIncrement:\n    \"\"\"Collect into a [`PolarsIncrement`][metaxy.versioning.types.PolarsIncrement].\n\n    !!! tip\n        Leverages [`polars.collect_all`](https://docs.pola.rs/api/python/stable/reference/api/polars.collect_all.html)\n        to optimize the collection process and take advantage of common subplan elimination.\n\n    Args:\n        **kwargs: backend-specific keyword arguments to pass to the collect method of the lazy frames.\n\n    Returns:\n        PolarsIncrement: The collected increment.\n    \"\"\"\n    added, changed, removed = pl.collect_all([self.added, self.changed, self.removed], **kwargs)\n    return PolarsIncrement(added, changed, removed)\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm","title":"metaxy.HashAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Supported hash algorithms for field provenance calculation.</p> <p>These algorithms are chosen for: - Speed (non-cryptographic hashes preferred) - Cross-database availability - Good collision resistance for field provenance calculation</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH64","title":"metaxy.HashAlgorithm.XXHASH64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH64 = 'xxhash64'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH32","title":"metaxy.HashAlgorithm.XXHASH32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH32 = 'xxhash32'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.WYHASH","title":"metaxy.HashAlgorithm.WYHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WYHASH = 'wyhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.SHA256","title":"metaxy.HashAlgorithm.SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'sha256'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.MD5","title":"metaxy.HashAlgorithm.MD5  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MD5 = 'md5'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.FARMHASH","title":"metaxy.HashAlgorithm.FARMHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FARMHASH = 'farmhash'\n</code></pre>"},{"location":"reference/api/types/#keys","title":"Keys","text":"<p>Types for working with feature and field keys.</p>"},{"location":"reference/api/types/#canonical-keys","title":"Canonical Keys","text":""},{"location":"reference/api/types/#metaxy.FeatureKey","title":"metaxy.FeatureKey","text":"<pre><code>FeatureKey(parts: str)\n</code></pre><pre><code>FeatureKey(parts: Sequence[str])\n</code></pre><pre><code>FeatureKey(parts: FeatureKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Feature key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFeatureKey(\"a/b/c\")  # String format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey([\"a\", \"b\", \"c\"])  # List format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(FeatureKey([\"a\", \"b\", \"c\"]))  # FeatureKey copy\n# FeatureKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(\n    self,\n    parts: str | Sequence[str] | FeatureKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FeatureKey.model_dump","title":"metaxy.FeatureKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to string format for JSON dict key compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to string format for JSON dict key compatibility.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__hash__","title":"metaxy.FeatureKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__eq__","title":"metaxy.FeatureKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.to_column_suffix","title":"metaxy.FeatureKey.to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey","title":"metaxy.FieldKey","text":"<pre><code>FieldKey(parts: str)\n</code></pre><pre><code>FieldKey(parts: Sequence[str])\n</code></pre><pre><code>FieldKey(parts: FieldKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(\n    self,\n    parts: str | Sequence[str] | FieldKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FieldKey.model_dump","title":"metaxy.FieldKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to string format for JSON dict key compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to string format for JSON dict key compatibility.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__hash__","title":"metaxy.FieldKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__eq__","title":"metaxy.FieldKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#type-annotations","title":"Type Annotations","text":"<p>These are typically used to annotate function parameters. Most APIs in Metaxy accepts them and perform type coercion into canonical types.</p>"},{"location":"reference/api/types/#metaxy.CoercibleToFeatureKey","title":"metaxy.CoercibleToFeatureKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFeatureKey: TypeAlias = \"str | Sequence[str] | FeatureKey | type[BaseFeature] | FeatureDefinition | FeatureSpec\"\n</code></pre> <p>Type alias for values that can be coerced to a <code>FeatureKey</code>.</p> <p>Accepted formats:</p> <ul> <li><code>str</code>: Slash-separated string like <code>\"raw/video\"</code> or <code>\"ml/embeddings/v2\"</code></li> <li><code>Sequence[str]</code>: sequences of parts like <code>[\"user\", \"profile\"]</code></li> <li><code>FeatureKey</code>: Pass through unchanged</li> <li><code>type[BaseFeature]</code>: Any <code>BaseFeature</code> subclass - extracts its key via <code>.spec().key</code></li> <li><code>FeatureDefinition</code>: Extracts its key via <code>.key</code></li> <li><code>FeatureSpec</code>: Extracts its key via <code>.key</code></li> </ul> Example <pre><code>key1 = \"raw/video\"\nkey2 = [\"raw\", \"video\"]\nkey3 = mx.FeatureKey(\"raw/video\")\nkey4 = MyFeatureClass  # where MyFeatureClass is a BaseFeature subclass\nkey5 = mx.FeatureDefinition(\"raw/video\", ...)\nkey6 = mx.FeatureSpec(key=\"raw/video\", id_columns=(\"id\",))\n</code></pre>"},{"location":"reference/api/types/#metaxy.CoercibleToFieldKey","title":"metaxy.CoercibleToFieldKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFieldKey: TypeAlias = (\n    str | Sequence[str] | FieldKey\n)\n</code></pre> <p>Type alias for values that can be coerced to a <code>FieldKey</code>.</p> <p>Accepted formats:</p> <ul> <li><code>str</code>: Slash-separated string like <code>\"audio/english\"</code></li> <li><code>Sequence[str]</code>: sequence of parts like <code>[\"audio\", \"english\"]</code></li> <li><code>FieldKey</code>: Pass through unchanged</li> </ul> Example <pre><code>key1 = \"audio/english\"\nkey2 = [\"audio\", \"english\"]\nkey3 = mx.FieldKey(\"audio/english\")\n</code></pre>"},{"location":"reference/api/types/#pydantic-type-annotations","title":"Pydantic Type Annotations","text":"<p>These types are used for type coercion into canonical types with Pydantic.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKey","title":"metaxy.ValidatedFeatureKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKey: TypeAlias = FeatureKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKey","title":"metaxy.ValidatedFieldKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKey: TypeAlias = FieldKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequence","title":"metaxy.ValidatedFeatureKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequence: TypeAlias = Sequence[\n    ValidatedFeatureKey\n]\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequence","title":"metaxy.ValidatedFieldKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequence: TypeAlias = Sequence[\n    ValidatedFieldKey\n]\n</code></pre>"},{"location":"reference/api/types/#adapters","title":"Adapters","text":"<p>These can perform type coercsion into canonical types in non-pydantic code.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeyAdapter","title":"metaxy.ValidatedFeatureKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeyAdapter: TypeAdapter[\n    ValidatedFeatureKey\n] = TypeAdapter(ValidatedFeatureKey)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequenceAdapter","title":"metaxy.ValidatedFeatureKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequenceAdapter: TypeAdapter[\n    ValidatedFeatureKeySequence\n] = TypeAdapter(ValidatedFeatureKeySequence)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeyAdapter","title":"metaxy.ValidatedFieldKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeyAdapter: TypeAdapter[ValidatedFieldKey] = (\n    TypeAdapter(ValidatedFieldKey)\n)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequenceAdapter","title":"metaxy.ValidatedFieldKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequenceAdapter: TypeAdapter[\n    ValidatedFieldKeySequence\n] = TypeAdapter(ValidatedFieldKeySequence)\n</code></pre>"},{"location":"reference/api/types/#other-types","title":"Other Types","text":""},{"location":"reference/api/types/#metaxy.models.types.SnapshotPushResult","title":"metaxy.models.types.SnapshotPushResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of recording a feature graph snapshot.</p> <p>Attributes:</p> <ul> <li> <code>snapshot_version</code>               (<code>str</code>)           \u2013            <p>The deterministic hash of the graph snapshot</p> </li> <li> <code>already_pushed</code>               (<code>bool</code>)           \u2013            <p>True if this snapshot_version was already pushed previously</p> </li> <li> <code>updated_features</code>               (<code>list[str]</code>)           \u2013            <p>List of feature keys with updated information (changed definition_version)</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.IDColumns","title":"metaxy.IDColumns  <code>module-attribute</code>","text":"<pre><code>IDColumns: TypeAlias = Sequence[str]\n</code></pre>"},{"location":"reference/api/utils/","title":"Metaxy Utils API","text":""},{"location":"reference/api/utils/#metaxy.utils","title":"metaxy.utils","text":"<p>Utility modules for Metaxy.</p>"},{"location":"reference/api/utils/#metaxy.utils-classes","title":"Classes","text":""},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter","title":"metaxy.utils.BatchedMetadataWriter","text":"<pre><code>BatchedMetadataWriter(\n    store: MetadataStore,\n    flush_batch_size: int | None = None,\n    flush_interval: float = 2.0,\n)\n</code></pre> <p>Batched metadata writer with background flush thread.</p> <p>Queues data and writes to a MetadataStore in batches either when:</p> <ul> <li>The batch reaches <code>flush_batch_size</code> rows (if set)</li> <li><code>flush_interval</code> seconds have passed since last flush</li> </ul> <p>The writer runs a background thread that handles flushing, allowing the main thread to continue processing data without blocking on writes.</p> Example <pre><code>import polars as pl\n\nwith mx.BatchedMetadataWriter(store) as writer:\n    batch = {\n        MyFeature: pl.DataFrame(\n            {\n                \"id\": [\"x\"],\n                \"metaxy_provenance_by_field\": [{\"part_1\": \"h1\", \"part_2\": \"h2\"}],\n            }\n        )\n    }\n    writer.put(batch)\n\nwith store:\n    assert len(store.read(MyFeature).collect()) == 1\n</code></pre> Manual lifecycle management <p> <pre><code>writer = mx.BatchedMetadataWriter(store)\nwriter.start()\ntry:\n    for batch_dict in data_stream:\n        writer.put(batch_dict)\nfinally:\n    rows_written = writer.stop()\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>The MetadataStore to write to. Must be opened before use.</p> </li> <li> <code>flush_batch_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows to accumulate before flushing. If not set, flushes are only triggered by <code>flush_interval</code> or when stopping the writer.</p> <p>Note</p> <p>Setting this triggers row counting which materializes lazy frames.</p> </li> <li> <code>flush_interval</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Maximum seconds between flushes. The timer resets after the end of each flush.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the background thread encounters an error during flush.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __init__(\n    self,\n    store: MetadataStore,\n    flush_batch_size: int | None = None,\n    flush_interval: float = 2.0,\n) -&gt; None:\n    self._store = store\n    self._flush_batch_size = flush_batch_size\n    self._flush_interval = flush_interval\n\n    self._queue: queue.Queue[QueueItem] = queue.Queue()\n    self._should_stop = threading.Event()\n    self._stopped = threading.Event()\n    self._num_written: dict[FeatureKey, int] = {}\n    self._lock = threading.Lock()\n    self._error: BaseException | None = None\n\n    self._thread: threading.Thread | None = None\n    self._started = False\n\n    # Capture context at construction time to propagate to background thread\n    # This is necessary because ContextVars don't propagate to child threads\n    self._graph = FeatureGraph.get_active()\n    self._config = MetaxyConfig.get()\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter-attributes","title":"Attributes","text":""},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.num_written","title":"num_written  <code>property</code>","text":"<pre><code>num_written: dict[FeatureKey, int]\n</code></pre> <p>Number of rows written so far per feature.</p> <p>This property is thread-safe and can be called while the writer is still running to check progress.</p> <p>Returns:</p> <ul> <li> <code>dict[FeatureKey, int]</code>           \u2013            <p>Dict mapping feature keys to number of rows successfully flushed to the store.</p> </li> </ul>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.has_error","title":"has_error  <code>property</code>","text":"<pre><code>has_error: bool\n</code></pre> <p>Check if the writer has encountered an error.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the background thread encountered an error, False otherwise.</p> </li> </ul>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter-functions","title":"Functions","text":""},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the background flush thread.</p> <p>This method must be called before putting data. When using the writer as a context manager, this is called automatically on entry.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the writer has already been started.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the background flush thread.\n\n    This method must be called before putting data. When using the writer\n    as a context manager, this is called automatically on entry.\n\n    Raises:\n        RuntimeError: If the writer has already been started.\n    \"\"\"\n    if self._started:\n        raise RuntimeError(\"Writer has already been started\")\n\n    self._thread = threading.Thread(target=self._run, daemon=True)\n    self._thread.start()\n    self._started = True\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.put","title":"put","text":"<pre><code>put(\n    batches: Mapping[CoercibleToFeatureKey, IntoFrame],\n) -&gt; None\n</code></pre> <p>Queue batches for writing.</p> <p>The batches are accumulated per-feature and written together using <code>[MetadataStore.write_multi][]</code>.</p> <p>Parameters:</p> <ul> <li> <code>batches</code>               (<code>Mapping[CoercibleToFeatureKey, IntoFrame]</code>)           \u2013            <p>Mapping from feature keys to dataframes. Dataframes can be of any type supported by Narwhals.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the writer has not been started, has been stopped, or encountered an error.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def put(self, batches: Mapping[CoercibleToFeatureKey, IntoFrame]) -&gt; None:\n    \"\"\"Queue batches for writing.\n\n    The batches are accumulated per-feature and written together using\n    `[MetadataStore.write_multi][]`.\n\n    Args:\n        batches: Mapping from feature keys to dataframes.\n            Dataframes can be of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n\n    Raises:\n        RuntimeError: If the writer has not been started, has been stopped,\n            or encountered an error.\n    \"\"\"\n    self._check_can_put()\n\n    # Convert all keys and values\n    converted: dict[FeatureKey, Frame] = {}\n    for key, batch in batches.items():\n        feature_key = ValidatedFeatureKeyAdapter.validate_python(key)\n        batch_nw = self._to_narwhals(batch)\n        converted[feature_key] = batch_nw\n\n    self._queue.put(converted)\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.stop","title":"stop","text":"<pre><code>stop(timeout: float = 30.0) -&gt; dict[FeatureKey, int]\n</code></pre> <p>Signal stop and wait for flush to complete.</p> <p>This method signals the background thread to stop, waits for it to finish flushing any remaining data, and returns the number of rows written per feature.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Maximum seconds to wait for the background thread. Defaults to 30.0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[FeatureKey, int]</code>           \u2013            <p>Dict mapping feature keys to number of rows written.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the background thread encountered an error during flush.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def stop(self, timeout: float = 30.0) -&gt; dict[FeatureKey, int]:\n    \"\"\"Signal stop and wait for flush to complete.\n\n    This method signals the background thread to stop, waits for it to\n    finish flushing any remaining data, and returns the number of rows\n    written per feature.\n\n    Args:\n        timeout: Maximum seconds to wait for the background thread.\n            Defaults to 30.0.\n\n    Returns:\n        Dict mapping feature keys to number of rows written.\n\n    Raises:\n        RuntimeError: If the background thread encountered an error during flush.\n    \"\"\"\n    if not self._started or self._thread is None:\n        return {}\n\n    self._should_stop.set()\n    self._thread.join(timeout=timeout)\n\n    if self._thread.is_alive():\n        logger.warning(\"BatchedMetadataWriter did not stop within %.1fs\", timeout)\n\n    if self._error is not None:\n        raise RuntimeError(f\"Writer encountered an error: {self._error}\") from self._error\n\n    with self._lock:\n        return dict(self._num_written)\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; BatchedMetadataWriter\n</code></pre> <p>Enter context manager, starting the background thread.</p> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __enter__(self) -&gt; BatchedMetadataWriter:\n    \"\"\"Enter context manager, starting the background thread.\"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BatchedMetadataWriter.__exit__","title":"__exit__","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit context manager, stopping the writer.</p> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit context manager, stopping the writer.\"\"\"\n    self.stop()\n</code></pre>"},{"location":"reference/api/definitions/","title":"Definitions","text":"<p>Metaxy's dependency specification system allows users to express dependencies between their features and their fields.</p>"},{"location":"reference/api/definitions/#featurespec","title":"<code>FeatureSpec</code>","text":"<p>FeatureSpec is the core of Metaxy's dependency specification system: it stores all the information about the parents, field mappings, and other metadata associated with a feature.</p>"},{"location":"reference/api/definitions/#feature","title":"Feature","text":"<p>A feature in Metaxy is used to model user-defined metadata. It must have a <code>FeatureSpec</code> instance associated with it. A <code>Feature</code> class is typically associated with a single table in the MetadataStore.</p>"},{"location":"reference/api/definitions/#fieldspec","title":"FieldSpec","text":"<p>A [field] in Metaxy is a logical slices of the data represented by feature metadata. Users are free to define their own fields as is suitable for them.</p> <p>Dependencies between fields are modeled with FieldDep and can be automatic (via field mappings) or explicitly set by users.</p>"},{"location":"reference/api/definitions/#graph","title":"Graph","text":"<p>All features live on a FeatureGraph object. The users don't typically interact with it outside of advanced use cases.</p>"},{"location":"reference/api/definitions/feature-spec/","title":"Feature Spec","text":"<p>Feature specs act as source of truth for all metadata related to features: their dependencies, fields, code versions, and so on.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec","title":"metaxy.FeatureSpec  <code>pydantic-model</code>","text":"<pre><code>FeatureSpec(\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n)\n</code></pre><pre><code>FeatureSpec(\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"additionalProperties\": false,\n      \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", columns=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\",\n          \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        },\n        \"filters\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"SQL-like filter strings applied to this dependency.\",\n          \"title\": \"Filters\"\n        },\n        \"lineage\": {\n          \"$ref\": \"#/$defs/LineageRelationship\",\n          \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n        },\n        \"optional\": {\n          \"default\": false,\n          \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n          \"title\": \"Optional\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"id_columns\": {\n      \"description\": \"Columns that uniquely identify a sample in this feature.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    },\n    \"deps\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FeatureDep\"\n      },\n      \"title\": \"Deps\",\n      \"type\": \"array\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"additionalProperties\": false,\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": true,\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Human-readable description of this feature.\",\n      \"title\": \"Description\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"id_columns\"\n  ],\n  \"title\": \"FeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FeatureKey</code>)             </li> <li> <code>id_columns</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>deps</code>                 (<code>list[FeatureDep]</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldSpec]</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, Any]</code>)             </li> <li> <code>description</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_unique_field_keys</code> </li> <li> <code>validate_id_columns</code> </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(\n    self,\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.id_columns","title":"metaxy.FeatureSpec.id_columns  <code>pydantic-field</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Columns that uniquely identify a sample in this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.metadata","title":"metaxy.FeatureSpec.metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, Any]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.description","title":"metaxy.FeatureSpec.description  <code>pydantic-field</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Human-readable description of this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.deps_by_key","title":"metaxy.FeatureSpec.deps_by_key  <code>cached</code> <code>property</code>","text":"<pre><code>deps_by_key: Mapping[FeatureKey, FeatureDep]\n</code></pre> <p>Get dependencies indexed by their feature key.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.code_version","title":"metaxy.FeatureSpec.code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.feature_spec_version","title":"metaxy.FeatureSpec.feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = mx.FeatureSpec(\n    key=mx.FeatureKey([\"my\", \"feature\"]),\n    id_columns=[\"id\"],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.table_name","title":"metaxy.FeatureSpec.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_unique_field_keys","title":"metaxy.FeatureSpec.validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; Self\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; Self:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(f\"Duplicate field key found: {field.key}. All fields must have unique keys.\")\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_id_columns","title":"metaxy.FeatureSpec.validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; Self\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; Self:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\"id_columns must be non-empty if specified. Use None for default.\")\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#feature-dependencies","title":"Feature Dependencies","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep","title":"metaxy.FeatureDep  <code>pydantic-model</code>","text":"<pre><code>FeatureDep(\n    *,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | type[BaseFeature],\n    columns: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    filters: Sequence[str] | None = None,\n    lineage: LineageRelationship | None = None,\n    optional: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature dependency specification with optional column selection, renaming, and lineage.</p> <p>Attributes:</p> <ul> <li> <code>feature</code>               (<code>ValidatedFeatureKey</code>)           \u2013            <p>The feature key to depend on. Accepts string (\"a/b/c\"), list ([\"a\", \"b\", \"c\"]), FeatureKey instance, or BaseFeature class.</p> </li> <li> <code>columns</code>               (<code>tuple[str, ...] | None</code>)           \u2013            <p>Optional tuple of column names to select from upstream feature. - None (default): Keep all columns from upstream - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.) - Tuple of names: Keep only specified columns (plus system columns)</p> </li> <li> <code>rename</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Optional mapping of old column names to new names. Applied after column selection.</p> </li> <li> <code>fields_mapping</code>               (<code>FieldsMapping</code>)           \u2013            <p>Optional field mapping configuration for automatic field dependency resolution. When provided, fields without explicit deps will automatically map to matching upstream fields. Defaults to using <code>[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]</code>.</p> </li> <li> <code>filters</code>               (<code>tuple[Expr, ...]</code>)           \u2013            <p>Optional SQL-like filter strings applied to this dependency. Automatically parsed into Narwhals expressions (accessible via the <code>filters</code> property). Filters are automatically applied by FeatureDepTransformer after renames during all FeatureDep operations (including resolve_update and version computation).</p> </li> <li> <code>lineage</code>               (<code>LineageRelationship</code>)           \u2013            <p>The lineage relationship between this upstream dependency and the downstream feature. - <code>LineageRelationship.identity()</code> (default): 1:1 relationship, same cardinality - <code>LineageRelationship.aggregation(on=...)</code>: N:1, multiple upstream rows aggregate to one downstream - <code>LineageRelationship.expansion(on=...)</code>: 1:N, one upstream row expands to multiple downstream rows</p> </li> <li> <code>optional</code>               (<code>bool</code>)           \u2013            <p>Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata. Defaults to False (required dependency).</p> </li> </ul> Basic Usage <pre><code># Keep all columns with default field mapping (1:1 lineage)\nmx.FeatureDep(feature=\"upstream\")\n\n# Keep only specific columns\nmx.FeatureDep(feature=\"upstream/feature\", columns=(\"col1\", \"col2\"))\n\n# Rename columns to avoid conflicts\nmx.FeatureDep(feature=\"upstream/feature\", rename={\"old_name\": \"new_name\"})\n\n# SQL filters\nmx.FeatureDep(feature=\"upstream\", filters=[\"age &gt;= 25\", \"status = 'active'\"])\n\n# Optional dependency (left join - samples preserved even if no match)\nmx.FeatureDep(feature=\"enrichment/data\", optional=True)\n</code></pre> Lineage Relationships <pre><code>from metaxy.models.lineage import LineageRelationship\n\n# Aggregation: many sensor readings aggregate to one hourly stat\nmx.FeatureDep(feature=\"sensor_readings\", lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"]))\n\n# Expansion: one video expands to many frames\nmx.FeatureDep(feature=\"video\", lineage=LineageRelationship.expansion(on=[\"video_id\"]))\n\n# Mixed lineage: aggregate from one parent, identity from another\n# In FeatureSpec:\ndeps = [\n    mx.FeatureDep(feature=\"readings\", lineage=LineageRelationship.aggregation(on=[\"sensor_id\"])),\n    mx.FeatureDep(feature=\"sensor_info\", lineage=LineageRelationship.identity()),\n]\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", columns=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\",\n      \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n    },\n    \"columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Columns\"\n    },\n    \"rename\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Rename\"\n    },\n    \"fields_mapping\": {\n      \"$ref\": \"#/$defs/FieldsMapping\"\n    },\n    \"filters\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"SQL-like filter strings applied to this dependency.\",\n      \"title\": \"Filters\"\n    },\n    \"lineage\": {\n      \"$ref\": \"#/$defs/LineageRelationship\",\n      \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n    },\n    \"optional\": {\n      \"default\": false,\n      \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n      \"title\": \"Optional\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FeatureDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>forbid</code></li> </ul> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>ValidatedFeatureKey</code>)             </li> <li> <code>columns</code>                 (<code>tuple[str, ...] | None</code>)             </li> <li> <code>rename</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>fields_mapping</code>                 (<code>FieldsMapping</code>)             </li> <li> <code>sql_filters</code>                 (<code>tuple[str, ...] | None</code>)             </li> <li> <code>lineage</code>                 (<code>LineageRelationship</code>)             </li> <li> <code>optional</code>                 (<code>bool</code>)             </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(\n    self,\n    *,\n    feature: str | Sequence[str] | FeatureKey | type[BaseFeature],\n    columns: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    filters: Sequence[str] | None = None,\n    lineage: LineageRelationship | None = None,\n    optional: bool = False,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.sql_filters","title":"metaxy.FeatureDep.sql_filters  <code>pydantic-field</code>","text":"<pre><code>sql_filters: tuple[str, ...] | None = None\n</code></pre> <p>SQL-like filter strings applied to this dependency.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.lineage","title":"metaxy.FeatureDep.lineage  <code>pydantic-field</code>","text":"<pre><code>lineage: LineageRelationship\n</code></pre> <p>Lineage relationship between this upstream dependency and the downstream feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.optional","title":"metaxy.FeatureDep.optional  <code>pydantic-field</code>","text":"<pre><code>optional: bool = False\n</code></pre> <p>Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.filters","title":"metaxy.FeatureDep.filters  <code>cached</code> <code>property</code>","text":"<pre><code>filters: tuple[Expr, ...]\n</code></pre> <p>Parse sql_filters into Narwhals expressions.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.table_name","title":"metaxy.FeatureDep.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.feature.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature/","title":"Feature","text":"<p><code>BaseFeature</code> is the most important class in Metaxy. Features are defined by extending it.</p> <p>Code Version Access</p> <p>Retrieve a feature's code version from its spec: <code>MyFeature.spec().code_version</code>.</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature","title":"metaxy.BaseFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_snapshot_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph snapshot\",\n      \"title\": \"Metaxy Snapshot Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    }\n  },\n  \"title\": \"BaseFeature\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>metaxy_provenance_by_field</code>                 (<code>dict[str, str]</code>)             </li> <li> <code>metaxy_provenance</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_feature_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_snapshot_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version_by_field</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>metaxy_data_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_created_at</code>                 (<code>AwareDatetime | None</code>)             </li> <li> <code>metaxy_materialization_id</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_id_columns_exist</code> </li> </ul>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance_by_field","title":"metaxy.BaseFeature.metaxy_provenance_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance_by_field: dict[str, str]\n</code></pre> <p>Field-level provenance hashes (maps field names to hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance","title":"metaxy.BaseFeature.metaxy_provenance  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance: str | None = None\n</code></pre> <p>Hash of metaxy_provenance_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_feature_version","title":"metaxy.BaseFeature.metaxy_feature_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_feature_version: str | None = None\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_snapshot_version","title":"metaxy.BaseFeature.metaxy_snapshot_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_snapshot_version: str | None = None\n</code></pre> <p>Hash of the entire feature graph snapshot</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version_by_field","title":"metaxy.BaseFeature.metaxy_data_version_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version_by_field: dict[str, str] | None = None\n</code></pre> <p>Field-level data version hashes (maps field names to version hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version","title":"metaxy.BaseFeature.metaxy_data_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version: str | None = None\n</code></pre> <p>Hash of metaxy_data_version_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_created_at","title":"metaxy.BaseFeature.metaxy_created_at  <code>pydantic-field</code>","text":"<pre><code>metaxy_created_at: AwareDatetime | None = None\n</code></pre> <p>Timestamp when the metadata row was created (UTC)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_materialization_id","title":"metaxy.BaseFeature.metaxy_materialization_id  <code>pydantic-field</code>","text":"<pre><code>metaxy_materialization_id: str | None = None\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_project","title":"metaxy.BaseFeature.metaxy_project  <code>classmethod</code>","text":"<pre><code>metaxy_project() -&gt; str\n</code></pre> <p>Return the project this feature belongs to.</p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef metaxy_project(cls) -&gt; str:\n    \"\"\"Return the project this feature belongs to.\"\"\"\n    return cls.__metaxy_project__\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.table_name","title":"metaxy.BaseFeature.table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/processing\", id_columns=[\"id\"])):\n    id: str\n\n\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/processing\", id_columns=[\"id\"])):\n            id: str\n\n\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_version","title":"metaxy.BaseFeature.feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature\", id_columns=[\"id\"])):\n    id: str\n\n\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature\", id_columns=[\"id\"])):\n            id: str\n\n\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_spec_version","title":"metaxy.BaseFeature.feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature2\", id_columns=[\"id\"])):\n    id: str\n\n\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for reproducibility and audit purposes.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature2\", id_columns=[\"id\"])):\n            id: str\n\n\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.provenance_by_field","title":"metaxy.BaseFeature.provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.load_input","title":"metaxy.BaseFeature.load_input  <code>classmethod</code>","text":"<pre><code>load_input(\n    joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]\n) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.get_feature_by_key","title":"metaxy.get_feature_by_key","text":"<pre><code>get_feature_by_key(\n    key: CoercibleToFeatureKey,\n) -&gt; FeatureDefinition\n</code></pre> <p>Get a FeatureDefinition by its key from the current graph.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up (can be FeatureKey, list of strings, slash-separated string, etc.)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>FeatureDefinition for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>@public\ndef get_feature_by_key(key: CoercibleToFeatureKey) -&gt; FeatureDefinition:\n    \"\"\"Get a FeatureDefinition by its key from the current graph.\n\n    Args:\n        key: Feature key to look up (can be FeatureKey, list of strings, slash-separated string, etc.)\n\n    Returns:\n        FeatureDefinition for the feature\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    return current_graph().get_feature_definition(key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition","title":"metaxy.FeatureDefinition  <code>pydantic-model</code>","text":"<p>               Bases: <code>FrozenBaseModel</code></p> <p>Complete feature definition wrapping all feature information.</p> <p>Attributes:</p> <ul> <li> <code>spec</code>               (<code>FeatureSpec</code>)           \u2013            <p>The complete feature specification</p> </li> <li> <code>feature_schema</code>               (<code>Json[dict[str, Any]]</code>)           \u2013            <p>Pydantic JSON schema dict for the feature model</p> </li> <li> <code>feature_class_path</code>               (<code>str | None</code>)           \u2013            <p>Python import path (e.g., 'myapp.features.VideoFeature')</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project this feature belongs to</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"additionalProperties\": false,\n      \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", columns=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\",\n          \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        },\n        \"filters\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"SQL-like filter strings applied to this dependency.\",\n          \"title\": \"Filters\"\n        },\n        \"lineage\": {\n          \"$ref\": \"#/$defs/LineageRelationship\",\n          \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n        },\n        \"optional\": {\n          \"default\": false,\n          \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n          \"title\": \"Optional\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FeatureSpec\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"key\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"id_columns\": {\n          \"description\": \"Columns that uniquely identify a sample in this feature.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Id Columns\",\n          \"type\": \"array\"\n        },\n        \"deps\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FeatureDep\"\n          },\n          \"title\": \"Deps\",\n          \"type\": \"array\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"additionalProperties\": false,\n            \"properties\": {\n              \"key\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"code_version\": {\n                \"default\": \"__metaxy_initial__\",\n                \"title\": \"Code Version\",\n                \"type\": \"string\"\n              },\n              \"deps\": {\n                \"anyOf\": [\n                  {\n                    \"$ref\": \"#/$defs/SpecialFieldDep\"\n                  },\n                  {\n                    \"items\": {\n                      \"$ref\": \"#/$defs/FieldDep\"\n                    },\n                    \"type\": \"array\"\n                  }\n                ],\n                \"title\": \"Deps\"\n              }\n            },\n            \"title\": \"FieldSpec\",\n            \"type\": \"object\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"metadata\": {\n          \"additionalProperties\": true,\n          \"description\": \"Metadata attached to this feature.\",\n          \"title\": \"Metadata\",\n          \"type\": \"object\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Human-readable description of this feature.\",\n          \"title\": \"Description\"\n        }\n      },\n      \"required\": [\n        \"key\",\n        \"id_columns\"\n      ],\n      \"title\": \"FeatureSpec\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Complete feature definition wrapping all feature information.\\n\\nAttributes:\\n    spec: The complete feature specification\\n    feature_schema: Pydantic JSON schema dict for the feature model\\n    feature_class_path: Python import path (e.g., 'myapp.features.VideoFeature')\\n    project: The metaxy project this feature belongs to\",\n  \"properties\": {\n    \"spec\": {\n      \"$ref\": \"#/$defs/FeatureSpec\",\n      \"description\": \"Complete feature specification\"\n    },\n    \"feature_schema\": {\n      \"contentMediaType\": \"application/json\",\n      \"contentSchema\": {\n        \"additionalProperties\": true,\n        \"type\": \"object\"\n      },\n      \"description\": \"Pydantic JSON schema dict\",\n      \"title\": \"Feature Schema\",\n      \"type\": \"string\"\n    },\n    \"feature_class_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Python import path\",\n      \"title\": \"Feature Class Path\"\n    },\n    \"project\": {\n      \"description\": \"The metaxy project this feature belongs to\",\n      \"minLength\": 1,\n      \"title\": \"Project\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"spec\",\n    \"feature_schema\",\n    \"project\"\n  ],\n  \"title\": \"FeatureDefinition\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>spec</code>                 (<code>FeatureSpec</code>)             </li> <li> <code>feature_schema</code>                 (<code>Json[dict[str, Any]]</code>)             </li> <li> <code>feature_class_path</code>                 (<code>str | None</code>)             </li> <li> <code>project</code>                 (<code>str</code>)             </li> <li> <code>_feature_class</code>                 (<code>type[BaseFeature] | None</code>)             </li> <li> <code>_is_external</code>                 (<code>bool</code>)             </li> <li> <code>_provenance_by_field</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>_on_version_mismatch</code>                 (<code>Literal['warn', 'error']</code>)             </li> <li> <code>_source</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_convert_dict_to_json</code>                 \u2192                   <code>feature_schema</code> </li> </ul>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.spec","title":"metaxy.FeatureDefinition.spec  <code>pydantic-field</code>","text":"<pre><code>spec: FeatureSpec\n</code></pre> <p>Complete feature specification</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_schema","title":"metaxy.FeatureDefinition.feature_schema  <code>pydantic-field</code>","text":"<pre><code>feature_schema: Json[dict[str, Any]]\n</code></pre> <p>Pydantic JSON schema dict</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_class_path","title":"metaxy.FeatureDefinition.feature_class_path  <code>pydantic-field</code>","text":"<pre><code>feature_class_path: str | None = None\n</code></pre> <p>Python import path</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.project","title":"metaxy.FeatureDefinition.project  <code>pydantic-field</code>","text":"<pre><code>project: str\n</code></pre> <p>The metaxy project this feature belongs to</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_definition_version","title":"metaxy.FeatureDefinition.feature_definition_version  <code>cached</code> <code>property</code>","text":"<pre><code>feature_definition_version: str\n</code></pre> <p>Hash of spec + schema (excludes project).</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.key","title":"metaxy.FeatureDefinition.key  <code>property</code>","text":"<pre><code>key: FeatureKey\n</code></pre> <p>Get the feature key from the spec.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.table_name","title":"metaxy.FeatureDefinition.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.id_columns","title":"metaxy.FeatureDefinition.id_columns  <code>property</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Get ID columns from the spec.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.columns","title":"metaxy.FeatureDefinition.columns  <code>cached</code> <code>property</code>","text":"<pre><code>columns: Sequence[str]\n</code></pre> <p>Get column names from the feature schema.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.is_external","title":"metaxy.FeatureDefinition.is_external  <code>property</code>","text":"<pre><code>is_external: bool\n</code></pre> <p>Check if this is an external feature definition.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.provenance_by_field_override","title":"metaxy.FeatureDefinition.provenance_by_field_override  <code>property</code>","text":"<pre><code>provenance_by_field_override: dict[str, str]\n</code></pre> <p>The manually-specified field provenance map.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If no provenance override was set.</p> </li> </ul>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.has_provenance_override","title":"metaxy.FeatureDefinition.has_provenance_override  <code>property</code>","text":"<pre><code>has_provenance_override: bool\n</code></pre> <p>True if this external feature has a provenance override.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.on_version_mismatch","title":"metaxy.FeatureDefinition.on_version_mismatch  <code>property</code>","text":"<pre><code>on_version_mismatch: Literal['warn', 'error']\n</code></pre> <p>What to do when actual feature version differs from expected.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.source","title":"metaxy.FeatureDefinition.source  <code>property</code>","text":"<pre><code>source: str\n</code></pre> <p>Human-readable string describing where this definition came from.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.from_feature_class","title":"metaxy.FeatureDefinition.from_feature_class  <code>classmethod</code>","text":"<pre><code>from_feature_class(\n    feature_cls: type[BaseFeature],\n) -&gt; FeatureDefinition\n</code></pre> <p>Create a FeatureDefinition from a Feature class.</p> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef from_feature_class(cls, feature_cls: type[BaseFeature]) -&gt; FeatureDefinition:\n    \"\"\"Create a FeatureDefinition from a Feature class.\"\"\"\n    spec = feature_cls.spec()\n\n    # Inject class docstring as description if not already set\n    if spec.description is None and feature_cls.__doc__:\n        spec = spec.model_copy(update={\"description\": inspect.cleandoc(feature_cls.__doc__)})\n\n    schema = feature_cls.model_json_schema()\n    class_path = f\"{feature_cls.__module__}.{feature_cls.__name__}\"\n    project = feature_cls.metaxy_project()\n\n    definition = cls(\n        spec=spec,\n        feature_schema=schema,\n        feature_class_path=class_path,\n        project=project,\n    )\n    definition._feature_class = feature_cls\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.from_stored_data","title":"metaxy.FeatureDefinition.from_stored_data  <code>classmethod</code>","text":"<pre><code>from_stored_data(\n    feature_spec: dict[str, Any] | str,\n    feature_schema: dict[str, Any] | str,\n    feature_class_path: str,\n    project: str,\n    source: str | None = None,\n) -&gt; FeatureDefinition\n</code></pre> <p>Create a FeatureDefinition from stored data.</p> <p>Handles JSON string or dict inputs for spec and schema fields.</p> <p>Parameters:</p> <ul> <li> <code>feature_spec</code>               (<code>dict[str, Any] | str</code>)           \u2013            <p>Feature specification as dict or JSON string.</p> </li> <li> <code>feature_schema</code>               (<code>dict[str, Any] | str</code>)           \u2013            <p>Pydantic JSON schema as dict or JSON string.</p> </li> <li> <code>feature_class_path</code>               (<code>str</code>)           \u2013            <p>Python import path of the feature class.</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project name.</p> </li> <li> <code>source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Human-readable string describing where this definition came from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>A new FeatureDefinition instance.</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef from_stored_data(\n    cls,\n    feature_spec: dict[str, Any] | str,\n    feature_schema: dict[str, Any] | str,\n    feature_class_path: str,\n    project: str,\n    source: str | None = None,\n) -&gt; FeatureDefinition:\n    \"\"\"Create a FeatureDefinition from stored data.\n\n    Handles JSON string or dict inputs for spec and schema fields.\n\n    Args:\n        feature_spec: Feature specification as dict or JSON string.\n        feature_schema: Pydantic JSON schema as dict or JSON string.\n        feature_class_path: Python import path of the feature class.\n        project: The metaxy project name.\n        source: Human-readable string describing where this definition came from.\n\n    Returns:\n        A new FeatureDefinition instance.\n    \"\"\"\n    import json\n\n    if isinstance(feature_spec, str):\n        feature_spec = json.loads(feature_spec)\n    if isinstance(feature_schema, str):\n        feature_schema = json.loads(feature_schema)\n\n    spec = FeatureSpec.model_validate(feature_spec)\n    definition = cls(\n        spec=spec,\n        feature_schema=feature_schema,\n        feature_class_path=feature_class_path,\n        project=project,\n    )\n    definition._source = source\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.external","title":"metaxy.FeatureDefinition.external  <code>classmethod</code>","text":"<pre><code>external(\n    *,\n    spec: FeatureSpec,\n    project: str,\n    feature_schema: dict[str, Any] | None = None,\n    provenance_by_field: dict[CoercibleToFieldKey, str]\n    | None = None,\n    on_version_mismatch: Literal[\"warn\", \"error\"] = \"warn\",\n    source: str | None = None,\n) -&gt; FeatureDefinition\n</code></pre> <p>Create an external FeatureDefinition without a Feature class.</p> <p>External features are definitions loaded from another project or system that don't have corresponding Python Feature classes in the current codebase.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>FeatureSpec</code>)           \u2013            <p>The feature specification.</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project this feature belongs to.</p> </li> <li> <code>feature_schema</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pydantic JSON schema dict describing the feature's fields. Typically doesn't have to be provided, unless some user code attempts to use it before the real feature definition is loaded from the metadata store. This argument is experimental and may be changed in the future.</p> </li> <li> <code>provenance_by_field</code>               (<code>dict[CoercibleToFieldKey, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional manually-specified field provenance map. Use this argument to avoid providing too many upstream external features. Make sure to provide the actual values from the real external feature.</p> </li> <li> <code>on_version_mismatch</code>               (<code>Literal['warn', 'error']</code>, default:                   <code>'warn'</code> )           \u2013            <p>How to handle a version mismatch if the actual feature loaded from the metadata store has a different version than the version specified in the corresponding external feature.</p> </li> <li> <code>source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Human-readable string describing where this definition came from. If not provided, captures the call site location automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>A new FeatureDefinition marked as external.</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef external(\n    cls,\n    *,\n    spec: FeatureSpec,\n    project: str,\n    feature_schema: dict[str, Any] | None = None,\n    provenance_by_field: dict[CoercibleToFieldKey, str] | None = None,\n    on_version_mismatch: Literal[\"warn\", \"error\"] = \"warn\",\n    source: str | None = None,\n) -&gt; FeatureDefinition:\n    \"\"\"Create an external FeatureDefinition without a Feature class.\n\n    External features are definitions loaded from another project or system\n    that don't have corresponding Python Feature classes in the current codebase.\n\n    Args:\n        spec: The feature specification.\n        project: The metaxy project this feature belongs to.\n        feature_schema: Pydantic JSON schema dict describing the feature's fields.\n            Typically doesn't have to be provided, unless some user code attempts\n            to use it before the real feature definition is loaded from the metadata store.\n            This argument is experimental and may be changed in the future.\n        provenance_by_field: Optional manually-specified field provenance map.\n            Use this argument to avoid providing too many upstream external features.\n            Make sure to provide the actual values from the real external feature.\n        on_version_mismatch: How to handle a version mismatch if the actual feature loaded from the\n            metadata store has a different version than the version specified in the corresponding external feature.\n        source: Human-readable string describing where this definition came from.\n            If not provided, captures the call site location automatically.\n\n    Returns:\n        A new FeatureDefinition marked as external.\n    \"\"\"\n    normalized_provenance: dict[str, str] | None = None\n    if provenance_by_field is not None:\n        normalized_provenance = {\n            ValidatedFieldKeyAdapter.validate_python(k).to_string(): v for k, v in provenance_by_field.items()\n        }\n\n    # Capture call site location if source is not provided\n    if source is None:\n        source = cls._capture_call_site()\n\n    definition = cls(\n        spec=spec,\n        feature_schema=feature_schema or {},\n        feature_class_path=None,\n        project=project,\n    )\n    definition._is_external = True\n    definition._provenance_by_field = normalized_provenance\n    definition._on_version_mismatch = on_version_mismatch\n    definition._source = source\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.check_version_mismatch","title":"metaxy.FeatureDefinition.check_version_mismatch","text":"<pre><code>check_version_mismatch(\n    *,\n    expected_version: str,\n    actual_version: str,\n    expected_version_by_field: dict[str, str],\n    actual_version_by_field: dict[str, str],\n) -&gt; None\n</code></pre> <p>Check if the actual feature version matches expected version.</p> <p>Called by load_feature_definitions after loading external features from the metadata store, comparing provenance-carrying feature versions.</p> <p>Parameters:</p> <ul> <li> <code>expected_version</code>               (<code>str</code>)           \u2013            <p>The feature version before loading (from graph).</p> </li> <li> <code>actual_version</code>               (<code>str</code>)           \u2013            <p>The feature version after loading (from graph).</p> </li> <li> <code>expected_version_by_field</code>               (<code>dict[str, str]</code>)           \u2013            <p>Field-level versions before loading.</p> </li> <li> <code>actual_version_by_field</code>               (<code>dict[str, str]</code>)           \u2013            <p>Field-level versions after loading.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If versions mismatch and on_version_mismatch is \"error\".</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>def check_version_mismatch(\n    self,\n    *,\n    expected_version: str,\n    actual_version: str,\n    expected_version_by_field: dict[str, str],\n    actual_version_by_field: dict[str, str],\n) -&gt; None:\n    \"\"\"Check if the actual feature version matches expected version.\n\n    Called by load_feature_definitions after loading external features from\n    the metadata store, comparing provenance-carrying feature versions.\n\n    Args:\n        expected_version: The feature version before loading (from graph).\n        actual_version: The feature version after loading (from graph).\n        expected_version_by_field: Field-level versions before loading.\n        actual_version_by_field: Field-level versions after loading.\n\n    Raises:\n        ValueError: If versions mismatch and on_version_mismatch is \"error\".\n    \"\"\"\n    if not self.is_external:\n        return\n\n    if expected_version == actual_version:\n        return\n\n    # Find which fields differ\n    mismatched_fields = []\n    all_fields = set(expected_version_by_field.keys()) | set(actual_version_by_field.keys())\n    for field in sorted(all_fields):\n        expected_field_ver = expected_version_by_field.get(field, \"&lt;missing&gt;\")\n        actual_field_ver = actual_version_by_field.get(field, \"&lt;missing&gt;\")\n        if expected_field_ver != actual_field_ver:\n            mismatched_fields.append(f\"  - {field}: expected '{expected_field_ver}', got '{actual_field_ver}'\")\n\n    field_details = \"\\n\".join(mismatched_fields) if mismatched_fields else \"  (no field-level details available)\"\n\n    message = (\n        f\"Version mismatch for external feature '{self.key}': \"\n        f\"expected feature version '{expected_version}', got '{actual_version}'.\\n\"\n        f\"Field-level mismatches:\\n{field_details}\\n\"\n        f\"The external feature definition may be out of sync with the metadata store.\"\n    )\n\n    if self._on_version_mismatch == \"error\":\n        raise ValueError(message)\n    warnings.warn(message, stacklevel=3)\n</code></pre>"},{"location":"reference/api/definitions/field/","title":"Field","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec","title":"metaxy.FieldSpec  <code>pydantic-model</code>","text":"<pre><code>FieldSpec(\n    *,\n    key: CoercibleToFieldKey | None = None,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FieldKey\"\n    },\n    \"code_version\": {\n      \"default\": \"__metaxy_initial__\",\n      \"title\": \"Code Version\",\n      \"type\": \"string\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/SpecialFieldDep\"\n        },\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldDep\"\n          },\n          \"type\": \"array\"\n        }\n      ],\n      \"title\": \"Deps\"\n    }\n  },\n  \"title\": \"FieldSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'extra': 'forbid'}</code></li> </ul> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FieldKey</code>)             </li> <li> <code>code_version</code>                 (<code>str</code>)             </li> <li> <code>deps</code>                 (<code>SpecialFieldDep | list[FieldDep]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    *,\n    key: CoercibleToFieldKey | None = None,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec.__get_pydantic_core_schema__","title":"metaxy.FieldSpec.__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(source_type, handler)\n</code></pre> <p>Add custom validator to coerce strings to FieldSpec.</p> Source code in <code>src/metaxy/models/field.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler):\n    \"\"\"Add custom validator to coerce strings to FieldSpec.\"\"\"\n    from pydantic_core import core_schema\n\n    # Get the default schema\n    python_schema = handler(source_type)\n\n    # Wrap it with a before validator that converts strings\n    return core_schema.no_info_before_validator_function(\n        _validate_field_spec_from_string,\n        python_schema,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldDep","title":"metaxy.FieldDep  <code>pydantic-model</code>","text":"<pre><code>FieldDep(\n    *,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | FeatureSpec\n    | type[BaseFeature],\n    fields: list[CoercibleToFieldKey] | Literal[ALL] = ALL,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"const\": \"__METAXY_ALL_DEP__\",\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"__METAXY_ALL_DEP__\",\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FieldDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'extra': 'forbid'}</code></li> </ul> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldKey] | Literal[ALL]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    *,\n    feature: str | Sequence[str] | FeatureKey | \"FeatureSpec\" | type[\"BaseFeature\"],\n    fields: list[CoercibleToFieldKey] | Literal[SpecialFieldDep.ALL] = SpecialFieldDep.ALL,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/","title":"Fields Mapping","text":"<p>Metaxy provides a few helpers when defining field-level dependencies:</p> <ul> <li>the default mapping that matches on field names or suffixes with metaxy.models.fields_mapping.FieldsMapping.default</li> <li><code>specific</code> mapping with metaxy.models.fields_mapping.FieldsMapping.specific</li> <li><code>all</code> mapping with metaxy.models.fields_mapping.FieldsMapping.all</li> <li><code>none</code> mapping with metaxy.models.fields_mapping.FieldsMapping.none]</li> </ul> <p>Always use these classmethods to create instances of lineage relationships. Under the hood, they use Pydantic's discriminated union to ensure that the correct type is constructed based on the provided data.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping","title":"metaxy.models.fields_mapping.FieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for field mapping configurations.</p> <p>Field mappings define how a field automatically resolves its dependencies based on upstream feature fields. This is separate from explicit field dependencies which are defined directly.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n  \"properties\": {\n    \"mapping\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"all\": \"#/$defs/AllFieldsMapping\",\n          \"default\": \"#/$defs/DefaultFieldsMapping\",\n          \"none\": \"#/$defs/NoneFieldsMapping\",\n          \"specific\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/AllFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/NoneFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n        }\n      ],\n      \"title\": \"Mapping\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"FieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>mapping</code>                 (<code>AllFieldsMapping | SpecificFieldsMapping | NoneFieldsMapping | DefaultFieldsMapping</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve field dependencies based on upstream feature fields.</p> <p>Invokes the provided mapping to resolve dependencies.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    \"\"\"Resolve field dependencies based on upstream feature fields.\n\n    Invokes the provided mapping to resolve dependencies.\n\n    Args:\n        context: The resolution context containing field key and upstream feature.\n\n    Returns:\n        Set of [FieldKey][metaxy.models.types.FieldKey] instances for matching fields\n    \"\"\"\n    return self.mapping.resolve_field_deps(context)\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.default","title":"metaxy.models.fields_mapping.FieldsMapping.default  <code>classmethod</code>","text":"<pre><code>default(\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self\n</code></pre> <p>Create a default field mapping configuration.</p> <p>Parameters:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef default(\n    cls,\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self:\n    \"\"\"Create a default field mapping configuration.\n\n    Args:\n        match_suffix: If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")\n        exclude_fields: List of field keys to exclude from auto-mapping\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(\n        mapping=DefaultFieldsMapping(\n            match_suffix=match_suffix,\n            exclude_fields=exclude_fields or [],\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.specific","title":"metaxy.models.fields_mapping.FieldsMapping.specific  <code>classmethod</code>","text":"<pre><code>specific(\n    mapping: dict[\n        CoercibleToFieldKey, set[CoercibleToFieldKey]\n    ],\n) -&gt; Self\n</code></pre> <p>Create a field mapping that maps downstream field keys into specific upstream field keys.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]</code>)           \u2013            <p>Mapping of downstream field keys to sets of upstream field keys. Keys and values can be strings, sequences, or FieldKey instances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef specific(cls, mapping: dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]) -&gt; Self:\n    \"\"\"Create a field mapping that maps downstream field keys into specific upstream field keys.\n\n    Args:\n        mapping: Mapping of downstream field keys to sets of upstream field keys.\n            Keys and values can be strings, sequences, or FieldKey instances.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    # Validate and coerce the mapping keys and values\n    validated_mapping: dict[FieldKey, set[FieldKey]] = {}\n    for key, value_set in mapping.items():\n        validated_key = ValidatedFieldKeyAdapter.validate_python(key)\n        validated_values = {ValidatedFieldKeyAdapter.validate_python(v) for v in value_set}\n        validated_mapping[validated_key] = validated_values\n\n    return cls(mapping=SpecificFieldsMapping(mapping=validated_mapping))\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.all","title":"metaxy.models.fields_mapping.FieldsMapping.all  <code>classmethod</code>","text":"<pre><code>all() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on all upstream fields.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef all(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on all upstream fields.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=AllFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.none","title":"metaxy.models.fields_mapping.FieldsMapping.none  <code>classmethod</code>","text":"<pre><code>none() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on no upstream fields.</p> <p>This is typically useful when explicitly defining FieldSpec.deps instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on no upstream fields.\n\n    This is typically useful when explicitly defining [FieldSpec.deps][metaxy.models.field.FieldSpec] instead.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=NoneFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMappingType","title":"metaxy.models.fields_mapping.FieldsMappingType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of fields mapping between a field key and the upstream field keys.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping","title":"metaxy.models.fields_mapping.DefaultFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Default automatic field mapping configuration.</p> <p>When used, automatically maps fields to matching upstream fields based on field keys.</p> <p>Attributes:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>)           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey]</code>)           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"default\",\n      \"default\": \"default\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"match_suffix\": {\n      \"default\": false,\n      \"title\": \"Match Suffix\",\n      \"type\": \"boolean\"\n    },\n    \"exclude_fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Exclude Fields\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"DefaultFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[DEFAULT]</code>)             </li> <li> <code>match_suffix</code>                 (<code>bool</code>)             </li> <li> <code>exclude_fields</code>                 (<code>list[FieldKey]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    res = set()\n\n    for upstream_field_key in context.upstream_feature_fields:\n        # Skip excluded fields\n        if upstream_field_key in self.exclude_fields:\n            continue\n\n        # Check for exact match\n        if upstream_field_key == context.field_key:\n            res.add(upstream_field_key)\n        # Check for suffix match if enabled\n        elif self.match_suffix and self._is_suffix_match(context.field_key, upstream_field_key):\n            res.add(upstream_field_key)\n\n    # If no fields matched, return ALL fields from this upstream feature\n    # (excluding any explicitly excluded fields)\n    if not res:\n        for upstream_field_key in context.upstream_feature_fields:\n            if upstream_field_key not in self.exclude_fields:\n                res.add(upstream_field_key)\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping","title":"metaxy.models.fields_mapping.SpecificFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on specific upstream fields.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"specific\",\n      \"default\": \"specific\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"mapping\": {\n      \"additionalProperties\": {\n        \"items\": {\n          \"$ref\": \"#/$defs/FieldKey\"\n        },\n        \"type\": \"array\",\n        \"uniqueItems\": true\n      },\n      \"propertyNames\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Mapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"SpecificFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[SPECIFIC]</code>)             </li> <li> <code>mapping</code>                 (<code>dict[FieldKey, set[FieldKey]]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    desired_upstream_fields = self.mapping.get(context.field_key, set())\n    return desired_upstream_fields &amp; context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping","title":"metaxy.models.fields_mapping.AllFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on all upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"all\",\n      \"default\": \"all\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"AllFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[ALL]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping","title":"metaxy.models.fields_mapping.NoneFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that never matches any upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that never matches any upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"none\",\n      \"default\": \"none\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"NoneFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[NONE]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return set()\n</code></pre>"},{"location":"reference/api/definitions/filters/","title":"String Filters","text":""},{"location":"reference/api/definitions/filters/#metaxy.models.filter_expression.parse_filter_string","title":"metaxy.models.filter_expression.parse_filter_string","text":"<pre><code>parse_filter_string(filter_string: str) -&gt; Expr\n</code></pre> <p>Parse a SQL WHERE-like string into a Narwhals expression.</p> <p>The parser understands SQL <code>WHERE</code> clauses composed of comparison operators, logical operators, parentheses, dotted identifiers, and literal values (strings, numbers, booleans, <code>NULL</code>).</p> <p>This functionality is implemented with SQLGlot.</p> Example <pre><code>parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n# Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n</code></pre> Source code in <code>src/metaxy/models/filter_expression.py</code> <pre><code>@public\ndef parse_filter_string(filter_string: str) -&gt; nw.Expr:\n    \"\"\"Parse a SQL WHERE-like string into a Narwhals expression.\n\n    The parser understands SQL `WHERE` clauses composed of comparison operators, logical operators, parentheses,\n    dotted identifiers, and literal values (strings, numbers, booleans, ``NULL``).\n\n    This functionality is implemented with [SQLGlot](https://sqlglot.com/).\n\n    Example:\n        ```python\n        parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n        # Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n        ```\n    \"\"\"\n    return NarwhalsFilter.model_validate(filter_string).to_expr()\n</code></pre>"},{"location":"reference/api/definitions/graph/","title":"Feature Graph","text":"<p><code>FeatureGraph</code> is a global \"God\" object that holds all the features loaded by Metaxy via the feature discovery mechanism.</p> <p>Users may interact with <code>FeatureGraph</code> when writing custom migrations, otherwise they are not exposed to it.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph","title":"metaxy.FeatureGraph","text":"<pre><code>FeatureGraph()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def __init__(self):\n    # Primary storage: FeatureDefinition objects\n    self.feature_definitions_by_key: dict[FeatureKey, FeatureDefinition] = {}\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.snapshot_version","title":"metaxy.FeatureGraph.snapshot_version  <code>property</code>","text":"<pre><code>snapshot_version: str\n</code></pre> <p>Generate a snapshot version for the current project's features.</p> <p>Uses feature_definition_version (spec + schema only), excluding external features. The project is determined from MetaxyConfig.project if set, otherwise from the graph's single project (via the <code>project</code> property).</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If MetaxyConfig.project is not set and the graph is empty or spans multiple projects.</p> </li> </ul>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.has_external_features","title":"metaxy.FeatureGraph.has_external_features  <code>property</code>","text":"<pre><code>has_external_features: bool\n</code></pre> <p>Check if any feature in the graph is an external feature.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.project","title":"metaxy.FeatureGraph.project  <code>property</code>","text":"<pre><code>project: str\n</code></pre> <p>The single project for all non-external features in this graph.</p> <p>Returns the project name if all non-external features belong to a single project.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the graph is empty or features span multiple projects.</p> </li> </ul>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-functions","title":"Functions","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature","title":"metaxy.FeatureGraph.add_feature","text":"<pre><code>add_feature(feature: type[BaseFeature]) -&gt; None\n</code></pre> <p>Add a feature class to the graph.</p> <p>Creates a FeatureDefinition from the class and delegates to add_feature_definition.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to register</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a feature with a different import path but the same key is already registered        or if duplicate column names would result from renaming operations</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature(self, feature: type[\"BaseFeature\"]) -&gt; None:\n    \"\"\"Add a feature class to the graph.\n\n    Creates a FeatureDefinition from the class and delegates to add_feature_definition.\n\n    Args:\n        feature: Feature class to register\n\n    Raises:\n        ValueError: If a feature with a different import path but the same key is already registered\n                   or if duplicate column names would result from renaming operations\n    \"\"\"\n    definition = FeatureDefinition.from_feature_class(feature)\n    self.add_feature_definition(definition)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature_definition","title":"metaxy.FeatureGraph.add_feature_definition","text":"<pre><code>add_feature_definition(\n    definition: FeatureDefinition,\n    on_conflict: Literal[\"raise\", \"ignore\"] = \"raise\",\n) -&gt; None\n</code></pre> <p>Add a feature to the graph.</p> <p>Interactions with External Features</p> <p>Normal features take priority over external features with the same key.</p> <p>Parameters:</p> <ul> <li> <code>definition</code>               (<code>FeatureDefinition</code>)           \u2013            <p>FeatureDefinition to register</p> </li> <li> <code>on_conflict</code>               (<code>Literal['raise', 'ignore']</code>, default:                   <code>'raise'</code> )           \u2013            <p>What to do if a feature with the same key is already registered</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a non-external feature with a different import path but the same key is already registered and <code>on_conflict</code> is <code>\"raise\"</code></p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature_definition(\n    self, definition: FeatureDefinition, on_conflict: Literal[\"raise\", \"ignore\"] = \"raise\"\n) -&gt; None:\n    \"\"\"Add a feature to the graph.\n\n    !!! note \"Interactions with External Features\"\n\n        Normal features take priority over external features with the same key.\n\n    Args:\n        definition: FeatureDefinition to register\n        on_conflict: What to do if a feature with the same key is already registered\n\n    Raises:\n        ValueError: If a non-external feature with a different import path but\n            the same key is already registered and `on_conflict` is `\"raise\"`\n    \"\"\"\n    key = definition.key\n\n    if key not in self.feature_definitions_by_key:\n        self.feature_definitions_by_key[key] = definition\n    elif definition.is_external and not self.feature_definitions_by_key[key].is_external:\n        # External features never overwrite non-external features\n        return\n    elif not definition.is_external and self.feature_definitions_by_key[key].is_external:\n        # Non-external features always replace external features\n        # Note: version mismatch checking is done in load_feature_definitions,\n        # not here, because we need the full graph context to compute\n        # provenance-carrying versions.\n        self.feature_definitions_by_key[key] = definition\n    elif definition.feature_class_path == self.feature_definitions_by_key[key].feature_class_path:\n        # Same class path - allow quiet replacement\n        self.feature_definitions_by_key[key] = definition\n    elif on_conflict == \"ignore\":\n        # Conflict exists but we're ignoring - keep existing definition\n        return\n    elif definition.is_external:\n        # Both external with different class paths - raise to be safe\n        raise ValueError(f\"External feature with key {key.to_string()} is already registered.\")\n    else:\n        # Both non-external with different class paths\n        raise ValueError(\n            f\"Feature with key {key.to_string()} already registered. \"\n            f\"Existing: {self.feature_definitions_by_key[key].feature_class_path}, \"\n            f\"New: {definition.feature_class_path}. \"\n            f\"Each feature key must be unique within a graph.\"\n        )\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_definition","title":"metaxy.FeatureGraph.get_feature_definition","text":"<pre><code>get_feature_definition(\n    key: CoercibleToFeatureKey,\n) -&gt; FeatureDefinition\n</code></pre> <p>Get a FeatureDefinition by its key.</p> <p>This is the primary method for accessing feature information.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>FeatureDefinition for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_definition(self, key: CoercibleToFeatureKey) -&gt; FeatureDefinition:\n    \"\"\"Get a FeatureDefinition by its key.\n\n    This is the primary method for accessing feature information.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        FeatureDefinition for the feature\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    if validated_key not in self.feature_definitions_by_key:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.feature_definitions_by_key.keys()]}\"\n        )\n    return self.feature_definitions_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.remove_feature","title":"metaxy.FeatureGraph.remove_feature","text":"<pre><code>remove_feature(key: CoercibleToFeatureKey) -&gt; None\n</code></pre> <p>Remove a feature from the graph.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to remove. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def remove_feature(self, key: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Remove a feature from the graph.\n\n    Args:\n        key: Feature key to remove. Accepts types that can be converted into a feature key..\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    if validated_key not in self.feature_definitions_by_key:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.feature_definitions_by_key]}\"\n        )\n\n    del self.feature_definitions_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.list_features","title":"metaxy.FeatureGraph.list_features","text":"<pre><code>list_features(\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]\n</code></pre> <p>List all feature keys in the graph, optionally filtered by project(s).</p> <p>By default, filters features by the current project (first part of feature key). This prevents operations from affecting features in other projects.</p> <p>Parameters:</p> <ul> <li> <code>projects</code>               (<code>list[str] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name(s) to filter by. Can be: - None: Use current project from MetaxyConfig (if only_current_project=True) - str: Single project name - list[str]: Multiple project names</p> </li> <li> <code>only_current_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filter by current/specified project(s). If False, return all features regardless of project.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys</p> </li> </ul> Example <pre><code># Get features for specific project\nfeatures = graph.list_features(projects=\"myproject\")\n\n# Get all features regardless of project\nall_features = graph.list_features(only_current_project=False)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def list_features(\n    self,\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]:\n    \"\"\"List all feature keys in the graph, optionally filtered by project(s).\n\n    By default, filters features by the current project (first part of feature key).\n    This prevents operations from affecting features in other projects.\n\n    Args:\n        projects: Project name(s) to filter by. Can be:\n            - None: Use current project from MetaxyConfig (if only_current_project=True)\n            - str: Single project name\n            - list[str]: Multiple project names\n        only_current_project: If True, filter by current/specified project(s).\n            If False, return all features regardless of project.\n\n    Returns:\n        List of feature keys\n\n    Example:\n        ```py\n        # Get features for specific project\n        features = graph.list_features(projects=\"myproject\")\n\n        # Get all features regardless of project\n        all_features = graph.list_features(only_current_project=False)\n        ```\n    \"\"\"\n    if not only_current_project:\n        # Return all features (both class-based and definition-only)\n        return list(self.feature_definitions_by_key.keys())\n\n    # Normalize projects to list\n    project_list: list[str]\n    if projects is None:\n        # Try to get from config context\n        try:\n            from metaxy.config import MetaxyConfig\n\n            config = MetaxyConfig.get()\n            if config.project is None:\n                # No project configured - return all features\n                return list(self.feature_definitions_by_key.keys())\n            project_list = [config.project]\n        except RuntimeError:\n            # Config not initialized - in tests or non-CLI usage\n            # Return all features (can't determine project)\n            return list(self.feature_definitions_by_key.keys())\n    elif isinstance(projects, str):\n        project_list = [projects]\n    else:\n        project_list = projects\n\n    # Filter by project(s) using FeatureDefinition.project\n    return [key for key, defn in self.feature_definitions_by_key.items() if defn.project in project_list]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_plan","title":"metaxy.FeatureGraph.get_feature_plan","text":"<pre><code>get_feature_plan(key: CoercibleToFeatureKey) -&gt; FeaturePlan\n</code></pre> <p>Get a feature plan for a given feature key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get plan for. Accepts types that can be converted into a feature key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeaturePlan</code>           \u2013            <p>FeaturePlan instance with feature spec and dependencies.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetaxyMissingFeatureDependency</code>             \u2013            <p>If any dependency is not in the graph.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_plan(self, key: CoercibleToFeatureKey) -&gt; FeaturePlan:\n    \"\"\"Get a feature plan for a given feature key.\n\n    Args:\n        key: Feature key to get plan for. Accepts types that can be converted into a feature key.\n\n    Returns:\n        FeaturePlan instance with feature spec and dependencies.\n\n    Raises:\n        MetaxyMissingFeatureDependency: If any dependency is not in the graph.\n    \"\"\"\n    from metaxy.utils.exceptions import MetaxyMissingFeatureDependency\n\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    definition = self.feature_definitions_by_key[validated_key]\n    spec = definition.spec\n\n    # Check all dependencies are present and collect their specs\n    dep_specs = []\n    for dep in spec.deps or []:\n        if dep.feature not in self.feature_definitions_by_key:\n            raise MetaxyMissingFeatureDependency(\n                f\"Feature '{validated_key.to_string()}' depends on '{dep.feature.to_string()}' \"\n                f\"which is not in the graph.\"\n            )\n        dep_specs.append(self.feature_definitions_by_key[dep.feature].spec)\n\n    return FeaturePlan(\n        feature=spec,\n        deps=dep_specs or None,\n        feature_deps=spec.deps,\n    )\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version_by_field","title":"metaxy.FeatureGraph.get_feature_version_by_field","text":"<pre><code>get_feature_version_by_field(\n    key: CoercibleToFeatureKey,\n) -&gt; dict[str, str]\n</code></pre> <p>Computes the field provenance map for a feature.</p> <p>Hash together field provenance entries with the feature code version.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get field versions for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>dict[str, str]: The provenance hash for each field in the feature plan. Keys are field names as strings.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version_by_field(self, key: CoercibleToFeatureKey) -&gt; dict[str, str]:\n    \"\"\"Computes the field provenance map for a feature.\n\n    Hash together field provenance entries with the feature code version.\n\n    Args:\n        key: Feature key to get field versions for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        dict[str, str]: The provenance hash for each field in the feature plan.\n            Keys are field names as strings.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    res = {}\n\n    plan = self.get_feature_plan(validated_key)\n\n    for k, v in plan.feature.fields_by_key.items():\n        res[k.to_string()] = self.get_field_version(FQFieldKey(field=k, feature=validated_key))\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version","title":"metaxy.FeatureGraph.get_feature_version","text":"<pre><code>get_feature_version(key: CoercibleToFeatureKey) -&gt; str\n</code></pre> <p>Computes the feature version as a single string.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get version for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Truncated SHA256 hash representing the feature version.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version(self, key: CoercibleToFeatureKey) -&gt; str:\n    \"\"\"Computes the feature version as a single string.\n\n    Args:\n        key: Feature key to get version for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        Truncated SHA256 hash representing the feature version.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    hasher = hashlib.sha256()\n    provenance_by_field = self.get_feature_version_by_field(validated_key)\n    for field_key in sorted(provenance_by_field):\n        hasher.update(field_key.encode())\n        hasher.update(provenance_by_field[field_key].encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_downstream_features","title":"metaxy.FeatureGraph.get_downstream_features","text":"<pre><code>get_downstream_features(\n    sources: Sequence[CoercibleToFeatureKey],\n) -&gt; list[FeatureKey]\n</code></pre> <p>Get all features downstream of sources, topologically sorted.</p> <p>Performs a depth-first traversal of the dependency graph to find all features that transitively depend on any of the source features.</p> <p>Parameters:</p> <ul> <li> <code>sources</code>               (<code>Sequence[CoercibleToFeatureKey]</code>)           \u2013            <p>List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of downstream feature keys in topological order (dependencies first).</p> </li> <li> <code>list[FeatureKey]</code>           \u2013            <p>Does not include the source features themselves.</p> </li> </ul> Example <pre><code># Build a DAG: a -&gt; b -&gt; d, a -&gt; c -&gt; d\nclass FeatureA(mx.BaseFeature, spec=mx.FeatureSpec(key=\"a\", id_columns=[\"id\"])):\n    id: str\n\n\nclass FeatureB(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"b\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n):\n    id: str\n\n\nclass FeatureC(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"c\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n):\n    id: str\n\n\nclass FeatureD(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"d\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureB), mx.FeatureDep(feature=FeatureC)]\n    ),\n):\n    id: str\n\n\ngraph.get_downstream_features([\"a\"])\n# [FeatureKey(['b']), FeatureKey(['c']), FeatureKey(['d'])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_downstream_features(self, sources: Sequence[CoercibleToFeatureKey]) -&gt; list[FeatureKey]:\n    \"\"\"Get all features downstream of sources, topologically sorted.\n\n    Performs a depth-first traversal of the dependency graph to find all\n    features that transitively depend on any of the source features.\n\n    Args:\n        sources: List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.\n\n    Returns:\n        List of downstream feature keys in topological order (dependencies first).\n        Does not include the source features themselves.\n\n    Example:\n        ```py\n        # Build a DAG: a -&gt; b -&gt; d, a -&gt; c -&gt; d\n        class FeatureA(mx.BaseFeature, spec=mx.FeatureSpec(key=\"a\", id_columns=[\"id\"])):\n            id: str\n\n\n        class FeatureB(\n            mx.BaseFeature, spec=mx.FeatureSpec(key=\"b\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n        ):\n            id: str\n\n\n        class FeatureC(\n            mx.BaseFeature, spec=mx.FeatureSpec(key=\"c\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n        ):\n            id: str\n\n\n        class FeatureD(\n            mx.BaseFeature,\n            spec=mx.FeatureSpec(\n                key=\"d\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureB), mx.FeatureDep(feature=FeatureC)]\n            ),\n        ):\n            id: str\n\n\n        graph.get_downstream_features([\"a\"])\n        # [FeatureKey(['b']), FeatureKey(['c']), FeatureKey(['d'])]\n        ```\n    \"\"\"\n    # Validate and coerce the source keys\n    validated_sources = ValidatedFeatureKeySequenceAdapter.validate_python(sources)\n\n    source_set = set(validated_sources)\n    visited = set()\n    post_order = []\n    source_set = set(sources)\n    visited = set()\n    post_order = []  # Reverse topological order\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS traversal.\"\"\"\n        if key in visited:\n            return\n        visited.add(key)\n\n        # Find all features that depend on this one\n        for feature_key, definition in self.feature_definitions_by_key.items():\n            if definition.spec.deps:\n                for dep in definition.spec.deps:\n                    if dep.feature == key:\n                        # This feature depends on 'key', so visit it\n                        visit(feature_key)\n\n        post_order.append(key)\n\n    # Visit all sources\n    for source in validated_sources:\n        visit(source)\n\n    # Remove sources from result, reverse to get topological order\n    result = [k for k in reversed(post_order) if k not in source_set]\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.topological_sort_features","title":"metaxy.FeatureGraph.topological_sort_features","text":"<pre><code>topological_sort_features(\n    feature_keys: Sequence[CoercibleToFeatureKey]\n    | None = None,\n    *,\n    descending: bool = False,\n) -&gt; list[FeatureKey]\n</code></pre> <p>Sort feature keys in topological order.</p> <p>Uses stable alphabetical ordering when multiple nodes are at the same level. This ensures deterministic output for diff comparisons and migrations.</p> <p>Implemented using depth-first search with post-order traversal.</p> <p>Parameters:</p> <ul> <li> <code>feature_keys</code>               (<code>Sequence[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of feature keys to sort. Each element can be string, sequence, FeatureKey, or BaseFeature class. If None, sorts all features (both Feature classes and standalone specs) in the graph.</p> </li> <li> <code>descending</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If False (default), dependencies appear before dependents. For a chain A -&gt; B -&gt; C, returns [A, B, C]. If True, dependents appear before dependencies. For a chain A -&gt; B -&gt; C, returns [C, B, A].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys sorted in topological order</p> </li> </ul> Example <pre><code>class VideoRaw(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/raw\", id_columns=[\"id\"])):\n    id: str\n\n\nclass VideoScene(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(key=\"video/scene\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=VideoRaw)]),\n):\n    id: str\n\n\ngraph.topological_sort_features([\"video/raw\", \"video/scene\"])\n# [FeatureKey(['video', 'raw']), FeatureKey(['video', 'scene'])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def topological_sort_features(\n    self,\n    feature_keys: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    descending: bool = False,\n) -&gt; list[FeatureKey]:\n    \"\"\"Sort feature keys in topological order.\n\n    Uses stable alphabetical ordering when multiple nodes are at the same level.\n    This ensures deterministic output for diff comparisons and migrations.\n\n    Implemented using depth-first search with post-order traversal.\n\n    Args:\n        feature_keys: List of feature keys to sort. Each element can be string, sequence,\n            FeatureKey, or BaseFeature class. If None, sorts all features\n            (both Feature classes and standalone specs) in the graph.\n        descending: If False (default), dependencies appear before dependents.\n            For a chain A -&gt; B -&gt; C, returns [A, B, C].\n            If True, dependents appear before dependencies.\n            For a chain A -&gt; B -&gt; C, returns [C, B, A].\n\n    Returns:\n        List of feature keys sorted in topological order\n\n    Example:\n        ```py\n        class VideoRaw(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/raw\", id_columns=[\"id\"])):\n            id: str\n\n\n        class VideoScene(\n            mx.BaseFeature,\n            spec=mx.FeatureSpec(key=\"video/scene\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=VideoRaw)]),\n        ):\n            id: str\n\n\n        graph.topological_sort_features([\"video/raw\", \"video/scene\"])\n        # [FeatureKey(['video', 'raw']), FeatureKey(['video', 'scene'])]\n        ```\n    \"\"\"\n    # Determine which features to sort\n    if feature_keys is None:\n        # Include all features\n        keys_to_sort = set(self.feature_definitions_by_key.keys())\n    else:\n        # Validate and coerce the feature keys\n        validated_keys = ValidatedFeatureKeySequenceAdapter.validate_python(feature_keys)\n        keys_to_sort = set(validated_keys)\n\n    visited = set()\n    result = []  # Topological order (dependencies first)\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS visit with post-order traversal.\"\"\"\n        if key in visited or key not in keys_to_sort:\n            return\n        visited.add(key)\n\n        # Get dependencies from feature definition\n        definition = self.feature_definitions_by_key.get(key)\n        if definition and definition.spec.deps:\n            # Sort dependencies alphabetically for deterministic ordering\n            sorted_deps = sorted(\n                (dep.feature for dep in definition.spec.deps),\n                key=lambda k: k.to_string().lower(),\n            )\n            for dep_key in sorted_deps:\n                if dep_key in keys_to_sort:\n                    visit(dep_key)\n\n        # Add to result after visiting dependencies (post-order)\n        result.append(key)\n\n    # Visit all keys in sorted order for deterministic traversal\n    for key in sorted(keys_to_sort, key=lambda k: k.to_string().lower()):\n        visit(key)\n\n    # Post-order DFS gives topological order (dependencies before dependents)\n    if descending:\n        return list(reversed(result))\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_project_snapshot_version","title":"metaxy.FeatureGraph.get_project_snapshot_version","text":"<pre><code>get_project_snapshot_version(project: str) -&gt; str\n</code></pre> <p>Generate a snapshot version for features belonging to a specific project.</p> <p>Uses feature_definition_version (spec + schema only), excluding external features. This makes the project snapshot independent of external feature changes.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The project name to compute snapshot version for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A hash representing the project's feature definitions.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_project_snapshot_version(self, project: str) -&gt; str:\n    \"\"\"Generate a snapshot version for features belonging to a specific project.\n\n    Uses feature_definition_version (spec + schema only), excluding external features.\n    This makes the project snapshot independent of external feature changes.\n\n    Args:\n        project: The project name to compute snapshot version for.\n\n    Returns:\n        A hash representing the project's feature definitions.\n    \"\"\"\n    project_features = sorted(\n        (\n            (key, defn)\n            for key, defn in self.feature_definitions_by_key.items()\n            if defn.project == project and not defn.is_external\n        ),\n        key=lambda x: x[0],\n    )\n    return self._compute_snapshot_version(project_features)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.to_snapshot","title":"metaxy.FeatureGraph.to_snapshot","text":"<pre><code>to_snapshot(\n    *, project: str | None = None\n) -&gt; dict[str, SerializedFeature]\n</code></pre> <p>Serialize graph to snapshot format.</p> <p>Returns a dict mapping feature_key (string) to feature data dict, including the import path of the Feature class for reconstruction.</p> <p>External features are excluded from the snapshot as they should not be pushed to the metadata store.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Only include features from this project. If not provided, uses the graph's single project (via the <code>project</code> property).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, SerializedFeature]</code>           \u2013            <p>Dictionary mapping feature_key (string) to feature data dict.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If no project is provided and features span multiple projects.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def to_snapshot(self, *, project: str | None = None) -&gt; dict[str, SerializedFeature]:\n    \"\"\"Serialize graph to snapshot format.\n\n    Returns a dict mapping feature_key (string) to feature data dict,\n    including the import path of the Feature class for reconstruction.\n\n    External features are excluded from the snapshot as they should not be\n    pushed to the metadata store.\n\n    Args:\n        project: Only include features from this project. If not provided,\n            uses the graph's single project (via the `project` property).\n\n    Returns:\n        Dictionary mapping feature_key (string) to feature data dict.\n\n    Raises:\n        RuntimeError: If no project is provided and features span multiple projects.\n    \"\"\"\n    if project is None:\n        project = self.project\n\n    snapshot: dict[str, SerializedFeature] = {}\n\n    for feature_key, definition in self.feature_definitions_by_key.items():\n        # Skip external features - they should not be pushed to the metadata store\n        if definition.is_external:\n            continue\n\n        # Skip features from other projects\n        if definition.project != project:\n            continue\n\n        feature_key_str = feature_key.to_string()\n        feature_spec_dict = definition.spec.model_dump(mode=\"json\")\n        feature_schema_dict = definition.feature_schema\n        feature_version = self.get_feature_version(feature_key)\n        definition_version = definition.feature_definition_version\n        project = definition.project\n        class_path = definition.feature_class_path\n\n        snapshot[feature_key_str] = {  # ty: ignore[invalid-assignment]\n            \"feature_spec\": feature_spec_dict,\n            \"feature_schema\": feature_schema_dict,\n            FEATURE_VERSION_COL: feature_version,\n            FEATURE_TRACKING_VERSION_COL: definition_version,\n            \"feature_class_path\": class_path,\n            \"project\": project,\n        }\n\n    return snapshot\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.from_snapshot","title":"metaxy.FeatureGraph.from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(\n    snapshot_data: Mapping[str, Mapping[str, Any]],\n) -&gt; FeatureGraph\n</code></pre> <p>Reconstruct graph from snapshot by creating FeatureDefinition objects.</p> <p>This method creates FeatureDefinition objects directly from the snapshot data without any dynamic imports. The resulting graph contains all feature metadata needed for operations like migrations and comparisons.</p> <p>Parameters:</p> <ul> <li> <code>snapshot_data</code>               (<code>Mapping[str, Mapping[str, Any]]</code>)           \u2013            <p>Dict of feature_key -&gt; dict containing all required fields: - feature_spec (dict): The feature specification - feature_schema (dict): The JSON schema for the feature - feature_class_path (str): The import path of the feature class - project (str): The project name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>New FeatureGraph with FeatureDefinition objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If required fields are missing from snapshot data</p> </li> </ul> Example <pre><code>snapshot_data = {}  # Loaded from metadata store\n# Load snapshot from metadata store\nhistorical_graph = FeatureGraph.from_snapshot(snapshot_data)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef from_snapshot(\n    cls,\n    snapshot_data: Mapping[str, Mapping[str, Any]],\n) -&gt; \"FeatureGraph\":\n    \"\"\"Reconstruct graph from snapshot by creating FeatureDefinition objects.\n\n    This method creates FeatureDefinition objects directly from the snapshot data\n    without any dynamic imports. The resulting graph contains all feature metadata\n    needed for operations like migrations and comparisons.\n\n    Args:\n        snapshot_data: Dict of feature_key -&gt; dict containing all required fields:\n            - feature_spec (dict): The feature specification\n            - feature_schema (dict): The JSON schema for the feature\n            - feature_class_path (str): The import path of the feature class\n            - project (str): The project name\n\n    Returns:\n        New FeatureGraph with FeatureDefinition objects\n\n    Raises:\n        KeyError: If required fields are missing from snapshot data\n\n    Example:\n        ```py\n        snapshot_data = {}  # Loaded from metadata store\n        # Load snapshot from metadata store\n        historical_graph = FeatureGraph.from_snapshot(snapshot_data)\n        ```\n    \"\"\"\n    graph = cls()\n\n    required_fields = (\"feature_spec\", \"feature_schema\", \"feature_class_path\", \"project\")\n\n    for feature_key_str, feature_data in snapshot_data.items():\n        # Validate all required fields are present\n        missing_fields = [f for f in required_fields if f not in feature_data]\n        if missing_fields:\n            raise KeyError(\n                f\"Feature '{feature_key_str}' snapshot is missing required fields: {missing_fields}. \"\n                f\"All snapshots must include: {required_fields}\"\n            )\n\n        definition = FeatureDefinition.from_stored_data(\n            feature_spec=feature_data[\"feature_spec\"],\n            feature_schema=feature_data[\"feature_schema\"],\n            feature_class_path=feature_data[\"feature_class_path\"],\n            project=feature_data[\"project\"],\n            source=\"snapshot\",\n        )\n        graph.add_feature_definition(definition)\n\n    return graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_active","title":"metaxy.FeatureGraph.get_active  <code>classmethod</code>","text":"<pre><code>get_active() -&gt; FeatureGraph\n</code></pre> <p>Get the currently active graph.</p> <p>Returns the graph from the context variable if set, otherwise returns the default global graph.</p> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>Active FeatureGraph instance</p> </li> </ul> Example <pre><code>graph = mx.FeatureGraph.get_active()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef get_active(cls) -&gt; \"FeatureGraph\":\n    \"\"\"Get the currently active graph.\n\n    Returns the graph from the context variable if set, otherwise returns\n    the default global graph.\n\n    Returns:\n        Active FeatureGraph instance\n\n    Example:\n        ```py\n        graph = mx.FeatureGraph.get_active()\n        ```\n    \"\"\"\n    return _active_graph.get() or graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.set_active","title":"metaxy.FeatureGraph.set_active  <code>classmethod</code>","text":"<pre><code>set_active(reg: FeatureGraph) -&gt; None\n</code></pre> <p>Set the active graph for the current context.</p> <p>This sets the context variable that will be returned by get_active(). Typically used in application setup code or test fixtures.</p> <p>Parameters:</p> <ul> <li> <code>reg</code>               (<code>FeatureGraph</code>)           \u2013            <p>FeatureGraph to activate</p> </li> </ul> Example <pre><code>my_graph = mx.FeatureGraph()\nmx.FeatureGraph.set_active(my_graph)\nmx.FeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef set_active(cls, reg: \"FeatureGraph\") -&gt; None:\n    \"\"\"Set the active graph for the current context.\n\n    This sets the context variable that will be returned by get_active().\n    Typically used in application setup code or test fixtures.\n\n    Args:\n        reg: FeatureGraph to activate\n\n    Example:\n        ```py\n        my_graph = mx.FeatureGraph()\n        mx.FeatureGraph.set_active(my_graph)\n        mx.FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    _active_graph.set(reg)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.use","title":"metaxy.FeatureGraph.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Context manager to temporarily use this graph as active.</p> <p>This is the recommended way to use custom registries, especially in tests. The graph is automatically restored when the context exits.</p> <p>Yields:</p> <ul> <li> <code>FeatureGraph</code> (              <code>Self</code> )          \u2013            <p>This graph instance</p> </li> </ul> Example <pre><code>with graph.use():\n\n    class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test\", id_columns=[\"id\"])):\n        id: str\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Context manager to temporarily use this graph as active.\n\n    This is the recommended way to use custom registries, especially in tests.\n    The graph is automatically restored when the context exits.\n\n    Yields:\n        FeatureGraph: This graph instance\n\n    Example:\n        ```py\n        with graph.use():\n\n            class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test\", id_columns=[\"id\"])):\n                id: str\n        ```\n    \"\"\"\n    token = _active_graph.set(self)\n    try:\n        yield self\n    finally:\n        _active_graph.reset(token)\n</code></pre>"},{"location":"reference/api/definitions/relationship/","title":"Lineage Relationships","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship","title":"metaxy.models.lineage.LineageRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for lineage relationship configurations with convenient constructors.</p> <p>This provides a cleaner API for creating lineage relationships while maintaining type safety through discriminated unions.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n  \"properties\": {\n    \"relationship\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"1:1\": \"#/$defs/IdentityRelationship\",\n          \"1:N\": \"#/$defs/ExpansionRelationship\",\n          \"N:1\": \"#/$defs/AggregationRelationship\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/IdentityRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/AggregationRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/ExpansionRelationship\"\n        }\n      ],\n      \"title\": \"Relationship\"\n    }\n  },\n  \"required\": [\n    \"relationship\"\n  ],\n  \"title\": \"LineageRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>relationship</code>                 (<code>LineageRelationshipUnion</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.identity","title":"metaxy.models.lineage.LineageRelationship.identity  <code>classmethod</code>","text":"<pre><code>identity() -&gt; Self\n</code></pre> <p>Create an identity (1:1) relationship.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:1 relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.identity()\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; Self:\n    \"\"\"Create an identity (1:1) relationship.\n\n    Returns:\n        Configured LineageRelationship for 1:1 relationship.\n\n    Example:\n        ```python\n        mx.LineageRelationship.identity()\n        ```\n    \"\"\"\n    return cls(relationship=IdentityRelationship())\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.aggregation","title":"metaxy.models.lineage.LineageRelationship.aggregation  <code>classmethod</code>","text":"<pre><code>aggregation(on: Sequence[str] | None = None) -&gt; Self\n</code></pre> <p>Create an aggregation (N:1) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Columns to group by for aggregation. If None, uses all target ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for N:1 relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef aggregation(cls, on: Sequence[str] | None = None) -&gt; Self:\n    \"\"\"Create an aggregation (N:1) relationship.\n\n    Args:\n        on: Columns to group by for aggregation. If None, uses all target ID columns.\n\n    Returns:\n        Configured LineageRelationship for N:1 relationship.\n\n    Example:\n        ```py\n        mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n        ```\n    \"\"\"\n    return cls(relationship=AggregationRelationship(on=on))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.expansion","title":"metaxy.models.lineage.LineageRelationship.expansion  <code>classmethod</code>","text":"<pre><code>expansion(\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self\n</code></pre> <p>Create an expansion (1:N) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. Required - must explicitly specify which columns link parent to child.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Pattern for generating child IDs. Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:N relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef expansion(\n    cls,\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self:\n    \"\"\"Create an expansion (1:N) relationship.\n\n    Args:\n        on: Parent ID columns that identify the parent record. Child records with\n            the same parent IDs will share the same upstream provenance.\n            Required - must explicitly specify which columns link parent to child.\n        id_generation_pattern: Pattern for generating child IDs.\n            Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().\n\n    Returns:\n        Configured LineageRelationship for 1:N relationship.\n\n    Example:\n        ```py\n        mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n        ```\n    \"\"\"\n    return cls(relationship=ExpansionRelationship(on=on, id_generation_pattern=id_generation_pattern))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.get_aggregation_columns","title":"metaxy.models.lineage.LineageRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>Get columns to aggregate on for this relationship.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>Columns to group by for aggregation, or None if no aggregation needed.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(self, target_id_columns: Sequence[str]) -&gt; Sequence[str] | None:\n    \"\"\"Get columns to aggregate on for this relationship.\n\n    Args:\n        target_id_columns: The target feature's ID columns.\n\n    Returns:\n        Columns to group by for aggregation, or None if no aggregation needed.\n    \"\"\"\n    return self.relationship.get_aggregation_columns(target_id_columns)\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationshipType","title":"metaxy.models.lineage.LineageRelationshipType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of lineage relationship between features.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship","title":"metaxy.models.lineage.IdentityRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-one relationship where each child row maps to exactly one parent row.</p> <p>This is the default relationship type. Parent and child features share the same ID columns and have the same cardinality.</p> <p>Construct this relationship via <code>LineageRelationship.identity</code> classmethod.</p> Example <pre><code>mx.LineageRelationship.identity()\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:1\",\n      \"default\": \"1:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"IdentityRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[IDENTITY]</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","title":"metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>No aggregation needed for identity relationships.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"No aggregation needed for identity relationships.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship","title":"metaxy.models.lineage.ExpansionRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-many relationship where one parent row expands to multiple child rows.</p> <p>Child features have more granular ID columns than the parent. Each parent row generates multiple child rows with additional ID columns.</p> <p>Construct this relationship via <code>LineageRelationship.expansion</code> classmethod.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. If not specified, will be inferred from the available columns.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>)           \u2013            <p>Optional pattern for generating child IDs. Can be \"sequential\", \"hash\", or a custom pattern. If not specified, the feature's load_input() method is responsible for ID generation.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:N\",\n      \"default\": \"1:N\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"On\",\n      \"type\": \"array\"\n    },\n    \"id_generation_pattern\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n      \"title\": \"Id Generation Pattern\"\n    }\n  },\n  \"required\": [\n    \"on\"\n  ],\n  \"title\": \"ExpansionRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[EXPANSION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str]</code>)             </li> <li> <code>id_generation_pattern</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.on","title":"metaxy.models.lineage.ExpansionRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str]\n</code></pre> <p>Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.id_generation_pattern","title":"metaxy.models.lineage.ExpansionRelationship.id_generation_pattern  <code>pydantic-field</code>","text":"<pre><code>id_generation_pattern: str | None = None\n</code></pre> <p>Pattern for generating child IDs. If None, handled by load_input().</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","title":"metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>Get aggregation columns for the joiner phase.</p> <p>For expansion relationships, returns None because aggregation happens during diff resolution, not during joining. The joiner should pass through all parent records without aggregation.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target (child) feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>None - no aggregation during join phase for expansion relationships.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"Get aggregation columns for the joiner phase.\n\n    For expansion relationships, returns None because aggregation\n    happens during diff resolution, not during joining. The joiner\n    should pass through all parent records without aggregation.\n\n    Args:\n        target_id_columns: The target (child) feature's ID columns.\n\n    Returns:\n        None - no aggregation during join phase for expansion relationships.\n    \"\"\"\n    # Expansion relationships don't aggregate during join phase\n    # Aggregation happens later during diff resolution\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship","title":"metaxy.models.lineage.AggregationRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>Many-to-one relationship where multiple parent rows aggregate to one child row.</p> <p>Parent features have more granular ID columns than the child. The child aggregates multiple parent rows by grouping on a subset of the parent's ID columns.</p> <p>Construct this relationship via <code>LineageRelationship.aggregation</code> classmethod.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>)           \u2013            <p>Columns to group by for aggregation. These should be a subset of the target feature's ID columns. If not specified, uses all target ID columns.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"N:1\",\n      \"default\": \"N:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n      \"title\": \"On\"\n    }\n  },\n  \"title\": \"AggregationRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[AGGREGATION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str] | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.on","title":"metaxy.models.lineage.AggregationRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str] | None = None\n</code></pre> <p>Columns to group by for aggregation. Defaults to all target ID columns.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","title":"metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]\n</code></pre> <p>Get columns to aggregate on.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]:\n    \"\"\"Get columns to aggregate on.\"\"\"\n    return self.on if self.on is not None else target_id_columns\n</code></pre>"},{"location":"reference/api/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata behind an interface called <code>MetadataStore</code>.</p> <p>Users can extend this class to implement support for arbitrary metadata storage such as databases, lakehouse formats, or really any kind of external system.</p> <p>Here are some of the built-in metadata store types:</p>"},{"location":"reference/api/metadata-stores/#databases","title":"Databases","text":"<ul> <li> <p>BigQuery</p> </li> <li> <p>ClickHouse</p> </li> <li> <p>DuckDB</p> </li> <li> <p>LanceDB</p> </li> <li> <p><code>IbisMetadataStore</code> (a base class) - see Ibis integration</p> </li> </ul>"},{"location":"reference/api/metadata-stores/#storage-only","title":"Storage Only","text":"<ul> <li>DeltaMetadataStore</li> </ul> <p>The full list can be found here</p>"},{"location":"reference/api/metadata-stores/#metadata-store-interface","title":"Metadata Store Interface","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore","title":"metaxy.MetadataStore","text":"<pre><code>MetadataStore(\n    *,\n    name: str | None = None,\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_create_tables: bool | None = None,\n    materialization_id: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for metadata storage backends.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for this store. For representation purposes only. Is typically included into <code>.display()</code>.</p> </li> <li> <code>hash_algorithm</code>               (<code>HashAlgorithm | None</code>, default:                   <code>None</code> )           \u2013            <p>Hash algorithm to use for the versioning engine.</p> </li> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use.</p> <ul> <li> <p>\"auto\": Prefer the store's native engine and fall back to Polars if needed</p> </li> <li> <p>\"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if at some point the metadata store has to switch to Polars</p> </li> <li> <p>\"polars\": Always use the Polars engine</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. Used when upstream features are not in this store. <code>VersioningEngineMismatchError</code> is not raised when reading from fallback stores.</p> </li> <li> <code>auto_create_tables</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Whether to automatically create tables when writing new features. If not provided, the global Metaxy configuration <code>auto_created_tables</code> option will be used (which can be set via <code>METAXY_AUTO_CREATE_TABLES</code> env var).</p> <p>Warning</p> <p>This is intended for development/testing purposes. Use proper database migration tools like Alembic to manage table infrastructure in production.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID. If provided, all metadata writes will include this ID in the <code>metaxy_materialization_id</code> column. Can be overridden per <code>MetadataStore.write</code> call.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If fallback stores use different hash algorithms or truncation lengths</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If the versioning engine is attempted to be switched to Polars and <code>versioning_engine</code> is set to <code>native</code>.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str | None = None,\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_create_tables: bool | None = None,\n    materialization_id: str | None = None,\n):\n    \"\"\"\n    Initialize the metadata store.\n\n    Args:\n        name: Optional name for this store. For representation purposes only.\n            Is typically included into `.display()`.\n        hash_algorithm: Hash algorithm to use for the versioning engine.\n        versioning_engine: Which versioning engine to use.\n\n            - \"auto\": Prefer the store's native engine and fall back to Polars if needed\n\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if at some point the metadata store has to switch to Polars\n\n            - \"polars\": Always use the Polars engine\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            Used when upstream features are not in this store.\n            `VersioningEngineMismatchError` is not raised when reading from fallback stores.\n        auto_create_tables: Whether to automatically create tables when writing new features.\n            If not provided, the global Metaxy configuration `auto_created_tables` option will\n            be used (which can be set via `METAXY_AUTO_CREATE_TABLES` env var).\n\n            !!! warning\n                This is intended for development/testing purposes.\n                Use proper database migration tools like Alembic to manage table infrastructure in production.\n\n        materialization_id: Optional external orchestration ID.\n            If provided, all metadata writes will include this ID in the `metaxy_materialization_id` column.\n            Can be overridden per [`MetadataStore.write`][metaxy.MetadataStore.write] call.\n\n    Raises:\n        ValueError: If fallback stores use different hash algorithms or truncation lengths\n        VersioningEngineMismatchError: If the versioning engine is attempted to be switched\n            to Polars and `versioning_engine` is set to `native`.\n    \"\"\"\n    # Initialize state early so properties can check it\n    self._name = name\n    self._is_open = False\n    self._context_depth = 0\n    self._versioning_engine = versioning_engine\n    self._materialization_id = materialization_id\n    self._open_cm: AbstractContextManager[Self] | None = None  # Track the open() context manager\n    self._transaction_timestamp: datetime | None = None  # Shared timestamp for write operations\n    self._soft_delete_in_progress: bool = False  # Track if we're inside a soft delete operation\n\n    # Resolve auto_create_tables from global config if not explicitly provided\n    if auto_create_tables is None:\n        from metaxy.config import MetaxyConfig\n\n        self.auto_create_tables = MetaxyConfig.get().auto_create_tables\n    else:\n        self.auto_create_tables = auto_create_tables\n\n    # Use store's default algorithm if not specified\n    if hash_algorithm is None:\n        hash_algorithm = self._get_default_hash_algorithm()\n\n    self.hash_algorithm = hash_algorithm\n\n    self.fallback_stores = fallback_stores or []\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.materialization_id","title":"metaxy.MetadataStore.materialization_id  <code>property</code>","text":"<pre><code>materialization_id: str | None\n</code></pre> <p>The external orchestration ID for this store instance.</p> <p>If set, all metadata writes include this ID in the <code>metaxy_materialization_id</code> column, allowing filtering of rows written during a specific materialization run.</p>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.name","title":"metaxy.MetadataStore.name  <code>property</code>","text":"<pre><code>name: str | None\n</code></pre> <p>The configured name of this store, if any.</p>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.qualified_class_name","title":"metaxy.MetadataStore.qualified_class_name  <code>property</code>","text":"<pre><code>qualified_class_name: str\n</code></pre> <p>The fully qualified class name (module.classname).</p>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.resolve_update","title":"metaxy.MetadataStore.resolve_update","text":"<pre><code>resolve_update(\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]]\n    | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    target_filters: Sequence[Expr] | None = None,\n    lazy: Literal[False] = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"]\n    | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; Increment\n</code></pre><pre><code>resolve_update(\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]]\n    | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    target_filters: Sequence[Expr] | None = None,\n    lazy: Literal[True],\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"]\n    | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>IntoFrame | Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>A dataframe with joined upstream metadata and <code>\"metaxy_provenance_by_field\"</code> column set. When provided, <code>MetadataStore</code> skips loading upstream feature metadata and provenance calculations.</p> <p>Required for root features</p> <p>Metaxy doesn't know how to populate input metadata for root features, so <code>samples</code> argument for must be provided for them.</p> <p>Tip</p> <p>For non-root features, use <code>samples</code> to customize the automatic upstream loading and field provenance calculation. For example, it can be used to requires processing for specific sample IDs.</p> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping from feature keys to lists of Narwhals filter expressions. Keys can be feature classes, FeatureKey objects, or string paths. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: <code>{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}</code></p> </li> <li> <code>global_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of Narwhals filter expressions applied to all features (both upstream and target). These filters are combined with any feature-specific filters from <code>filters</code>. Must reference columns that exist in ALL features. Useful for filtering by common columns like <code>sample_uid</code> across all features. Example: <code>[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]</code></p> </li> <li> <code>target_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of Narwhals filter expressions applied ONLY to the target feature (not to upstream features). Use this when filtering by columns that only exist in the target feature. Example: <code>[nw.col(\"height\").is_null()]</code></p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a metaxy.versioning.types.LazyIncrement or a metaxy.versioning.types.Increment.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>skip_comparison</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, skip the increment comparison logic and return all upstream samples in <code>Increment.added</code>. The <code>changed</code> and <code>removed</code> frames will be empty.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> dataframe has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If <code>versioning_engine</code> has been set to <code>\"native\"</code> and a dataframe of a different implementation has been encountered during <code>resolve_update</code>.</p> </li> </ul> <p>Note</p> <p>This method automatically loads feature definitions from the metadata store before computing the update. This ensures that any external feature dependencies are resolved with their actual definitions from the store, preventing incorrect version calculations from stale external feature definitions.</p> <p>With a root feature</p> <pre><code>import narwhals as nw\nimport polars as pl\n\nsamples = pl.DataFrame(\n    {\n        \"id\": [\"x\", \"y\", \"z\"],\n        \"metaxy_provenance_by_field\": [\n            {\"part_1\": \"h1\", \"part_2\": \"h2\"},\n            {\"part_1\": \"h3\", \"part_2\": \"h4\"},\n            {\"part_1\": \"h5\", \"part_2\": \"h6\"},\n        ],\n    }\n)\nwith store.open(mode=\"w\"):\n    result = store.resolve_update(MyFeature, samples=nw.from_native(samples))\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[nw.Expr]] | None = None,\n    global_filters: Sequence[nw.Expr] | None = None,\n    target_filters: Sequence[nw.Expr] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: A dataframe with joined upstream metadata and `\"metaxy_provenance_by_field\"` column set.\n            When provided, `MetadataStore` skips loading upstream feature metadata and provenance calculations.\n\n            !!! info \"Required for root features\"\n                Metaxy doesn't know how to populate input metadata for root features,\n                so `samples` argument for **must** be provided for them.\n\n            !!! tip\n                For non-root features, use `samples` to customize the automatic upstream loading and field provenance calculation.\n                For example, it can be used to requires processing for specific sample IDs.\n\n            Setting this parameter during normal operations is not required.\n\n        filters: A mapping from feature keys to lists of Narwhals filter expressions.\n            Keys can be feature classes, FeatureKey objects, or string paths.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: `{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}`\n        global_filters: A list of Narwhals filter expressions applied to all features\n            (both upstream and target). These filters are combined with any feature-specific\n            filters from `filters`. Must reference columns that exist in ALL features.\n            Useful for filtering by common columns like `sample_uid` across all features.\n            Example: `[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]`\n        target_filters: A list of Narwhals filter expressions applied ONLY to the target\n            feature (not to upstream features). Use this when filtering by columns that\n            only exist in the target feature.\n            Example: `[nw.col(\"height\").is_null()]`\n        lazy: Whether to return a [metaxy.versioning.types.LazyIncrement][] or a [metaxy.versioning.types.Increment][].\n        versioning_engine: Override the store's versioning engine for this operation.\n        skip_comparison: If True, skip the increment comparison logic and return all\n            upstream samples in `Increment.added`. The `changed` and `removed` frames will\n            be empty.\n\n    Raises:\n        ValueError: If no `samples` dataframe has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If `versioning_engine` has been set to `\"native\"`\n            and a dataframe of a different implementation has been encountered during `resolve_update`.\n\n    !!! note\n        This method automatically loads feature definitions from the metadata store\n        before computing the update. This ensures that any external feature dependencies\n        are resolved with their actual definitions from the store, preventing incorrect\n        version calculations from stale external feature definitions.\n\n    !!! example \"With a root feature\"\n\n        ```py\n        import narwhals as nw\n        import polars as pl\n\n        samples = pl.DataFrame(\n            {\n                \"id\": [\"x\", \"y\", \"z\"],\n                \"metaxy_provenance_by_field\": [\n                    {\"part_1\": \"h1\", \"part_2\": \"h2\"},\n                    {\"part_1\": \"h3\", \"part_2\": \"h4\"},\n                    {\"part_1\": \"h5\", \"part_2\": \"h6\"},\n                ],\n            }\n        )\n        with store.open(mode=\"w\"):\n            result = store.resolve_update(MyFeature, samples=nw.from_native(samples))\n        ```\n    \"\"\"\n    import narwhals as nw\n\n    import metaxy as mx\n    from metaxy.config import MetaxyConfig\n\n    # Sync external feature definitions from the store to replace any external feature placeholders.\n    # This ensures version hashes are computed correctly against actual stored definitions.\n    # it is acceptable to call this here automatically for three reasons:\n    # 1. `resolve_update` is typically only called once at the start of the workflow\n    # 2. `resolve_update` is already doing heavy computations so an extra little call won't hurt performance\n    # 3. it is extremely important to get the result right\n    if MetaxyConfig.get(_allow_default_config=True).sync:\n        mx.sync_external_features(self)\n\n    # Convert samples to Narwhals frame if not already\n    samples_nw: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None\n    if samples is not None:\n        if isinstance(samples, (nw.DataFrame, nw.LazyFrame)):\n            samples_nw = samples\n        else:\n            samples_nw = nw.from_native(samples)  # ty: ignore[invalid-assignment]\n\n    # Normalize filter keys to FeatureKey\n    normalized_filters: dict[FeatureKey, list[nw.Expr]] = {}\n    if filters:\n        for key, exprs in filters.items():\n            feature_key = self._resolve_feature_key(key)\n            normalized_filters[feature_key] = list(exprs)\n\n    # Convert global_filters and target_filters to lists for easy concatenation\n    global_filter_list = list(global_filters) if global_filters else []\n    target_filter_list = list(target_filters) if target_filters else []\n\n    feature_key = self._resolve_feature_key(feature)\n    if self._is_system_table(feature_key):\n        raise NotImplementedError(\"Delete operations are not yet supported for system tables.\")\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature_key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples_nw is None:\n        raise ValueError(\n            f\"Feature {feature_key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    # Combine feature-specific filters, global filters, and target filters for current feature\n    # target_filters are ONLY applied to the current feature, not to upstream features\n    current_feature_filters = [\n        *normalized_filters.get(feature_key, []),\n        *global_filter_list,\n        *target_filter_list,\n    ]\n\n    # Read current metadata with deduplication (with_sample_history=False by default)\n    # Use allow_fallback=False since we only want metadata from THIS store\n    # to determine what needs to be updated locally\n    try:\n        current_metadata: nw.LazyFrame[Any] | None = self.read(\n            feature_key,\n            filters=current_feature_filters if current_feature_filters else None,\n            allow_fallback=False,\n            with_feature_history=False,  # filters by current feature_version\n            with_sample_history=False,  # deduplicates by id_columns, keeping latest\n        )\n    except FeatureNotFoundError:\n        current_metadata = None\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples_nw is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples_nw\n        if current_feature_filters:\n            filtered_samples = samples_nw.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples_nw = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n\n        # For root features, add data_version columns if they don't exist\n        # (root features have no computation, so data_version equals provenance)\n        # Use collect_schema().names() to avoid PerformanceWarning on lazy frames\n        if METAXY_DATA_VERSION_BY_FIELD not in samples_nw.collect_schema().names():\n            samples_nw = samples_nw.with_columns(\n                nw.col(METAXY_PROVENANCE_BY_FIELD).alias(METAXY_DATA_VERSION_BY_FIELD),\n                nw.col(METAXY_PROVENANCE).alias(METAXY_DATA_VERSION),\n            )\n    else:\n        for upstream_spec in plan.deps or []:\n            # Combine feature-specific filters with global filters for upstream\n            upstream_filters = [\n                *normalized_filters.get(upstream_spec.key, []),\n                *global_filter_list,\n            ]\n            upstream_feature_metadata = self.read(\n                upstream_spec.key,\n                filters=upstream_filters if upstream_filters else None,\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = versioning_engine if versioning_engine is not None else self._versioning_engine\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (engine_mode == \"native\" and self.fallback_stores):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if samples_nw is not None and samples_nw.implementation != self.native_implementation():\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples_nw.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples_nw.implementation,\n                        message=f\"Provided `samples` have implementation {samples_nw.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples_nw:\n            samples_nw = switch_implementation_to_polars(samples_nw)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(plan=plan, implementation=implementation) as engine:\n        if skip_comparison:\n            # Skip comparison: return all upstream samples as added\n            if samples_nw is not None:\n                # Root features or user-provided samples: use samples directly\n                # Note: samples already has metaxy_provenance computed\n                added = samples_nw.lazy()\n                input_df = None  # Root features have no upstream input\n            else:\n                # Non-root features: load all upstream with provenance\n                added = engine.load_upstream_with_provenance(\n                    upstream=upstream_by_key,\n                    hash_algo=self.hash_algorithm,\n                    filters=filters_by_key,\n                )\n                input_df = added  # Input is the same as added when skipping comparison\n            changed = None\n            removed = None\n        else:\n            added, changed, removed, input_df = engine.resolve_increment_with_provenance(\n                current=current_metadata,\n                upstream=upstream_by_key,\n                hash_algorithm=self.hash_algorithm,\n                filters=filters_by_key,\n                sample=samples_nw.lazy() if samples_nw is not None else None,\n            )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added if isinstance(added, nw.LazyFrame) else nw.from_native(added),\n            changed=changed if isinstance(changed, nw.LazyFrame) else nw.from_native(changed),\n            removed=removed if isinstance(removed, nw.LazyFrame) else nw.from_native(removed),\n            input=input_df if input_df is None or isinstance(input_df, nw.LazyFrame) else nw.from_native(input_df),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect() if isinstance(changed, nw.LazyFrame) else changed,\n            removed=removed.collect() if isinstance(removed, nw.LazyFrame) else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.compute_provenance","title":"metaxy.MetadataStore.compute_provenance","text":"<pre><code>compute_provenance(\n    feature: CoercibleToFeatureKey, df: FrameT\n) -&gt; FrameT\n</code></pre> <p>Compute provenance columns for a DataFrame with pre-joined upstream data.</p> <p>Note</p> <p>This method may be useful in very rare cases. Rely on <code>MetadataStore.resolve_update</code> instead.</p> <p>Use this method when you perform custom joins outside of Metaxy's auto-join system but still want Metaxy to compute provenance. The method computes metaxy_provenance_by_field, metaxy_provenance, metaxy_data_version_by_field, and metaxy_data_version columns based on the upstream metadata.</p> <p>Info</p> <p>The input DataFrame must contain the renamed metaxy_data_version_by_field columns from each upstream feature. The naming convention follows the pattern <code>metaxy_data_version_by_field__&lt;feature_key.to_column_suffix()&gt;</code>. For example, for an upstream feature with key <code>[\"video\", \"raw\"]</code>, the column should be named <code>metaxy_data_version_by_field__video_raw</code>.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The feature to compute provenance for.</p> </li> <li> <code>df</code>               (<code>FrameT</code>)           \u2013            <p>A DataFrame containing pre-joined upstream data with renamed metaxy_data_version_by_field columns from each upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FrameT</code>           \u2013            <p>The input DataFrame with provenance columns added. Returns the same</p> </li> <li> <code>FrameT</code>           \u2013            <p>frame type as the input, either an eager DataFrame or a LazyFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If the store is not open.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If required upstream <code>metaxy_data_version_by_field</code> columns are missing from the DataFrame.</p> </li> </ul> Example <pre><code>    # Read upstream metadata\n    video_df = store.read(VideoFeature).collect()\n    audio_df = store.read(AudioFeature).collect()\n\n    # Rename data_version_by_field columns to the expected convention\n    video_df = video_df.rename({\n        \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__video_raw\"\n    })\n    audio_df = audio_df.rename({\n        \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__audio_raw\"\n    })\n\n    # Perform custom join\n    joined = video_df.join(audio_df, on=\"sample_uid\", how=\"inner\")\n\n    # Compute provenance\n    with_provenance = store.compute_provenance(MyFeature, joined)\n\n    # Pass to resolve_update\n    increment = store.resolve_update(MyFeature, samples=with_provenance)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def compute_provenance(\n    self,\n    feature: CoercibleToFeatureKey,\n    df: FrameT,\n) -&gt; FrameT:\n    \"\"\"Compute provenance columns for a DataFrame with pre-joined upstream data.\n\n    !!! note\n        This method may be useful in very rare cases.\n        Rely on [`MetadataStore.resolve_update`][metaxy.metadata_store.base.MetadataStore.resolve_update] instead.\n\n    Use this method when you perform custom joins outside of Metaxy's auto-join\n    system but still want Metaxy to compute provenance. The method computes\n    metaxy_provenance_by_field, metaxy_provenance, metaxy_data_version_by_field,\n    and metaxy_data_version columns based on the upstream metadata.\n\n    !!! info\n        The input DataFrame must contain the renamed metaxy_data_version_by_field\n        columns from each upstream feature. The naming convention follows the pattern\n        `metaxy_data_version_by_field__&lt;feature_key.to_column_suffix()&gt;`. For example, for an\n        upstream feature with key `[\"video\", \"raw\"]`, the column should be named\n        `metaxy_data_version_by_field__video_raw`.\n\n    Args:\n        feature: The feature to compute provenance for.\n        df: A DataFrame containing pre-joined upstream data with renamed\n            metaxy_data_version_by_field columns from each upstream feature.\n\n    Returns:\n        The input DataFrame with provenance columns added. Returns the same\n        frame type as the input, either an eager DataFrame or a LazyFrame.\n\n    Raises:\n        StoreNotOpenError: If the store is not open.\n        ValueError: If required upstream `metaxy_data_version_by_field` columns\n            are missing from the DataFrame.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n\n            # Read upstream metadata\n            video_df = store.read(VideoFeature).collect()\n            audio_df = store.read(AudioFeature).collect()\n\n            # Rename data_version_by_field columns to the expected convention\n            video_df = video_df.rename({\n                \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__video_raw\"\n            })\n            audio_df = audio_df.rename({\n                \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__audio_raw\"\n            })\n\n            # Perform custom join\n            joined = video_df.join(audio_df, on=\"sample_uid\", how=\"inner\")\n\n            # Compute provenance\n            with_provenance = store.compute_provenance(MyFeature, joined)\n\n            # Pass to resolve_update\n            increment = store.resolve_update(MyFeature, samples=with_provenance)\n        ```\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature_key)\n\n    # Use native implementation if DataFrame matches, otherwise fall back to Polars\n    implementation = self.native_implementation()\n    if df.implementation != implementation:\n        implementation = nw.Implementation.POLARS\n        df = switch_implementation_to_polars(df)  # ty: ignore[no-matching-overload]\n\n    with self.create_versioning_engine(plan=plan, implementation=implementation) as engine:\n        # Validate required upstream columns exist\n        expected_columns = {\n            dep.feature: engine.get_renamed_data_version_by_field_col(dep.feature)\n            for dep in (plan.feature_deps or [])\n        }\n\n        df_columns = set(df.collect_schema().names())  # ty: ignore[invalid-argument-type]\n        missing_columns = [\n            f\"{col} (from upstream feature {key.to_string()})\"\n            for key, col in expected_columns.items()\n            if col not in df_columns\n        ]\n\n        if missing_columns:\n            raise ValueError(\n                f\"DataFrame is missing required upstream columns for computing \"\n                f\"provenance of feature {feature_key.to_string()}. \"\n                f\"Missing columns: {missing_columns}. \"\n                f\"Make sure to rename metaxy_data_version_by_field columns from \"\n                f\"each upstream feature using the pattern \"\n                f\"metaxy_data_version_by_field__&lt;feature_key.table_name&gt;.\"\n            )\n\n        return engine.compute_provenance_columns(df, hash_algo=self.hash_algorithm)  # ty: ignore[invalid-argument-type]\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read","title":"metaxy.MetadataStore.read","text":"<pre><code>read(\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: Literal[False] = False,\n) -&gt; LazyFrame[Any]\n</code></pre><pre><code>read(\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: Literal[True],\n) -&gt; tuple[LazyFrame[Any], MetadataStore]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>By default, does not include:    - rows from historical feature versions (configured via <code>with_feature_history=False</code>)    - rows that have been overwritten by subsequent writes with the same feature version (configured via <code>with_sample_history=False</code>)    - soft-deleted with <code>metaxy_deleted_at</code> set to a non-null value (configured via <code>include_soft_deleted=False</code>)</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with with_feature_history=False)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to allow fallback to upstream stores if the requested feature is not found in the current store.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include rows from all historical feature versions. By default (False), only returns rows with the currently registered feature version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations per sample. By default (False), deduplicates samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> <li> <code>include_soft_deleted</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, include soft-deleted rows in the result. Previous historical materializations of the same feature version will be effectively removed from the output otherwise.</p> </li> <li> <code>with_store_info</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return a tuple of (LazyFrame, MetadataStore) where the MetadataStore is the store that actually contained the feature (which may be a fallback store if allow_fallback=True).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | tuple[LazyFrame[Any], MetadataStore]</code>           \u2013            <p>Narwhals <code>LazyFrame</code> with metadata, or, if <code>with_store_info=True</code>, a tuple of (<code>LazyFrame</code>, <code>MetadataStore</code>) containing the metadata store which has been actually used to retrieve the feature (may be a fallback store).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existent Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and with_feature_history=False are provided</p> </li> </ul> <p>Info</p> <p>When this method is called with default arguments, it will return the latest (by <code>metaxy_created_at</code>) metadata for the current feature version excluding soft-deleted rows. Therefore, it's perfectly suitable for most use cases.</p> <p>Warning</p> <p>The order of rows is not guaranteed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: bool = False,\n) -&gt; nw.LazyFrame[Any] | tuple[nw.LazyFrame[Any], MetadataStore]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    By default, does not include:\n       - rows from historical feature versions (configured via `with_feature_history=False`)\n       - rows that have been overwritten by subsequent writes with the same feature version (configured via `with_sample_history=False`)\n       - soft-deleted with `metaxy_deleted_at` set to a non-null value (configured via `include_soft_deleted=False`)\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with with_feature_history=False)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: Whether to allow fallback to upstream stores if the requested feature is not found in the current store.\n        with_feature_history: If True, include rows from all historical feature versions.\n            By default (False), only returns rows with the currently registered feature version.\n        with_sample_history: If True, include all historical materializations per sample.\n            By default (False), deduplicates samples within `id_columns` groups ordered by `metaxy_created_at`.\n        include_soft_deleted: If `True`, include soft-deleted rows in the result. Previous historical materializations of the same feature version will be effectively removed from the output otherwise.\n        with_store_info: If `True`, return a tuple of (LazyFrame, MetadataStore) where\n            the MetadataStore is the store that actually contained the feature (which\n            may be a fallback store if allow_fallback=True).\n\n    Returns:\n        Narwhals `LazyFrame` with metadata, or, if `with_store_info=True`, a tuple of (`LazyFrame`, `MetadataStore`) containing the metadata store which has been actually used to retrieve the feature (may be a fallback store).\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existent Metaxy system data\n        ValueError: If both feature_version and with_feature_history=False are provided\n\n    !!! info\n        When this method is called with default arguments, it will return the latest (by `metaxy_created_at`)\n        metadata for the current feature version excluding soft-deleted rows. Therefore, it's perfectly suitable\n        for most use cases.\n\n    !!! warning\n        The order of rows is not guaranteed.\n    \"\"\"\n    import metaxy as mx\n    from metaxy.config import MetaxyConfig\n\n    self._check_open()\n\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Sync external features if auto-sync is enabled (default)\n    # This call is a no-op most of the time and is very lightweight when it's not\n    # Skip for system tables to avoid infinite recursion (sync_external_features reads system tables)\n    if not is_system_table and MetaxyConfig.get(_allow_default_config=True).sync:\n        mx.sync_external_features(self)\n\n    # If caller wants soft-deleted records, do not filter them out later\n    filter_deleted = not include_soft_deleted and not is_system_table\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and not with_feature_history:\n        raise ValueError(\n            \"Cannot specify both feature_version and with_feature_history=False. \"\n            \"Use with_feature_history=True with feature_version parameter.\"\n        )\n\n    # Separate system filters (applied before dedup) from user filters (applied after dedup)\n    # System filters like feature_version need to be applied early to reduce data volume\n    # User filters should see the deduplicated view of the data\n    system_filters: list[nw.Expr] = []\n    user_filters = list(filters) if filters else []\n\n    # Add feature_version filter only when needed (this is a system filter)\n    if not with_feature_history or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key) if not with_feature_history else feature_version\n        )\n        system_filters.append(version_filter)\n\n    # If user filters are provided, we need to read all columns since filters may\n    # reference columns not in the requested columns list. Column selection happens\n    # after filtering\n    if user_filters:\n        read_columns = None\n    elif columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [c for c in ALL_SYSTEM_COLUMNS if c not in columns_set]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        # Only pass system filters to _read_feature\n        # User filters will be applied after deduplication\n        lazy_frame = self._read_feature(\n            feature, filters=system_filters if system_filters else None, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self}. Invoke `metaxy push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where _read_feature returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self}. Invoke `metaxy push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table:\n        # Deduplicate first, then filter soft-deleted rows\n        if not with_sample_history:\n            id_cols = list(self._resolve_feature_plan(feature_key).feature.id_columns)\n            # Treat soft-deletes like hard deletes by ordering on the\n            # most recent lifecycle timestamp.\n            lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n                df=lazy_frame,\n                group_columns=id_cols,\n                timestamp_columns=[METAXY_DELETED_AT, METAXY_UPDATED_AT],\n            )\n\n        if filter_deleted:\n            lazy_frame = lazy_frame.filter(nw.col(METAXY_DELETED_AT).is_null())\n\n        # Apply user filters AFTER deduplication so they see the latest version of each row\n        for user_filter in user_filters:\n            lazy_frame = lazy_frame.filter(user_filter)\n\n    # For system tables, apply user filters directly (no dedup needed)\n    if lazy_frame is not None and is_system_table:\n        for user_filter in user_filters:\n            lazy_frame = lazy_frame.filter(user_filter)\n\n    if lazy_frame is not None:\n        # After dedup and user filters, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        if with_store_info:\n            return lazy_frame, self\n        return lazy_frame\n\n    # Try fallback stores (opened on demand)\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Open fallback store on demand for reading\n                with store:\n                    # Use full read to handle nested fallback chains\n                    if with_store_info:\n                        return store.read(\n                            feature,\n                            feature_version=feature_version,\n                            filters=filters,\n                            columns=columns,\n                            allow_fallback=True,\n                            with_feature_history=with_feature_history,\n                            with_sample_history=with_sample_history,\n                            include_soft_deleted=include_soft_deleted,\n                            with_store_info=True,\n                        )\n                    return store.read(\n                        feature,\n                        feature_version=feature_version,\n                        filters=filters,\n                        columns=columns,\n                        allow_fallback=True,\n                        with_feature_history=with_feature_history,\n                        with_sample_history=with_sample_history,\n                        include_soft_deleted=include_soft_deleted,\n                    )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\" + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write","title":"metaxy.MetadataStore.write","text":"<pre><code>write(\n    feature: CoercibleToFeatureKey,\n    df: IntoFrame,\n    materialization_id: str | None = None,\n) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for this write. Overrides the store's default <code>materialization_id</code> if provided. Useful for tracking which orchestration run produced this metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> <p>Note:     - Must be called within a <code>MetadataStore.open(mode=\"w\")</code> context manager.</p> <pre><code>- Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.\n\n- Fallback stores are never used for writes.\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write(\n    self,\n    feature: CoercibleToFeatureKey,\n    df: IntoFrame,\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n        materialization_id: Optional external orchestration ID for this write.\n            Overrides the store's default `materialization_id` if provided.\n            Useful for tracking which orchestration run produced this metadata.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"w\")` context manager.\n\n        - Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.\n\n        - Fallback stores are never used for writes.\n\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, (nw.DataFrame, nw.LazyFrame)), f\"df must be a Narwhals DataFrame, got {type(df_nw)}\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self._write_feature(feature_key, df_nw)\n        return\n\n    # Use collect_schema().names() to avoid PerformanceWarning on lazy frames\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.collect_schema().names():\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\")\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadataStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature, materialization_id=materialization_id)\n\n    self._validate_schema(df_nw)\n    self._write_feature(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_multi","title":"metaxy.MetadataStore.write_multi","text":"<pre><code>write_multi(\n    metadata: Mapping[Any, IntoFrame],\n    materialization_id: str | None = None,\n) -&gt; None\n</code></pre> <p>Write metadata for multiple features in reverse topological order.</p> <p>Processes features so that dependents are written before their dependencies. This ordering ensures that downstream features are written first, which can be useful for certain data consistency requirements or when features need to be processed in a specific order.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Mapping[Any, IntoFrame]</code>)           \u2013            <p>Mapping from feature keys to metadata DataFrames. Keys can be any type coercible to FeatureKey (string, sequence, FeatureKey, or BaseFeature class). Values must be DataFrames compatible with Narwhals, containing required system columns.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for all writes. Overrides the store's default <code>materialization_id</code> if provided. Applied to all feature writes in this batch.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If any DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Must be called within a <code>MetadataStore.open(mode=\"w\")</code> context manager.</li> <li>Empty mappings are handled gracefully (no-op).</li> <li>Each feature's metadata is written via <code>write</code>, so all   validation and system column handling from that method applies.</li> </ul> Example <pre><code>with store.open(mode=\"w\"):\n    store.write_multi(\n        {\n            ChildFeature: child_df,\n            ParentFeature: parent_df,\n        }\n    )\n# Features are written in reverse topological order:\n# ChildFeature first, then ParentFeature\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_multi(\n    self,\n    metadata: Mapping[Any, IntoFrame],\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for multiple features in reverse topological order.\n\n    Processes features so that dependents are written before their dependencies.\n    This ordering ensures that downstream features are written first, which can\n    be useful for certain data consistency requirements or when features need\n    to be processed in a specific order.\n\n    Args:\n        metadata: Mapping from feature keys to metadata DataFrames.\n            Keys can be any type coercible to FeatureKey (string, sequence,\n            FeatureKey, or BaseFeature class). Values must be DataFrames\n            compatible with Narwhals, containing required system columns.\n        materialization_id: Optional external orchestration ID for all writes.\n            Overrides the store's default `materialization_id` if provided.\n            Applied to all feature writes in this batch.\n\n    Raises:\n        MetadataSchemaError: If any DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"w\")` context manager.\n        - Empty mappings are handled gracefully (no-op).\n        - Each feature's metadata is written via `write`, so all\n          validation and system column handling from that method applies.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        with store.open(mode=\"w\"):\n            store.write_multi(\n                {\n                    ChildFeature: child_df,\n                    ParentFeature: parent_df,\n                }\n            )\n        # Features are written in reverse topological order:\n        # ChildFeature first, then ParentFeature\n        ```\n    \"\"\"\n    if not metadata:\n        return\n\n    # Build mapping from resolved keys to dataframes in one pass\n    resolved_metadata = {self._resolve_feature_key(key): df for key, df in metadata.items()}\n\n    # Get reverse topological order (dependents first)\n    graph = current_graph()\n    sorted_keys = graph.topological_sort_features(list(resolved_metadata.keys()), descending=True)\n\n    # Write metadata in reverse topological order\n    for feature_key in sorted_keys:\n        self.write(\n            feature_key,\n            resolved_metadata[feature_key],\n            materialization_id=materialization_id,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.config_model","title":"metaxy.MetadataStore.config_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[MetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef config_model(cls) -&gt; type[MetadataStoreConfig]:\n    \"\"\"Return the configuration model class for this store type.\n\n    Subclasses must override this to return their specific config class.\n\n    Returns:\n        The config class type (e.g., DuckDBMetadataStoreConfig)\n\n    Note:\n        Subclasses override this with a more specific return type.\n        Type checkers may show a warning about incompatible override,\n        but this is intentional - each store returns its own config type.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.from_config","title":"metaxy.MetadataStore.from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: MetadataStoreConfig, **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a store instance from a configuration object.</p> <p>This method creates a store by: 1. Converting the config to a dict 2. Resolving fallback store names to actual store instances 3. Calling the store's init with the config parameters</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetadataStoreConfig</code>)           \u2013            <p>Configuration object (should be the type returned by config_model())</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed directly to the store constructor (e.g., materialization_id for runtime parameters not in config)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>A new store instance configured according to the config object</p> </li> </ul> Example <pre><code>from metaxy.metadata_store.duckdb import (\n    DuckDBMetadataStore,\n    DuckDBMetadataStoreConfig,\n)\n\nconfig = DuckDBMetadataStoreConfig(\n    database=\"metadata.db\",\n    fallback_stores=[\"prod\"],\n)\n\nstore = DuckDBMetadataStore.from_config(config)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MetadataStoreConfig, **kwargs: Any) -&gt; Self:\n    \"\"\"Create a store instance from a configuration object.\n\n    This method creates a store by:\n    1. Converting the config to a dict\n    2. Resolving fallback store names to actual store instances\n    3. Calling the store's __init__ with the config parameters\n\n    Args:\n        config: Configuration object (should be the type returned by config_model())\n        **kwargs: Additional arguments passed directly to the store constructor\n            (e.g., materialization_id for runtime parameters not in config)\n\n    Returns:\n        A new store instance configured according to the config object\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```python\n        from metaxy.metadata_store.duckdb import (\n            DuckDBMetadataStore,\n            DuckDBMetadataStoreConfig,\n        )\n\n        config = DuckDBMetadataStoreConfig(\n            database=\"metadata.db\",\n            fallback_stores=[\"prod\"],\n        )\n\n        store = DuckDBMetadataStore.from_config(config)\n        ```\n    \"\"\"\n    # Convert config to dict, excluding unset values\n    config_dict = config.model_dump(exclude_unset=True)\n\n    # Pop and resolve fallback store names to actual store instances\n    fallback_store_names = config_dict.pop(\"fallback_stores\", [])\n    fallback_stores = [MetaxyConfig.get().get_store(name) for name in fallback_store_names]\n\n    # Create store with resolved fallback stores, config, and extra kwargs\n    return cls(fallback_stores=fallback_stores, **config_dict, **kwargs)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.native_implementation","title":"metaxy.MetadataStore.native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.create_versioning_engine","title":"metaxy.MetadataStore.create_versioning_engine","text":"<pre><code>create_versioning_engine(\n    plan: FeaturePlan, implementation: Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.open","title":"metaxy.MetadataStore.open  <code>abstractmethod</code>","text":"<pre><code>open(mode: AccessMode = 'r') -&gt; Iterator[Self]\n</code></pre> <p>Open/initialize the store for operations.</p> <p>Context manager that opens the store with specified access mode. Called internally by <code>__enter__</code>. Child classes should implement backend-specific connection setup/teardown here.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'r'</code> )           \u2013            <p>Access mode for this connection session.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Note <p>Users should prefer using <code>with store:</code> pattern except when write access mode is needed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef open(self, mode: AccessMode = \"r\") -&gt; Iterator[Self]:\n    \"\"\"Open/initialize the store for operations.\n\n    Context manager that opens the store with specified access mode.\n    Called internally by `__enter__`.\n    Child classes should implement backend-specific connection setup/teardown here.\n\n    Args:\n        mode: Access mode for this connection session.\n\n    Yields:\n        Self: The store instance with connection open\n\n    Note:\n        Users should prefer using `with store:` pattern except when write access mode is needed.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__enter__","title":"metaxy.MetadataStore.__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = \"w\" if self.auto_create_tables else \"r\"\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)  # ty: ignore[invalid-assignment]\n    self._open_cm.__enter__()  # ty: ignore[possibly-missing-attribute]\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.validate_hash_algorithm","title":"metaxy.MetadataStore.validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(\n    check_fallback_stores: bool = True,\n) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.drop_feature_metadata","title":"metaxy.MetadataStore.drop_feature_metadata","text":"<pre><code>drop_feature_metadata(\n    feature: CoercibleToFeatureKey,\n) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>with store_with_data.open(mode=\"w\"):\n    assert store_with_data.has_feature(MyFeature)\n    store_with_data.drop_feature_metadata(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        with store_with_data.open(mode=\"w\"):\n            assert store_with_data.has_feature(MyFeature)\n            store_with_data.drop_feature_metadata(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    if self._is_system_table(feature_key):\n        raise NotImplementedError(f\"{self.__class__.__name__} does not support deletes for system tables\")\n    self._drop_feature(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.delete","title":"metaxy.MetadataStore.delete","text":"<pre><code>delete(\n    feature: CoercibleToFeatureKey,\n    filters: Sequence[Expr] | Expr | None,\n    *,\n    soft: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n) -&gt; None\n</code></pre> <p>Delete records matching provided filters.</p> <p>Performs a soft delete by default. This is achieved by setting metaxy_deleted_at to the current timestamp. Subsequent [[MetadataStore.read]] calls would ignore these records by default.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to delete from.</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | Expr | None</code>)           \u2013            <p>One or more Narwhals expressions or a sequence of expressions that determine which records to delete. If <code>None</code>, deletes all records (subject to <code>with_feature_history</code> and <code>with_sample_history</code> constraints).</p> </li> <li> <code>soft</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform a soft delete.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, delete across all historical feature versions. If False (default), only current version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations. If False (default), deduplicate to latest rows.</p> </li> </ul> <p>Critical</p> <p>By default, deletions target historical records. Even when <code>with_feature_history</code> is <code>False</code>, records with the same feature version but an older <code>metaxy_created_at</code> would be targeted as well. Consider adding additional conditions to <code>filters</code> if you want to avoid that.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def delete(\n    self,\n    feature: CoercibleToFeatureKey,\n    filters: Sequence[nw.Expr] | nw.Expr | None,\n    *,\n    soft: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n) -&gt; None:\n    \"\"\"Delete records matching provided filters.\n\n    Performs a soft delete by default. This is achieved by setting metaxy_deleted_at to the current timestamp.\n    Subsequent [[MetadataStore.read]] calls would ignore these records by default.\n\n    Args:\n        feature: Feature to delete from.\n        filters: One or more Narwhals expressions or a sequence of expressions that determine which records to delete.\n            If `None`, deletes all records (subject to `with_feature_history` and `with_sample_history` constraints).\n        soft: Whether to perform a soft delete.\n        with_feature_history: If True, delete across all historical feature versions. If False (default), only current version.\n        with_sample_history: If True, include all historical materializations. If False (default), deduplicate to latest rows.\n\n    !!! critical\n        By default, deletions target historical records. Even when `with_feature_history` is `False`,\n        records with the same feature version but an older `metaxy_created_at` would be targeted as\n        well. Consider adding additional conditions to `filters` if you want to avoid that.\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n\n    # Normalize filters to list\n    if filters is None:\n        filter_list: list[nw.Expr] = []\n    elif isinstance(filters, nw.Expr):\n        filter_list = [filters]\n    else:\n        filter_list = list(filters)\n\n    if soft:\n        # Soft delete: mark records with deletion timestamp, preserving original updated_at\n        lazy = self.read(\n            feature_key,\n            filters=filter_list,\n            include_soft_deleted=False,\n            with_feature_history=with_feature_history,\n            with_sample_history=with_sample_history,\n            allow_fallback=True,\n        )\n        with self._shared_transaction_timestamp(soft_delete=True) as ts:\n            soft_deletion_marked = lazy.with_columns(\n                nw.lit(ts).alias(METAXY_DELETED_AT),\n            )\n            self.write(feature_key, soft_deletion_marked.to_native())\n    else:\n        # Hard delete: add version filter if needed, then delegate to backend\n        if not with_feature_history and not self._is_system_table(feature_key):\n            version_filter = nw.col(METAXY_FEATURE_VERSION) == current_graph().get_feature_version(feature_key)\n            filter_list = [version_filter, *filter_list]\n\n        self._delete_feature(feature_key, filter_list, with_feature_history=with_feature_history)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_feature_schema_from_store","title":"metaxy.MetadataStore.read_feature_schema_from_store","text":"<pre><code>read_feature_schema_from_store(\n    feature: CoercibleToFeatureKey,\n) -&gt; Schema\n</code></pre> <p>Read the schema for a feature from the store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read schema for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>Narwhals schema for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in the store</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_feature_schema_from_store(\n    self,\n    feature: CoercibleToFeatureKey,\n) -&gt; nw.Schema:\n    \"\"\"Read the schema for a feature from the store.\n\n    Args:\n        feature: Feature to read schema for\n\n    Returns:\n        Narwhals schema for the feature\n\n    Raises:\n        FeatureNotFoundError: If feature not found in the store\n    \"\"\"\n    lazy = self.read(\n        feature,\n        allow_fallback=False,\n    )\n    return lazy.collect_schema()\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.has_feature","title":"metaxy.MetadataStore.has_feature","text":"<pre><code>has_feature(\n    feature: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = False,\n) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self._read_feature(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.display","title":"metaxy.MetadataStore.display  <code>abstractmethod</code>","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return a human-readable display string for this store.</p> <p>Used in warnings, logs, and CLI output to identify the store.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef display(self) -&gt; str:\n    \"\"\"Return a human-readable display string for this store.\n\n    Used in warnings, logs, and CLI output to identify the store.\n\n    Returns:\n        Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__repr__","title":"metaxy.MetadataStore.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return the display string with optional name prefix.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the display string with optional name prefix.\"\"\"\n    return self._format_display_with_name(self.display())\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.find_store_for_feature","title":"metaxy.MetadataStore.find_store_for_feature","text":"<pre><code>find_store_for_feature(\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; MetadataStore | None\n</code></pre> <p>Find the store that contains the given feature.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to find</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores when the feature is not found in the current store</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore | None</code>           \u2013            <p>The store containing the feature, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def find_store_for_feature(\n    self,\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; MetadataStore | None:\n    \"\"\"Find the store that contains the given feature.\n\n    Args:\n        feature_key: Feature to find\n        check_fallback: Whether to check fallback stores when the feature\n            is not found in the current store\n\n    Returns:\n        The store containing the feature, or None if not found\n    \"\"\"\n    self._check_open()\n\n    # Check if feature exists in this store\n    if self.has_feature(feature_key):\n        return self\n\n    # Try fallback stores if enabled (opened on demand)\n    if check_fallback:\n        for store in self.fallback_stores:\n            with store:\n                found = store.find_store_for_feature(feature_key, check_fallback=True)\n                if found is not None:\n                    return found\n\n    return None\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.get_store_metadata","title":"metaxy.MetadataStore.get_store_metadata","text":"<pre><code>get_store_metadata(\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; dict[str, Any]\n</code></pre> <p>Arbitrary key-value pairs with useful metadata for logging purposes (like a path in storage).</p> <p>This method should not expose sensitive information.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to get metadata for</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores when the feature is not found in the current store</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary with store-specific metadata. Contains:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>name</code>, <code>display</code>: The queried store (self)</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>resolved_from</code>: Where the feature was actually found (may be a fallback store), includes <code>name</code>, <code>display</code>, and store-specific fields like <code>table_name</code> or <code>uri</code></li> </ul> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def get_store_metadata(\n    self,\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Arbitrary key-value pairs with useful metadata for logging purposes (like a path in storage).\n\n    This method should not expose sensitive information.\n\n    Args:\n        feature_key: Feature to get metadata for\n        check_fallback: Whether to check fallback stores when the feature\n            is not found in the current store\n\n    Returns:\n        Dictionary with store-specific metadata. Contains:\n        - `name`, `display`: The queried store (self)\n        - `resolved_from`: Where the feature was actually found (may be a fallback store),\n            includes `name`, `display`, and store-specific fields like `table_name` or `uri`\n    \"\"\"\n    resolved_store = self.find_store_for_feature(feature_key, check_fallback=check_fallback)\n    result: dict[str, Any] = {\n        \"name\": self.name,\n        \"display\": self.display(),\n    }\n    if resolved_store is not None:\n        result[\"resolved_from\"] = resolved_store.get_store_info(feature_key)\n    return result\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.get_store_info","title":"metaxy.MetadataStore.get_store_info","text":"<pre><code>get_store_info(\n    feature_key: CoercibleToFeatureKey,\n) -&gt; dict[str, Any]\n</code></pre> <p>Build a dictionary with store identification and feature-specific metadata.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to get metadata for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary containing:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>name</code>: The store name</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>type</code>: The fully qualified class name (module.classname)</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>display</code>: Human-readable store description</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li>Store-specific metadata from <code>_get_store_metadata_impl</code> (e.g., <code>table_name</code>, <code>uri</code>)</li> </ul> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def get_store_info(self, feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]:\n    \"\"\"Build a dictionary with store identification and feature-specific metadata.\n\n    Args:\n        feature_key: Feature to get metadata for\n\n    Returns:\n        Dictionary containing:\n        - `name`: The store name\n        - `type`: The fully qualified class name (module.classname)\n        - `display`: Human-readable store description\n        - Store-specific metadata from `_get_store_metadata_impl` (e.g., `table_name`, `uri`)\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"type\": self.qualified_class_name,\n        \"display\": self.display(),\n        **self._get_store_metadata_impl(feature_key),\n    }\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.calculate_input_progress","title":"metaxy.MetadataStore.calculate_input_progress","text":"<pre><code>calculate_input_progress(\n    lazy_increment: LazyIncrement,\n    feature_key: CoercibleToFeatureKey,\n) -&gt; float | None\n</code></pre> <p>Calculate progress percentage from lazy increment.</p> <p>Uses the <code>input</code> field from LazyIncrement to count total input units and compares with <code>added</code> to determine how many are missing.</p> <p>Progress represents the percentage of input units that have been processed at least once. Stale samples (in <code>changed</code>) are counted as processed since they have existing metadata, even though they may need re-processing due to upstream changes.</p> <p>Parameters:</p> <ul> <li> <code>lazy_increment</code>               (<code>LazyIncrement</code>)           \u2013            <p>The lazy increment containing input and added dataframes.</p> </li> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The feature key to look up lineage information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float | None</code>           \u2013            <p>Progress percentage (0-100), or None if input is not available.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def calculate_input_progress(\n    self,\n    lazy_increment: LazyIncrement,\n    feature_key: CoercibleToFeatureKey,\n) -&gt; float | None:\n    \"\"\"Calculate progress percentage from lazy increment.\n\n    Uses the `input` field from LazyIncrement to count total input units\n    and compares with `added` to determine how many are missing.\n\n    Progress represents the percentage of input units that have been processed\n    at least once. Stale samples (in `changed`) are counted as processed since\n    they have existing metadata, even though they may need re-processing due to\n    upstream changes.\n\n    Args:\n        lazy_increment: The lazy increment containing input and added dataframes.\n        feature_key: The feature key to look up lineage information.\n\n    Returns:\n        Progress percentage (0-100), or None if input is not available.\n    \"\"\"\n    if lazy_increment.input is None:\n        return None\n\n    key = self._resolve_feature_key(feature_key)\n    graph = current_graph()\n    plan = graph.get_feature_plan(key)\n\n    # Get the columns that define input units from the feature plan\n    input_id_columns = plan.input_id_columns\n\n    # Count distinct input units using two separate queries\n    # We can't use concat because input and added may have different schemas\n    # (e.g., nullable vs non-nullable columns)\n    total_units: int = lazy_increment.input.select(input_id_columns).unique().select(nw.len()).collect().item()\n\n    if total_units == 0:\n        return None  # No input available from upstream\n\n    missing_units: int = lazy_increment.added.select(input_id_columns).unique().select(nw.len()).collect().item()\n\n    processed_units = total_units - missing_units\n    return (processed_units / total_units) * 100\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.copy_metadata","title":"metaxy.MetadataStore.copy_metadata","text":"<pre><code>copy_metadata(\n    from_store: MetadataStore,\n    features: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    filters: Mapping[str, Sequence[Expr]] | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    with_feature_history: bool = True,\n    with_sample_history: bool = False,\n) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened for reading)</p> </li> <li> <code>features</code>               (<code>Sequence[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>Features to copy. Can be:</p> <ul> <li> <p><code>None</code>: copies all features from the active graph</p> </li> <li> <p>Sequence of specific features to copy</p> </li> </ul> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>global_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions applied to all features. These filters are combined with any feature-specific filters from <code>filters</code>. Example: [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), include rows from all historical feature versions. If False, only copy rows with the current feature_version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations per sample. If False (default), deduplicate within <code>id_columns</code> groups by keeping latest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If source or destination store is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Copy all features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        features=[mx.FeatureKey(\"my_feature\")],\n    )\n</code></pre> <pre><code># Copy with global filters applied to all features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        global_filters=[nw.col(\"id\").is_in([\"a\", \"b\"])],\n    )\n</code></pre> <pre><code># Copy specific features with per-feature filters\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        features=[\n            mx.FeatureKey(\"feature_a\"),\n            mx.FeatureKey(\"feature_b\"),\n        ],\n        filters={\n            \"feature_a\": [nw.col(\"field_a\") &gt; 10],\n            \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n        },\n    )\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    global_filters: Sequence[nw.Expr] | None = None,\n    with_feature_history: bool = True,\n    with_sample_history: bool = False,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened for reading)\n        features: Features to copy. Can be:\n\n            - `None`: copies all features from the active graph\n\n            - Sequence of specific features to copy\n\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        global_filters: Sequence of Narwhals filter expressions applied to all features.\n            These filters are combined with any feature-specific filters from `filters`.\n            Example: [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]\n        with_feature_history: If True (default), include rows from all historical feature versions.\n            If False, only copy rows with the current feature_version.\n        with_sample_history: If True, include all historical materializations per sample.\n            If False (default), deduplicate within `id_columns` groups by keeping latest.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If source or destination store is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy all features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy specific features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                features=[mx.FeatureKey(\"my_feature\")],\n            )\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy with global filters applied to all features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                global_filters=[nw.col(\"id\").is_in([\"a\", \"b\"])],\n            )\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy specific features with per-feature filters\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                features=[\n                    mx.FeatureKey(\"feature_a\"),\n                    mx.FeatureKey(\"feature_b\"),\n                ],\n                filters={\n                    \"feature_a\": [nw.col(\"field_a\") &gt; 10],\n                    \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n                },\n            )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate both stores are open\n    if not self._is_open:\n        raise ValueError('Destination store must be opened with store.open(\"w\") before use')\n    if not from_store._is_open:\n        raise ValueError(\"Source store must be opened with store before use\")\n\n    return self._copy_metadata_impl(\n        from_store=from_store,\n        features=features,\n        filters=filters,\n        global_filters=global_filters,\n        with_feature_history=with_feature_history,\n        with_sample_history=with_sample_history,\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode","title":"metaxy.metadata_store.types.AccessMode  <code>module-attribute</code>","text":"<pre><code>AccessMode = Literal['r', 'w']\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.base.VersioningEngineOptions","title":"metaxy.metadata_store.base.VersioningEngineOptions  <code>module-attribute</code>","text":"<pre><code>VersioningEngineOptions: TypeAlias = Literal[\n    \"auto\", \"native\", \"polars\"\n]\n</code></pre>"},{"location":"reference/api/metadata-stores/#base-configuration-class","title":"Base Configuration Class","text":"<p>The following base configuration class is typically used by child metadata stores:</p>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.base.MetadataStoreConfig","title":"metaxy.metadata_store.base.MetadataStoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base configuration class for metadata stores.</p> <p>This class defines common configuration fields shared by all metadata store types. Store-specific config classes should inherit from this and add their own fields.</p> Example <pre><code>from metaxy.metadata_store.duckdb import DuckDBMetadataStoreConfig\n\nconfig = DuckDBMetadataStoreConfig(\n    database=\"metadata.db\",\n    hash_algorithm=HashAlgorithm.MD5,\n)\n\nstore = DuckDBMetadataStore.from_config(config)\n</code></pre>"},{"location":"reference/api/metadata-stores/#configuration","title":"Configuration","text":"<p>The base <code>MetadataStoreConfig</code> class injects the following configuration options:</p>"},{"location":"reference/api/metadata-stores/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"reference/api/metadata-stores/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"reference/api/metadata-stores/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"reference/api/metadata-stores/exceptions/","title":"Metadata Store Exceptions","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions","title":"metaxy.metadata_store.exceptions","text":"<p>Exceptions for metadata store operations.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-classes","title":"Classes","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataStoreError","title":"metaxy.metadata_store.exceptions.MetadataStoreError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for metadata store errors.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FeatureNotFoundError","title":"metaxy.metadata_store.exceptions.FeatureNotFoundError","text":"<pre><code>FeatureNotFoundError(\n    message: str, *, keys: list[FeatureKey] | None = None\n)\n</code></pre> <p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a feature is not found in the store.</p> Source code in <code>src/metaxy/metadata_store/exceptions.py</code> <pre><code>def __init__(self, message: str, *, keys: list[FeatureKey] | None = None):\n    super().__init__(message)\n    self.keys = keys or []\n</code></pre>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.SystemDataNotFoundError","title":"metaxy.metadata_store.exceptions.SystemDataNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when system features are not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FieldNotFoundError","title":"metaxy.metadata_store.exceptions.FieldNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a field is not found for a feature.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataSchemaError","title":"metaxy.metadata_store.exceptions.MetadataSchemaError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when metadata DataFrame has invalid schema.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.DependencyError","title":"metaxy.metadata_store.exceptions.DependencyError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when upstream dependencies are missing or invalid.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.StoreNotOpenError","title":"metaxy.metadata_store.exceptions.StoreNotOpenError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when attempting to use a store that hasn't been opened.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","title":"metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a hash algorithm is not supported by the store or its components.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.TableNotFoundError","title":"metaxy.metadata_store.exceptions.TableNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a table does not exist and auto_create_tables is disabled.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.VersioningEngineMismatchError","title":"metaxy.metadata_store.exceptions.VersioningEngineMismatchError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when versioning_engine='native' is requested but data has wrong implementation.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-functions","title":"Functions","text":""}]}