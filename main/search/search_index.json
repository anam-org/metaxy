{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metaxy \ud83c\udf0c","text":"<p>Metaxy is a metadata layer for multi-modal Data and ML pipelines that manages and tracks metadata: sample versions, dependencies, and data lineage across complex computational graphs.</p> <p>It's agnostic to everything: compute engines, data storage, metadata storage.</p> <p>It has no strict infrastructure requirements and can use external databases for computations or run locally.</p> <p>It can scale to handle large amounts of big metadata.</p> <p>Giga Alpha</p> <p>This project is as raw as a steak still saying \u2018moo.\u2019</p>"},{"location":"#what-problem-exactly-does-metaxy-solve","title":"What problem exactly does Metaxy solve?","text":"<p>Data, ML and AI workloads processing large amounts of images, videos, audios, texts, or any other kind of data can be very expensive to run. In contrast to traditional data engineering, re-running the whole pipeline on changes is no longer an option. Therefore, it becomes crucially important to correctly implement incremental processing and sample-level versioning.</p> <p>Typically, a feature has to be re-computed in one of the following scenarios:</p> <ul> <li> <p>upstream data changes</p> </li> <li> <p>bug fixes or algorithmic changes</p> </li> </ul> <p>But correctly distinguishing these scenarios from cases where the feature should not be re-computed is a surprisingly challenging. Here are some of the cases where it would be undesirable:</p> <ul> <li> <p>merging two consecutive steps into one (refactoring the graph topology)</p> </li> <li> <p>partial data updates, e.g. changing only the audio track inside a video file</p> </li> <li> <p>backfilling metadata from another source</p> </li> </ul> <p>Tracking and propagating these changes correctly to the right subset of samples and features can become incredibly complicated. Until now, a general solution for this problem did not exist, but this is not the case anymore.</p>"},{"location":"#metaxy-to-the-rescue","title":"Metaxy to the rescue","text":"<p>Metaxy solves the first set of problems with a feature and field dependency system, and the second with a migrations system.</p> <p>Metaxy builds a versioned graphs from feature definitions and tracks version changes. This graph can be snapshotted and saved at any point of time, typically during pipeline deployment. Here is an example of a graph diff produced by a code_version update on the <code>audio</code> field of the <code>example/video</code> feature:</p> <pre><code>---\ntitle: Propagation Of Changes\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>The key observation here is that <code>example/face_detection</code>'s <code>faces</code> field did not receive a new version, because it does not depend on the <code>audio</code> field that has been updated upstream.</p>"},{"location":"#about-metaxy","title":"About Metaxy","text":"<p>Metaxy is:</p> <ul> <li> <p>\ud83e\udde9 composable --- bring your own everything!</p> <ul> <li>supports DuckDB, ClickHouse, and 20+ databases via Ibis</li> <li>supports lakehouse storage formats such as DeltaLake or DuckLake</li> <li>is agnostic to tabular compute engines: Polars, Spark, Pandas, and databases thanks to Narwhals</li> <li>we totally don't care how is the multi-modal data produced or where is it stored: Metaxy is responsible for yielding input metadata and writing output metadata</li> </ul> </li> <li> <p>\ud83e\udd38 flexible to work around restrictions consciously:</p> <ul> <li>features are defined as Pydantic models, leveraging Pydantic's type safety guarantees, rich validation system, and allowing inheritance patterns to stay DRY</li> <li>has a migrations system to compensate for reconciling field provenances and metadata when computations are not desired</li> </ul> </li> <li> <p>\ud83e\udea8 rock solid when it matters:</p> <ul> <li>field provenance is guaranteed to be consistent across DBs or in-memory compute engines. We really have tested this very well!</li> <li>changes to topology, feature versioning, or individual samples ruthlessly propagate downstream</li> <li>unique field-level dependency system prevents unnecessary recomputations for features that depend on partial data</li> <li>metadata is append-only to ensure data integrity and immutability. Users can perform cleanup if needed (Metaxy provides tools for this).</li> </ul> </li> <li> <p>\ud83d\udcc8 scalable:</p> <ul> <li>supports feature organization and discovery patterns such as packaging entry points. This enables collaboration across teams and projects.</li> <li>is built with performance in mind: all operations default to run in the DB, Metaxy does not stand in the way of metadata flow</li> </ul> </li> <li> <p>\ud83e\uddd1\u200d\ud83d\udcbb dev friendly:</p> <ul> <li>clean, intuitive Python API that stays out of your way when you don't need it</li> <li>feature discovery system for effortless dependency management</li> <li>comprehensive type hints and Pydantic integration for excellent IDE support</li> <li>first-class support for local development, testing, preview environments, CI/CD</li> <li>CLI tool for easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats</li> <li>integrations with popular tools such as SQLModel, Dagster, and Ray.</li> <li>testing helpers that you're going to appreciate</li> </ul> </li> </ul>"},{"location":"#feature-dependencies","title":"Feature Dependencies","text":"<p>Features form a DAG where each feature declares its upstream dependencies. Consider an video processing pipeline:</p> <pre><code>class Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"video\",\n        fields=[\n            # simple field with only the key defined and the default code_version used\n            \"frames\",\n            # let's version this one!\n            FieldSpec(name=\"audio\", code_version=\"1\"),\n        ],\n    ),\n):\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n\n\nclass VoiceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"voice_detection\",\n        deps=[Video],\n        fields=[\n            \"frames\",   # dependency automatically mapped into Video.frames\n            \"audio\",  # dependency automatically mapped into Video.audio\n        ]\n    ),\n):\n    path: str = Field(description=\"Path to the voice detection json file\")\n</code></pre> <p>Note</p> <p>This API will be improved with more ergonomic alternatives. See issue #70 for details.</p> <p>When <code>Video</code> changes, Metaxy automatically identifies that <code>VoiceDetection</code> requires recomputation.</p>"},{"location":"#versioned-change-propagation","title":"Versioned Change Propagation","text":"<p>Every feature definition produces a deterministic version hash computed from its dependencies, fields, and code versions. When you modify a feature\u2014whether changing its dependencies, adding fields, or updating transformation logic, Metaxy detects the change and propagates it downstream. This is done on multiple levels: <code>Feature</code> level, field level, and of course on sample level: each row in the metadata store tracks the version of each field and the feature-level version.</p> <p>This ensures that when feature definitions evolve, every feature that transitively depends on it can be systematically updated. Because Metaxy supports declaring dependencies on fields, it can identify when a feature does not require recomputation, even if one of its parents has been changed (but only irrelevant fields did). This is a huge factor in improving efficiency and reducing unnecessary computations (and costs!).</p> <p>Because Metaxy feature graphs are static, Metaxy can calculate field provenance changes ahead of the actual computation. This enables patterns such as computation preview and computation cost prediction.</p>"},{"location":"#typical-user-workflow","title":"Typical User Workflow","text":""},{"location":"#1-record-metaxy-feature-graph-in-cicd","title":"1. Record Metaxy feature graph in CI/CD","text":"<p>Invoke the <code>metaxy</code> CLI:</p> <pre><code>metaxy graph push\n</code></pre> <p>This can be skipped in non-production environments.</p>"},{"location":"#2-get-a-resolved-metadata-increment-from-metaxy","title":"2. Get a resolved metadata increment from Metaxy","text":"<p>Use <code>metaxy.MetadataStore.resolve_update</code> to identify samples requiring recomputation:</p> <pre><code>import metaxy as mx\n\n# discover and load Metaxy features\nmx.init_metaxy()\n\n# can be DuckDBMetadataStore locally and ClickHouseMetadataStore in production\nstore: mx.MetadataStore = ...\ndiff = store.resolve_update(VoiceDetection)\n</code></pre> <p><code>resolve_update</code> runs in the database with an optional fallback to use Polars in-memory (and the two workflows are guaranteed to produce consistent results). The returned object provides Narwhals lazy dataframes which are backend agnostic -- can run on Polars, Pandas, PySpark, or an extenral DB, and have all the field provenances already computed.</p>"},{"location":"#3-run-user-defined-computation-over-the-metadata-increment","title":"3. Run user-defined computation over the metadata increment","text":"<p>Metaxy is not involved in this step at all.</p> <pre><code>if (len(diff.added) + len(diff.changed)) &gt; 0:\n    # run your computation, this can be done in a distributed manner\n    results = run_voice_detection(diff, ...)\n</code></pre>"},{"location":"#4-record-metadata-for-computed-samples","title":"4. Record metadata for computed samples","text":"<p>This can be done in a distributed manner as well, and the recommended pattern is to write metadata as soon as it becomes available to avoid losing progress in case of interruptions or failures.</p> <pre><code>store.write_metadata(VoiceDetection, results)\n</code></pre> <p>We have now successfully recorded the metadata for the computed samples! Processed samples will no longer be returned by <code>MetadataStore.resolve_update</code> during future pipeline runs.</p> <p>No Write Time Uniqueness Checks!</p> <p>Metaxy doesn't enforce deduplication or uniqueness checks at write time for performance reasons. While <code>MetadataStore.resolve_update</code> is guaranteed to never return the same versioned sample twice, it's up to the user to ensure that samples are not written multiple times to the metadata store. Configuring deduplication or uniqueness checks in the store (database) is a good idea. For example, the SQLModel integration can inject a composite primary key on <code>metaxy_data_version</code>, <code>metaxy_created_at</code> and the user-defined ID columns. However, Metaxy only uses the latest version (by <code>metaxy_created_at</code>) at read time.</p>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>Learn more about feature definitions or versioning</p> </li> <li> <p>Use Metaxy from the command line</p> </li> <li> <p>Learn how to configure Metaxy</p> </li> <li> <p>Get lost in our API Reference</p> </li> </ul>"},{"location":"examples/one-to-many/","title":"One-to-Many Expansion","text":"<p> View Example Source on GitHub</p> <p>This example demonstrates how to implement <code>1:N</code> transformations with Metaxy. In such relationships a single parent sample can map into multiple child samples.</p> <p>In Metaxy they can be modeled with LineageRelationship.expansion lineage type.</p> <p>We will use a hypothetical video chunking pipeline as an example. We are also going to demonstrate that other Metaxy features such as fields mapping work with non-standard lineage types.</p>"},{"location":"examples/one-to-many/#the-pipeline","title":"The Pipeline","text":"<p>We are going to define a typical video processing pipeline with three features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces&lt;/div&gt;\"]\n        video_raw --&gt; video_chunk\n        video_chunk --&gt; video_faces</code></pre>"},{"location":"examples/one-to-many/#defining-features-video","title":"Defining features: <code>Video</code>","text":"<p>Each video-like feature in our pipeline is going to have two fields: <code>audio</code> and <code>frames</code>.</p> <p>Let's set the code version of <code>audio</code> to <code>\"1\"</code> in order to change it in the future. <code>frames</code> field will have a default version.</p> src/example_one_to_many/features.py<pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.Feature,\n    spec=mx.FeatureSpec(\n        key=\"video/raw\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            \"frames\",\n        ],\n    ),\n):\n    video_id: str\n    path: str  # where the video is stored\n</code></pre>"},{"location":"examples/one-to-many/#defining-features-videochunk","title":"Defining features: <code>VideoChunk</code>","text":"<p><code>VideoChunk</code> represents a piece of the upstream <code>Video</code> feature. Since each <code>Video</code> sample can be split into multiple chunks, we need to tell Metaxy how to map each chunk to its parent video.</p> src/example_one_to_many/features.py<pre><code>class VideoChunk(\n    mx.Feature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"chunk\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[mx.FeatureDep(feature=Video)],\n        fields=[\"audio\", \"frames\"],\n        lineage=mx.LineageRelationship.expansion(on=[\"video_id\"]),\n    ),\n):\n    video_id: str  # points to the parent video\n    video_chunk_id: str\n    path: str  # where the video chunk is stored\n</code></pre> <p>We do not specify custom versions on its fields. Metaxy will automatically assign field-level dependencies by matching on field names: <code>VideoChunk.frames</code> depends on <code>Video.frames</code> and <code>VideoChunk.audio</code> depends on <code>Video.audio</code>.</p>"},{"location":"examples/one-to-many/#defining-features-facerecognition","title":"Defining features: <code>FaceRecognition</code>","text":"<p><code>FaceRecognition</code> processes video chunks and only depends on the <code>frames</code> field. This can be expressed with a <code>SpecificFieldsMapping</code>.</p> src/example_one_to_many/features.py<pre><code>class FaceRecognition(\n    mx.Feature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"faces\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=VideoChunk,\n                fields_mapping=mx.FieldsMapping.specific(\n                    mapping={mx.FieldKey(\"faces\"): {mx.FieldKey(\"frames\")}}\n                ),\n            )\n        ],\n        fields=[\"faces\"],\n    ),\n):\n    video_chunk_id: str\n    num_faces: int  # number of faces detected\n</code></pre> <p>This completes the feature definitions. Let's proceed to running the pipeline.</p>"},{"location":"examples/one-to-many/#walkthrough","title":"Walkthrough","text":"<p>Here is a toy pipeline for computing the feature graph described above:</p> <code>pipeline.py</code> pipeline.py<pre><code>import random\n\nimport narwhals as nw\nimport polars as pl\nfrom example_one_to_many.features import FaceRecognition, Video, VideoChunk\nfrom example_one_to_many.utils import split_video_into_chunks\n\nfrom metaxy import init_metaxy\n\n\ndef main():\n    cfg = init_metaxy()\n    store = cfg.get_store(\"dev\")\n\n    # let's pretend somebody has already created the videos for us\n    samples = pl.DataFrame(\n        {\n            \"video_id\": [1, 2, 3],\n            \"path\": [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"],\n            \"metaxy_provenance_by_field\": [\n                {\"audio\": \"v1\", \"frames\": \"v1\"},\n                {\"audio\": \"v2\", \"frames\": \"v2\"},\n                {\"audio\": \"v3\", \"frames\": \"v3\"},\n            ],\n        }\n    )\n\n    with store:\n        # showcase: resolve incremental update for a root feature\n        diff = store.resolve_update(Video, samples=nw.from_native(samples))\n        if len(diff.added) &gt; 0:\n            print(f\"Found {len(diff.added)} new videos\")\n            store.write_metadata(Video, diff.added)\n\n    # now we are going to resolve the videos that have to be split to chunks\n    with store:\n        diff = store.resolve_update(VideoChunk)\n        # the DataFrame dimensions matches Video (with ID column renamed)\n\n        print(\n            f\"Found {len(diff.added)} videos and {len(diff.changed)} videos that need chunking\"\n        )\n\n        for row_dict in pl.concat(\n            [diff.added.to_polars(), diff.changed.to_polars()]\n        ).iter_rows(named=True):\n            print(f\"Processing video: {row_dict}\")\n            # let's split each video to 3-5 chunks randomly\n\n            video_id = row_dict[\"video_id\"]\n            path = row_dict[\"path\"]\n\n            provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n            provenance = row_dict[\"metaxy_provenance\"]\n\n            # pretend we split the video into chunks\n            chunk_paths = split_video_into_chunks(path)\n\n            # Generate chunk IDs based on the parent video ID\n            chunk_ids = [f\"{video_id}_{i}\" for i in range(len(chunk_paths))]\n\n            # write the chunks to the store\n            # CRUSIAL: all the chunks **must share the same provenance values**\n            chunk_df = pl.DataFrame(\n                {\n                    \"video_id\": [video_id] * len(chunk_paths),\n                    \"video_chunk_id\": chunk_ids,\n                    \"path\": chunk_paths,\n                    \"metaxy_provenance_by_field\": [provenance_by_field]\n                    * len(chunk_paths),\n                    \"metaxy_provenance\": [provenance] * len(chunk_paths),\n                }\n            )\n            print(f\"Writing {len(chunk_paths)} chunks for video {video_id}\")\n            store.write_metadata(VideoChunk, nw.from_native(chunk_df))\n\n    # now process face recognition on video chunks\n    with store:\n        diff = store.resolve_update(FaceRecognition)\n        print(\n            f\"Found {len(diff.added)} video chunks and {len(diff.changed)} video chunks that need face recognition\"\n        )\n\n        if len(diff.added) &gt; 0:\n            # simulate face detection on each chunk\n            face_data = []\n            for row_dict in pl.concat(\n                [diff.added.to_polars(), diff.changed.to_polars()]\n            ).iter_rows(named=True):\n                video_chunk_id = row_dict[\"video_chunk_id\"]\n                provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n                provenance = row_dict[\"metaxy_provenance\"]\n\n                # simulate detecting random number of faces\n                num_faces = random.randint(0, 10)\n\n                face_data.append(\n                    {\n                        \"video_chunk_id\": video_chunk_id,\n                        \"num_faces\": num_faces,\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            face_df = pl.DataFrame(face_data)\n            print(f\"Writing face recognition results for {len(face_data)} chunks\")\n            store.write_metadata(FaceRecognition, nw.from_native(face_df))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/one-to-many/#step-1-launch-initial-run","title":"Step 1: Launch Initial Run","text":"<p>Run the pipeline to create videos, chunks, and face recognition results:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 4 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 3 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 5 chunks for video 3\nFound 12 video chunks and 0 video chunks that need face recognition\nWriting face recognition results for 12 chunks\n</code></pre> <p>All three features have been materialized. Note that the <code>VideoChunk</code> feature may dynamically create as many samples as needed: Metaxy doesn't need to know anything about this in advance, except the relationship type.</p>"},{"location":"examples/one-to-many/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 0 videos and 0 videos that need chunking\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/one-to-many/#step-3-change-audio-code-version","title":"Step 3: Change Audio Code Version","text":"<p>Now let's bump the code version on the <code>audio</code> field of <code>Video</code> feature:</p> <code>patches/01_update_video_code_version.patch</code> patches/01_update_video_code_version.patch<pre><code>--- a/src/example_one_to_many/features.py\n+++ b/src/example_one_to_many/features.py\n@@ -9,6 +9,6 @@ class Video(\n         id_columns=[\"video_id\"],\n         fields=[\n-            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n+            mx.FieldSpec(key=\"audio\", code_version=\"2\"),\n             \"frames\",\n         ],\n     ),\n</code></pre> <p>This represents updating the audio processing algorithm, and therefore the audio data.</p>"},{"location":"examples/one-to-many/#step-4-observe-field-level-tracking","title":"Step 4: Observe Field-Level Tracking","text":"<p>Run the pipeline again after the code change:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 3 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 5 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 4 chunks for video 3\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Key observation:</p> <ul> <li><code>VideoChunk</code> has been recomputed since the <code>audio</code> field on it has been affected by the upstream change</li> <li><code>FaceRecognition</code> did not require a recompute, because it only depends on the <code>frames</code> field (which did not change)</li> </ul>"},{"location":"examples/one-to-many/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling <code>1:N</code> relationships: LineageRelationship.expansion. Other Metaxy features such as field-level versioning continue to work seamlessly when declaring <code>1:N</code> relationships.</p>"},{"location":"examples/one-to-many/#related-materials","title":"Related materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>Fields Mapping</li> </ul>"},{"location":"learn/data-versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static, deterministic and idempotent.</p>"},{"location":"learn/data-versioning/#versioning_1","title":"Versioning","text":"<p>Feature and field versions are defined by the feature graph topology and the user-provided code versions of fields. Sample versions are defined by upstream sample versions and the code versions of the fields defined on the sample's feature.</p> <p>All versions are computed ahead of time: feature and field versions can be immediately derived from code (and we keep historical graph snapshots for them), and calculating sample versions requires access to the metadata store.</p> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"learn/data-versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them.</p>"},{"location":"learn/data-versioning/#field-level","title":"Field Level","text":"<ul> <li>Field Code Version is defined on the field and is provided by the user (defaults to <code>\"__metaxy_initial__\"</code>)</li> </ul> <p>Code Version Value</p> <p>The value can be arbitrary, but in the future we might implement something around semantic versioning.</p> <ul> <li>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</li> </ul>"},{"location":"learn/data-versioning/#feature-level","title":"Feature Level","text":"<ul> <li>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</li> <li>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</li> </ul>"},{"location":"learn/data-versioning/#graph-level","title":"Graph Level","text":"<ul> <li>Snapshot Version: is computed from the Feature Versions of all features defined on the graph.</li> </ul> <p>Why Do We Need Snapshot Version?</p> <p>This value is used to uniquely encode versioned feature graph topology in historical snapshots.</p>"},{"location":"learn/data-versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to compute them.</p> <ul> <li>Provenance By Field is computed from the upstream Provenance By Fields (with respect to defined field-level dependencies and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</li> </ul> sample_uid provenance_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <ul> <li>Sample Version is derived from the Provenance By Field by simply hashing it.</li> </ul> <p>This is the end game of the versioning system. It ensures that only the necessary samples are recomputed when a feature version changes. It acts as source of truth for resolving incremental updates for feature metadata.</p>"},{"location":"learn/data-versioning/#practical-example","title":"Practical Example","text":"<p>Consider a video processing pipeline with these features:</p> <pre><code>from metaxy import (\n    Feature,\n    FeatureDep,\n    FeatureSpec,\n    FieldDep,\n    FieldSpec,\n)\n\n\nclass Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/video\",\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/crop\",\n        deps=[FeatureDep(feature=Video)],\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass  # omit columns for the sake of simplicity\n\n\nclass FaceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/face_detection\",\n        deps=[\n            FeatureDep(\n                feature=Crop,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"faces\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Crop,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n\n\nclass SpeechToText(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/stt\",\n        deps=[\n            FeatureDep(\n                feature=Video,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"transcription\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n</code></pre> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: 8468950d\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc9ca835)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 22742381)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: 794116a9)&lt;/small&gt;&lt;/div&gt;\"]\n        example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 3ac04df8)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 76c8bdc9)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: abc79017)&lt;/small&gt;&lt;/div&gt;\"]\n        example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 1ac83b07)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces &lt;small&gt;(v: 2d75f0bd)&lt;/small&gt;&lt;/div&gt;\"]\n        example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: c83a754a)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 transcription &lt;small&gt;(v: ac412b3c)&lt;/small&gt;&lt;/div&gt;\"]\n        example_video --&gt; example_crop\n        example_crop --&gt; example_face_detection\n        example_video --&gt; example_stt</code></pre>"},{"location":"learn/data-versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>Video</code> feature changes (perhaps denoising was applied):</p> <pre><code>         key=\"example/video\",\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n</code></pre> <p>Run <code>metaxy graph diff</code> to see what changed:</p> <pre><code>---\ntitle: Merged Graph Diff\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>Notice:</p> <ul> <li><code>Video</code>, <code>Crop</code>, and <code>SpeechToText</code> changed (highlighted)</li> <li><code>FaceDetection</code> remained unchanged (depends only on <code>frames</code>, not <code>audio</code>)</li> <li>Audio field versions changed throughout the graph</li> <li>Frame field versions stayed the same</li> </ul>"},{"location":"learn/data-versioning/#incremental-computation","title":"Incremental Computation","text":"<p>The metadata store's <code>calculate_provenance_by_field()</code> method:</p> <ol> <li>Joins upstream feature metadata</li> <li>Computes sample versions</li> <li>Compares against existing metadata</li> <li>Returns diff: added, changed, removed samples</li> </ol> <p>Typically, steps 1-3 can be run directly in the database. Analytical databases such as ClickHouse or Snowflake can efficiently handle these operations.</p> <p>The Python pipeline then processes only the delta</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes provenance_by_field and identifies changes\n    diff = store.resolve_update(MyFeature)\n\n    # Process only changed samples\n</code></pre> <p>The <code>diff</code> object has attributes for new upstream samples, samples with new versions, and samples that have been removed from upstream metadata.</p> <p>This approach avoids expensive recomputation when nothing changed, while ensuring correctness when dependencies update.</p>"},{"location":"learn/feature-definitions/","title":"Feature System","text":"<p>Metaxy has a declarative (defined statically at class level), expressive, flexible feature system. It has been inspired by Dagster's Software-Defined Assets and Nix.</p> <p>Features represent tabular metadata, typically containing references to external multi-modal data such as files, images, or videos. But it can be just pure metadata as well.</p> <p>I will highlight data and metadata with bold so it really stands out.</p> <p>Metaxy is responsible for providing correct metadata to users. During incremental processing, Metaxy will automatically resolve added, changed and deleted metadata rows and calculate the right sample versions for them. Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage (it's a good idea to inject the sample version into the data sample identifier). Metaxy is designed to be used with systems that do not overwrite existing metadata (Metaxy only appends metadata) and therefore data as well (while we cannot enforce that since the user is responsible for writing the data, it's easily achievable by including the sample version into the data sample identifier).</p> <p>I hope we can stop using bold for data and metadata from now on, hopefully we've made our point.</p> <p>Include sample version in your data path</p> <p>Include the sample version in your data path to ensure strong consistency guarantees. I mean it. Really do it!</p> <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly). Features are bound to a specific Metaxy project, but can be moved between projects over time. Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p>"},{"location":"learn/feature-definitions/#feature-definitions","title":"Feature Definitions","text":"<p>Metaxy provides a <code>BaseFeature</code> class that can be extended to create user-defined features. It's a Pydantic model.</p> <pre><code>from metaxy import BaseFeature, FeatureSpec\n\n\nclass VideoFeature(\n    BaseFeature, spec=FeatureSpec(key=\"/raw/video\", id_columns=[\"video_id\"])\n):\n    path: str\n</code></pre> <p>Metaxy must know how to uniquely identify feature samples and join metadata tables, therefore, you need to attach one or more ID columns to your <code>FeatureSpec</code>.</p> <p>That's it! Since it's a root feature, it doesn't have any dependencies. Easy.</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    BaseFeature,\n    spec=FeatureSpec(key=\"/processed/transcript\", id_columns=[\"video_id\"] deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> <p>Hurray! You get the idea.</p>"},{"location":"learn/feature-definitions/#field-level-dependencies","title":"Field-Level Dependencies","text":"<p>A core (I'll be straight: a killer) feature of Metaxy is the concept of field-level dependencies. These are used to define dependencies between logical fields of features.</p> <p>A field is not to be confused with metadata column (Pydantic fields). Fields are completely independent from them.</p> <p>Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy.</p> <p>Fields refer to data and are logical -- users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>Video</code> feature -- an <code>.mp4</code> file -- may have <code>frames</code> and <code>audio</code> fields.</p> <p>Downstream features can depend on specific fields of upstream features. This enables fine-grained control over field provenance, avoiding unnecessary reprocessing.</p> <p>At this point, careful readers have probably noticed that the <code>Transcript</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express that with Metaxy:</p> <pre><code>from metaxy import FieldDep, FieldSpec\n\nvideo_spec = FeatureSpec(key=\"/raw/video\", fields=[\"audio\", \"frames\"])\n\n\nclass VideoFeature(BaseFeature, spec=video_spec):\n    path: str\n\n\ntranscript_spec = TranscriptFeatureSpec(\n    key=\"/raw/transcript\",\n    id_columns=[\"video_id\"],\n    fields=[\n        FieldSpec(\n            key=\"text\",\n            deps=[FieldDep(feature=VideoFeature.spec().key, fields=[\"audio\"])],\n        )\n    ],\n)\n\n\nclass TranscriptFeature(BaseTranscriptFeature, spec=transcript_spec):\n    path: str\n</code></pre> <p>Voil\u00e0!</p> <p>Use boilerplate-free API</p> <p>Metaxy allows passing simplified types to some of the models like <code>FeatureSpec</code> or <code>FeatureKey</code>. See syntactic sugar for more details.</p> <p>The Data Versioning docs explain more about how Metaxy calculates versions for different components of a feature graph.</p>"},{"location":"learn/feature-definitions/#attaching-user-defined-metadata","title":"Attaching user-defined metadata","text":"<p>Users can attach arbitrary JSON-like metadata dictionary to feature specs, typically used for declaring ownership, providing information to third-party tooling, or documentation purposes. This metadata does not influence graph topology or the versioning system.</p>"},{"location":"learn/feature-definitions/#fully-qualified-field-key","title":"Fully Qualified Field Key","text":"<p>A fully qualified field key (FQFK) is an identifier that uniquely identifies a field within the whole feature graph. It consists of the feature key and the field key, separated by a colon, for example: <code>/raw/video:frames</code>, <code>/raw/video:audio/english</code>.</p>"},{"location":"learn/feature-discovery/","title":"Feature Discovery","text":"<p>Metaxy provides automatic feature discovery through Python's entrypoint system. This enables modular architecture patterns essential for scaling Metaxy projects.</p>"},{"location":"learn/feature-discovery/#why-feature-discovery","title":"Why Feature Discovery?","text":"<p>Manual feature registration doesn't scale. As your system grows, you need:</p> <ul> <li>Plugin architectures - Third-party teams contribute features without modifying core code</li> <li>Feature collections - Package and distribute related features as installable units</li> <li>Monorepo support - Discover features across multiple packages in a monorepo</li> <li>Internal packages - Share features between projects via private package registries</li> </ul> <p>Feature discovery solves these problems through automatic registration at import time.</p>"},{"location":"learn/feature-discovery/#package-entry-points","title":"Package Entry Points","text":"<p>The most powerful discovery mechanism uses Python's standard entry point system via a well-known <code>\"metaxy.project\"</code> entrypoint group in the package metadata.</p>"},{"location":"learn/feature-discovery/#creating-a-feature-plugin","title":"Creating a Feature Plugin","text":"<p>Structure your feature package:</p> <pre><code>my-video-features/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_video_features/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 detection.py\n        \u2514\u2500\u2500 transcription.py\n</code></pre> <p>Declare entry points in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-video-features\"\nversion = \"1.0.0\"\ndependencies = [\"metaxy\"]\n\n[project.entry-points.\"metaxy.project\"]\nmy-video-features = \"my_video_features\"\n</code></pre> <p>The entry point name is your project name. The value can be either:</p> <ul> <li>Function syntax (<code>module:function</code>) - Points to a callable function that will be invoked to load features. Useful when you need conditional loading or setup logic.</li> <li>Module syntax (<code>module</code>) - Points directly to a module containing Feature definitions. Simply importing the module registers the features.</li> </ul> <p>One Entry Point Per Package</p> <p>Each package can only declare one entry point in the <code>metaxy.project</code> group, since <code>metaxy.toml</code> only supports a single <code>project</code> field.</p> <p>To organize features into logical groups within a package, use submodules and import them from your entry point function.</p>"},{"location":"learn/feature-discovery/#installing-and-using-feature-plugins","title":"Installing and Using Feature Plugins","text":"<p>Install the package:</p> <pre><code>pip install my-video-features\n# Or in a monorepo:\npip install -e ./packages/my-video-features\n</code></pre> <p>UV Package Manager: Entry Point Changes</p> <p>If you're using <code>uv</code> and modify entry points in <code>pyproject.toml</code>, <code>uv sync</code> will not recreate the editable package metadata. You must explicitly reinstall:</p> <pre><code>uv sync --reinstall-package my-video-features my-video-features\n</code></pre>"},{"location":"learn/feature-discovery/#monorepo-patterns","title":"Monorepo Patterns","text":"<p>In monorepos, use entry points to manage feature collections across teams:</p>"},{"location":"learn/feature-discovery/#team-owned-feature-packages","title":"Team-Owned Feature Packages","text":"<pre><code>monorepo/\n\u251c\u2500\u2500 packages/\n\u2502   \u251c\u2500\u2500 core-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u251c\u2500\u2500 ml-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u2514\u2500\u2500 experimental-features/\n\u2502       \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2514\u2500\u2500 apps/\n    \u2514\u2500\u2500 main-pipeline/\n        \u2514\u2500\u2500 pyproject.toml  # depends on feature packages\n</code></pre> <p>Each team maintains their features independently:</p> <pre><code># packages/ml-features/pyproject.toml\n[project.entry-points.\"metaxy.project\"]\nml-features = \"ml_features.load\"\n</code></pre> <pre><code># packages/core-features/pyproject.toml\n[project.entry-points.\"metaxy.project\"]\ncore-features = \"core_features.load\"\n</code></pre> <p>The main application imports features from all installed packages, and each feature automatically knows its project based on the entry point.</p>"},{"location":"learn/feature-discovery/#config-based-discovery","title":"Config-Based Discovery","text":"<p>For simpler use cases that don't require distribution, you can specify module paths directly in configuration:</p> metaxy.tomlpyproject.toml <pre><code>project = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre>"},{"location":"learn/feature-discovery/#best-practices","title":"Best Practices","text":"<ol> <li>Use entry points for distribution - Any features intended for reuse should use entry points</li> <li>Version your feature packages - Use semantic versioning for feature collections</li> <li>Test in isolation - Load feature packages into test graphs to verify behavior</li> </ol> <p>The entry point system transforms feature management from a manual process to an automatic, scalable system that grows with your organization.</p>"},{"location":"learn/filters/","title":"Specifying Filters As Text","text":"<p>There are a few occasions with Metaxy where users may want to define custom filter expressions via text, mainly being CLI arguments or configuration files. For this purpose, Metaxy implements <code>parse_filter_string</code>, which converts SQL-like <code>WHERE</code> clauses into Narwhals filter expressions.</p> <p>The following syntax is supported:</p> <ul> <li> <p>Comparisons: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></p> </li> <li> <p>Logical operators: <code>AND</code>, <code>OR</code>, <code>NOT</code></p> </li> <li> <p>Parentheses for grouping</p> </li> <li> <p>Column references (identifiers or dotted paths)</p> </li> <li> <p>Literals: strings (<code>'value'</code>), numbers, booleans (<code>TRUE</code>/<code>FALSE</code>), and <code>NULL</code></p> </li> <li> <p>Implicit boolean columns (e.g., <code>NOT is_active</code>)</p> </li> </ul> <p>Example usage:</p> <pre><code>from metaxy.models.filter_expression import parse_filter_string\n\ndf = ...  # a Narwhals frame\n\n# Parse a SQL WHERE clause into a backend-agnostic Narwhals expression\nexpr = parse_filter_string(\"(age &gt; 25 OR age &lt; 18) AND status != 'deleted'\")\n\ndf = df.filter(expr)\n</code></pre>"},{"location":"learn/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: <code>MetadataStore</code>.</p> <p>Metadata stores expose methods for reading, writing, deleting metadata, and the most important one: resolve_update for receiving a metadata increment. Metaxy intentionally does not support mutating metadata in-place for performance reasons. Deletes are not required during normal operations, but they are still supported since users would want to eventually delete stale metadata and data.</p> <p>Metadata reads/writes are not guaranteed to be ACID: Metaxy is designed to interact with analytical databases which lack ACID guarantees by definition and design (for performance reasons). However, Metaxy guarantees to never attempt to retrieve the same sample version twice, so as long as users do not write it twice (or have deduplication configured inside the metadata store) we should be all good.</p> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store. This includes joining upstream features, hashing their versions, and filtering out samples that have already been processed.</p> <p>There are 3 cases where this is done in-memory instead (with the help of polars-hash):</p> <ol> <li>The metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</li> <li>The user explicitly requested to keep the computations in-memory (<code>MetadataStore(..., prefer_native=False)</code>)</li> <li>When having to use a fallback store to retrieve one of the parent features.</li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p> <p>Learn about configuring metadata stores here</p>"},{"location":"learn/metadata-stores/#fallback-stores","title":"Fallback Stores","text":"<p>Fallback stores are a powerful feature that allow stores to read feature metadata from other stores (only if it's missing in the primary store). This is very useful for development, as production data can be retrieved immediately without populating the development environment. This is especially useful for ephemeral environments such as branch/preview deployments (typically created by CI/CD for pull requests) or integration testing environments.</p>"},{"location":"learn/metadata-stores/#project-write-validation","title":"Project Write Validation","text":"<p>By default, <code>MetadataStore</code> raises a <code>ValueError</code> when attempting to write to a project that doesn't match the expected project from <code>MetaxyConfig.get().project</code>.</p> <p>For legitimate cross-project operations (such as migrations that need to update features across multiple projects), an escape hatch is provided via the <code>allow_cross_project_writes()</code> context manager:</p> <pre><code># Normal operation - writes are validated against expected project\nwith store:\n    store.write_metadata(feature_from_my_project, metadata)  # OK\n    store.write_metadata(feature_from_other_project, metadata)  # Raises ValueError\n\n# Migration scenario - temporarily allow cross-project writes\nwith store:\n    with store.allow_cross_project_writes():\n        store.write_metadata(feature_from_project_a, metadata_a)  # OK\n        store.write_metadata(feature_from_project_b, metadata_b)  # OK\n</code></pre>"},{"location":"learn/relationship/","title":"Lineage Relationships","text":"<p>Metaxy supports a few common mappings from parent to child samples out of the box. These include:</p> <ul> <li><code>1:1</code> mapping with [metaxy.LineageRelationshipType.identity] (the default one)</li> <li><code>1:N</code> mapping with [metaxy.LineageRelationshipType.expansion]</li> <li><code>N:1</code> mapping with [metaxy.LineageRelationshipType.aggregation]</li> </ul> <p>Always use these classmethods to create instances of lineage relationships. Under the hood, they use Pydantic's discriminated union to ensure that the correct type is constructed based on the provided data.</p>"},{"location":"learn/relationship/#examples","title":"Examples","text":"<ul> <li>1:N example provides a toy pipeline demonstrating how to define and use <code>1:N</code> lineage relationships in Metaxy.</li> </ul>"},{"location":"learn/syntactic-sugar/","title":"Syntactic Sugar","text":""},{"location":"learn/syntactic-sugar/#type-coercion-for-input-types","title":"Type Coercion For Input Types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent feature keys, their fields, and the dependencies between them.</p> <p>To avoid boilerplate, Metaxy also has syntactic sugar for construction of these classes. Different ways to provide them are automatically coerced into canonical internal models. This is fully typed and only affects constructor arguments, so accessing attributes on Metaxy models will always return only the canonical types.</p> <p>Some examples:</p> <pre><code>from metaxy import FeatureKey\n\nkey = FeatureKey(\"prefix/feature\")\nkey = FeatureKey([\"prefix\", \"feature\"])\nkey = FeatureKey(\"prefix\", \"feature\")\nsame_key = FeatureKey(key)\n</code></pre> <p>Metaxy really loves you, the user!</p>"},{"location":"learn/syntactic-sugar/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> accept:</p> <ul> <li>String format: <code>FeatureKey(\"prefix/feature\")</code></li> <li>Sequence format: <code>FeatureKey([\"prefix\", \"feature\"])</code></li> <li>Variadic format: <code>FeatureKey(\"prefix\", \"feature\")</code></li> <li>Same type: <code>FeatureKey(another_feature_key)</code> -- for full Inception mode</li> </ul> <p>All formats produce equivalent keys, internally represented as a sequence of parts.</p>"},{"location":"learn/syntactic-sugar/#feature-dep","title":"Feature Dep","text":"<p><code>FeatureDep</code> accepts types coercible to <code>FeatureKey</code> and additionally subclasses of <code>BaseFeature</code>:</p> <pre><code>dep = FeatureDep(feature=MyFeature)\n</code></pre>"},{"location":"learn/syntactic-sugar/#feature-spec","title":"Feature Spec","text":"<p><code>FeatureSpec</code> has some syntactic sugar implemented as well.</p>"},{"location":"learn/syntactic-sugar/#deps","title":"Deps","text":"<p>The <code>deps</code> argument accepts a sequence of types coercible to <code>FeatureDep</code>:</p> <pre><code>spec = FeatureSpec(\n    ...,\n    deps=[\n        MyFeature,\n        FeatureDep(feature=[\"my/feature/key\"]),\n        [\"another/key\"],\n        \"very/nice\",\n    ],\n)\n</code></pre>"},{"location":"learn/syntactic-sugar/#fields","title":"Fields","text":"<p><code>fields</code> elements can omit the full <code>FieldsSpec</code> and be strings (field keys) instead:</p> <pre><code>spec = FeatureSpec(\n    ..., fields=[\"my/field\", FieldSpec(key=\"field/with/version\", code_version=\"v1.2.3\")]\n)\n</code></pre>"},{"location":"learn/syntactic-sugar/#fields-mapping","title":"Fields Mapping","text":"<p>Metaxy uses a bunch of common sense heuristics automatically find parent fields by matching on their names. This is enabled by default. For example, using the same field names in upstream and downstream features will automatically create a dependency between these fields:</p> <pre><code>class Parent(BaseFeature, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n\nclass Child(Parent, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n</code></pre> <p>is equivalent to:</p> <pre><code>class Child(Parent, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n\nclass Grandchild(Child, spec=FeatureSpec(fields=[FieldSpec(key=\"my_field\", deps=[FieldDep(feature=Parent.spec().key, field=\"my_field\")])], ...):\n    ...\n</code></pre>"},{"location":"learn/system-columns/","title":"System Column Registry","text":"<p>Metaxy reserves a small set of system-managed columns that it attaches to feature metadata tables. These columns are part of the platform contract and are used by the metadata store, versioning engine, and migration tooling to keep track of feature lineage.</p> <p>All system column names start with the <code>metaxy_</code> prefix to avoid collisions with user-defined feature fields. Only the prefixed forms are supported.</p>"},{"location":"learn/system-columns/#canonical-column-names","title":"Canonical column names","text":"Canonical name Explanation Level Type <code>metaxy_provenance_by_field</code> Derived from upstream data versions and code version per field sample struct <code>metaxy_provenance</code> Hash of <code>metaxy_provenance_by_field</code> sample string <code>metaxy_data_version_by_field</code> Defaults to <code>metaxy_provenance_by_field</code>, can be user-defined sample struct <code>metaxy_data_version</code> Hash of <code>metaxy_data_version_by_field</code> sample string <code>metaxy_feature_version</code> Derived from versions of relevant upstream fields feature string <code>metaxy_snapshot_version</code> Derived from the entire Metaxy feature graph graph string <code>metaxy_feature_spec_version</code> Derived from the part of the feature spec responsible for versioning sample string <code>metaxy_full_definition_version</code> Hash of the entire feature Pydanitc model schema and the Metaxy project string true"},{"location":"learn/testing/","title":"Testing Metaxy Features","text":"<p>This guide covers patterns for testing your features when using Metaxy.</p>"},{"location":"learn/testing/#graph-isolation","title":"Graph Isolation","text":"<p>By default, Metaxy uses a single global feature graph where all features register themselves automatically. During testing, you might want to construct your own, clean and isolated graphs.</p>"},{"location":"learn/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>@pytest.fixture(autouse=True)\ndef graph():\n    with FeatureGraph().use():\n        yield graph\n\n\ndef test_my_feature(graph: FeatureGraph):\n    class MyFeature(Feature, spec=...):\n        pass\n\n    # Test operations here\n\n    # inspect the graph object if needed\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one. Multiple graphs can exist at the same time, but only one will be used for feature registration.</p>"},{"location":"learn/testing/#graph-context-management","title":"Graph Context Management","text":"<p>The active graph uses context variables to support multiple graphs:</p> <pre><code># Default global graph (used in production)\ngraph = FeatureGraph()\n\n# Get active graph\nactive = FeatureGraph.get_active()\n\n# Use custom graph temporarily\nwith custom_graph.use():\n    # All operations use custom_graph\n    pass\n</code></pre> <p>This enables:</p> <ul> <li>Isolated testing: Each test gets its own feature registry</li> <li>Migration testing: Load historical graphs for migration scenarios</li> <li>Multi-environment testing: Test different feature configurations</li> </ul>"},{"location":"learn/testing/#testing-metadata-store-operations","title":"Testing Metadata Store Operations","text":""},{"location":"learn/testing/#testing-with-different-backends","title":"Testing with Different Backends","text":"<p>Use parametrized tests to verify behavior across backends:</p> <pre><code>import pytest\n\n\n@pytest.mark.parametrize(\n    \"store_cls\",\n    [\n        InMemoryMetadataStore,\n        DuckDBMetadataStore,\n    ],\n)\ndef test_store_behavior(store_cls, tmp_path):\n    # Use tmp_path for file-based stores\n    store_kwargs = {}\n    if store_cls != InMemoryMetadataStore:\n        store_kwargs[\"path\"] = tmp_path / \"test.db\"\n\n    with store_cls(**store_kwargs) as store:\n        # Test your feature operations\n        pass\n</code></pre>"},{"location":"learn/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing AUTO_CREATE_TABLES Warnings","text":"<p>When testing with <code>auto_create_tables=True</code>, Metaxy emits warnings to remind you not to use this in production. These warnings are important for production safety, but can clutter test output.</p> <p>To suppress these warnings in your test suite, use pytest's <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but pytest filters it from test output.</p> <p>Testing the Warning Itself</p> <p>If you need to verify that the warning is actually emitted, use <code>pytest.warns()</code>:</p> <pre><code>import pytest\n\n\ndef test_auto_create_tables_warning():\n    with pytest.warns(\n        UserWarning, match=r\"AUTO_CREATE_TABLES is enabled.*do not use in production\"\n    ):\n        with DuckDBMetadataStore(\":memory:\", auto_create_tables=True) as store:\n            pass  # Warning is emitted and captured\n</code></pre> <p>This works even with <code>filterwarnings</code> configured, because <code>pytest.warns()</code> explicitly captures and verifies the warning.</p>"},{"location":"learn/integrations/sqlmodel/","title":"SQLModel Integration","text":"<p>The SQLModel integration enables Metaxy features to function as both metadata-tracked features and SQLAlchemy ORM models.</p> <p>This integration combines Metaxy's versioning and dependency tracking with SQLModel's database mapping and query capabilities.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores. The benefits of using SQLModel are mostly in the ability to use migration systems such as Alembic that can ensure schema consistency with Metaxy features, and provide the tools for schema evolution as the features change over time.</p>"},{"location":"learn/integrations/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install metaxy[sqlmodel]\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#basic-usage","title":"Basic Usage","text":"<p>The integration has to be enabled in the configuration file:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_ENABLE=true\n</code></pre> <p>This will expose Metaxy's system tables to SQLAlchemy.</p> <p>First, as always with Metaxy features, we would have to define our ID columns:</p> <pre><code>from metaxy import FeatureSpec\nfrom metaxy.ext.sqlmodel import BaseSQLModelFeature\n\n\nclass SampleFeatureSpec(FeatureSpec):\n    id_columns: tuple[str] = \"sample_id\"\n\n\nclass SampleFeature(BaseSQLModelFeature, table=False, spec=None):\n    sample_id: str\n</code></pre> <p>Note that ID columns cannot be server-generated, so the cannot include a Primary Key.</p> <p>Now we can define feature class that inherits from <code>SampleFeature</code> and specify both Metaxy's <code>spec</code> parameter and SQLModel's <code>table=True</code> parameter:</p> <pre><code>from metaxy import FeatureKey, FieldSpec, FieldKey\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    SampleFeature,\n    table=True,\n    spec=SampleFeatureSpec(\n        key=FeatureKey([\"video\"]),\n        # Root feature with no dependencies\n        fields=[\n            FieldSpec(key=FieldKey([\"frames\"]), code_version=\"1\"),\n            FieldSpec(key=FieldKey([\"duration\"]), code_version=\"1\"),\n        ],\n    ),\n):\n    # User-defined metadata columns\n    path: str\n    duration: float\n</code></pre> <p>This class serves dual purposes:</p> <ul> <li>Metaxy feature: Tracks feature version, field versions, and dependencies</li> <li>SQLModel table: Maps to database schema with ORM functionality</li> </ul> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\"])</code>, the table name becomes <code>\"video\"</code>. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in Metaxy's configuration.</p>"},{"location":"learn/integrations/sqlmodel/#system-managed-columns","title":"System-Managed Columns","text":"<p>Metaxy's metadata store automatically manages versioning columns:</p> <ul> <li><code>provenance_by_field</code>: Struct column mapping field keys to hashes</li> <li><code>feature_version</code>: Hash of feature specification</li> <li><code>snapshot_version</code>: Hash of entire graph state</li> </ul> <p>These columns need not be defined in your SQLModel class. The metadata store injects them during write and read operations.</p>"},{"location":"learn/integrations/sqlmodel/#id-columns","title":"ID Columns","text":"<p>ID columns must exist before database insertion</p> <p>ID columns are used for joins between features, so their values must exist before insertion into the database. This means you cannot use server-generated values (autoincrement, sequences, server_default) for ID columns.</p> <pre><code>Metaxy validates against autoincrement primary keys but cannot detect all server-generated patterns. Ensure your ID columns use client-provided values.\n</code></pre> <p>Example:</p> <pre><code># \u2705 Good: Client-generated ID columns\nclass UserActivity(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"user\", \"activity\"]),\n        id_columns=[\"user_id\", \"session_id\"],  # Client provides these\n        ...\n    ),\n):\n    user_id: str = Field(primary_key=True)  # Client-generated\n    session_id: str = Field(primary_key=True)  # Client-generated\n    created_at: str = Field(sa_column_kwargs={\"server_default\": \"NOW()\"})  # OK - not an ID column\n\n# \u274c Bad: Autoincrement ID column\nclass BadFeature(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"bad\"]),\n        id_columns=[\"id\"],  # This is listed as an ID column\n        ...\n    ),\n):\n    id: int = Field(primary_key=True, sa_column_kwargs={\"autoincrement\": True})  # Will raise error\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#loading-features-and-populating-metadata","title":"Loading Features and Populating Metadata","text":"<p>When using metaxy.init_metaxy to discover and import feature modules, all <code>SQLModelFeature</code> classes are automatically registered in SQLModel's metadata:</p> <pre><code>from metaxy import init_metaxy, init_metaxy\nfrom sqlmodel import SQLModel\n\n# Load all features from configured entrypoints\ngraph = init_metaxy()\n\n# All SQLModelFeature tables are now registered in SQLModel.metadata\n# This metadata can be used with Alembic for migrations\nprint(f\"Tables registered: {list(SQLModel.metadata.tables.keys())}\")\n</code></pre> <p>This is particularly useful when:</p> <ul> <li>Generating Alembic migrations that need to discover all tables</li> <li>Setting up database connections that require the complete schema</li> <li>Using SQLModel's <code>create_all()</code> for development/testing (Metaxy's <code>auto_create_tables</code> setting should be preferred over <code>create_all()</code>)</li> </ul> <p>Migration Generation</p> <p>After calling <code>init_metaxy</code>, you can use Alembic to automatically detect all your SQLModelFeature tables and generate migration scripts.</p>"},{"location":"learn/integrations/sqlmodel/#configuration","title":"Configuration","text":"<p>Configure automatic table naming behavior:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#database-migrations-with-alembic","title":"Database Migrations with Alembic","text":"<p>Metaxy provides SQLModel definitions for its system tables that integrate with Alembic for database migrations. This allows you to version control schema changes alongside your application code. Note that you might want to keep separate migrations per each DB-backed <code>MetadataStore</code> used with Metaxy.</p>"},{"location":"learn/integrations/sqlmodel/#separate-migration-management","title":"Separate Migration Management","text":"<p>Metaxy system tables and user application tables should be managed in separate Alembic migration directories. This separation provides critical safety guarantees:</p> <p>System Table Isolation: Metaxy system tables (<code>metaxy-system__feature_versions</code>, <code>metaxy-system__migration_events</code>) have schemas managed by the framework. User migrations cannot accidentally modify these internal structures.</p> <p>Independent Evolution: Metaxy can evolve its system table schemas independently through framework updates without conflicts with user migrations.</p> <p>Failure Isolation: User migration failures remain isolated from metaxy's internal state tracking. A failed user migration leaves system tables intact for debugging and recovery.</p> <p>Clear Audit Trail: Separate migration histories make it trivial to distinguish framework schema changes from application schema changes. This clarity is essential during rollbacks and incident investigation.</p>"},{"location":"learn/integrations/sqlmodel/#setup","title":"Setup","text":"<p>Enable SQLModel system tables in your metaxy configuration and set up two Alembic directories:</p> <pre><code># Standard structure\nproject/\n\u251c\u2500\u2500 alembic/              # User application migrations\n\u2502   \u251c\u2500\u2500 versions/\n\u2502   \u2514\u2500\u2500 env.py\n\u251c\u2500\u2500 .metaxy/\n\u2502   \u2514\u2500\u2500 alembic-system/   # Metaxy system table migrations\n\u2502       \u251c\u2500\u2500 versions/\n\u2502       \u2514\u2500\u2500 env.py\n\u2514\u2500\u2500 metaxy.toml\n</code></pre> <p>Initialize both Alembic directories:</p> <pre><code># Initialize user migrations\nalembic init alembic\n\n# Initialize metaxy system migrations\nalembic init .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#metaxy-system-tables-configuration","title":"Metaxy System Tables Configuration","text":"<p>Configure <code>.metaxy/alembic-system/env.py</code> to manage only metaxy system tables:</p> <pre><code># typical Alembic boilerplate\nfrom metaxy.ext.alembic import get_metaxy_metadata\n\nmetaxy_system_metadata = get_metaxy_metadata()\n\n# metaxy_system_metadata has system tables\n\n# continue with alembic boilerplate\n</code></pre> <p>Configure <code>.metaxy/alembic-system/alembic.ini</code> with your database URL:</p> <pre><code>[alembic]\nscript_location = .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#user-application-tables-configuration","title":"User Application Tables Configuration","text":"<p>Configure <code>alembic/env.py</code> to manage user tables, excluding metaxy system tables:</p> <pre><code># standard Alembic boilerplate\nfrom sqlmodel import SQLModel\nfrom metaxy import init_metaxy\n\ninit_metaxy()\n\n# SQLModel.metadata now has user-defined Metaxy tables\n\n\n# continue with alembic boilerplate\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#migration-workflow","title":"Migration Workflow","text":"<p>Generate and apply migrations separately for each concern:</p> <pre><code># 1. Create metaxy system tables (run once during initial setup)\nalembic -c .metaxy/alembic-system/alembic.ini revision --autogenerate -m \"create metaxy system tables\"\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head\n\n# 2. Create and apply user table migrations\nalembic revision --autogenerate -m \"add video feature table\"\nalembic upgrade head\n\n# 3. When modifying user tables, only user migrations change\nalembic revision --autogenerate -m \"add processing timestamp\"\nalembic upgrade head\n</code></pre> <p>When deploying to production, always apply system table migrations before user migrations:</p> <pre><code># Production deployment order\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head  # System tables first\nalembic upgrade head                                 # Then user tables\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#disabling-sqlmodel-system-tables","title":"Disabling SQLModel System Tables","text":"<p>If required, disable SQLModel system tables in <code>metaxy.toml</code>:</p> <pre><code>[ext.sqlmodel]\nenabled = true\nsystem_tables = false\n</code></pre>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Metaxy CLI.</p> <p>Auto-discovers configuration (<code>metaxy.toml</code> or <code>pyproject.toml</code>) in current or parent directories. Feature definitions are collected via feature discovery.</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>migrations</code><ul> <li><code>generate</code></li> <li><code>apply</code></li> <li><code>status</code></li> <li><code>list</code></li> <li><code>explain</code></li> <li><code>describe</code></li> </ul> </li> <li><code>graph</code><ul> <li><code>push</code></li> <li><code>history</code></li> <li><code>describe</code></li> <li><code>render</code></li> </ul> </li> <li><code>graph-diff</code><ul> <li><code>render</code></li> </ul> </li> <li><code>list</code><ul> <li><code>features</code></li> </ul> </li> <li><code>metadata</code><ul> <li><code>copy</code></li> <li><code>drop</code></li> </ul> </li> </ul> <pre><code>metaxy COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>graph</code>: Manage feature graphs</li> <li><code>graph-diff</code>: Compare and visualize graph snapshots</li> <li><code>list</code>: List Metaxy entities</li> <li><code>metadata</code>: Manage Metaxy metadata</li> <li><code>migrations</code>: Metadata migration commands</li> <li><code>shell</code>: Start interactive shell.</li> </ul> <p>Arguments:</p> <p>Parameters:</p> <ul> <li><code>--config-file</code>: Global option. Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Global option. Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Global option. Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: --no-all-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-shell","title":"metaxy shell","text":"<p>Start interactive shell.</p> <pre><code>metaxy shell\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations","title":"metaxy migrations","text":"<p>Metadata migration commands</p>"},{"location":"reference/cli/#metaxy-migrations-generate","title":"metaxy migrations generate","text":"<p>Generate migration from detected feature changes.</p> <p>Compares the latest snapshot in the store (or specified from_snapshot) with the current active graph to detect changes.</p> <p>The migration is recorded in the system tables (not a YAML file).</p> <pre><code>metaxy migrations generate --op LIST[STR] [OPTIONS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>--name</code>: Migration name (creates {timestamp}_{name} ID)</li> <li><code>--store</code>: Store name (defaults to default)</li> <li><code>--from-snapshot</code>: Compare from this historical snapshot version (defaults to latest)</li> <li><code>--OP</code>: Operation class path to use (can be repeated). Example: metaxy.migrations.ops.DataVersionReconciliation  [required]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-apply","title":"metaxy migrations apply","text":"<p>Apply migration(s) from YAML files.</p> <p>Reads migration definitions from .metaxy/migrations/ directory (git). Follows parent chain to ensure correct order. Tracks execution state in database (events).</p> <pre><code>metaxy migrations apply [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to apply (applies all unapplied if not specified)</li> <li><code>STORE, --store</code>: Metadata store to use.</li> <li><code>--dry-run, --no-dry-run</code>: Preview changes without executing  [default: --no-dry-run]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-status","title":"metaxy migrations status","text":"<p>Show migrations and execution status.</p> <p>Reads migration definitions from YAML files (git). Shows execution status from database events. Displays the parent chain in order.</p> <pre><code>metaxy migrations status\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-list","title":"metaxy migrations list","text":"<p>List all migrations in chain order as defined in code.</p> <p>Displays a simple table showing migration ID, creation time, and operations.</p> <pre><code>metaxy migrations list\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-explain","title":"metaxy migrations explain","text":"<p>Show detailed diff for a migration.</p> <p>Reads migration from YAML file. Computes and displays the GraphDiff between the two snapshots on-demand.</p> <pre><code>metaxy migrations explain [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to explain (explains latest if not specified)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-describe","title":"metaxy migrations describe","text":"<p>Show verbose description of migration(s).</p> <p>Displays detailed information about what the migration will do: - Migration metadata (ID, parent, snapshots, created timestamp) - Operations to execute - Affected features with row counts - Execution status if already run</p> <pre><code>metaxy migrations describe [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-IDS, --migration-ids, --empty-migration-ids</code>: Migration IDs to describe (default: all migrations in order)  [default: []]</li> <li><code>STORE, --store</code>: Metadata store to use.</li> </ul>"},{"location":"reference/cli/#metaxy-graph","title":"metaxy graph","text":"<p>Manage feature graphs</p>"},{"location":"reference/cli/#metaxy-graph-push","title":"metaxy graph push","text":"<p>Serialize all Metaxy features to the metadata store.</p> <p>This is intended to be invoked in a CD pipeline before running Metaxy code in production.</p> <pre><code>metaxy graph push [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-t, --tags</code>: Arbitrary key-value pairs to attach to the pushed snapshot. Example: <code>--tags.git_commit abc123def</code>.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"metaxy graph history","text":"<p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing snapshot versions, when they were recorded, and feature counts.</p> <pre><code>metaxy graph history [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-describe","title":"metaxy graph describe","text":"<p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no dependents) - Project breakdown (if multi-project)</p> <pre><code>metaxy graph describe [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"metaxy graph render","text":"<p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:   - graph (default): Hierarchical tree view   - cards: Panel/card-based view with dependency edges - mermaid: Mermaid flowchart markup - graphviz: Graphviz DOT format</p> <pre><code>metaxy graph render [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>-f, --format</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>-t, --type</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>--snapshot</code>: Snapshot version to render (default: current graph from code)</li> <li><code>--store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>--show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-graph-diff","title":"metaxy graph-diff","text":"<p>Compare and visualize graph snapshots</p>"},{"location":"reference/cli/#metaxy-graph-diff-render","title":"metaxy graph-diff render","text":"<p>Render merged graph visualization comparing two snapshots.</p> <p>Shows all features color-coded by status (added/removed/changed/unchanged). Uses the unified rendering system - same renderers as 'metaxy graph render'.</p> <p>Special snapshot literals: - \"latest\": Most recent snapshot in the store - \"current\": Current graph state from code</p> <p>Output formats: - terminal: Hierarchical tree view (default) - cards: Panel/card-based view - mermaid: Mermaid flowchart diagram - graphviz: Graphviz DOT format</p> <pre><code>metaxy graph-diff render [OPTIONS] FROM-SNAPSHOT [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>FROM-SNAPSHOT</code>: First snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [required]</li> <li><code>TO-SNAPSHOT, --to-snapshot</code>: Second snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [default: current]</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-f, --format</code>: Output format: terminal, cards, mermaid, graphviz, json, or yaml  [choices: terminal, cards, mermaid, graphviz, json, yaml] [default: terminal]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>--show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-list","title":"metaxy list","text":"<p>List Metaxy entities</p>"},{"location":"reference/cli/#metaxy-list-features","title":"metaxy list features","text":"<p>List Metaxy features.</p> <pre><code>metaxy list features\n</code></pre>"},{"location":"reference/cli/#metaxy-metadata","title":"metaxy metadata","text":"<p>Manage Metaxy metadata</p>"},{"location":"reference/cli/#metaxy-metadata-copy","title":"metaxy metadata copy","text":"<p>Copy metadata between stores.</p> <p>Copies metadata for specified features from one store to another, optionally using a historical version. Useful for: - Migrating data between environments - Backfilling metadata - Copying specific feature versions</p> <p>Incremental Mode (default):     By default, performs an anti-join on sample_uid to skip rows that already exist     in the destination for the same snapshot_version. This prevents duplicate writes.</p> <pre><code>Disabling incremental (--no-incremental) may improve performance when:\n- The destination store is empty or has no overlap with source\n- The destination store has eventual deduplication\n</code></pre> <pre><code>metaxy metadata copy FROM TO [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>FROM</code>: Source store name (must be configured in metaxy.toml)  [required]</li> <li><code>TO</code>: Destination store name (must be configured in metaxy.toml)  [required]</li> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to copy (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Copy all features from source store  [default: --no-all-features]</li> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to copy (defaults to latest in source store). The snapshot_version is preserved in the destination.</li> <li><code>INCREMENTAL, --incremental, --no-incremental</code>: Use incremental copy (compare provenance_by_field to skip existing rows). Disable for better performance if destination is empty or uses deduplication.  [default: --incremental]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-drop","title":"metaxy metadata drop","text":"<p>Drop metadata from a store.</p> <p>Removes metadata for specified features from the store. This is a destructive operation and requires --confirm flag.</p> <p>When using --all-features, drops metadata for all features defined in the current project's feature graph.</p> <p>Useful for: - Cleaning up test data - Re-computing feature metadata from scratch - Removing obsolete features</p> <pre><code>metaxy metadata drop [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Store name to drop metadata from (defaults to configured default store)</li> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to drop (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Drop metadata for all features defined in the current project's feature graph  [default: --no-all-features]</li> <li><code>CONFIRM, --confirm, --no-confirm</code>: Confirm the drop operation (required to prevent accidental deletion)  [default: --no-confirm]</li> </ul>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Metaxy can be configured using TOML configuration files or environment variables.</p>"},{"location":"reference/configuration/#default-configuration","title":"Default Configuration","text":"<p>Here is the complete default configuration with all available options:</p> !metaxy.tomlpyproject.toml <pre><code># Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy]\n# Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[tool.metaxy.ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre>"},{"location":"reference/configuration/#configuration-fields","title":"Configuration Fields","text":"<p>Each field can be set via TOML configuration or environment variables.</p>"},{"location":"reference/configuration/#store","title":"<code>store</code>","text":"<p>Default metadata store to use</p> <p>Type: <code>str</code>  | Default: <code>\"dev\"</code></p> !metaxy.tomlpyproject.toml <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORE=dev\n</code></pre>"},{"location":"reference/configuration/#stores","title":"<code>stores</code>","text":"<p>Named store configurations</p> <p>Type: dict[str, metaxy.config.StoreConfig]</p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# stores = {}\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# stores = {}\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORES=...\n</code></pre>"},{"location":"reference/configuration/#migrations_dir","title":"<code>migrations_dir</code>","text":"<p>Directory where migration files are stored</p> <p>Type: <code>str</code>  | Default: <code>\".metaxy/migrations\"</code></p> !metaxy.tomlpyproject.toml <pre><code>migrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>[tool.metaxy]\nmigrations_dir = \".metaxy/migrations\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_MIGRATIONS_DIR=.metaxy/migrations\n</code></pre>"},{"location":"reference/configuration/#entrypoints","title":"<code>entrypoints</code>","text":"<p>List of Python module paths to load for feature discovery</p> <p>Type: <code>list[str]</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# entrypoints = []\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_ENTRYPOINTS=...\n</code></pre>"},{"location":"reference/configuration/#theme","title":"<code>theme</code>","text":"<p>Graph rendering theme for CLI visualization</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>theme = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\ntheme = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_THEME=default\n</code></pre>"},{"location":"reference/configuration/#hash_truncation_length","title":"<code>hash_truncation_length</code>","text":"<p>Truncate hash values to this length (minimum 8 characters). None = no truncation.</p> <p>Type: <code>int | None</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# hash_truncation_length = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# hash_truncation_length = null\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=...\n</code></pre>"},{"location":"reference/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre>"},{"location":"reference/configuration/#project","title":"<code>project</code>","text":"<p>Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>project = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_PROJECT=default\n</code></pre>"},{"location":"reference/configuration/#ext-sqlmodel-configuration","title":"Ext &gt; Sqlmodel Configuration","text":""},{"location":"reference/configuration/#extsqlmodelenable","title":"<code>ext.sqlmodel.enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__ENABLE=false\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelinfer_db_table_names","title":"<code>ext.sqlmodel.infer_db_table_names</code>","text":"<p>Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's tablename value.</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelsystem_tables","title":"<code>ext.sqlmodel.system_tables</code>","text":"<p>Whether to use SQLModel definitions for system tables (for Alembic migrations).</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nsystem_tables = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__SYSTEM_TABLES=true\n</code></pre>"},{"location":"reference/configuration/#configuration-types","title":"Configuration Types","text":""},{"location":"reference/configuration/#storeconfig","title":"StoreConfig","text":"<p>Configuration for a single metadata store backend.</p> <p>Fields:</p> <ul> <li><code>type</code> (str): Full import path to the store class</li> <li><code>config</code> (dict[str, Any]): Store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#extconfig","title":"ExtConfig","text":"<p>Configuration for Metaxy integrations with third-party tools.</p> <p>Fields:</p> <ul> <li><code>sqlmodel</code> (SQLModelConfig): SQLModel integration configuration</li> </ul>"},{"location":"reference/configuration/#sqlmodelconfig","title":"SQLModelConfig","text":"<p>Configuration for SQLModel integration.</p> <p>Fields:</p> <ul> <li><code>enable</code> (bool): Whether to enable the plugin (default: <code>false</code>)</li> <li><code>infer_db_table_names</code> (bool): Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's <code>__tablename__</code> value (default: <code>true</code>)</li> <li><code>system_tables</code> (bool): Whether to use SQLModel definitions for system tables (default: <code>true</code>)</li> </ul>"},{"location":"reference/configuration/#store-configuration","title":"Store Configuration","text":"<p>The <code>stores</code> field configures metadata store backends. Each store is defined by:</p> <ul> <li><code>type</code>: Full import path to the store class (e.g., <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code>)</li> <li><code>config</code>: Dictionary of store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#example-multiple-stores-with-fallback-chain","title":"Example: Multiple Stores with Fallback Chain","text":"!metaxy.tomlpyproject.toml <pre><code># Default store to use\nstore = \"dev\"\n\n# Development store (in-memory) with fallback to production\n[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n# Production store\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n\n[tool.metaxy.stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n[tool.metaxy.stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre>"},{"location":"reference/configuration/#available-store-types","title":"Available Store Types","text":"Store Type Import Path Description DuckDB <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code> File-based or in-memory DuckDB backend ClickHouse <code>metaxy.metadata_store.clickhouse.ClickHouseMetadataStore</code> ClickHouse database backend In-Memory <code>metaxy.metadata_store.memory.InMemoryMetadataStore</code> In-memory backend for testing"},{"location":"reference/configuration/#getting-a-store-instance","title":"Getting a Store Instance","text":"<pre><code>from metaxy.config import MetaxyConfig\n\nconfig = MetaxyConfig.load()\n\n# Get the default store\nwith config.get_store() as store:\n    # Use store\n    pass\n\n# Get a specific store by name\nwith config.get_store(\"prod\") as store:\n    # Use store\n    pass\n</code></pre>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol> <p>This allows you to override file-based configuration with environment variables, which is useful for CI/CD pipelines and different deployment environments.</p>"},{"location":"reference/configuration/#loading-configuration","title":"Loading Configuration","text":""},{"location":"reference/configuration/#auto-discovery","title":"Auto-Discovery","text":"<pre><code>from metaxy.config import MetaxyConfig\n\n# Auto-discover config file in current or parent directories\nconfig = MetaxyConfig.load()\n</code></pre>"},{"location":"reference/configuration/#explicit-file","title":"Explicit File","text":"<pre><code># Load from specific file\nconfig = MetaxyConfig.load(\"path/to/metaxy.toml\")\n</code></pre>"},{"location":"reference/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code># Create configuration programmatically\nconfig = MetaxyConfig(\n    store=\"prod\",\n    migrations_dir=\".migrations\",\n)\n</code></pre>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#metaxy","title":"<code>metaxy</code>","text":"<p>The top-level <code>metaxy</code> module provides the main public API for Metaxy.</p>"},{"location":"reference/api/#initialization","title":"Initialization","text":""},{"location":"reference/api/#metaxy.init_metaxy","title":"init_metaxy","text":"<pre><code>init_metaxy(config_file: Path | None = None, search_parents: bool = True) -&gt; MetaxyConfig\n</code></pre> <p>Main user-facing initialization function for Metaxy. It loads the configuration and features.</p> <p>Features are discovered from installed Python packages metadata.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. Defaults to None.</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to search parent directories for configuration files. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code> (              <code>MetaxyConfig</code> )          \u2013            <p>The initialized Metaxy configuration.</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>def init_metaxy(\n    config_file: Path | None = None, search_parents: bool = True\n) -&gt; MetaxyConfig:\n    \"\"\"Main user-facing initialization function for Metaxy. It loads the configuration and features.\n\n    Features are [discovered](../../learn/feature-discovery.md) from installed Python packages metadata.\n\n    Args:\n        config_file (Path | None, optional): Path to the configuration file. Defaults to None.\n        search_parents (bool, optional): Whether to search parent directories for configuration files. Defaults to True.\n\n    Returns:\n        MetaxyConfig: The initialized Metaxy configuration.\n    \"\"\"\n    cfg = MetaxyConfig.load(\n        config_file=config_file,\n        search_parents=search_parents,\n    )\n    load_features(cfg.entrypoints)\n    return cfg\n</code></pre>"},{"location":"reference/api/#metadata-stores","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata through the MetadaStore interface.</p>"},{"location":"reference/api/#dependency-specification","title":"Dependency Specification","text":"<p>Metaxy has a declarative feature specification system that allows users to express dependencies between their features and their versioned fields.</p>"},{"location":"reference/api/config/","title":"Configuration","text":"<p>This is the Python SDK for Metaxy's configuration. See config file reference to learn how to configure Metaxy via <code>TOML</code> files.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig","title":"MetaxyConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> <p>The default store is <code>\"dev\"</code>; <code>METAXY_STORE</code> can be used to override it.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.plugins","title":"plugins  <code>property</code>","text":"<pre><code>plugins: list[str]\n</code></pre> <p>Returns all enabled plugin names from ext configuration.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_project","title":"validate_project  <code>classmethod</code>","text":"<pre><code>validate_project(v: str) -&gt; str\n</code></pre> <p>Validate project name follows naming rules.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"project\")\n@classmethod\ndef validate_project(cls, v: str) -&gt; str:\n    \"\"\"Validate project name follows naming rules.\"\"\"\n    if not v:\n        raise ValueError(\"project name cannot be empty\")\n    if \"/\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain forward slashes (/). \"\n            f\"Forward slashes are reserved for FeatureKey separation\"\n        )\n    if \"__\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain double underscores (__). \"\n            f\"Double underscores are reserved for table name generation\"\n        )\n    import re\n\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", v):\n        raise ValueError(\n            f\"project name '{v}' must contain only alphanumeric characters, underscores, and hyphens\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_hash_truncation_length","title":"validate_hash_truncation_length  <code>classmethod</code>","text":"<pre><code>validate_hash_truncation_length(v: int | None) -&gt; int | None\n</code></pre> <p>Validate hash truncation length is at least 8 if set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"hash_truncation_length\")\n@classmethod\ndef validate_hash_truncation_length(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate hash truncation length is at least 8 if set.\"\"\"\n    if v is not None and v &lt; 8:\n        raise ValueError(\n            f\"hash_truncation_length must be at least 8 characters, got {v}\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(settings_cls: type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources: init \u2192 env \u2192 TOML.</p> <p>Priority (first wins): 1. Init arguments 2. Environment variables 3. TOML file</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources: init \u2192 env \u2192 TOML.\n\n    Priority (first wins):\n    1. Init arguments\n    2. Environment variables\n    3. TOML file\n    \"\"\"\n    toml_settings = TomlConfigSettingsSource(settings_cls)\n    return (init_settings, env_settings, toml_settings)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get","title":"get  <code>classmethod</code>","text":"<pre><code>get() -&gt; MetaxyConfig\n</code></pre> <p>Get the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get(cls) -&gt; \"MetaxyConfig\":\n    \"\"\"Get the current Metaxy configuration.\"\"\"\n    cfg = _metaxy_config.get()\n    if cfg is None:\n        warnings.warn(\n            UserWarning(\n                \"Global Metaxy configuration not initialized. It can be set with MetaxyConfig.set(config) typically after loading it from a toml file. Returning default configuration (with environment variables and other pydantic settings sources resolved, project='default').\"\n            )\n        )\n        return cls(project=\"default\")\n    else:\n        return cfg\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.set","title":"set  <code>classmethod</code>","text":"<pre><code>set(config: Self | None) -&gt; None\n</code></pre> <p>Set the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef set(cls, config: Self | None) -&gt; None:\n    \"\"\"Set the current Metaxy configuration.\"\"\"\n    _metaxy_config.set(config)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.is_set","title":"is_set  <code>classmethod</code>","text":"<pre><code>is_set() -&gt; bool\n</code></pre> <p>Check if the current Metaxy configuration is set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef is_set(cls) -&gt; bool:\n    \"\"\"Check if the current Metaxy configuration is set.\"\"\"\n    return _metaxy_config.get() is not None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.reset","title":"reset  <code>classmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the current Metaxy configuration to None.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset the current Metaxy configuration to None.\"\"\"\n    _metaxy_config.set(None)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.use","title":"use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Use this configuration temporarily, restoring previous config on exit.</p> Example <pre><code>config = MetaxyConfig(project=\"test\")\nwith config.use():\n    # Code here uses test config\n    assert MetaxyConfig.get().project == \"test\"\n# Previous config restored\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Use this configuration temporarily, restoring previous config on exit.\n\n    Example:\n        ```py\n        config = MetaxyConfig(project=\"test\")\n        with config.use():\n            # Code here uses test config\n            assert MetaxyConfig.get().project == \"test\"\n        # Previous config restored\n        ```\n    \"\"\"\n    previous = _metaxy_config.get()\n    _metaxy_config.set(self)\n    try:\n        yield self\n    finally:\n        _metaxy_config.set(previous)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(config_file: str | Path | None = None, *, search_parents: bool = True, auto_discovery_start: Path | None = None) -&gt; MetaxyConfig\n</code></pre> <p>Load config with auto-discovery and parent directory search.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional config file path (overrides auto-discovery)</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Search parent directories for config file (default: True)</p> </li> <li> <code>auto_discovery_start</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to start search from (defaults to cwd)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>Loaded config (TOML + env vars merged)</p> </li> </ul> Example <pre><code># Auto-discover with parent search\nconfig = MetaxyConfig.load()\n\n# Explicit file\nconfig = MetaxyConfig.load(\"custom.toml\")\n\n# Auto-discover without parent search\nconfig = MetaxyConfig.load(search_parents=False)\n\n# Auto-discover from a specific directory\nconfig = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; \"MetaxyConfig\":\n    \"\"\"Load config with auto-discovery and parent directory search.\n\n    Args:\n        config_file: Optional config file path (overrides auto-discovery)\n        search_parents: Search parent directories for config file (default: True)\n        auto_discovery_start: Directory to start search from (defaults to cwd)\n\n    Returns:\n        Loaded config (TOML + env vars merged)\n\n    Example:\n        ```py\n        # Auto-discover with parent search\n        config = MetaxyConfig.load()\n\n        # Explicit file\n        config = MetaxyConfig.load(\"custom.toml\")\n\n        # Auto-discover without parent search\n        config = MetaxyConfig.load(search_parents=False)\n\n        # Auto-discover from a specific directory\n        config = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n        ```\n    \"\"\"\n    # Search for config file if not explicitly provided\n    if config_file is None and search_parents:\n        config_file = cls._discover_config_with_parents(auto_discovery_start)\n\n    # For explicit file, temporarily patch the TomlConfigSettingsSource\n    # to use that file, then use normal instantiation\n    # This ensures env vars still work\n\n    if config_file:\n        # Create a custom settings source class for this file\n        toml_path = Path(config_file)\n\n        class CustomTomlSource(TomlConfigSettingsSource):\n            def __init__(self, settings_cls: type[BaseSettings]):\n                # Skip auto-discovery, use explicit file\n                super(TomlConfigSettingsSource, self).__init__(settings_cls)\n                self.toml_file = toml_path\n                self.toml_data = self._load_toml()\n\n        # Customize sources to use custom TOML file\n        original_method = cls.settings_customise_sources\n\n        @classmethod  # type: ignore[misc]\n        def custom_sources(\n            cls_inner,\n            settings_cls,\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n        ):\n            toml_settings = CustomTomlSource(settings_cls)\n            return (init_settings, env_settings, toml_settings)\n\n        # Temporarily replace method\n        cls.settings_customise_sources = custom_sources  # type: ignore[assignment]\n        config = cls()\n        cls.settings_customise_sources = original_method  # type: ignore[method-assign]\n    else:\n        # Use default sources (auto-discovery + env vars)\n        config = cls()\n\n    cls.set(config)\n\n    return config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_store","title":"get_store","text":"<pre><code>get_store(name: str | None = None) -&gt; MetadataStore\n</code></pre> <p>Instantiate metadata store by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Store name (uses config.store if None)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore</code>           \u2013            <p>Instantiated metadata store</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store name not found in config, or if fallback stores have different hash algorithms than the parent store</p> </li> <li> <code>ImportError</code>             \u2013            <p>If store class cannot be imported</p> </li> </ul> Example <pre><code>config = MetaxyConfig.load()\nstore = config.get_store(\"prod\")\n\n# Use default store\nstore = config.get_store()\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>def get_store(self, name: str | None = None) -&gt; \"MetadataStore\":\n    \"\"\"Instantiate metadata store by name.\n\n    Args:\n        name: Store name (uses config.store if None)\n\n    Returns:\n        Instantiated metadata store\n\n    Raises:\n        ValueError: If store name not found in config, or if fallback stores\n            have different hash algorithms than the parent store\n        ImportError: If store class cannot be imported\n\n    Example:\n        ```py\n        config = MetaxyConfig.load()\n        store = config.get_store(\"prod\")\n\n        # Use default store\n        store = config.get_store()\n        ```\n    \"\"\"\n    from metaxy.versioning.types import HashAlgorithm\n\n    if len(self.stores) == 0:\n        raise ValueError(\n            \"No Metaxy stores available. They should be configured in metaxy.toml|pyproject.toml or via environment variables.\"\n        )\n\n    name = name or self.store\n\n    if name not in self.stores:\n        raise ValueError(\n            f\"Store '{name}' not found in config. \"\n            f\"Available stores: {list(self.stores.keys())}\"\n        )\n\n    store_config = self.stores[name]\n\n    # Get store class (already imported by Pydantic's ImportString)\n    store_class = store_config.type\n\n    # Extract configuration\n    config_copy = store_config.config.copy()\n    fallback_store_names = config_copy.pop(\"fallback_stores\", [])\n\n    # Get hash_algorithm from config (if specified) and convert to enum\n    configured_hash_algorithm = config_copy.get(\"hash_algorithm\")\n    if configured_hash_algorithm is not None:\n        # Convert string to enum if needed\n        if isinstance(configured_hash_algorithm, str):\n            configured_hash_algorithm = HashAlgorithm(configured_hash_algorithm)\n            config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n    else:\n        # Don't set a default here - let the store choose its own default\n        configured_hash_algorithm = None\n\n    # Get hash_truncation_length from global config (unless overridden in store config)\n    if (\n        \"hash_truncation_length\" not in config_copy\n        and self.hash_truncation_length is not None\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"hash_truncation_length\"] = self.hash_truncation_length\n\n    # Get auto_create_tables from global config (unless overridden in store config)\n    if (\n        \"auto_create_tables\" not in config_copy\n        and self.auto_create_tables is not None\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"auto_create_tables\"] = self.auto_create_tables\n\n    # Build fallback stores recursively\n    fallback_stores = []\n    for fallback_name in fallback_store_names:\n        fallback_store = self.get_store(fallback_name)\n        fallback_stores.append(fallback_store)\n\n    # Instantiate store with config + fallback_stores\n    store = store_class(\n        fallback_stores=fallback_stores,\n        **config_copy,\n    )\n\n    # Verify the store actually uses the hash algorithm we configured\n    # (in case a store subclass overrides the default or ignores the parameter)\n    # Only check if we explicitly configured a hash algorithm\n    if (\n        configured_hash_algorithm is not None\n        and store.hash_algorithm != configured_hash_algorithm\n    ):\n        raise ValueError(\n            f\"Store '{name}' ({store_class.__name__}) was configured with \"\n            f\"hash_algorithm='{configured_hash_algorithm.value}' but is using \"\n            f\"'{store.hash_algorithm.value}'. The store class may have overridden \"\n            f\"the hash algorithm. All stores must use the same hash algorithm.\"\n        )\n\n    return store\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig","title":"StoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for a single metadata store.</p> Example <pre><code>config = StoreConfig(\n    type=\"metaxy_delta.DeltaMetadataStore\",\n    config={\n        \"table_uri\": \"s3://bucket/metadata\",\n        \"region\": \"us-west-2\",\n        \"fallback_stores\": [\"prod\"],\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/","title":"Constants","text":""},{"location":"reference/api/constants/#metaxy.models.constants","title":"Constants","text":"<p>Shared constants for system-managed column names.</p> <p>All system columns use the metaxy_ prefix to avoid conflicts with user columns.</p>"},{"location":"reference/api/constants/#metaxy.models.constants-attributes","title":"Attributes","text":""},{"location":"reference/api/constants/#metaxy.models.constants.DEFAULT_CODE_VERSION","title":"DEFAULT_CODE_VERSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CODE_VERSION = '__metaxy_initial__'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMN_PREFIX","title":"SYSTEM_COLUMN_PREFIX  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMN_PREFIX = 'metaxy_'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD","title":"METAXY_PROVENANCE_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE_BY_FIELD = f'{SYSTEM_COLUMN_PREFIX}provenance_by_field'\n</code></pre> <p>Field-level provenance hashes (struct column mapping field names to hashes).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE","title":"METAXY_PROVENANCE  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE = f'{SYSTEM_COLUMN_PREFIX}provenance'\n</code></pre> <p>Hash of<code>metaxy_provenance_by_field</code> -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_VERSION","title":"METAXY_FEATURE_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_VERSION = f'{SYSTEM_COLUMN_PREFIX}feature_version'\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_SNAPSHOT_VERSION","title":"METAXY_SNAPSHOT_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_SNAPSHOT_VERSION = f'{SYSTEM_COLUMN_PREFIX}snapshot_version'\n</code></pre> <p>Hash of the entire feature graph snapshot (recorded during deployment).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_SPEC_VERSION","title":"METAXY_FEATURE_SPEC_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_SPEC_VERSION = f'{SYSTEM_COLUMN_PREFIX}feature_spec_version'\n</code></pre> <p>Hash of the complete feature specification (used for migration detection).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FULL_DEFINITION_VERSION","title":"METAXY_FULL_DEFINITION_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FULL_DEFINITION_VERSION = f'{SYSTEM_COLUMN_PREFIX}full_definition_version'\n</code></pre> <p>Hash of the complete feature definition including Pydantic schema, feature spec, and project.</p> <p>This comprehensive hash captures ALL aspects of a feature definition: - Pydantic model schema (field types, descriptions, validators, serializers, etc.) - Feature specification (dependencies, fields, code_versions, metadata) - Project name</p> <p>Used in system tables to detect when ANY part of a feature changes.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD","title":"METAXY_DATA_VERSION_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION_BY_FIELD = f'{SYSTEM_COLUMN_PREFIX}data_version_by_field'\n</code></pre> <p>Field-level data version hashes (struct column mapping field names to version hashes).</p> <p>Similar to provenance_by_field, but can be user-overridden to implement custom versioning (e.g., content hashes, timestamps, semantic versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION","title":"METAXY_DATA_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION = f'{SYSTEM_COLUMN_PREFIX}data_version'\n</code></pre> <p>Hash of metaxy_data_version_by_field -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_CREATED_AT","title":"METAXY_CREATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_CREATED_AT = f'{SYSTEM_COLUMN_PREFIX}created_at'\n</code></pre> <p>Timestamp when the metadata row was created.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.ALL_SYSTEM_COLUMNS","title":"ALL_SYSTEM_COLUMNS  <code>module-attribute</code>","text":"<pre><code>ALL_SYSTEM_COLUMNS = frozenset({METAXY_PROVENANCE_BY_FIELD, METAXY_PROVENANCE, METAXY_FEATURE_VERSION, METAXY_SNAPSHOT_VERSION, METAXY_DATA_VERSION_BY_FIELD, METAXY_DATA_VERSION, METAXY_CREATED_AT})\n</code></pre> <p>All Metaxy-managed column names that are injected into feature tables.</p>"},{"location":"reference/api/constants/#metaxy.models.constants._DROPPABLE_COLUMNS","title":"_DROPPABLE_COLUMNS  <code>module-attribute</code>","text":"<pre><code>_DROPPABLE_COLUMNS = frozenset({METAXY_FEATURE_VERSION, METAXY_SNAPSHOT_VERSION, METAXY_CREATED_AT, METAXY_DATA_VERSION_BY_FIELD, METAXY_DATA_VERSION})\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants-functions","title":"Functions","text":""},{"location":"reference/api/constants/#metaxy.models.constants.is_system_column","title":"is_system_column","text":"<pre><code>is_system_column(name: str) -&gt; bool\n</code></pre> <p>Check whether a column name is a system-managed column.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Column name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the column is a system column, False otherwise</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_system_column(\"metaxy_feature_version\")\nTrue\n&gt;&gt;&gt; is_system_column(\"my_column\")\nFalse\n</code></pre> Source code in <code>src/metaxy/models/constants.py</code> <pre><code>def is_system_column(name: str) -&gt; bool:\n    \"\"\"Check whether a column name is a system-managed column.\n\n    Args:\n        name: Column name to check\n\n    Returns:\n        True if the column is a system column, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_system_column(\"metaxy_feature_version\")\n        True\n        &gt;&gt;&gt; is_system_column(\"my_column\")\n        False\n    \"\"\"\n    return name in ALL_SYSTEM_COLUMNS\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.is_droppable_system_column","title":"is_droppable_system_column","text":"<pre><code>is_droppable_system_column(name: str) -&gt; bool\n</code></pre> <p>Check whether a column should be dropped when joining upstream features.</p> <p>Droppable columns (feature_version, snapshot_version) are recalculated for each feature, so keeping them from upstream would cause conflicts.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Column name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the column should be dropped during joins, False otherwise</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_droppable_system_column(\"metaxy_feature_version\")\nTrue\n&gt;&gt;&gt; is_droppable_system_column(\"metaxy_provenance_by_field\")\nFalse\n</code></pre> Source code in <code>src/metaxy/models/constants.py</code> <pre><code>def is_droppable_system_column(name: str) -&gt; bool:\n    \"\"\"Check whether a column should be dropped when joining upstream features.\n\n    Droppable columns (feature_version, snapshot_version) are recalculated for\n    each feature, so keeping them from upstream would cause conflicts.\n\n    Args:\n        name: Column name to check\n\n    Returns:\n        True if the column should be dropped during joins, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_droppable_system_column(\"metaxy_feature_version\")\n        True\n        &gt;&gt;&gt; is_droppable_system_column(\"metaxy_provenance_by_field\")\n        False\n    \"\"\"\n    return name in _DROPPABLE_COLUMNS\n</code></pre>"},{"location":"reference/api/types/","title":"Types","text":"<p>A few types used in Metaxy here and there.</p>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement","title":"LazyIncrement","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing lazy dataframes.</p> <p>Contains three sets of samples: - added: New samples from upstream not present in current metadata - changed: Samples with different provenance - removed: Samples in current metadata but not in upstream state</p>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.collect","title":"collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>Collect all lazy frames to eager DataFrames.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; Increment:\n    \"\"\"Collect all lazy frames to eager DataFrames.\"\"\"\n    return Increment(\n        added=self.added.collect(),\n        changed=self.changed.collect(),\n        removed=self.removed.collect(),\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment","title":"Increment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing eager dataframes.</p> <p>Contains three sets of samples: - added: New samples from upstream not present in current metadata - changed: Samples with different provenance - removed: Samples in current metadata but not in upstream state</p>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.Increment.collect","title":"collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>No-op for eager Increment (already collected).</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; \"Increment\":\n    \"\"\"No-op for eager Increment (already collected).\"\"\"\n    return self\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm","title":"HashAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Supported hash algorithms for field provenance calculation.</p> <p>These algorithms are chosen for: - Speed (non-cryptographic hashes preferred) - Cross-database availability - Good collision resistance for field provenance calculation</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH64","title":"XXHASH64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH64 = 'xxhash64'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH32","title":"XXHASH32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH32 = 'xxhash32'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.WYHASH","title":"WYHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WYHASH = 'wyhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.SHA256","title":"SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'sha256'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.MD5","title":"MD5  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MD5 = 'md5'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.FARMHASH","title":"FARMHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FARMHASH = 'farmhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.models.types.SnapshotPushResult","title":"SnapshotPushResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of recording a feature graph snapshot.</p> <p>Attributes:</p> <ul> <li> <code>snapshot_version</code>               (<code>str</code>)           \u2013            <p>The deterministic hash of the graph snapshot</p> </li> <li> <code>already_pushed</code>               (<code>bool</code>)           \u2013            <p>True if this snapshot_version was already pushed previously</p> </li> <li> <code>updated_features</code>               (<code>list[str]</code>)           \u2013            <p>List of feature keys with updated information (changed full_definition_version)</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.IDColumns","title":"IDColumns  <code>module-attribute</code>","text":"<pre><code>IDColumns: TypeAlias = Sequence[str]\n</code></pre>"},{"location":"reference/api/definitions/","title":"Definitions","text":"<p>Metaxy's dependency specification system allows users to express dependencies between their features and their fields.</p>"},{"location":"reference/api/definitions/#featurespec","title":"<code>FeatureSpec</code>","text":"<p>FeatureSpec is the core of Metaxy's dependency specification system: it stores all the information about the parents, field mappings, and other metadata associated with a feature.</p>"},{"location":"reference/api/definitions/#feature","title":"Feature","text":"<p>A feature in Metaxy is used to model user-defined metadata. It must have a <code>FeatureSpec</code> instance associated with it. A <code>Feature</code> class is typically associated with a single table in the MetadataStore.</p>"},{"location":"reference/api/definitions/#fieldspec","title":"FieldSpec","text":"<p>A [field] in Metaxy is a logical slices of the data represented by feature metadata. Users are free to define their own fields as is suitable for them.</p> <p>Dependencies between fields are modeled with FieldDep and can be automatic (via field mappings) or explicitly set by users.</p>"},{"location":"reference/api/definitions/#graph","title":"Graph","text":"<p>All features live on a FeatureGraph object. The users don't typically interact with it outside of advanced use cases.</p>"},{"location":"reference/api/definitions/feature-spec/","title":"Feature Spec","text":"<p>Feature specs act as source of truth for all metadata related to features: their dependencies, fields, code versions, and so on.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec","title":"FeatureSpec  <code>pydantic-model</code>","text":"<pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[CoercibleToFeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        or FeatureKey instance.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"JsonValue\": {},\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"id_columns\": {\n      \"description\": \"Columns that uniquely identify a sample in this feature.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    },\n    \"deps\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FeatureDep\"\n      },\n      \"title\": \"Deps\",\n      \"type\": \"array\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"lineage\": {\n      \"$ref\": \"#/$defs/LineageRelationship\",\n      \"description\": \"Lineage relationship of this feature.\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/JsonValue\"\n      },\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"id_columns\"\n  ],\n  \"title\": \"FeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FeatureKey</code>)             </li> <li> <code>id_columns</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>deps</code>                 (<code>list[FeatureDep]</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldSpec]</code>)             </li> <li> <code>lineage</code>                 (<code>LineageRelationship</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, JsonValue]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_unique_field_keys</code> </li> <li> <code>validate_id_columns</code> </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    lineage: LineageRelationship | None = None,\n    metadata: Mapping[str, JsonValue] | None = None,\n    **kwargs: Any,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.id_columns","title":"id_columns  <code>pydantic-field</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Columns that uniquely identify a sample in this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.lineage","title":"lineage  <code>pydantic-field</code>","text":"<pre><code>lineage: LineageRelationship\n</code></pre> <p>Lineage relationship of this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.metadata","title":"metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, JsonValue]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.code_version","title":"code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.feature_spec_version","title":"feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]))],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.table_name","title":"table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_unique_field_keys","title":"validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; Self\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; Self:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(\n                f\"Duplicate field key found: {field.key}. \"\n                f\"All fields must have unique keys.\"\n            )\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_id_columns","title":"validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; Self\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; Self:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\n            \"id_columns must be non-empty if specified. Use None for default.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#feature-dependencies","title":"Feature Dependencies","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep","title":"FeatureDep  <code>pydantic-model</code>","text":"<pre><code>FeatureDep(*, feature: str | Sequence[str] | FeatureKey | FeatureSpecProtocol | type[BaseFeature], columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature dependency specification with optional column selection and renaming.</p> <p>Attributes:</p> <ul> <li> <code>key</code>           \u2013            <p>The feature key to depend on. Accepts string (\"a/b/c\"), list ([\"a\", \"b\", \"c\"]), or FeatureKey instance.</p> </li> <li> <code>columns</code>               (<code>tuple[str, ...] | None</code>)           \u2013            <p>Optional tuple of column names to select from upstream feature. - None (default): Keep all columns from upstream - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.) - Tuple of names: Keep only specified columns (plus system columns)</p> </li> <li> <code>rename</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Optional mapping of old column names to new names. Applied after column selection.</p> </li> <li> <code>fields_mapping</code>               (<code>FieldsMapping</code>)           \u2013            <p>Optional field mapping configuration for automatic field dependency resolution. When provided, fields without explicit deps will automatically map to matching upstream fields. Defaults to using <code>[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]</code>.</p> </li> </ul> <p>Examples:</p> <pre><code># Keep all columns with default field mapping\nFeatureDep(feature=\"upstream\")\n\n# Keep all columns with suffix matching\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.default(match_suffix=True))\n\n# Keep all columns with all fields mapping\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.all())\n\n# Keep only specific columns\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\")\n)\n\n# Rename columns to avoid conflicts\nFeatureDep(\n    feature=\"upstream/feature\",\n    rename={\"old_name\": \"new_name\"}\n)\n\n# Select and rename\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\"),\n    rename={\"col1\": \"upstream_col1\"}\n)\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        or FeatureKey instance.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n    ```\",\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Columns\"\n    },\n    \"rename\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Rename\"\n    },\n    \"fields_mapping\": {\n      \"$ref\": \"#/$defs/FieldsMapping\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FeatureDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>columns</code>                 (<code>tuple[str, ...] | None</code>)             </li> <li> <code>rename</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>fields_mapping</code>                 (<code>FieldsMapping</code>)             </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    *,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | FeatureSpecProtocol\n    | type[BaseFeature],\n    columns: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.table_name","title":"table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.feature.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature/","title":"Feature","text":"<p><code>BaseFeature</code> is the most important class in Metaxy.</p> <p>Users can extend this class to define their features.</p> <p>! \"Code Version Access\"     Retrieve a feature's code version from its spec: <code>MyFeature.spec().code_version</code>.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey","title":"FeatureKey  <code>pydantic-model</code>","text":"<pre><code>FeatureKey(key: str)\n</code></pre><pre><code>FeatureKey(key: Sequence[str])\n</code></pre><pre><code>FeatureKey(key: FeatureKey)\n</code></pre><pre><code>FeatureKey(*parts: str)\n</code></pre><pre><code>FeatureKey(*, parts: Sequence[str])\n</code></pre> <pre><code>FeatureKey(key: str | Sequence[str] | FeatureKey | None = None, *parts: str, **kwargs: Any)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Feature key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Examples:</p> <pre><code>FeatureKey(\"a/b/c\")  # String format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey([\"a\", \"b\", \"c\"])  # List format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(FeatureKey([\"a\", \"b\", \"c\"]))  # FeatureKey copy\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(\"a\", \"b\", \"c\")  # Variadic format\n# FeatureKey(parts=['a', 'b', 'c'])\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>str | _CoercibleToKey | Self</code>, default:                   <code>()</code> )           \u2013            <p>Variadic positional arguments: - Single str: Split on \"/\" separator (\"a/b/c\" -&gt; [\"a\", \"b\", \"c\"]) - Single Sequence[str]: Use as parts ([\"a\", \"b\", \"c\"]) - Single Key instance: Copy parts - Multiple str: Use as parts (\"a\", \"b\", \"c\" -&gt; [\"a\", \"b\", \"c\"])</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for BaseModel (e.g., parts=...)</p> </li> </ul> <p>Examples:</p> <pre><code>FeatureKey(\"a/b/c\")  # String with separator\nFeatureKey([\"a\", \"b\", \"c\"])  # List\nFeatureKey(\"a\", \"b\", \"c\")  # Variadic\nFeatureKey(parts=[\"a\", \"b\", \"c\"])  # Keyword argument\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n  \"properties\": {\n    \"parts\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Parts\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"parts\"\n  ],\n  \"title\": \"FeatureKey\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>parts</code>                 (<code>tuple[str, ...]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_input</code> </li> <li> <code>_validate_parts_content</code>                 \u2192                   <code>parts</code> </li> </ul> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    key: str | Sequence[str] | FeatureKey | None = None,\n    *parts: str,\n    **kwargs: Any,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.to_string","title":"to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.to_struct_key","title":"to_struct_key","text":"<pre><code>to_struct_key() -&gt; str\n</code></pre> <p>Convert to a name that can be used as struct key in databases</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_struct_key(self) -&gt; str:\n    \"\"\"Convert to a name that can be used as struct key in databases\"\"\"\n    return \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__le__","title":"__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__contains__","title":"__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__reversed__","title":"__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__get_validators__","title":"__get_validators__  <code>classmethod</code>","text":"<pre><code>__get_validators__()\n</code></pre> <p>Pydantic validator for when used as a field type.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef __get_validators__(cls):\n    \"\"\"Pydantic validator for when used as a field type.\"\"\"\n    yield cls.validate\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.validate","title":"validate  <code>classmethod</code>","text":"<pre><code>validate(value: Any) -&gt; FeatureKey\n</code></pre> <p>Convert various inputs to FeatureKey.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef validate(cls, value: Any) -&gt; FeatureKey:\n    \"\"\"Convert various inputs to FeatureKey.\"\"\"\n    if isinstance(value, cls):\n        return value\n    return cls(value)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.model_dump","title":"model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.to_column_suffix","title":"to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature","title":"BaseFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"properties\": {},\n  \"title\": \"BaseFeature\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.table_name","title":"table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_version","title":"feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_spec_version","title":"feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.full_definition_version","title":"full_definition_version  <code>classmethod</code>","text":"<pre><code>full_definition_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature definition including Pydantic schema.</p> <p>This method computes a hash of the entire feature class definition, including: - Pydantic model schema - Project name</p> <p>Used in the <code>metaxy_full_definition_version</code> column of system tables.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete definition</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef full_definition_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature definition including Pydantic schema.\n\n    This method computes a hash of the entire feature class definition, including:\n    - Pydantic model schema\n    - Project name\n\n    Used in the `metaxy_full_definition_version` column of system tables.\n\n    Returns:\n        SHA256 hex digest of the complete definition\n    \"\"\"\n    import json\n\n    hasher = hashlib.sha256()\n\n    # Hash the Pydantic schema (includes field types, descriptions, validators, etc.)\n    schema = cls.model_json_schema()\n    schema_json = json.dumps(schema, sort_keys=True)\n    hasher.update(schema_json.encode())\n\n    # Hash the feature specification\n    hasher.update(cls.feature_spec_version().encode())\n\n    # Hash the project name\n    hasher.update(cls.project.encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.provenance_by_field","title":"provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.load_input","title":"load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.resolve_data_version_diff","title":"resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: Any, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>Any</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: Any,\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.versioning.types import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature","title":"Feature  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFeature</code></p> <p>A default specialization of BaseFeature that uses a <code>sample_uid</code> ID column.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A default specialization of BaseFeature that uses a `sample_uid` ID column.\",\n  \"properties\": {},\n  \"title\": \"Feature\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.Feature.table_name","title":"table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.feature_version","title":"feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.feature_spec_version","title":"feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.full_definition_version","title":"full_definition_version  <code>classmethod</code>","text":"<pre><code>full_definition_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature definition including Pydantic schema.</p> <p>This method computes a hash of the entire feature class definition, including: - Pydantic model schema - Project name</p> <p>Used in the <code>metaxy_full_definition_version</code> column of system tables.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete definition</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef full_definition_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature definition including Pydantic schema.\n\n    This method computes a hash of the entire feature class definition, including:\n    - Pydantic model schema\n    - Project name\n\n    Used in the `metaxy_full_definition_version` column of system tables.\n\n    Returns:\n        SHA256 hex digest of the complete definition\n    \"\"\"\n    import json\n\n    hasher = hashlib.sha256()\n\n    # Hash the Pydantic schema (includes field types, descriptions, validators, etc.)\n    schema = cls.model_json_schema()\n    schema_json = json.dumps(schema, sort_keys=True)\n    hasher.update(schema_json.encode())\n\n    # Hash the feature specification\n    hasher.update(cls.feature_spec_version().encode())\n\n    # Hash the project name\n    hasher.update(cls.project.encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.provenance_by_field","title":"provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.load_input","title":"load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.resolve_data_version_diff","title":"resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: Any, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>Any</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: Any,\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.versioning.types import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.get_feature_by_key","title":"get_feature_by_key","text":"<pre><code>get_feature_by_key(key: FeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key from the active graph.</p> <p>Convenience function that retrieves Metaxy feature class from the currently active feature graph. Can be useful when receiving a feature key from storage or across process boundaries.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>from metaxy import get_feature_by_key, FeatureKey\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = get_feature_by_key(parent_key)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(key: \"FeatureKey\") -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key from the active graph.\n\n    Convenience function that retrieves Metaxy feature class from the currently active [feature graph][metaxy.FeatureGraph]. Can be useful when receiving a feature key from storage or across process boundaries.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        from metaxy import get_feature_by_key, FeatureKey\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = get_feature_by_key(parent_key)\n        ```\n    \"\"\"\n    graph = FeatureGraph.get_active()\n    return graph.get_feature_by_key(key)\n</code></pre>"},{"location":"reference/api/definitions/field/","title":"Field","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey","title":"FieldKey  <code>pydantic-model</code>","text":"<pre><code>FieldKey(key: str | Sequence[str] | FieldKey | None = None, *parts: str, **kwargs: Any)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Examples:</p> <pre><code>FieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(\"a\", \"b\", \"c\")  # Variadic format\n# FieldKey(parts=['a', 'b', 'c'])\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>str | _CoercibleToKey | Self</code>, default:                   <code>()</code> )           \u2013            <p>Variadic positional arguments: - Single str: Split on \"/\" separator (\"a/b/c\" -&gt; [\"a\", \"b\", \"c\"]) - Single Sequence[str]: Use as parts ([\"a\", \"b\", \"c\"]) - Single Key instance: Copy parts - Multiple str: Use as parts (\"a\", \"b\", \"c\" -&gt; [\"a\", \"b\", \"c\"])</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for BaseModel (e.g., parts=...)</p> </li> </ul> <p>Examples:</p> <pre><code>FeatureKey(\"a/b/c\")  # String with separator\nFeatureKey([\"a\", \"b\", \"c\"])  # List\nFeatureKey(\"a\", \"b\", \"c\")  # Variadic\nFeatureKey(parts=[\"a\", \"b\", \"c\"])  # Keyword argument\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n  \"properties\": {\n    \"parts\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Parts\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"parts\"\n  ],\n  \"title\": \"FieldKey\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>parts</code>                 (<code>tuple[str, ...]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_input</code> </li> <li> <code>_validate_parts_content</code>                 \u2192                   <code>parts</code> </li> </ul> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    key: str | Sequence[str] | FieldKey | None = None,\n    *parts: str,\n    **kwargs: Any,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_string","title":"to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_struct_key","title":"to_struct_key","text":"<pre><code>to_struct_key() -&gt; str\n</code></pre> <p>Convert to a name that can be used as struct key in databases</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_struct_key(self) -&gt; str:\n    \"\"\"Convert to a name that can be used as struct key in databases\"\"\"\n    return \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_column_suffix","title":"to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__le__","title":"__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__contains__","title":"__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__reversed__","title":"__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__get_validators__","title":"__get_validators__  <code>classmethod</code>","text":"<pre><code>__get_validators__()\n</code></pre> <p>Pydantic validator for when used as a field type.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef __get_validators__(cls):\n    \"\"\"Pydantic validator for when used as a field type.\"\"\"\n    yield cls.validate\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.validate","title":"validate  <code>classmethod</code>","text":"<pre><code>validate(value: Any) -&gt; FieldKey\n</code></pre> <p>Convert various inputs to FieldKey.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef validate(cls, value: Any) -&gt; FieldKey:\n    \"\"\"Convert various inputs to FieldKey.\"\"\"\n    if isinstance(value, cls):\n        return value\n    return cls(value)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.model_dump","title":"model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec","title":"FieldSpec  <code>pydantic-model</code>","text":"<pre><code>FieldSpec(key: CoercibleToFieldKey, code_version: str = DEFAULT_CODE_VERSION, deps: SpecialFieldDep | list[FieldDep] | None = None, *args, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FieldKey\"\n    },\n    \"code_version\": {\n      \"default\": \"__metaxy_initial__\",\n      \"title\": \"Code Version\",\n      \"type\": \"string\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/SpecialFieldDep\"\n        },\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldDep\"\n          },\n          \"type\": \"array\"\n        }\n      ],\n      \"title\": \"Deps\"\n    }\n  },\n  \"title\": \"FieldSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FieldKey</code>)             </li> <li> <code>code_version</code>                 (<code>str</code>)             </li> <li> <code>deps</code>                 (<code>SpecialFieldDep | list[FieldDep]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    key: CoercibleToFieldKey,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n    *args,\n    **kwargs: Any,\n) -&gt; None:\n    validated_key = FieldKeyAdapter.validate_python(key)\n\n    # Handle None deps - use empty list as default\n    if deps is None:\n        deps = []\n\n    super().__init__(\n        key=validated_key,\n        code_version=code_version,\n        deps=deps,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec.__get_pydantic_core_schema__","title":"__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(source_type, handler)\n</code></pre> <p>Add custom validator to coerce strings to FieldSpec.</p> Source code in <code>src/metaxy/models/field.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler):\n    \"\"\"Add custom validator to coerce strings to FieldSpec.\"\"\"\n    from pydantic_core import core_schema\n\n    # Get the default schema\n    python_schema = handler(source_type)\n\n    # Wrap it with a before validator that converts strings\n    return core_schema.no_info_before_validator_function(\n        _validate_field_spec_from_string,\n        python_schema,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldDep","title":"FieldDep  <code>pydantic-model</code>","text":"<pre><code>FieldDep(feature: str | Sequence[str] | FeatureKey | FeatureSpec | type[Feature], fields: list[CoercibleToFieldKey] | Literal[ALL] = ALL, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    }\n  },\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"const\": \"__METAXY_ALL_DEP__\",\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"__METAXY_ALL_DEP__\",\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FieldDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldKey] | Literal[ALL]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    feature: str | Sequence[str] | FeatureKey | \"FeatureSpec\" | type[\"Feature\"],\n    fields: list[CoercibleToFieldKey]\n    | Literal[SpecialFieldDep.ALL] = SpecialFieldDep.ALL,\n    **kwargs: Any,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/","title":"Fields Mapping","text":"<p>Metaxy provides a few helpers when defining field-level dependencies:</p> <ul> <li>the default mapping that matches on field names or suffixes with [[metaxy.FieldsMapping.identity]] (the default one)</li> <li><code>specific</code> mapping with [[metaxy.FieldsMapping.default]]</li> <li><code>all</code> mapping with [[metaxy.FieldsMapping.all]]</li> </ul> <p>Always use these classmethods to create instances of lineage relationships. Under the hood, they use Pydantic's discriminated union to ensure that the correct type is constructed based on the provided data.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping","title":"FieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for field mapping configurations.</p> <p>Field mappings define how a field automatically resolves its dependencies based on upstream feature fields. This is separate from explicit field dependencies which are defined directly.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n  \"properties\": {\n    \"mapping\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"all\": \"#/$defs/AllFieldsMapping\",\n          \"default\": \"#/$defs/DefaultFieldsMapping\",\n          \"none\": \"#/$defs/NoneFieldsMapping\",\n          \"specific\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/AllFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/NoneFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n        }\n      ],\n      \"title\": \"Mapping\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"FieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>mapping</code>                 (<code>AllFieldsMapping | SpecificFieldsMapping | NoneFieldsMapping | DefaultFieldsMapping</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","title":"resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve field dependencies based on upstream feature fields.</p> <p>Invokes the provided mapping to resolve dependencies.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    \"\"\"Resolve field dependencies based on upstream feature fields.\n\n    Invokes the provided mapping to resolve dependencies.\n\n    Args:\n        context: The resolution context containing field key and upstream feature.\n\n    Returns:\n        Set of [FieldKey][metaxy.models.types.FieldKey] instances for matching fields\n    \"\"\"\n    return self.mapping.resolve_field_deps(context)\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.default","title":"default  <code>classmethod</code>","text":"<pre><code>default(*, match_suffix: bool = False, exclude_fields: list[FieldKey] | None = None) -&gt; Self\n</code></pre> <p>Create a default field mapping configuration.</p> <p>Parameters:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef default(\n    cls,\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self:\n    \"\"\"Create a default field mapping configuration.\n\n    Args:\n        match_suffix: If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")\n        exclude_fields: List of field keys to exclude from auto-mapping\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(\n        mapping=DefaultFieldsMapping(\n            match_suffix=match_suffix,\n            exclude_fields=exclude_fields or [],\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.specific","title":"specific  <code>classmethod</code>","text":"<pre><code>specific(mapping: dict[FieldKey, set[FieldKey]]) -&gt; Self\n</code></pre> <p>Create a field mapping that maps downstream field keys into specific upstream field keys.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[FieldKey, set[FieldKey]]</code>)           \u2013            <p>Mapping of downstream field keys to sets of upstream field keys</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef specific(cls, mapping: dict[FieldKey, set[FieldKey]]) -&gt; Self:\n    \"\"\"Create a field mapping that maps downstream field keys into specific upstream field keys.\n\n    Args:\n        mapping: Mapping of downstream field keys to sets of upstream field keys\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=SpecificFieldsMapping(mapping=mapping))\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.all","title":"all  <code>classmethod</code>","text":"<pre><code>all() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on all upstream fields.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Use in field specifications\n&gt;&gt;&gt; FieldSpec(\n...     key=\"combined\",\n...     fields_mapping=FieldsMapping.all()\n... )\n</code></pre> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef all(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on all upstream fields.\n\n    Returns:\n        Configured FieldsMapping instance.\n\n    Examples:\n        &gt;&gt;&gt; # Use in field specifications\n        &gt;&gt;&gt; FieldSpec(\n        ...     key=\"combined\",\n        ...     fields_mapping=FieldsMapping.all()\n        ... )\n    \"\"\"\n    return cls(mapping=AllFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.none","title":"none  <code>classmethod</code>","text":"<pre><code>none() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on no upstream fields.</p> <p>This is typically useful when explicitly defining FieldSpec.deps instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on no upstream fields.\n\n    This is typically useful when explicitly defining [FieldSpec.deps][metaxy.models.field.FieldSpec] instead.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=NoneFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMappingType","title":"FieldsMappingType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of fields mapping between a field key and the upstream field keys.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping","title":"DefaultFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Default automatic field mapping configuration.</p> <p>When used, automatically maps fields to matching upstream fields based on field keys.</p> <p>Attributes:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>)           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey]</code>)           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"default\",\n      \"default\": \"default\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"match_suffix\": {\n      \"default\": false,\n      \"title\": \"Match Suffix\",\n      \"type\": \"boolean\"\n    },\n    \"exclude_fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Exclude Fields\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"DefaultFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[DEFAULT]</code>)             </li> <li> <code>match_suffix</code>                 (<code>bool</code>)             </li> <li> <code>exclude_fields</code>                 (<code>list[FieldKey]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","title":"resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    res = set()\n\n    for upstream_field_key in context.upstream_feature_fields:\n        # Skip excluded fields\n        if upstream_field_key in self.exclude_fields:\n            continue\n\n        # Check for exact match\n        if upstream_field_key == context.field_key:\n            res.add(upstream_field_key)\n        # Check for suffix match if enabled\n        elif self.match_suffix and self._is_suffix_match(\n            context.field_key, upstream_field_key\n        ):\n            res.add(upstream_field_key)\n\n    # If no fields matched, return ALL fields from this upstream feature\n    # (excluding any explicitly excluded fields)\n    if not res:\n        for upstream_field_key in context.upstream_feature_fields:\n            if upstream_field_key not in self.exclude_fields:\n                res.add(upstream_field_key)\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping","title":"SpecificFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on specific upstream fields.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"specific\",\n      \"default\": \"specific\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"mapping\": {\n      \"additionalProperties\": {\n        \"items\": {\n          \"$ref\": \"#/$defs/FieldKey\"\n        },\n        \"type\": \"array\",\n        \"uniqueItems\": true\n      },\n      \"propertyNames\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Mapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"SpecificFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[SPECIFIC]</code>)             </li> <li> <code>mapping</code>                 (<code>dict[FieldKey, set[FieldKey]]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","title":"resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    desired_upstream_fields = self.mapping.get(context.field_key, set())\n    return desired_upstream_fields &amp; context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping","title":"AllFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on all upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"all\",\n      \"default\": \"all\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"AllFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[ALL]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","title":"resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping","title":"NoneFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that never matches any upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that never matches any upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"none\",\n      \"default\": \"none\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"NoneFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[NONE]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","title":"resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return set()\n</code></pre>"},{"location":"reference/api/definitions/filters/","title":"String Filters","text":""},{"location":"reference/api/definitions/filters/#metaxy.models.filter_expression.parse_filter_string","title":"parse_filter_string","text":"<pre><code>parse_filter_string(filter_string: str) -&gt; Expr\n</code></pre> <p>Parse a SQL WHERE-like string into a Narwhals expression.</p> <p>The parser understands SQL <code>WHERE</code> clauses composed of comparison operators, logical operators, parentheses, dotted identifiers, and literal values (strings, numbers, booleans, <code>NULL</code>).</p> <p>This functionality is implemented with SQLGlot.</p> Example <pre><code>parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n# Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n</code></pre> Source code in <code>src/metaxy/models/filter_expression.py</code> <pre><code>def parse_filter_string(filter_string: str) -&gt; nw.Expr:\n    \"\"\"Parse a SQL WHERE-like string into a Narwhals expression.\n\n    The parser understands SQL `WHERE` clauses composed of comparison operators, logical operators, parentheses,\n    dotted identifiers, and literal values (strings, numbers, booleans, ``NULL``).\n\n    This functionality is implemented with [SQLGlot](https://sqlglot.com/).\n\n    Example:\n        ```python\n        parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n        # Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n        ```\n    \"\"\"\n    return NarwhalsFilter.model_validate(filter_string).to_expr()\n</code></pre>"},{"location":"reference/api/definitions/graph/","title":"Feature Graph","text":"<p><code>FeatureGraph</code> is a global \"God\" object that holds all the features loaded by Metaxy via the feature discovery mechanism.</p> <p>Users may interact with <code>FeatureGraph</code> when writing custom migrations, otherwise they are not exposed to it.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph","title":"FeatureGraph","text":"<pre><code>FeatureGraph()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def __init__(self):\n    self.features_by_key: dict[FeatureKey, type[BaseFeature]] = {}\n    self.feature_specs_by_key: dict[FeatureKey, FeatureSpec] = {}\n    # Standalone specs registered without Feature classes (for migrations)\n    self.standalone_specs_by_key: dict[FeatureKey, FeatureSpec] = {}\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.snapshot_version","title":"snapshot_version  <code>property</code>","text":"<pre><code>snapshot_version: str\n</code></pre> <p>Generate a snapshot version representing the current topology + versions of the feature graph</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-functions","title":"Functions","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature","title":"add_feature","text":"<pre><code>add_feature(feature: type[BaseFeature]) -&gt; None\n</code></pre> <p>Add a feature to the graph.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to register</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a feature with the same key is already registered        or if duplicate column names would result from renaming operations</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature(self, feature: type[\"BaseFeature\"]) -&gt; None:\n    \"\"\"Add a feature to the graph.\n\n    Args:\n        feature: Feature class to register\n\n    Raises:\n        ValueError: If a feature with the same key is already registered\n                   or if duplicate column names would result from renaming operations\n    \"\"\"\n    if feature.spec().key in self.features_by_key:\n        existing = self.features_by_key[feature.spec().key]\n        raise ValueError(\n            f\"Feature with key {feature.spec().key.to_string()} already registered. \"\n            f\"Existing: {existing.__name__}, New: {feature.__name__}. \"\n            f\"Each feature key must be unique within a graph.\"\n        )\n\n    # Validate that there are no duplicate column names across dependencies after renaming\n    if feature.spec().deps:\n        self._validate_no_duplicate_columns(feature.spec())\n\n    self.features_by_key[feature.spec().key] = feature\n    self.feature_specs_by_key[feature.spec().key] = feature.spec()\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.remove_feature","title":"remove_feature","text":"<pre><code>remove_feature(key: FeatureKey) -&gt; None\n</code></pre> <p>Remove a feature from the graph.</p> <p>Removes Feature class or standalone spec (whichever exists).</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to remove</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def remove_feature(self, key: FeatureKey) -&gt; None:\n    \"\"\"Remove a feature from the graph.\n\n    Removes Feature class or standalone spec (whichever exists).\n\n    Args:\n        key: Feature key to remove\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    # Check both Feature classes and standalone specs\n    combined = {**self.feature_specs_by_key, **self.standalone_specs_by_key}\n\n    if key not in combined:\n        raise KeyError(\n            f\"No feature with key {key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in combined]}\"\n        )\n\n    # Remove from all relevant dicts\n    if key in self.features_by_key:\n        del self.features_by_key[key]\n    if key in self.standalone_specs_by_key:\n        del self.standalone_specs_by_key[key]\n    if key in self.feature_specs_by_key:\n        del self.feature_specs_by_key[key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_by_key","title":"get_feature_by_key","text":"<pre><code>get_feature_by_key(key: FeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>graph = FeatureGraph.get_active()\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = graph.get_feature_by_key(parent_key)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(self, key: FeatureKey) -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        graph = FeatureGraph.get_active()\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = graph.get_feature_by_key(parent_key)\n        ```\n    \"\"\"\n    if key not in self.features_by_key:\n        raise KeyError(\n            f\"No feature with key {key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.features_by_key.keys()]}\"\n        )\n    return self.features_by_key[key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.list_features","title":"list_features","text":"<pre><code>list_features(projects: list[str] | str | None = None, *, only_current_project: bool = True) -&gt; list[FeatureKey]\n</code></pre> <p>List all feature keys in the graph, optionally filtered by project(s).</p> <p>By default, filters features by the current project (first part of feature key). This prevents operations from affecting features in other projects.</p> <p>Parameters:</p> <ul> <li> <code>projects</code>               (<code>list[str] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name(s) to filter by. Can be: - None: Use current project from MetaxyConfig (if only_current_project=True) - str: Single project name - list[str]: Multiple project names</p> </li> <li> <code>only_current_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filter by current/specified project(s). If False, return all features regardless of project.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys</p> </li> </ul> Example <pre><code># Get all features for current project\ngraph = FeatureGraph.get_active()\nfeatures = graph.list_features()\n\n# Get features for specific project\nfeatures = graph.list_features(projects=\"myproject\")\n\n# Get features for multiple projects\nfeatures = graph.list_features(projects=[\"project1\", \"project2\"])\n\n# Get all features regardless of project\nall_features = graph.list_features(only_current_project=False)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def list_features(\n    self,\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]:\n    \"\"\"List all feature keys in the graph, optionally filtered by project(s).\n\n    By default, filters features by the current project (first part of feature key).\n    This prevents operations from affecting features in other projects.\n\n    Args:\n        projects: Project name(s) to filter by. Can be:\n            - None: Use current project from MetaxyConfig (if only_current_project=True)\n            - str: Single project name\n            - list[str]: Multiple project names\n        only_current_project: If True, filter by current/specified project(s).\n            If False, return all features regardless of project.\n\n    Returns:\n        List of feature keys\n\n    Example:\n        ```py\n        # Get all features for current project\n        graph = FeatureGraph.get_active()\n        features = graph.list_features()\n\n        # Get features for specific project\n        features = graph.list_features(projects=\"myproject\")\n\n        # Get features for multiple projects\n        features = graph.list_features(projects=[\"project1\", \"project2\"])\n\n        # Get all features regardless of project\n        all_features = graph.list_features(only_current_project=False)\n        ```\n    \"\"\"\n    if not only_current_project:\n        # Return all features\n        return list(self.features_by_key.keys())\n\n    # Normalize projects to list\n    project_list: list[str]\n    if projects is None:\n        # Try to get from config context\n        try:\n            from metaxy.config import MetaxyConfig\n\n            config = MetaxyConfig.get()\n            project_list = [config.project]\n        except RuntimeError:\n            # Config not initialized - in tests or non-CLI usage\n            # Return all features (can't determine project)\n            return list(self.features_by_key.keys())\n    elif isinstance(projects, str):\n        project_list = [projects]\n    else:\n        project_list = projects\n\n    # Filter by project(s) using Feature.project attribute\n    return [\n        key\n        for key in self.features_by_key.keys()\n        if self.features_by_key[key].project in project_list\n    ]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version_by_field","title":"get_feature_version_by_field","text":"<pre><code>get_feature_version_by_field(key: FeatureKey) -&gt; dict[str, str]\n</code></pre> <p>Computes the field provenance map for a feature.</p> <p>Hash together field provenance entries with the feature code version.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>dict[str, str]: The provenance hash for each field in the feature plan. Keys are field names as strings.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version_by_field(self, key: FeatureKey) -&gt; dict[str, str]:\n    \"\"\"Computes the field provenance map for a feature.\n\n    Hash together field provenance entries with the feature code version.\n\n    Returns:\n        dict[str, str]: The provenance hash for each field in the feature plan.\n            Keys are field names as strings.\n    \"\"\"\n    res = {}\n\n    plan = self.get_feature_plan(key)\n\n    for k, v in plan.feature.fields_by_key.items():\n        res[k.to_string()] = self.get_field_version(\n            FQFieldKey(field=k, feature=key)\n        )\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version","title":"get_feature_version","text":"<pre><code>get_feature_version(key: FeatureKey) -&gt; str\n</code></pre> <p>Computes the feature version as a single string</p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version(self, key: FeatureKey) -&gt; str:\n    \"\"\"Computes the feature version as a single string\"\"\"\n    hasher = hashlib.sha256()\n    provenance_by_field = self.get_feature_version_by_field(key)\n    for field_key in sorted(provenance_by_field):\n        hasher.update(field_key.encode())\n        hasher.update(provenance_by_field[field_key].encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_downstream_features","title":"get_downstream_features","text":"<pre><code>get_downstream_features(sources: list[FeatureKey]) -&gt; list[FeatureKey]\n</code></pre> <p>Get all features downstream of sources, topologically sorted.</p> <p>Performs a depth-first traversal of the dependency graph to find all features that transitively depend on any of the source features.</p> <p>Parameters:</p> <ul> <li> <code>sources</code>               (<code>list[FeatureKey]</code>)           \u2013            <p>List of source feature keys</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of downstream feature keys in topological order (dependencies first).</p> </li> <li> <code>list[FeatureKey]</code>           \u2013            <p>Does not include the source features themselves.</p> </li> </ul> Example <pre><code># DAG: A -&gt; B -&gt; D\n#      A -&gt; C -&gt; D\ngraph.get_downstream_features([FeatureKey([\"A\"])])\n# [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_downstream_features(self, sources: list[FeatureKey]) -&gt; list[FeatureKey]:\n    \"\"\"Get all features downstream of sources, topologically sorted.\n\n    Performs a depth-first traversal of the dependency graph to find all\n    features that transitively depend on any of the source features.\n\n    Args:\n        sources: List of source feature keys\n\n    Returns:\n        List of downstream feature keys in topological order (dependencies first).\n        Does not include the source features themselves.\n\n    Example:\n        ```py\n        # DAG: A -&gt; B -&gt; D\n        #      A -&gt; C -&gt; D\n        graph.get_downstream_features([FeatureKey([\"A\"])])\n        # [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n        ```\n    \"\"\"\n    source_set = set(sources)\n    visited = set()\n    post_order = []  # Reverse topological order\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS traversal.\"\"\"\n        if key in visited:\n            return\n        visited.add(key)\n\n        # Find all features that depend on this one\n        for feature_key, feature_spec in self.feature_specs_by_key.items():\n            if feature_spec.deps:\n                for dep in feature_spec.deps:\n                    if dep.feature == key:\n                        # This feature depends on 'key', so visit it\n                        visit(feature_key)\n\n        post_order.append(key)\n\n    # Visit all sources\n    for source in sources:\n        visit(source)\n\n    # Remove sources from result, reverse to get topological order\n    result = [k for k in reversed(post_order) if k not in source_set]\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.topological_sort_features","title":"topological_sort_features","text":"<pre><code>topological_sort_features(feature_keys: list[FeatureKey] | None = None) -&gt; list[FeatureKey]\n</code></pre> <p>Sort feature keys in topological order (dependencies first).</p> <p>Uses stable alphabetical ordering when multiple nodes are at the same level. This ensures deterministic output for diff comparisons and migrations.</p> <p>Implemented using depth-first search with post-order traversal.</p> <p>Parameters:</p> <ul> <li> <code>feature_keys</code>               (<code>list[FeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of feature keys to sort. If None, sorts all features (both Feature classes and standalone specs) in the graph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys sorted so dependencies appear before dependents</p> </li> </ul> Example <pre><code>graph = FeatureGraph.get_active()\n# Sort specific features\nsorted_keys = graph.topological_sort_features([\n    FeatureKey([\"video\", \"raw\"]),\n    FeatureKey([\"video\", \"scene\"]),\n])\n\n# Sort all features in the graph (including standalone specs)\nall_sorted = graph.topological_sort_features()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def topological_sort_features(\n    self,\n    feature_keys: list[FeatureKey] | None = None,\n) -&gt; list[FeatureKey]:\n    \"\"\"Sort feature keys in topological order (dependencies first).\n\n    Uses stable alphabetical ordering when multiple nodes are at the same level.\n    This ensures deterministic output for diff comparisons and migrations.\n\n    Implemented using depth-first search with post-order traversal.\n\n    Args:\n        feature_keys: List of feature keys to sort. If None, sorts all features\n            (both Feature classes and standalone specs) in the graph.\n\n    Returns:\n        List of feature keys sorted so dependencies appear before dependents\n\n    Example:\n        ```py\n        graph = FeatureGraph.get_active()\n        # Sort specific features\n        sorted_keys = graph.topological_sort_features([\n            FeatureKey([\"video\", \"raw\"]),\n            FeatureKey([\"video\", \"scene\"]),\n        ])\n\n        # Sort all features in the graph (including standalone specs)\n        all_sorted = graph.topological_sort_features()\n        ```\n    \"\"\"\n    # Determine which features to sort\n    if feature_keys is None:\n        # Include both Feature classes and standalone specs\n        keys_to_sort = set(self.feature_specs_by_key.keys())\n    else:\n        keys_to_sort = set(feature_keys)\n\n    visited = set()\n    result = []  # Topological order (dependencies first)\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS visit with post-order traversal.\"\"\"\n        if key in visited or key not in keys_to_sort:\n            return\n        visited.add(key)\n\n        # Get dependencies from feature spec\n        spec = self.feature_specs_by_key.get(key)\n        if spec and spec.deps:\n            # Sort dependencies alphabetically for deterministic ordering\n            sorted_deps = sorted(\n                (dep.feature for dep in spec.deps),\n                key=lambda k: k.to_string().lower(),\n            )\n            for dep_key in sorted_deps:\n                if dep_key in keys_to_sort:\n                    visit(dep_key)\n\n        # Add to result after visiting dependencies (post-order)\n        result.append(key)\n\n    # Visit all keys in sorted order for deterministic traversal\n    for key in sorted(keys_to_sort, key=lambda k: k.to_string().lower()):\n        visit(key)\n\n    # Post-order DFS gives topological order (dependencies before dependents)\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.to_snapshot","title":"to_snapshot","text":"<pre><code>to_snapshot() -&gt; dict[str, SerializedFeature]\n</code></pre> <p>Serialize graph to snapshot format.</p> <p>Returns a dict mapping feature_key (string) to feature data dict, including the import path of the Feature class for reconstruction.</p> <p>Returns: dictionary mapping feature_key (string) to feature data dict</p> Example <pre><code>snapshot = graph.to_snapshot()\nsnapshot[\"video_processing\"][\"metaxy_feature_version\"]\n# 'abc12345'\nsnapshot[\"video_processing\"][\"metaxy_feature_spec_version\"]\n# 'def67890'\nsnapshot[\"video_processing\"][\"metaxy_full_definition_version\"]\n# 'xyz98765'\nsnapshot[\"video_processing\"][\"feature_class_path\"]\n# 'myapp.features.video.VideoProcessing'\nsnapshot[\"video_processing\"][\"project\"]\n# 'myapp'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def to_snapshot(self) -&gt; dict[str, SerializedFeature]:\n    \"\"\"Serialize graph to snapshot format.\n\n    Returns a dict mapping feature_key (string) to feature data dict,\n    including the import path of the Feature class for reconstruction.\n\n    Returns: dictionary mapping feature_key (string) to feature data dict\n\n    Example:\n        ```py\n        snapshot = graph.to_snapshot()\n        snapshot[\"video_processing\"][\"metaxy_feature_version\"]\n        # 'abc12345'\n        snapshot[\"video_processing\"][\"metaxy_feature_spec_version\"]\n        # 'def67890'\n        snapshot[\"video_processing\"][\"metaxy_full_definition_version\"]\n        # 'xyz98765'\n        snapshot[\"video_processing\"][\"feature_class_path\"]\n        # 'myapp.features.video.VideoProcessing'\n        snapshot[\"video_processing\"][\"project\"]\n        # 'myapp'\n        ```\n    \"\"\"\n    snapshot: dict[str, SerializedFeature] = {}\n\n    for feature_key, feature_cls in self.features_by_key.items():\n        feature_key_str = feature_key.to_string()\n        feature_spec_dict = feature_cls.spec().model_dump(mode=\"json\")  # type: ignore[attr-defined]\n        feature_schema_dict = feature_cls.model_json_schema()  # type: ignore[attr-defined]\n        feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n        feature_spec_version = feature_cls.spec().feature_spec_version  # type: ignore[attr-defined]\n        full_definition_version = feature_cls.full_definition_version()  # type: ignore[attr-defined]\n        project = feature_cls.project  # type: ignore[attr-defined]\n\n        # Get class import path (module.ClassName)\n        class_path = f\"{feature_cls.__module__}.{feature_cls.__name__}\"\n\n        snapshot[feature_key_str] = {  # pyright: ignore\n            \"feature_spec\": feature_spec_dict,\n            \"feature_schema\": feature_schema_dict,\n            FEATURE_VERSION_COL: feature_version,\n            FEATURE_SPEC_VERSION_COL: feature_spec_version,\n            FEATURE_TRACKING_VERSION_COL: full_definition_version,\n            \"feature_class_path\": class_path,\n            \"project\": project,\n        }\n\n    return snapshot\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.from_snapshot","title":"from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(snapshot_data: dict[str, dict[str, Any]], *, class_path_overrides: dict[str, str] | None = None, force_reload: bool = False) -&gt; FeatureGraph\n</code></pre> <p>Reconstruct graph from snapshot by importing Feature classes.</p> <p>Strictly requires Feature classes to exist at their recorded import paths. This ensures custom methods (like load_input) are available.</p> <p>If a feature has been moved/renamed, use class_path_overrides to specify the new location.</p> <p>Parameters:</p> <ul> <li> <code>snapshot_data</code>               (<code>dict[str, dict[str, Any]]</code>)           \u2013            <p>Dict of feature_key -&gt; dict containing feature_spec (dict), feature_class_path (str), and other fields as returned by to_snapshot() or loaded from DB</p> </li> <li> <code>class_path_overrides</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dict mapping feature_key to new class path                  for features that have been moved/renamed</p> </li> <li> <code>force_reload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, reload modules from disk to get current code state.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>New FeatureGraph with historical features</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If feature class cannot be imported at recorded path</p> </li> </ul> Example <pre><code># Load snapshot from metadata store\nhistorical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n# With override for moved feature\nhistorical_graph = FeatureGraph.from_snapshot(\n    snapshot_data,\n    class_path_overrides={\n        \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n    }\n)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef from_snapshot(\n    cls,\n    snapshot_data: dict[str, dict[str, Any]],\n    *,\n    class_path_overrides: dict[str, str] | None = None,\n    force_reload: bool = False,\n) -&gt; \"FeatureGraph\":\n    \"\"\"Reconstruct graph from snapshot by importing Feature classes.\n\n    Strictly requires Feature classes to exist at their recorded import paths.\n    This ensures custom methods (like load_input) are available.\n\n    If a feature has been moved/renamed, use class_path_overrides to specify\n    the new location.\n\n    Args:\n        snapshot_data: Dict of feature_key -&gt; dict containing\n            feature_spec (dict), feature_class_path (str), and other fields\n            as returned by to_snapshot() or loaded from DB\n        class_path_overrides: Optional dict mapping feature_key to new class path\n                             for features that have been moved/renamed\n        force_reload: If True, reload modules from disk to get current code state.\n\n    Returns:\n        New FeatureGraph with historical features\n\n    Raises:\n        ImportError: If feature class cannot be imported at recorded path\n\n    Example:\n        ```py\n        # Load snapshot from metadata store\n        historical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n        # With override for moved feature\n        historical_graph = FeatureGraph.from_snapshot(\n            snapshot_data,\n            class_path_overrides={\n                \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n            }\n        )\n        ```\n    \"\"\"\n    import importlib\n    import sys\n\n    graph = cls()\n    class_path_overrides = class_path_overrides or {}\n\n    # If force_reload, collect all module paths first to remove ALL features\n    # from those modules before reloading (modules can have multiple features)\n    modules_to_reload = set()\n    if force_reload:\n        for feature_key_str, feature_data in snapshot_data.items():\n            class_path = class_path_overrides.get(\n                feature_key_str\n            ) or feature_data.get(\"feature_class_path\")\n            if class_path:\n                module_path, _ = class_path.rsplit(\".\", 1)\n                if module_path in sys.modules:\n                    modules_to_reload.add(module_path)\n\n    # Use context manager to temporarily set the new graph as active\n    # This ensures imported Feature classes register to the new graph, not the current one\n    with graph.use():\n        for feature_key_str, feature_data in snapshot_data.items():\n            # Parse FeatureSpec for validation\n            feature_spec_dict = feature_data[\"feature_spec\"]\n            FeatureSpec.model_validate(feature_spec_dict)\n\n            # Get class path (check overrides first)\n            if feature_key_str in class_path_overrides:\n                class_path = class_path_overrides[feature_key_str]\n            else:\n                class_path = feature_data.get(\"feature_class_path\")\n                if not class_path:\n                    raise ValueError(\n                        f\"Feature '{feature_key_str}' has no feature_class_path in snapshot. \"\n                        f\"Cannot reconstruct historical graph.\"\n                    )\n\n            # Import the class\n            try:\n                module_path, class_name = class_path.rsplit(\".\", 1)\n\n                # Force reload module from disk if requested\n                # This is critical for migration detection - when code changes,\n                # we need fresh imports to detect the changes\n                if force_reload and module_path in modules_to_reload:\n                    # Before first reload of this module, remove ALL features from this module\n                    # (a module can define multiple features)\n                    if module_path in modules_to_reload:\n                        # Find all features from this module in snapshot and remove them\n                        for fk_str, fd in snapshot_data.items():\n                            fcp = class_path_overrides.get(fk_str) or fd.get(\n                                \"feature_class_path\"\n                            )\n                            if fcp and fcp.rsplit(\".\", 1)[0] == module_path:\n                                fspec_dict = fd[\"feature_spec\"]\n                                fspec = FeatureSpec.model_validate(fspec_dict)\n                                if fspec.key in graph.features_by_key:\n                                    graph.remove_feature(fspec.key)\n\n                        # Mark module as processed so we don't remove features again\n                        modules_to_reload.discard(module_path)\n\n                    module = importlib.reload(sys.modules[module_path])\n                else:\n                    module = __import__(module_path, fromlist=[class_name])\n\n                feature_cls = getattr(module, class_name)\n            except (ImportError, AttributeError):\n                # Feature class not importable - add as standalone spec instead\n                # This allows migrations to work even when old Feature classes are deleted/moved\n                import logging\n\n                logger = logging.getLogger(__name__)\n                logger.exception(\n                    f\"Cannot import Feature class '{class_path}' for '{feature_key_str}'. \"\n                    f\"Adding only the FeatureSpec. \"\n                )\n\n                feature_spec = FeatureSpec.model_validate(feature_spec_dict)\n                # Add the spec as a standalone spec\n                graph.add_feature_spec(feature_spec)\n                continue\n\n            # Validate the imported class matches the stored spec\n            if not hasattr(feature_cls, \"spec\"):\n                raise TypeError(\n                    f\"Imported class '{class_path}' is not a valid Feature class \"\n                    f\"(missing 'spec' attribute)\"\n                )\n\n            # Register the imported feature to this graph if not already present\n            # If the module was imported for the first time, the metaclass already registered it\n            # If the module was previously imported, we need to manually register it\n            if feature_cls.spec().key not in graph.features_by_key:\n                graph.add_feature(feature_cls)\n\n    return graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_active","title":"get_active  <code>classmethod</code>","text":"<pre><code>get_active() -&gt; FeatureGraph\n</code></pre> <p>Get the currently active graph.</p> <p>Returns the graph from the context variable if set, otherwise returns the default global graph.</p> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>Active FeatureGraph instance</p> </li> </ul> Example <pre><code># Normal usage - returns default graph\nreg = FeatureGraph.get_active()\n\n# With custom graph in context\nwith my_graph.use():\n    reg = FeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef get_active(cls) -&gt; \"FeatureGraph\":\n    \"\"\"Get the currently active graph.\n\n    Returns the graph from the context variable if set, otherwise returns\n    the default global graph.\n\n    Returns:\n        Active FeatureGraph instance\n\n    Example:\n        ```py\n        # Normal usage - returns default graph\n        reg = FeatureGraph.get_active()\n\n        # With custom graph in context\n        with my_graph.use():\n            reg = FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    return _active_graph.get() or graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.set_active","title":"set_active  <code>classmethod</code>","text":"<pre><code>set_active(reg: FeatureGraph) -&gt; None\n</code></pre> <p>Set the active graph for the current context.</p> <p>This sets the context variable that will be returned by get_active(). Typically used in application setup code or test fixtures.</p> <p>Parameters:</p> <ul> <li> <code>reg</code>               (<code>FeatureGraph</code>)           \u2013            <p>FeatureGraph to activate</p> </li> </ul> Example <pre><code># In application setup\nmy_graph = FeatureGraph()\nFeatureGraph.set_active(my_graph)\n\n# Now all operations use my_graph\nFeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef set_active(cls, reg: \"FeatureGraph\") -&gt; None:\n    \"\"\"Set the active graph for the current context.\n\n    This sets the context variable that will be returned by get_active().\n    Typically used in application setup code or test fixtures.\n\n    Args:\n        reg: FeatureGraph to activate\n\n    Example:\n        ```py\n        # In application setup\n        my_graph = FeatureGraph()\n        FeatureGraph.set_active(my_graph)\n\n        # Now all operations use my_graph\n        FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    _active_graph.set(reg)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.use","title":"use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Context manager to temporarily use this graph as active.</p> <p>This is the recommended way to use custom registries, especially in tests. The graph is automatically restored when the context exits.</p> <p>Yields:</p> <ul> <li> <code>FeatureGraph</code> (              <code>Self</code> )          \u2013            <p>This graph instance</p> </li> </ul> Example <pre><code>test_graph = FeatureGraph()\n\nwith test_graph.use():\n    # All operations use test_graph\n    class TestFeature(Feature, spec=...):\n        pass\n\n# Outside context, back to previous graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Context manager to temporarily use this graph as active.\n\n    This is the recommended way to use custom registries, especially in tests.\n    The graph is automatically restored when the context exits.\n\n    Yields:\n        FeatureGraph: This graph instance\n\n    Example:\n        ```py\n        test_graph = FeatureGraph()\n\n        with test_graph.use():\n            # All operations use test_graph\n            class TestFeature(Feature, spec=...):\n                pass\n\n        # Outside context, back to previous graph\n        ```\n    \"\"\"\n    token = _active_graph.set(self)\n    try:\n        yield self\n    finally:\n        _active_graph.reset(token)\n</code></pre>"},{"location":"reference/api/definitions/relationship/","title":"Lineage Relationships","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship","title":"LineageRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for lineage relationship configurations with convenient constructors.</p> <p>This provides a cleaner API for creating lineage relationships while maintaining type safety through discriminated unions.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n  \"properties\": {\n    \"relationship\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"1:1\": \"#/$defs/IdentityRelationship\",\n          \"1:N\": \"#/$defs/ExpansionRelationship\",\n          \"N:1\": \"#/$defs/AggregationRelationship\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/IdentityRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/AggregationRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/ExpansionRelationship\"\n        }\n      ],\n      \"title\": \"Relationship\"\n    }\n  },\n  \"required\": [\n    \"relationship\"\n  ],\n  \"title\": \"LineageRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>relationship</code>                 (<code>LineageRelationshipUnion</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.identity","title":"identity  <code>classmethod</code>","text":"<pre><code>identity() -&gt; Self\n</code></pre> <p>Create an identity (1:1) relationship.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:1 relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"feature\",\n...     lineage=LineageRelationship.identity()\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; Self:\n    \"\"\"Create an identity (1:1) relationship.\n\n    Returns:\n        Configured LineageRelationship for 1:1 relationship.\n\n    Examples:\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"feature\",\n        ...     lineage=LineageRelationship.identity()\n        ... )\n    \"\"\"\n    return cls(relationship=IdentityRelationship())\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.aggregation","title":"aggregation  <code>classmethod</code>","text":"<pre><code>aggregation(on: Sequence[str] | None = None) -&gt; Self\n</code></pre> <p>Create an aggregation (N:1) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Columns to group by for aggregation. If None, uses all target ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for N:1 relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Aggregate on specific columns\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"hourly_stats\",\n...     id_columns=[\"sensor_id\", \"hour\"],\n...     lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Aggregate on all ID columns (default)\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"user_summary\",\n...     id_columns=[\"user_id\"],\n...     lineage=LineageRelationship.aggregation()\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef aggregation(cls, on: Sequence[str] | None = None) -&gt; Self:\n    \"\"\"Create an aggregation (N:1) relationship.\n\n    Args:\n        on: Columns to group by for aggregation. If None, uses all target ID columns.\n\n    Returns:\n        Configured LineageRelationship for N:1 relationship.\n\n    Examples:\n        &gt;&gt;&gt; # Aggregate on specific columns\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"hourly_stats\",\n        ...     id_columns=[\"sensor_id\", \"hour\"],\n        ...     lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n        ... )\n\n        &gt;&gt;&gt; # Aggregate on all ID columns (default)\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"user_summary\",\n        ...     id_columns=[\"user_id\"],\n        ...     lineage=LineageRelationship.aggregation()\n        ... )\n    \"\"\"\n    return cls(relationship=AggregationRelationship(on=on))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.expansion","title":"expansion  <code>classmethod</code>","text":"<pre><code>expansion(on: Sequence[str], id_generation_pattern: str | None = None) -&gt; Self\n</code></pre> <p>Create an expansion (1:N) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. Required - must explicitly specify which columns link parent to child.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Pattern for generating child IDs. Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:N relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Sequential ID generation with explicit parent ID\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"video_frames\",\n...     id_columns=[\"video_id\", \"frame_id\"],\n...     lineage=LineageRelationship.expansion(\n...         on=[\"video_id\"],\n...         id_generation_pattern=\"sequential\"\n...     )\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom ID generation in load_input()\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"text_chunks\",\n...     id_columns=[\"doc_id\", \"chunk_id\"],\n...     lineage=LineageRelationship.expansion(on=[\"doc_id\"])\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef expansion(\n    cls,\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self:\n    \"\"\"Create an expansion (1:N) relationship.\n\n    Args:\n        on: Parent ID columns that identify the parent record. Child records with\n            the same parent IDs will share the same upstream provenance.\n            Required - must explicitly specify which columns link parent to child.\n        id_generation_pattern: Pattern for generating child IDs.\n            Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().\n\n    Returns:\n        Configured LineageRelationship for 1:N relationship.\n\n    Examples:\n        &gt;&gt;&gt; # Sequential ID generation with explicit parent ID\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"video_frames\",\n        ...     id_columns=[\"video_id\", \"frame_id\"],\n        ...     lineage=LineageRelationship.expansion(\n        ...         on=[\"video_id\"],\n        ...         id_generation_pattern=\"sequential\"\n        ...     )\n        ... )\n\n        &gt;&gt;&gt; # Custom ID generation in load_input()\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"text_chunks\",\n        ...     id_columns=[\"doc_id\", \"chunk_id\"],\n        ...     lineage=LineageRelationship.expansion(on=[\"doc_id\"])\n        ... )\n    \"\"\"\n    return cls(\n        relationship=ExpansionRelationship(\n            on=on, id_generation_pattern=id_generation_pattern\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.get_aggregation_columns","title":"get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>Get columns to aggregate on for this relationship.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>Columns to group by for aggregation, or None if no aggregation needed.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self, target_id_columns: Sequence[str]\n) -&gt; Sequence[str] | None:\n    \"\"\"Get columns to aggregate on for this relationship.\n\n    Args:\n        target_id_columns: The target feature's ID columns.\n\n    Returns:\n        Columns to group by for aggregation, or None if no aggregation needed.\n    \"\"\"\n    return self.relationship.get_aggregation_columns(target_id_columns)\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationshipType","title":"LineageRelationshipType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of lineage relationship between features.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship","title":"IdentityRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-one relationship where each child row maps to exactly one parent row.</p> <p>This is the default relationship type. Parent and child features share the same ID columns and have the same cardinality. No aggregation is performed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default 1:1 relationship\n&gt;&gt;&gt; IdentityRelationship()\n</code></pre> <pre><code>&gt;&gt;&gt; # Or use the classmethod\n&gt;&gt;&gt; LineageRelationship.identity()\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:1\",\n      \"default\": \"1:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"IdentityRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[IDENTITY]</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","title":"get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>No aggregation needed for identity relationships.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"No aggregation needed for identity relationships.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship","title":"ExpansionRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-many relationship where one parent row expands to multiple child rows.</p> <p>Child features have more granular ID columns than the parent. Each parent row generates multiple child rows with additional ID columns.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. If not specified, will be inferred from the available columns.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>)           \u2013            <p>Optional pattern for generating child IDs. Can be \"sequential\", \"hash\", or a custom pattern. If not specified, the feature's load_input() method is responsible for ID generation.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Video frames from video\n&gt;&gt;&gt; ExpansionRelationship(\n...     on=[\"video_id\"],  # Parent ID\n...     id_generation_pattern=\"sequential\"\n... )\n&gt;&gt;&gt; # Parent has: video_id\n&gt;&gt;&gt; # Child has: video_id, frame_id (generated)\n</code></pre> <pre><code>&gt;&gt;&gt; # Text chunks from document\n&gt;&gt;&gt; ExpansionRelationship(on=[\"doc_id\"])\n&gt;&gt;&gt; # Parent has: doc_id\n&gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:N\",\n      \"default\": \"1:N\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"On\",\n      \"type\": \"array\"\n    },\n    \"id_generation_pattern\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n      \"title\": \"Id Generation Pattern\"\n    }\n  },\n  \"required\": [\n    \"on\"\n  ],\n  \"title\": \"ExpansionRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[EXPANSION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str]</code>)             </li> <li> <code>id_generation_pattern</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.on","title":"on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str]\n</code></pre> <p>Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.id_generation_pattern","title":"id_generation_pattern  <code>pydantic-field</code>","text":"<pre><code>id_generation_pattern: str | None = None\n</code></pre> <p>Pattern for generating child IDs. If None, handled by load_input().</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","title":"get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>Get aggregation columns for the joiner phase.</p> <p>For expansion relationships, returns None because aggregation happens during diff resolution, not during joining. The joiner should pass through all parent records without aggregation.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target (child) feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>None - no aggregation during join phase for expansion relationships.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"Get aggregation columns for the joiner phase.\n\n    For expansion relationships, returns None because aggregation\n    happens during diff resolution, not during joining. The joiner\n    should pass through all parent records without aggregation.\n\n    Args:\n        target_id_columns: The target (child) feature's ID columns.\n\n    Returns:\n        None - no aggregation during join phase for expansion relationships.\n    \"\"\"\n    # Expansion relationships don't aggregate during join phase\n    # Aggregation happens later during diff resolution\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship","title":"AggregationRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>Many-to-one relationship where multiple parent rows aggregate to one child row.</p> <p>Parent features have more granular ID columns than the child. The child aggregates multiple parent rows by grouping on a subset of the parent's ID columns.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>)           \u2013            <p>Columns to group by for aggregation. These should be a subset of the target feature's ID columns. If not specified, uses all target ID columns.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Aggregate sensor readings by hour\n&gt;&gt;&gt; AggregationRelationship(on=[\"sensor_id\", \"hour\"])\n&gt;&gt;&gt; # Parent has: sensor_id, hour, minute\n&gt;&gt;&gt; # Child has: sensor_id, hour\n</code></pre> <pre><code>&gt;&gt;&gt; # Or use the classmethod\n&gt;&gt;&gt; LineageRelationship.aggregation(on=[\"user_id\", \"session_id\"])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"N:1\",\n      \"default\": \"N:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n      \"title\": \"On\"\n    }\n  },\n  \"title\": \"AggregationRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[AGGREGATION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str] | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.on","title":"on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str] | None = None\n</code></pre> <p>Columns to group by for aggregation. Defaults to all target ID columns.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","title":"get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str]\n</code></pre> <p>Get columns to aggregate on.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]:\n    \"\"\"Get columns to aggregate on.\"\"\"\n    return self.on if self.on is not None else target_id_columns\n</code></pre>"},{"location":"reference/api/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata behind an interface called MetadaStore.</p> <p>Users can extend this class to implement support for arbitrary metadata storage such as databases, lakehouse formats, or really any kind of external system. Metaxy has built-in support for the following metadata store types:</p>"},{"location":"reference/api/metadata-stores/#databases","title":"Databases","text":"<p>See IbisMetadataStore.</p>"},{"location":"reference/api/metadata-stores/#in-memory","title":"In-memory","text":"<p>See InMemoryMetadataStore.</p>"},{"location":"reference/api/metadata-stores/#metadata-store-interface","title":"Metadata Store Interface","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore","title":"MetadataStore","text":"<pre><code>MetadataStore(*, versioning_engine_cls: type[VersioningEngineT], hash_algorithm: HashAlgorithm | None = None, versioning_engine: VersioningEngineOptions = 'auto', fallback_stores: list[MetadataStore] | None = None, auto_create_tables: bool | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for metadata storage backends.</p> <p>Parameters:</p> <ul> <li> <code>hash_algorithm</code>               (<code>HashAlgorithm | None</code>, default:                   <code>None</code> )           \u2013            <p>Hash algorithm to use for the versioning engine.</p> </li> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use.</p> <ul> <li> <p>\"auto\": Prefer the store's native engine, fall back to Polars if needed</p> </li> <li> <p>\"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible</p> </li> <li> <p>\"polars\": Always use the Polars engine</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. Used when upstream features are not in this store. <code>VersioningEngineMismatchError</code> is not raised when reading from fallback stores.</p> </li> <li> <code>auto_create_tables</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, automatically create tables when opening the store. If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var). If False, never auto-create tables.</p> <p>Warning</p> <p>Auto-create is intended for development/testing only. Use proper database migration tools like Alembic for production deployments.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If fallback stores use different hash algorithms or truncation lengths</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If a user-provided dataframe has a wrong implementation and versioning_engine is set to <code>native</code></p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    versioning_engine_cls: type[VersioningEngineT],\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_create_tables: bool | None = None,\n):\n    \"\"\"\n    Initialize the metadata store.\n\n    Args:\n        hash_algorithm: Hash algorithm to use for the versioning engine.\n\n        versioning_engine: Which versioning engine to use.\n\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n\n            - \"polars\": Always use the Polars engine\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            Used when upstream features are not in this store.\n            `VersioningEngineMismatchError` is not raised when reading from fallback stores.\n        auto_create_tables: If True, automatically create tables when opening the store.\n            If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var).\n            If False, never auto-create tables.\n\n            !!! warning\n                Auto-create is intended for development/testing only.\n                Use proper database migration tools like Alembic for production deployments.\n\n    Raises:\n        ValueError: If fallback stores use different hash algorithms or truncation lengths\n        VersioningEngineMismatchError: If a user-provided dataframe has a wrong implementation\n            and versioning_engine is set to `native`\n    \"\"\"\n    # Initialize state early so properties can check it\n    self._is_open = False\n    self._context_depth = 0\n    self._versioning_engine = versioning_engine\n    self._allow_cross_project_writes = False\n    self._open_cm: AbstractContextManager[Self] | None = (\n        None  # Track the open() context manager\n    )\n    self.versioning_engine_cls = versioning_engine_cls\n\n    # Resolve auto_create_tables from global config if not explicitly provided\n    if auto_create_tables is None:\n        from metaxy.config import MetaxyConfig\n\n        self.auto_create_tables = MetaxyConfig.get().auto_create_tables\n    else:\n        self.auto_create_tables = auto_create_tables\n\n    # Use store's default algorithm if not specified\n    if hash_algorithm is None:\n        hash_algorithm = self._get_default_hash_algorithm()\n\n    self.hash_algorithm = hash_algorithm\n\n    self.fallback_stores = fallback_stores or []\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>PROVENANCE_BY_FIELD_COL</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>PROVENANCE_BY_FIELD_COL</code> from, so users must provide samples with manually computed <code>PROVENANCE_BY_FIELD_COL</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.versioning.types.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.versioning.types.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If versioning_engine=\"native\" and data has wrong implementation</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: Frame | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `PROVENANCE_BY_FIELD_COL` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `PROVENANCE_BY_FIELD_COL` from, so users\n            must provide samples with manually computed `PROVENANCE_BY_FIELD_COL` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.versioning.types.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.versioning.types.Increment][] with eager Narwhals DataFrames.\n        versioning_engine: Override the store's versioning engine for this operation.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If versioning_engine=\"native\" and data has wrong implementation\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    filters = filters or defaultdict(list)\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    current_feature_filters = [*filters.get(feature.spec().key.to_string(), [])]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples\n        if current_feature_filters:\n            filtered_samples = samples.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n    else:\n        for upstream_spec in plan.deps or []:\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=filters.get(upstream_spec.key.to_string(), []),\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples is not None\n            and samples.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples.implementation,\n                        message=f\"Provided `samples` have implementation {samples.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples:\n            samples = switch_implementation_to_polars(samples)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        added, changed, removed = engine.resolve_increment_with_provenance(\n            current=current_metadata,\n            upstream=upstream_by_key,\n            hash_algorithm=self.hash_algorithm,\n            filters=filters_by_key,\n            sample=samples.lazy() if samples is not None else None,\n        )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existant Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existant Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: IntoFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Never writes to fallback stores.</p> </li> <li> <p>Project validation is performed unless disabled via <code>allow_cross_project_writes()</code> context manager.</p> </li> <li> <p>Must be called within <code>store.open(mode=AccessMode.WRITE)</code> context manager.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: IntoFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Never writes to fallback stores.\n\n        - Project validation is performed unless disabled via `allow_cross_project_writes()` context manager.\n\n        - Must be called within `store.open(mode=AccessMode.WRITE)` context manager.\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadatStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature)\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.native_implementation","title":"native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.create_versioning_engine","title":"create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.open","title":"open  <code>abstractmethod</code>","text":"<pre><code>open(mode: AccessMode = READ) -&gt; Iterator[Self]\n</code></pre> <p>Open/initialize the store for operations.</p> <p>Context manager that opens the store with specified access mode. Called internally by <code>__enter__</code>. Child classes should implement backend-specific connection setup/teardown here.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>READ</code> )           \u2013            <p>Access mode for this connection session.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Note <p>Users should prefer using <code>with store:</code> pattern except when write access mode is needed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef open(self, mode: AccessMode = AccessMode.READ) -&gt; Iterator[Self]:\n    \"\"\"Open/initialize the store for operations.\n\n    Context manager that opens the store with specified access mode.\n    Called internally by `__enter__`.\n    Child classes should implement backend-specific connection setup/teardown here.\n\n    Args:\n        mode: Access mode for this connection session.\n\n    Yields:\n        Self: The store instance with connection open\n\n    Note:\n        Users should prefer using `with store:` pattern except when write access mode is needed.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = AccessMode.WRITE if self.auto_create_tables else AccessMode.READ\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata_to_store","title":"write_metadata_to_store  <code>abstractmethod</code>","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame) -&gt; None\n</code></pre> <p>Internal write implementation (backend-specific).</p> <p>Backends may convert to their specific type if needed (e.g., Polars, Ibis).</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>Narwhals-compatible DataFrame with metadata to write</p> </li> </ul> <p>Note: Subclasses implement this for their storage backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation (backend-specific).\n\n    Backends may convert to their specific type if needed (e.g., Polars, Ibis).\n\n    Args:\n        feature_key: Feature key to write to\n        df: [Narwhals](https://narwhals-dev.github.io/narwhals/)-compatible DataFrame with metadata to write\n\n    Note: Subclasses implement this for their storage backend.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata_in_store","title":"read_metadata_in_store  <code>abstractmethod</code>","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from THIS store only without using any fallbacks stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions for this specific feature.</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if feature not found in the store</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from THIS store only without using any fallbacks stores.\n\n    Args:\n        feature: Feature to read metadata for\n        filters: List of Narwhals filter expressions for this specific feature.\n        columns: Subset of columns to return\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if feature not found in the store\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.display","title":"display  <code>abstractmethod</code>","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return a human-readable display string for this store.</p> <p>Used in warnings, logs, and CLI output to identify the store.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef display(self) -&gt; str:\n    \"\"\"Return a human-readable display string for this store.\n\n    Used in warnings, logs, and CLI output to identify the store.\n\n    Returns:\n        Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            \"Destination store must be opened with store.open(AccessMode.WRITE) before use\"\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(AccessMode.READ):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode","title":"AccessMode","text":"<p>               Bases: <code>Enum</code></p> <p>Access mode for metadata store connections.</p> <p>Controls whether the store is opened in read-only or read-write mode. This is particularly important for stores like DuckDB that lock the database in write mode by default. Specific store implementations should handle this parameter accordingly.</p>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode.READ","title":"READ  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>READ = 'read'\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode.WRITE","title":"WRITE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WRITE = 'write'\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.base.VersioningEngineOptions","title":"VersioningEngineOptions  <code>module-attribute</code>","text":"<pre><code>VersioningEngineOptions = Literal['auto', 'native', 'polars']\n</code></pre>"},{"location":"reference/api/metadata-stores/exceptions/","title":"Metadata Store Exceptions","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions","title":"exceptions","text":"<p>Exceptions for metadata store operations.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-classes","title":"Classes","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataStoreError","title":"MetadataStoreError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for metadata store errors.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FeatureNotFoundError","title":"FeatureNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a feature is not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.SystemDataNotFoundError","title":"SystemDataNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when system features are not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FieldNotFoundError","title":"FieldNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a field is not found for a feature.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataSchemaError","title":"MetadataSchemaError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when metadata DataFrame has invalid schema.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.DependencyError","title":"DependencyError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when upstream dependencies are missing or invalid.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.StoreNotOpenError","title":"StoreNotOpenError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when attempting to use a store that hasn't been opened.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","title":"HashAlgorithmNotSupportedError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a hash algorithm is not supported by the store or its components.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.TableNotFoundError","title":"TableNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a table does not exist and auto_create_tables is disabled.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.VersioningEngineMismatchError","title":"VersioningEngineMismatchError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when versioning_engine='native' is requested but data has wrong implementation.</p>"},{"location":"reference/api/metadata-stores/memory/","title":"In-Memory Metadata Store","text":"<p>This one is mostly useful for testing.</p>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore","title":"InMemoryMetadataStore","text":"<pre><code>InMemoryMetadataStore(**kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>In-memory metadata store using dict-based storage.</p> <p>Features: - Simple dict storage: {FeatureKey: pl.DataFrame} - Fast for testing and prototyping - No persistence (data lost when process exits) - Schema validation on write - Uses Polars components for all operations</p> <p>Limitations: - Not suitable for production - Data lost on process exit - No concurrency support across processes - Memory-bound (all data in RAM)</p> Notes <p>Uses Narwhals LazyFrames (nw.LazyFrame) for all operations</p> Components <p>Components are created on-demand in resolve_update(). Uses Polars internally but exposes Narwhals interface. Only supports Polars components (no native backend).</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialize in-memory store.\n\n    Args:\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n    \"\"\"\n    # Use tuple as key (hashable) instead of string to avoid parsing issues\n    self._storage: dict[tuple[str, ...], pl.DataFrame] = {}\n    super().__init__(**kwargs, versioning_engine_cls=PolarsVersioningEngine)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>PROVENANCE_BY_FIELD_COL</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>PROVENANCE_BY_FIELD_COL</code> from, so users must provide samples with manually computed <code>PROVENANCE_BY_FIELD_COL</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.versioning.types.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.versioning.types.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If versioning_engine=\"native\" and data has wrong implementation</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: Frame | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `PROVENANCE_BY_FIELD_COL` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `PROVENANCE_BY_FIELD_COL` from, so users\n            must provide samples with manually computed `PROVENANCE_BY_FIELD_COL` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.versioning.types.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.versioning.types.Increment][] with eager Narwhals DataFrames.\n        versioning_engine: Override the store's versioning engine for this operation.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If versioning_engine=\"native\" and data has wrong implementation\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    filters = filters or defaultdict(list)\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    current_feature_filters = [*filters.get(feature.spec().key.to_string(), [])]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples\n        if current_feature_filters:\n            filtered_samples = samples.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n    else:\n        for upstream_spec in plan.deps or []:\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=filters.get(upstream_spec.key.to_string(), []),\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples is not None\n            and samples.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples.implementation,\n                        message=f\"Provided `samples` have implementation {samples.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples:\n            samples = switch_implementation_to_polars(samples)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        added, changed, removed = engine.resolve_increment_with_provenance(\n            current=current_metadata,\n            upstream=upstream_by_key,\n            hash_algorithm=self.hash_algorithm,\n            filters=filters_by_key,\n            sample=samples.lazy() if samples is not None else None,\n        )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existant Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existant Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: IntoFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Never writes to fallback stores.</p> </li> <li> <p>Project validation is performed unless disabled via <code>allow_cross_project_writes()</code> context manager.</p> </li> <li> <p>Must be called within <code>store.open(mode=AccessMode.WRITE)</code> context manager.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: IntoFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Never writes to fallback stores.\n\n        - Project validation is performed unless disabled via `allow_cross_project_writes()` context manager.\n\n        - Must be called within `store.open(mode=AccessMode.WRITE)` context manager.\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadatStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature)\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.native_implementation","title":"native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.create_versioning_engine","title":"create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = AccessMode.WRITE if self.auto_create_tables else AccessMode.READ\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            \"Destination store must be opened with store.open(AccessMode.WRITE) before use\"\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(AccessMode.READ):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.write_metadata_to_store","title":"write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame) -&gt; None\n</code></pre> <p>Internal write implementation for in-memory storage.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation for in-memory storage.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n    \"\"\"\n    df_polars: pl.DataFrame = collect_to_polars(df)\n\n    storage_key = self._get_storage_key(feature_key)\n\n    # Append or create\n    if storage_key in self._storage:\n        existing_df = self._storage[storage_key]\n\n        # Handle schema evolution: ensure both DataFrames have matching columns\n        # Add missing columns as null to the existing DataFrame\n        for col_name in df_polars.columns:\n            if col_name not in existing_df.columns:\n                # Get the data type from the new DataFrame\n                col_dtype = df_polars.schema[col_name]\n                # Add column with null values of the appropriate type\n                existing_df = existing_df.with_columns(\n                    pl.lit(None).cast(col_dtype).alias(col_name)\n                )\n\n        # Add missing columns to the new DataFrame\n        for col_name in existing_df.columns:\n            if col_name not in df_polars.columns:\n                # Get the data type from the existing DataFrame\n                col_dtype = existing_df.schema[col_name]\n                # Add column with null values of the appropriate type\n                df_polars = df_polars.with_columns(\n                    pl.lit(None).cast(col_dtype).alias(col_name)\n                )  # type: ignore[arg-type,union-attr]\n\n        # Ensure column order matches by selecting columns in consistent order\n        all_columns = sorted(set(existing_df.columns) | set(df_polars.columns))\n        existing_df = existing_df.select(all_columns)\n        df_polars = df_polars.select(all_columns)\n\n        # Now we can safely concat\n        self._storage[storage_key] = pl.concat(\n            [existing_df, df_polars],\n            how=\"vertical\",\n        )\n    else:\n        # Create new\n        self._storage[storage_key] = df_polars\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version\n        filters: List of Narwhals filter expressions\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    storage_key = self._get_storage_key(feature_key)\n\n    if storage_key not in self._storage:\n        return None\n\n    # Start with lazy Polars DataFrame, wrap with Narwhals\n    df_lazy = self._storage[storage_key].lazy()\n    nw_lazy = nw.from_native(df_lazy)\n\n    # Apply feature_version filter\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Check if result would be empty (we need to check the underlying frame)\n    # For now, return the lazy frame - emptiness check happens when materializing\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all metadata from store.</p> <p>Useful for testing.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all metadata from store.\n\n    Useful for testing.\n    \"\"\"\n    self._storage.clear()\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.open","title":"open","text":"<pre><code>open(mode: AccessMode = READ) -&gt; Iterator[Self]\n</code></pre> <p>Open the in-memory store (no-op for in-memory, but accepts mode for consistency).</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>READ</code> )           \u2013            <p>Access mode (accepted for consistency but ignored).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = AccessMode.READ) -&gt; Iterator[Self]:\n    \"\"\"Open the in-memory store (no-op for in-memory, but accepts mode for consistency).\n\n    Args:\n        mode: Access mode (accepted for consistency but ignored).\n\n    Yields:\n        Self: The store instance\n    \"\"\"\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # No actual connection needed for in-memory\n            # Mark store as open and validate\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            # Nothing to clean up\n            self._is_open = False\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    num_fallbacks = len(self.fallback_stores)\n    status = \"open\" if self._is_open else \"closed\"\n    return (\n        f\"InMemoryMetadataStore(status={status}, fallback_stores={num_fallbacks})\"\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    status = \"open\" if self._is_open else \"closed\"\n    return f\"InMemoryMetadataStore(status={status})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/","title":"Database Metadata Stores","text":"<p>Metadata stores that are backed by databases are implemented with Ibis.</p> <p>Users can extend IbisMetadataStore to work with databases that are not natively supported by Metaxy.</p>"},{"location":"reference/api/metadata-stores/ibis/#ibis-metadata-store","title":"Ibis Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore","title":"IbisMetadataStore","text":"<pre><code>IbisMetadataStore(versioning_engine: VersioningEngineOptions = 'auto', connection_string: str | None = None, *, backend: str | None = None, connection_params: dict[str, Any] | None = None, table_prefix: str | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code>, <code>ABC</code></p> <p>Generic SQL metadata store using Ibis.</p> <p>Supports any Ibis backend that supports struct types, such as: DuckDB, PostgreSQL, ClickHouse, and others.</p> Warning <p>Backends without native struct support (e.g., SQLite) are NOT supported.</p> <p>Storage layout: - Each feature gets its own table: {feature}__{key} - System tables: metaxy__system__feature_versions, metaxy__system__migrations - Uses Ibis for cross-database compatibility</p> <p>Note: Uses MD5 hash by default for cross-database compatibility. DuckDBMetadataStore overrides this with dynamic algorithm detection. For other backends, override the calculator instance variable with backend-specific implementations.</p> Example <pre><code># ClickHouse\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# PostgreSQL\nstore = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\n\n# DuckDB (use DuckDBMetadataStore instead for better hash support)\nstore = IbisMetadataStore(\"duckdb:///metadata.db\")\n\nwith store:\n    store.write_metadata(MyFeature, df)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use. - \"auto\": Prefer the store's native engine, fall back to Polars if needed - \"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible - \"polars\": Always use the Polars engine</p> </li> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis connection string (e.g., \"clickhouse://host:9000/db\") If provided, backend and connection_params are ignored.</p> </li> <li> <code>backend</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\") Used with connection_params for more control.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Backend-specific connection parameters e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}</p> </li> <li> <code>table_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional prefix applied to all feature and system table names. Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL identifier when combined with the generated table name.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor backend is provided</p> </li> <li> <code>ImportError</code>             \u2013            <p>If Ibis or required backend driver not installed</p> </li> </ul> Example <pre><code># Using connection string\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# Using backend + params\nstore = IbisMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\"host\": \"localhost\", \"port\": 9000}\n    )\n</code></pre> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def __init__(\n    self,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize Ibis metadata store.\n\n    Args:\n        versioning_engine: Which versioning engine to use.\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n            - \"polars\": Always use the Polars engine\n        connection_string: Ibis connection string (e.g., \"clickhouse://host:9000/db\")\n            If provided, backend and connection_params are ignored.\n        backend: Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\")\n            Used with connection_params for more control.\n        connection_params: Backend-specific connection parameters\n            e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}\n        table_prefix: Optional prefix applied to all feature and system table names.\n            Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL\n            identifier when combined with the generated table name.\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n\n    Raises:\n        ValueError: If neither connection_string nor backend is provided\n        ImportError: If Ibis or required backend driver not installed\n\n    Example:\n        ```py\n        # Using connection string\n        store = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n        # Using backend + params\n        store = IbisMetadataStore(\n            backend=\"clickhouse\",\n            connection_params={\"host\": \"localhost\", \"port\": 9000}\n            )\n        ```\n    \"\"\"\n    import ibis\n\n    self.connection_string = connection_string\n    self.backend = backend\n    self.connection_params = connection_params or {}\n    self._conn: ibis.BaseBackend | None = None\n    self._table_prefix = table_prefix or \"\"\n\n    super().__init__(\n        **kwargs,\n        versioning_engine=versioning_engine,\n        versioning_engine_cls=IbisVersioningEngine,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.sqlalchemy_url","title":"sqlalchemy_url  <code>property</code>","text":"<pre><code>sqlalchemy_url: str\n</code></pre> <p>Get SQLAlchemy-compatible connection URL for tools like Alembic.</p> <p>Returns the connection string if available. If the store was initialized with backend + connection_params instead of a connection string, raises an error since constructing a proper URL is backend-specific.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SQLAlchemy-compatible URL string</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If connection_string is not available</p> </li> </ul> Example <pre><code>store = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\nprint(store.sqlalchemy_url)  # postgresql://user:pass@host:5432/db\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.get_table_name","title":"get_table_name","text":"<pre><code>get_table_name(key: FeatureKey) -&gt; str\n</code></pre> <p>Generate the storage table name for a feature or system table.</p> <p>Applies the configured table_prefix (if any) to the feature key's table name. Subclasses can override this method to implement custom naming logic.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to convert to storage table name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Storage table name with optional prefix applied.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def get_table_name(\n    self,\n    key: FeatureKey,\n) -&gt; str:\n    \"\"\"Generate the storage table name for a feature or system table.\n\n    Applies the configured table_prefix (if any) to the feature key's table name.\n    Subclasses can override this method to implement custom naming logic.\n\n    Args:\n        key: Feature key to convert to storage table name.\n\n    Returns:\n        Storage table name with optional prefix applied.\n    \"\"\"\n    base_name = key.table_name\n\n    return f\"{self._table_prefix}{base_name}\" if self._table_prefix else base_name\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.open","title":"open","text":"<pre><code>open(mode: AccessMode = READ) -&gt; Iterator[Self]\n</code></pre> <p>Open connection to database via Ibis.</p> <p>Subclasses should override this to add backend-specific initialization (e.g., loading extensions) and must call this method via super().open(mode).</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>READ</code> )           \u2013            <p>Access mode. Subclasses may use this to set backend-specific connection parameters (e.g., <code>read_only</code> for DuckDB).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = AccessMode.READ) -&gt; Iterator[Self]:\n    \"\"\"Open connection to database via Ibis.\n\n    Subclasses should override this to add backend-specific initialization\n    (e.g., loading extensions) and must call this method via super().open(mode).\n\n    Args:\n        mode: Access mode. Subclasses may use this to set backend-specific connection\n            parameters (e.g., `read_only` for DuckDB).\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    import ibis\n\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # Setup: Connect to database\n            if self.connection_string:\n                # Use connection string\n                self._conn = ibis.connect(self.connection_string)\n            else:\n                # Use backend + params\n                # Get backend-specific connect function\n                assert self.backend is not None, (\n                    \"backend must be set if connection_string is None\"\n                )\n                backend_module = getattr(ibis, self.backend)\n                self._conn = backend_module.connect(**self.connection_params)\n\n            # Mark store as open and validate\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            # Teardown: Close connection\n            if self._conn is not None:\n                # Ibis connections may not have explicit close method\n                # but setting to None releases resources\n                self._conn = None\n            self._is_open = False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.write_metadata_to_store","title":"write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame) -&gt; None\n</code></pre> <p>Internal write implementation using Ibis.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If table doesn't exist and auto_create_tables is False</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation using Ibis.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n\n    Raises:\n        TableNotFoundError: If table doesn't exist and auto_create_tables is False\n    \"\"\"\n    if df.implementation == nw.Implementation.IBIS:\n        df_to_insert = df.to_native()  # Ibis expression\n    else:\n        from metaxy._utils import collect_to_polars\n\n        df_to_insert = collect_to_polars(df)  # Polars DataFrame\n\n    table_name = self.get_table_name(feature_key)\n\n    try:\n        self.conn.insert(table_name, obj=df_to_insert)  # type: ignore[attr-defined]  # pyright: ignore[reportAttributeAccessIssue]\n    except Exception as e:\n        import ibis.common.exceptions\n\n        if not isinstance(e, ibis.common.exceptions.TableNotFound):\n            raise\n        if self.auto_create_tables:\n            # Warn about auto-create (first time only)\n            if self._should_warn_auto_create_tables:\n                import warnings\n\n                warnings.warn(\n                    f\"AUTO_CREATE_TABLES is enabled - automatically creating table '{table_name}'. \"\n                    \"Do not use in production! \"\n                    \"Use proper database migration tools like Alembic for production deployments.\",\n                    UserWarning,\n                    stacklevel=4,\n                )\n\n            # Note: create_table(table_name, obj=df) both creates the table AND inserts the data\n            # No separate insert needed - the data from df is already written\n            self.conn.create_table(table_name, obj=df_to_insert)\n        else:\n            raise TableNotFoundError(\n                f\"Table '{table_name}' does not exist for feature {feature_key.to_string()}. \"\n                f\"Enable auto_create_tables=True to automatically create tables, \"\n                f\"or use proper database migration tools like Alembic to create the table first.\"\n            ) from e\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = self.get_table_name(feature_key)\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    return f\"{self.__class__.__name__}(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>PROVENANCE_BY_FIELD_COL</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>PROVENANCE_BY_FIELD_COL</code> from, so users must provide samples with manually computed <code>PROVENANCE_BY_FIELD_COL</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.versioning.types.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.versioning.types.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If versioning_engine=\"native\" and data has wrong implementation</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: Frame | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `PROVENANCE_BY_FIELD_COL` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `PROVENANCE_BY_FIELD_COL` from, so users\n            must provide samples with manually computed `PROVENANCE_BY_FIELD_COL` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.versioning.types.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.versioning.types.Increment][] with eager Narwhals DataFrames.\n        versioning_engine: Override the store's versioning engine for this operation.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If versioning_engine=\"native\" and data has wrong implementation\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    filters = filters or defaultdict(list)\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    current_feature_filters = [*filters.get(feature.spec().key.to_string(), [])]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples\n        if current_feature_filters:\n            filtered_samples = samples.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n    else:\n        for upstream_spec in plan.deps or []:\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=filters.get(upstream_spec.key.to_string(), []),\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples is not None\n            and samples.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples.implementation,\n                        message=f\"Provided `samples` have implementation {samples.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples:\n            samples = switch_implementation_to_polars(samples)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        added, changed, removed = engine.resolve_increment_with_provenance(\n            current=current_metadata,\n            upstream=upstream_by_key,\n            hash_algorithm=self.hash_algorithm,\n            filters=filters_by_key,\n            sample=samples.lazy() if samples is not None else None,\n        )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existant Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existant Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: IntoFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Never writes to fallback stores.</p> </li> <li> <p>Project validation is performed unless disabled via <code>allow_cross_project_writes()</code> context manager.</p> </li> <li> <p>Must be called within <code>store.open(mode=AccessMode.WRITE)</code> context manager.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: IntoFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Never writes to fallback stores.\n\n        - Project validation is performed unless disabled via `allow_cross_project_writes()` context manager.\n\n        - Must be called within `store.open(mode=AccessMode.WRITE)` context manager.\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadatStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature)\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.native_implementation","title":"native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.create_versioning_engine","title":"create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = AccessMode.WRITE if self.auto_create_tables else AccessMode.READ\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            \"Destination store must be opened with store.open(AccessMode.WRITE) before use\"\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(AccessMode.READ):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/","title":"Clickhouse Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","title":"ClickHouseMetadataStore","text":"<pre><code>ClickHouseMetadataStore(connection_string: str | None = None, *, connection_params: dict[str, Any] | None = None, fallback_stores: list[MetadataStore] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>ClickHouse metadata storeusing Ibis backend.</p> Connection Parameters <pre><code>store = ClickHouseMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\n        \"host\": \"localhost\",\n        \"port\": 9000,\n        \"database\": \"default\",\n        \"user\": \"default\",\n        \"password\": \"\"\n    },\n    hash_algorithm=HashAlgorithm.XXHASH64\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ClickHouse connection string.</p> <p>Format: <code>clickhouse://[user[:password]@]host[:port]/database[?param=value]</code></p> <p>Examples:     <pre><code>- \"clickhouse://localhost:9000/default\"\n- \"clickhouse://user:pass@host:9000/db\"\n- \"clickhouse://host:9000/db?secure=true\"\n</code></pre></p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Alternative to connection_string, specify params as dict:</p> <ul> <li> <p>host: Server host</p> </li> <li> <p>port: Server port (default: <code>9000</code>)</p> </li> <li> <p>database: Database name</p> </li> <li> <p>user: Username</p> </li> <li> <p>password: Password</p> </li> <li> <p>secure: Use secure connection (default: <code>False</code>)</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-clickhouse not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor connection_params provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/clickhouse.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [ClickHouse](https://clickhouse.com/) metadata store.\n\n    Args:\n        connection_string: ClickHouse connection string.\n\n            Format: `clickhouse://[user[:password]@]host[:port]/database[?param=value]`\n\n            Examples:\n                ```\n                - \"clickhouse://localhost:9000/default\"\n                - \"clickhouse://user:pass@host:9000/db\"\n                - \"clickhouse://host:9000/db?secure=true\"\n                ```\n\n        connection_params: Alternative to connection_string, specify params as dict:\n\n            - host: Server host\n\n            - port: Server port (default: `9000`)\n\n            - database: Database name\n\n            - user: Username\n\n            - password: Password\n\n            - secure: Use secure connection (default: `False`)\n\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Raises:\n        ImportError: If ibis-clickhouse not installed\n        ValueError: If neither connection_string nor connection_params provided\n    \"\"\"\n    if connection_string is None and connection_params is None:\n        raise ValueError(\n            \"Must provide either connection_string or connection_params. \"\n            \"Example: connection_string='clickhouse://localhost:9000/default'\"\n        )\n\n    # Initialize Ibis store with ClickHouse backend\n    super().__init__(\n        connection_string=connection_string,\n        backend=\"clickhouse\" if connection_string is None else None,\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.sqlalchemy_url","title":"sqlalchemy_url  <code>property</code>","text":"<pre><code>sqlalchemy_url: str\n</code></pre> <p>Get SQLAlchemy-compatible connection URL for tools like Alembic.</p> <p>Returns the connection string if available. If the store was initialized with backend + connection_params instead of a connection string, raises an error since constructing a proper URL is backend-specific.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SQLAlchemy-compatible URL string</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If connection_string is not available</p> </li> </ul> Example <pre><code>store = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\nprint(store.sqlalchemy_url)  # postgresql://user:pass@host:5432/db\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>PROVENANCE_BY_FIELD_COL</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>PROVENANCE_BY_FIELD_COL</code> from, so users must provide samples with manually computed <code>PROVENANCE_BY_FIELD_COL</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.versioning.types.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.versioning.types.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If versioning_engine=\"native\" and data has wrong implementation</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: Frame | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `PROVENANCE_BY_FIELD_COL` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `PROVENANCE_BY_FIELD_COL` from, so users\n            must provide samples with manually computed `PROVENANCE_BY_FIELD_COL` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.versioning.types.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.versioning.types.Increment][] with eager Narwhals DataFrames.\n        versioning_engine: Override the store's versioning engine for this operation.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If versioning_engine=\"native\" and data has wrong implementation\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    filters = filters or defaultdict(list)\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    current_feature_filters = [*filters.get(feature.spec().key.to_string(), [])]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples\n        if current_feature_filters:\n            filtered_samples = samples.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n    else:\n        for upstream_spec in plan.deps or []:\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=filters.get(upstream_spec.key.to_string(), []),\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples is not None\n            and samples.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples.implementation,\n                        message=f\"Provided `samples` have implementation {samples.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples:\n            samples = switch_implementation_to_polars(samples)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        added, changed, removed = engine.resolve_increment_with_provenance(\n            current=current_metadata,\n            upstream=upstream_by_key,\n            hash_algorithm=self.hash_algorithm,\n            filters=filters_by_key,\n            sample=samples.lazy() if samples is not None else None,\n        )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existant Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existant Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: IntoFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Never writes to fallback stores.</p> </li> <li> <p>Project validation is performed unless disabled via <code>allow_cross_project_writes()</code> context manager.</p> </li> <li> <p>Must be called within <code>store.open(mode=AccessMode.WRITE)</code> context manager.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: IntoFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Never writes to fallback stores.\n\n        - Project validation is performed unless disabled via `allow_cross_project_writes()` context manager.\n\n        - Must be called within `store.open(mode=AccessMode.WRITE)` context manager.\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadatStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature)\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.native_implementation","title":"native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.create_versioning_engine","title":"create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.open","title":"open","text":"<pre><code>open(mode: AccessMode = READ) -&gt; Iterator[Self]\n</code></pre> <p>Open connection to database via Ibis.</p> <p>Subclasses should override this to add backend-specific initialization (e.g., loading extensions) and must call this method via super().open(mode).</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>READ</code> )           \u2013            <p>Access mode. Subclasses may use this to set backend-specific connection parameters (e.g., <code>read_only</code> for DuckDB).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = AccessMode.READ) -&gt; Iterator[Self]:\n    \"\"\"Open connection to database via Ibis.\n\n    Subclasses should override this to add backend-specific initialization\n    (e.g., loading extensions) and must call this method via super().open(mode).\n\n    Args:\n        mode: Access mode. Subclasses may use this to set backend-specific connection\n            parameters (e.g., `read_only` for DuckDB).\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    import ibis\n\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # Setup: Connect to database\n            if self.connection_string:\n                # Use connection string\n                self._conn = ibis.connect(self.connection_string)\n            else:\n                # Use backend + params\n                # Get backend-specific connect function\n                assert self.backend is not None, (\n                    \"backend must be set if connection_string is None\"\n                )\n                backend_module = getattr(ibis, self.backend)\n                self._conn = backend_module.connect(**self.connection_params)\n\n            # Mark store as open and validate\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            # Teardown: Close connection\n            if self._conn is not None:\n                # Ibis connections may not have explicit close method\n                # but setting to None releases resources\n                self._conn = None\n            self._is_open = False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = AccessMode.WRITE if self.auto_create_tables else AccessMode.READ\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.write_metadata_to_store","title":"write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame) -&gt; None\n</code></pre> <p>Internal write implementation using Ibis.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If table doesn't exist and auto_create_tables is False</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation using Ibis.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n\n    Raises:\n        TableNotFoundError: If table doesn't exist and auto_create_tables is False\n    \"\"\"\n    if df.implementation == nw.Implementation.IBIS:\n        df_to_insert = df.to_native()  # Ibis expression\n    else:\n        from metaxy._utils import collect_to_polars\n\n        df_to_insert = collect_to_polars(df)  # Polars DataFrame\n\n    table_name = self.get_table_name(feature_key)\n\n    try:\n        self.conn.insert(table_name, obj=df_to_insert)  # type: ignore[attr-defined]  # pyright: ignore[reportAttributeAccessIssue]\n    except Exception as e:\n        import ibis.common.exceptions\n\n        if not isinstance(e, ibis.common.exceptions.TableNotFound):\n            raise\n        if self.auto_create_tables:\n            # Warn about auto-create (first time only)\n            if self._should_warn_auto_create_tables:\n                import warnings\n\n                warnings.warn(\n                    f\"AUTO_CREATE_TABLES is enabled - automatically creating table '{table_name}'. \"\n                    \"Do not use in production! \"\n                    \"Use proper database migration tools like Alembic for production deployments.\",\n                    UserWarning,\n                    stacklevel=4,\n                )\n\n            # Note: create_table(table_name, obj=df) both creates the table AND inserts the data\n            # No separate insert needed - the data from df is already written\n            self.conn.create_table(table_name, obj=df_to_insert)\n        else:\n            raise TableNotFoundError(\n                f\"Table '{table_name}' does not exist for feature {feature_key.to_string()}. \"\n                f\"Enable auto_create_tables=True to automatically create tables, \"\n                f\"or use proper database migration tools like Alembic to create the table first.\"\n            ) from e\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = self.get_table_name(feature_key)\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    return f\"{self.__class__.__name__}(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            \"Destination store must be opened with store.open(AccessMode.WRITE) before use\"\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(AccessMode.READ):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.get_table_name","title":"get_table_name","text":"<pre><code>get_table_name(key: FeatureKey) -&gt; str\n</code></pre> <p>Generate the storage table name for a feature or system table.</p> <p>Applies the configured table_prefix (if any) to the feature key's table name. Subclasses can override this method to implement custom naming logic.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to convert to storage table name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Storage table name with optional prefix applied.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def get_table_name(\n    self,\n    key: FeatureKey,\n) -&gt; str:\n    \"\"\"Generate the storage table name for a feature or system table.\n\n    Applies the configured table_prefix (if any) to the feature key's table name.\n    Subclasses can override this method to implement custom naming logic.\n\n    Args:\n        key: Feature key to convert to storage table name.\n\n    Returns:\n        Storage table name with optional prefix applied.\n    \"\"\"\n    base_name = key.table_name\n\n    return f\"{self._table_prefix}{base_name}\" if self._table_prefix else base_name\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/","title":"DuckDB Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore","title":"DuckDBMetadataStore","text":"<pre><code>DuckDBMetadataStore(database: str | Path, *, config: dict[str, str] | None = None, extensions: Sequence[ExtensionInput] | None = None, fallback_stores: list[MetadataStore] | None = None, ducklake: DuckLakeConfigInput | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>DuckDB metadata store using Ibis backend.</p> Local File <pre><code>store = DuckDBMetadataStore(\"metadata.db\")\n</code></pre> In-memory database <pre><code># In-memory database\nstore = DuckDBMetadataStore(\":memory:\")\n</code></pre> MotherDuck <pre><code># MotherDuck\nstore = DuckDBMetadataStore(\"md:my_database\")\n</code></pre> With extensions <pre><code># With extensions\nstore = DuckDBMetadataStore(\n    \"metadata.db\",\n    hash_algorithm=HashAlgorithm.XXHASH64,\n    extensions=[\"hashfuncs\"]\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>database</code>               (<code>str | Path</code>)           \u2013            <p>Database connection string or path. - File path: <code>\"metadata.db\"</code> or <code>Path(\"metadata.db\")</code></p> <ul> <li> <p>In-memory: <code>\":memory:\"</code></p> </li> <li> <p>MotherDuck: <code>\"md:my_database\"</code> or <code>\"md:my_database?motherduck_token=...\"</code></p> </li> <li> <p>S3: <code>\"s3://bucket/path/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>HTTPS: <code>\"https://example.com/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>Any valid DuckDB connection string</p> </li> </ul> </li> <li> <code>config</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})</p> </li> <li> <code>extensions</code>               (<code>Sequence[ExtensionInput] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of DuckDB extensions to install and load on open. Supports strings (community repo), mapping-like objects with <code>name</code>/<code>repository</code> keys, or metaxy.metadata_store.duckdb.ExtensionSpec instances.</p> </li> </ul> Optional DuckLake attachment configuration. Provide either a <p>mapping with 'metadata_backend' and 'storage_backend' entries or a DuckLakeAttachmentConfig instance. When supplied, the DuckDB connection is configured to ATTACH the DuckLake catalog after open(). fallback_stores: Ordered list of read-only fallback stores.</p> <p>**kwargs: Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> Warning <p>Parent directories are NOT created automatically. Ensure paths exist before initializing the store.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def __init__(\n    self,\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[ExtensionInput] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    ducklake: DuckLakeConfigInput | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize [DuckDB](https://duckdb.org/) metadata store.\n\n    Args:\n        database: Database connection string or path.\n            - File path: `\"metadata.db\"` or `Path(\"metadata.db\")`\n\n            - In-memory: `\":memory:\"`\n\n            - MotherDuck: `\"md:my_database\"` or `\"md:my_database?motherduck_token=...\"`\n\n            - S3: `\"s3://bucket/path/database.duckdb\"` (read-only via ATTACH)\n\n            - HTTPS: `\"https://example.com/database.duckdb\"` (read-only via ATTACH)\n\n            - Any valid DuckDB connection string\n\n        config: Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})\n        extensions: List of DuckDB extensions to install and load on open.\n            Supports strings (community repo), mapping-like objects with\n            ``name``/``repository`` keys, or [metaxy.metadata_store.duckdb.ExtensionSpec][] instances.\n\n    ducklake: Optional DuckLake attachment configuration. Provide either a\n        mapping with 'metadata_backend' and 'storage_backend' entries or a\n        DuckLakeAttachmentConfig instance. When supplied, the DuckDB\n        connection is configured to ATTACH the DuckLake catalog after open().\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Warning:\n        Parent directories are NOT created automatically. Ensure paths exist\n        before initializing the store.\n    \"\"\"\n    database_str = str(database)\n\n    # Build connection params for Ibis DuckDB backend\n    # Ibis DuckDB backend accepts config params directly (not nested under 'config')\n    connection_params = {\"database\": database_str}\n    if config:\n        connection_params.update(config)\n\n    self.database = database_str\n    base_extensions: list[NormalisedExtension] = _normalise_extensions(\n        extensions or []\n    )\n\n    self._ducklake_config: DuckLakeAttachmentConfig | None = None\n    self._ducklake_attachment: DuckLakeAttachmentManager | None = None\n    if ducklake is not None:\n        attachment_config, manager = build_ducklake_attachment(ducklake)\n        ensure_extensions_with_plugins(base_extensions, attachment_config.plugins)\n        self._ducklake_config = attachment_config\n        self._ducklake_attachment = manager\n\n    self.extensions = base_extensions\n\n    # Auto-add hashfuncs extension if not present (needed for default XXHASH64)\n    # But we'll fall back to MD5 if hashfuncs is not available\n    extension_names: list[str] = []\n    for ext in self.extensions:\n        if isinstance(ext, str):\n            extension_names.append(ext)\n        elif isinstance(ext, ExtensionSpec):\n            extension_names.append(ext.name)\n        else:\n            # After _normalise_extensions, this should not happen\n            # But keep defensive check for type safety\n            raise TypeError(\n                f\"Extension must be str or ExtensionSpec after normalization; got {type(ext)}\"\n            )\n    if \"hashfuncs\" not in extension_names:\n        self.extensions.append(\"hashfuncs\")\n\n    # Initialize Ibis store with DuckDB backend\n    super().__init__(\n        backend=\"duckdb\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment","title":"ducklake_attachment  <code>property</code>","text":"<pre><code>ducklake_attachment: DuckLakeAttachmentManager\n</code></pre> <p>DuckLake attachment manager (raises if not configured).</p>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment_config","title":"ducklake_attachment_config  <code>property</code>","text":"<pre><code>ducklake_attachment_config: DuckLakeAttachmentConfig\n</code></pre> <p>DuckLake attachment configuration (raises if not configured).</p>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.sqlalchemy_url","title":"sqlalchemy_url  <code>property</code>","text":"<pre><code>sqlalchemy_url: str\n</code></pre> <p>Get SQLAlchemy-compatible connection URL for tools like Alembic.</p> <p>Returns the connection string if available. If the store was initialized with backend + connection_params instead of a connection string, raises an error since constructing a proper URL is backend-specific.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SQLAlchemy-compatible URL string</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If connection_string is not available</p> </li> </ul> Example <pre><code>store = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\nprint(store.sqlalchemy_url)  # postgresql://user:pass@host:5432/db\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.open","title":"open","text":"<pre><code>open(mode: AccessMode = READ) -&gt; Iterator[Self]\n</code></pre> <p>Open DuckDB connection with specified access mode.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>READ</code> )           \u2013            <p>Access mode (READ or WRITE). Defaults to READ. READ mode sets read_only=True for concurrent access.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = AccessMode.READ) -&gt; Iterator[Self]:\n    \"\"\"Open DuckDB connection with specified access mode.\n\n    Args:\n        mode: Access mode (READ or WRITE). Defaults to READ.\n            READ mode sets read_only=True for concurrent access.\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    # Setup: Configure connection params based on mode\n    if mode == AccessMode.READ:\n        self.connection_params[\"read_only\"] = True\n    else:\n        # Remove read_only if present (switching to WRITE)\n        self.connection_params.pop(\"read_only\", None)\n\n    # Call parent context manager to establish connection\n    with super().open(mode):\n        try:\n            # Configure DuckLake if needed (only on first entry)\n            if self._ducklake_attachment is not None and self._context_depth == 1:\n                duckdb_conn = self._duckdb_raw_connection()\n                self._ducklake_attachment.configure(duckdb_conn)\n\n            yield self\n        finally:\n            # Cleanup is handled by parent's finally block\n            pass\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.preview_ducklake_sql","title":"preview_ducklake_sql","text":"<pre><code>preview_ducklake_sql() -&gt; list[str]\n</code></pre> <p>Return DuckLake attachment SQL if configured.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def preview_ducklake_sql(self) -&gt; list[str]:\n    \"\"\"Return DuckLake attachment SQL if configured.\"\"\"\n    return self.ducklake_attachment.preview_sql()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: Frame | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>PROVENANCE_BY_FIELD_COL</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>PROVENANCE_BY_FIELD_COL</code> from, so users must provide samples with manually computed <code>PROVENANCE_BY_FIELD_COL</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.versioning.types.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.versioning.types.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If versioning_engine=\"native\" and data has wrong implementation</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: Frame | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `PROVENANCE_BY_FIELD_COL` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `PROVENANCE_BY_FIELD_COL` from, so users\n            must provide samples with manually computed `PROVENANCE_BY_FIELD_COL` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.versioning.types.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.versioning.types.Increment][] with eager Narwhals DataFrames.\n        versioning_engine: Override the store's versioning engine for this operation.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If versioning_engine=\"native\" and data has wrong implementation\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            PROVENANCE_BY_FIELD_COL: [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    filters = filters or defaultdict(list)\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    current_feature_filters = [*filters.get(feature.spec().key.to_string(), [])]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples\n        if current_feature_filters:\n            filtered_samples = samples.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n    else:\n        for upstream_spec in plan.deps or []:\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=filters.get(upstream_spec.key.to_string(), []),\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples is not None\n            and samples.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples.implementation,\n                        message=f\"Provided `samples` have implementation {samples.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples:\n            samples = switch_implementation_to_polars(samples)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        added, changed, removed = engine.resolve_increment_with_provenance(\n            current=current_metadata,\n            upstream=upstream_by_key,\n            hash_algorithm=self.hash_algorithm,\n            filters=filters_by_key,\n            sample=samples.lazy() if samples is not None else None,\n        )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existant Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existant Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: IntoFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Never writes to fallback stores.</p> </li> <li> <p>Project validation is performed unless disabled via <code>allow_cross_project_writes()</code> context manager.</p> </li> <li> <p>Must be called within <code>store.open(mode=AccessMode.WRITE)</code> context manager.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: IntoFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Never writes to fallback stores.\n\n        - Project validation is performed unless disabled via `allow_cross_project_writes()` context manager.\n\n        - Must be called within `store.open(mode=AccessMode.WRITE)` context manager.\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadatStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature)\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.native_implementation","title":"native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.create_versioning_engine","title":"create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = AccessMode.WRITE if self.auto_create_tables else AccessMode.READ\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.write_metadata_to_store","title":"write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame) -&gt; None\n</code></pre> <p>Internal write implementation using Ibis.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If table doesn't exist and auto_create_tables is False</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation using Ibis.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n\n    Raises:\n        TableNotFoundError: If table doesn't exist and auto_create_tables is False\n    \"\"\"\n    if df.implementation == nw.Implementation.IBIS:\n        df_to_insert = df.to_native()  # Ibis expression\n    else:\n        from metaxy._utils import collect_to_polars\n\n        df_to_insert = collect_to_polars(df)  # Polars DataFrame\n\n    table_name = self.get_table_name(feature_key)\n\n    try:\n        self.conn.insert(table_name, obj=df_to_insert)  # type: ignore[attr-defined]  # pyright: ignore[reportAttributeAccessIssue]\n    except Exception as e:\n        import ibis.common.exceptions\n\n        if not isinstance(e, ibis.common.exceptions.TableNotFound):\n            raise\n        if self.auto_create_tables:\n            # Warn about auto-create (first time only)\n            if self._should_warn_auto_create_tables:\n                import warnings\n\n                warnings.warn(\n                    f\"AUTO_CREATE_TABLES is enabled - automatically creating table '{table_name}'. \"\n                    \"Do not use in production! \"\n                    \"Use proper database migration tools like Alembic for production deployments.\",\n                    UserWarning,\n                    stacklevel=4,\n                )\n\n            # Note: create_table(table_name, obj=df) both creates the table AND inserts the data\n            # No separate insert needed - the data from df is already written\n            self.conn.create_table(table_name, obj=df_to_insert)\n        else:\n            raise TableNotFoundError(\n                f\"Table '{table_name}' does not exist for feature {feature_key.to_string()}. \"\n                f\"Enable auto_create_tables=True to automatically create tables, \"\n                f\"or use proper database migration tools like Alembic to create the table first.\"\n            ) from e\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = self.get_table_name(feature_key)\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    return f\"{self.__class__.__name__}(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            \"Destination store must be opened with store.open(AccessMode.WRITE) before use\"\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(AccessMode.READ):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.get_table_name","title":"get_table_name","text":"<pre><code>get_table_name(key: FeatureKey) -&gt; str\n</code></pre> <p>Generate the storage table name for a feature or system table.</p> <p>Applies the configured table_prefix (if any) to the feature key's table name. Subclasses can override this method to implement custom naming logic.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to convert to storage table name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Storage table name with optional prefix applied.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def get_table_name(\n    self,\n    key: FeatureKey,\n) -&gt; str:\n    \"\"\"Generate the storage table name for a feature or system table.\n\n    Applies the configured table_prefix (if any) to the feature key's table name.\n    Subclasses can override this method to implement custom naming logic.\n\n    Args:\n        key: Feature key to convert to storage table name.\n\n    Returns:\n        Storage table name with optional prefix applied.\n    \"\"\"\n    base_name = key.table_name\n\n    return f\"{self._table_prefix}{base_name}\" if self._table_prefix else base_name\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.ExtensionSpec","title":"ExtensionSpec  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB extension specification accepted by DuckDBMetadataStore.</p> <p>Supports additional keys for forward compatibility.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\\n\\nSupports additional keys for forward compatibility.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"repository\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Repository\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"ExtensionSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>repository</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckLakeConfigInput","title":"DuckLakeConfigInput  <code>module-attribute</code>","text":"<pre><code>DuckLakeConfigInput = DuckLakeAttachmentConfig | Mapping[str, Any]\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig","title":"DuckLakeAttachmentConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration payload used to attach DuckLake to a DuckDB connection.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"Configuration payload used to attach DuckLake to a DuckDB connection.\",\n  \"properties\": {\n    \"metadata_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Metadata Backend\",\n      \"type\": \"object\"\n    },\n    \"storage_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Storage Backend\",\n      \"type\": \"object\"\n    },\n    \"alias\": {\n      \"default\": \"ducklake\",\n      \"title\": \"Alias\",\n      \"type\": \"string\"\n    },\n    \"plugins\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Plugins\",\n      \"type\": \"array\"\n    },\n    \"attach_options\": {\n      \"additionalProperties\": true,\n      \"title\": \"Attach Options\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"metadata_backend\",\n    \"storage_backend\"\n  ],\n  \"title\": \"DuckLakeAttachmentConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metadata_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>storage_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>alias</code>                 (<code>str</code>)             </li> <li> <code>plugins</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>attach_options</code>                 (<code>dict[str, Any]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_coerce_backends</code>                 \u2192                   <code>metadata_backend</code>,                   <code>storage_backend</code> </li> <li> <code>_coerce_alias</code>                 \u2192                   <code>alias</code> </li> <li> <code>_coerce_plugins</code>                 \u2192                   <code>plugins</code> </li> <li> <code>_coerce_attach_options</code>                 \u2192                   <code>attach_options</code> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.metadata_sql_parts","title":"metadata_sql_parts","text":"<pre><code>metadata_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed metadata SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef metadata_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed metadata SQL components for DuckLake attachments.\"\"\"\n    return resolve_metadata_backend(self.metadata_backend, self.alias)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.storage_sql_parts","title":"storage_sql_parts","text":"<pre><code>storage_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed storage SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef storage_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed storage SQL components for DuckLake attachments.\"\"\"\n    return resolve_storage_backend(self.storage_backend, self.alias)\n</code></pre>"}]}