{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metaxy \ud83c\udf0c","text":"<p>Metaxy is a metadata layer for multi-modal Data and ML pipelines that manages and tracks metadata: sample versions, dependencies, and data lineage across complex computational graphs.</p> <p>It's agnostic to everything: compute engines, data storage, metadata storage.</p> <p>It has no strict infrastructure requirements and can use external databases for computations or run locally.</p> <p>It can scale to handle large amounts of big metadata.</p> <p>Giga Alpha</p> <p>This project is as raw as a steak still saying \u2018moo.\u2019</p>"},{"location":"#what-problem-exactly-does-metaxy-solve","title":"What problem exactly does Metaxy solve?","text":"<p>Data, ML and AI workloads processing large amounts of images, videos, audios, texts, or any other kind of data can be very expensive to run. In contrast to traditional data engineering, re-running the whole pipeline on changes is no longer an option. Therefore, it becomes crucially important to correctly implement incremental processing and sample-level versioning.</p> <p>Typically, a feature has to be re-computed in one of the following scenarios:</p> <ul> <li> <p>upstream data changes</p> </li> <li> <p>bug fixes or algorithmic changes</p> </li> </ul> <p>But correctly distinguishing these scenarios from cases where the feature should not be re-computed is a surprisingly challenging. Here are some of the cases where it would be undesirable:</p> <ul> <li> <p>merging two consecutive steps into one (refactoring the graph topology)</p> </li> <li> <p>partial data updates, e.g. changing only the audio track inside a video file</p> </li> <li> <p>backfilling metadata from another source</p> </li> </ul> <p>Tracking and propagating these changes correctly to the right subset of samples and features can become incredibly complicated. Until now, a general solution for this problem did not exist, but this is not the case anymore.</p>"},{"location":"#metaxy-to-the-rescue","title":"Metaxy to the rescue","text":"<p>Metaxy solves the first set of problems with a feature and field dependency system, and the second with a migrations system.</p> <p>Metaxy builds a versioned graphs from feature definitions and tracks version changes. This graph can be snapshotted and saved at any point of time, typically during pipeline deployment. Here is an example of a graph diff produced by a code_version update on the <code>audio</code> field of the <code>example/video</code> feature:</p> <pre><code>---\ntitle: Propagation Of Changes\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>The key observation here is that <code>example/face_detection</code>'s <code>faces</code> field did not receive a new version, because it does not depend on the <code>audio</code> field that has been updated upstream.</p>"},{"location":"#about-metaxy","title":"About Metaxy","text":"<p>Metaxy is:</p> <ul> <li> <p>\ud83e\udde9 composable --- bring your own everything!</p> <ul> <li>supports DuckDB, ClickHouse, and 20+ databases via Ibis</li> <li>supports lakehouse storage formats such as DeltaLake or DuckLake</li> <li>is agnostic to tabular compute engines: Polars, Spark, Pandas, and databases thanks to Narwhals</li> <li>we totally don't care how is the multi-modal data produced or where is it stored: Metaxy is responsible for yielding input metadata and writing output metadata</li> </ul> </li> <li> <p>\ud83e\udd38 flexible to work around restrictions consciously:</p> <ul> <li>features are defined as Pydantic models, leveraging Pydantic's type safety guarantees, rich validation system, and allowing inheritance patterns to stay DRY</li> <li>has a migrations system to compensate for reconciling field provenances and metadata when computations are not desired</li> </ul> </li> <li> <p>\ud83e\udea8 rock solid when it matters:</p> <ul> <li>field provenance is guaranteed to be consistent across DBs or in-memory compute engines. We really have tested this very well!</li> <li>changes to topology, feature versioning, or individual samples ruthlessly propagate downstream</li> <li>unique field-level dependency system prevents unnecessary recomputations for features that depend on partial data</li> <li>metadata is append-only to ensure data integrity and immutability. Users can perform cleanup if needed (Metaxy provides tools for this).</li> </ul> </li> <li> <p>\ud83d\udcc8 scalable:</p> <ul> <li>supports feature organization and discovery patterns such as packaging entry points. This enables collaboration across teams and projects.</li> <li>is built with performance in mind: all operations default to run in the DB, Metaxy does not stand in the way of metadata flow</li> </ul> </li> <li> <p>\ud83e\uddd1\u200d\ud83d\udcbb dev friendly:</p> <ul> <li>clean, intuitive Python API that stays out of your way when you don't need it</li> <li>feature discovery system for effortless dependency management</li> <li>comprehensive type hints and Pydantic integration for excellent IDE support</li> <li>first-class support for local development, testing, preview environments, CI/CD</li> <li>CLI tool for easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats</li> <li>integrations with popular tools such as SQLModel, Dagster, and Ray.</li> <li>testing helpers that you're going to appreciate</li> </ul> </li> </ul>"},{"location":"#feature-dependencies","title":"Feature Dependencies","text":"<p>Features form a DAG where each feature declares its upstream dependencies. Consider an video processing pipeline:</p> <pre><code>class Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"video\",\n        fields=[\n            # simple field with only the key defined and the default code_version used\n            \"frames\",\n            # let's version this one!\n            FieldSpec(name=\"audio\", code_version=\"1\"),\n        ],\n    ),\n):\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n\n\nclass VoiceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"voice_detection\",\n        deps=[Video],\n        fields=[\n            \"frames\",   # dependency automatically mapped into Video.frames\n            \"audio\",  # dependency automatically mapped into Video.audio\n        ]\n    ),\n):\n    path: str = Field(description=\"Path to the voice detection json file\")\n</code></pre> <p>Note</p> <p>This API will be improved with more ergonomic alternatives. See issue #70 for details.</p> <p>When <code>Video</code> changes, Metaxy automatically identifies that <code>VoiceDetection</code> requires recomputation.</p>"},{"location":"#versioned-change-propagation","title":"Versioned Change Propagation","text":"<p>Every feature definition produces a deterministic version hash computed from its dependencies, fields, and code versions. When you modify a feature\u2014whether changing its dependencies, adding fields, or updating transformation logic, Metaxy detects the change and propagates it downstream. This is done on multiple levels: <code>Feature</code> level, field level, and of course on sample level: each row in the metadata store tracks the version of each field and the feature-level version.</p> <p>This ensures that when feature definitions evolve, every feature that transitively depends on it can be systematically updated. Because Metaxy supports declaring dependencies on fields, it can identify when a feature does not require recomputation, even if one of its parents has been changed (but only irrelevant fields did). This is a huge factor in improving efficiency and reducing unnecessary computations (and costs!).</p> <p>Because Metaxy feature graphs are static, Metaxy can calculate field provenance changes ahead of the actual computation. This enables patterns such as computation preview and computation cost prediction.</p>"},{"location":"#typical-user-workflow","title":"Typical User Workflow","text":""},{"location":"#1-record-metaxy-feature-graph-in-cicd","title":"1. Record Metaxy feature graph in CI/CD","text":"<p>Invoke the <code>metaxy</code> CLI:</p> <pre><code>metaxy graph push\n</code></pre> <p>This can be skipped in non-production environments.</p>"},{"location":"#2-get-a-resolved-metadata-increment-from-metaxy","title":"2. Get a resolved metadata increment from Metaxy","text":"<p>Use <code>metaxy.MetadataStore.resolve_update</code> to identify samples requiring recomputation:</p> <pre><code>from metaxy import init_metaxy\n\n# discover and load Metaxy features\ninit_metaxy()\n\nstore = (\n    ...\n)  # can be DuckDBMetadataStore locally and ClickHouseMetadataStore in production\ndiff = store.resolve_update(VoiceDetection)\n</code></pre> <p><code>metaxy.MetadataStore.resolve_update</code> runs in the database with an optional fallback to use Polars in-memory (and the two workflows are guaranteed to produce consistent results). The returned object provides Narwhals lazy dataframes which are backend agnostic -- can run on Polars, Pandas, PySpark, or an extenral DB, and have all the field provenances already computed.</p>"},{"location":"#3-run-user-defined-computation-over-the-metadata-increment","title":"3. Run user-defined computation over the metadata increment","text":"<p>Metaxy is not involved in this step at all.</p> <pre><code>if (len(diff.added) + len(diff.changed)) &gt; 0:\n    # run your computation, this can be done in a distributed manner\n    results = run_voice_detection(diff, ...)\n</code></pre>"},{"location":"#4-record-metadata-for-computed-samples","title":"4. Record metadata for computed samples","text":"<p>This can be done in a distributed manner as well, and the recommended pattern is to write metadata as soon as it becomes available to avoid losing progress in case of interruptions or failures.</p> <pre><code>store.write_metadata(VoiceDetection, results)\n</code></pre> <p>We have now successfully recorded the metadata for the computed samples! Processed samples will no longer be returned by <code>MetadataStore.resolve_update</code> during future pipeline runs.</p> <p>No Uniqueness Checks!</p> <p>Metaxy doesn't attempt to perform any deduplication or uniqueness checks for performance reasons. While <code>MetadataStore.resolve_update</code> is guaranteed to never return the same versioned sample twice (hey that's the whole point of Metaxy), it's up to the user to ensure that samples are not written multiple times to the metadata store. Configuring deduplication or uniqueness checks in the store (database) is a good idea.</p>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>Learn more about feature definitions or versioning</p> </li> <li> <p>Use Metaxy from the command line</p> </li> <li> <p>Learn how to configure Metaxy</p> </li> <li> <p>Get lost in our API Reference</p> </li> </ul>"},{"location":"learn/data-versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static, deterministic and idempotent.</p>"},{"location":"learn/data-versioning/#versioning_1","title":"Versioning","text":"<p>Feature and field versions are defined by the feature graph topology and the user-provided code versions of fields. Sample versions are defined by upstream sample versions and the code versions of the fields defined on the sample's feature.</p> <p>All versions are computed ahead of time: feature and field versions can be immediately derived from code (and we keep historical graph snapshots for them), and calculating sample versions requires access to the metadata store.</p> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"learn/data-versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them.</p>"},{"location":"learn/data-versioning/#field-level","title":"Field Level","text":"<ul> <li>Field Code Version is defined on the field and is provided by the user (defaults to <code>\"__metaxy_initial__\"</code>)</li> </ul> <p>Code Version Value</p> <p>The value can be arbitrary, but in the future we might implement something around semantic versioning.</p> <ul> <li>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</li> </ul>"},{"location":"learn/data-versioning/#feature-level","title":"Feature Level","text":"<ul> <li>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</li> <li>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</li> </ul>"},{"location":"learn/data-versioning/#graph-level","title":"Graph Level","text":"<ul> <li>Snapshot Version: is computed from the Feature Versions of all features defined on the graph.</li> </ul> <p>Why Do We Need Snapshot Version?</p> <p>This value is used to uniquely encode versioned feature graph topology in historical snapshots.</p>"},{"location":"learn/data-versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to compute them.</p> <ul> <li>Provenance By Field is computed from the upstream Provenance By Fields (with respect to defined field-level dependencies and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</li> </ul> sample_uid provenance_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <ul> <li>Sample Version is derived from the Provenance By Field by simply hashing it.</li> </ul> <p>This is the end game of the versioning system. It ensures that only the necessary samples are recomputed when a feature version changes. It acts as source of truth for resolving incremental updates for feature metadata.</p>"},{"location":"learn/data-versioning/#practical-example","title":"Practical Example","text":"<p>Consider a video processing pipeline with these features:</p> <pre><code>from metaxy import (\n    Feature,\n    FeatureDep,\n    FeatureSpec,\n    FieldDep,\n    FieldSpec,\n)\n\n\nclass Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/video\",\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/crop\",\n        deps=[FeatureDep(feature=Video)],\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass  # omit columns for the sake of simplicity\n\n\nclass FaceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/face_detection\",\n        deps=[\n            FeatureDep(\n                feature=Crop,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"faces\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Crop,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n\n\nclass SpeechToText(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/stt\",\n        deps=[\n            FeatureDep(\n                feature=Video,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"transcription\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n</code></pre> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: 8468950d\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc9ca835)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 22742381)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: 794116a9)&lt;/small&gt;&lt;/div&gt;\"]\n        example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 3ac04df8)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 76c8bdc9)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: abc79017)&lt;/small&gt;&lt;/div&gt;\"]\n        example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 1ac83b07)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces &lt;small&gt;(v: 2d75f0bd)&lt;/small&gt;&lt;/div&gt;\"]\n        example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: c83a754a)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 transcription &lt;small&gt;(v: ac412b3c)&lt;/small&gt;&lt;/div&gt;\"]\n        example_video --&gt; example_crop\n        example_crop --&gt; example_face_detection\n        example_video --&gt; example_stt</code></pre>"},{"location":"learn/data-versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>Video</code> feature changes (perhaps denoising was applied):</p> <pre><code>         key=\"example/video\",\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n</code></pre> <p>Run <code>metaxy graph diff</code> to see what changed:</p> <pre><code>---\ntitle: Merged Graph Diff\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>Notice:</p> <ul> <li><code>Video</code>, <code>Crop</code>, and <code>SpeechToText</code> changed (highlighted)</li> <li><code>FaceDetection</code> remained unchanged (depends only on <code>frames</code>, not <code>audio</code>)</li> <li>Audio field versions changed throughout the graph</li> <li>Frame field versions stayed the same</li> </ul>"},{"location":"learn/data-versioning/#incremental-computation","title":"Incremental Computation","text":"<p>The metadata store's <code>calculate_provenance_by_field()</code> method:</p> <ol> <li>Joins upstream feature metadata</li> <li>Computes sample versions</li> <li>Compares against existing metadata</li> <li>Returns diff: added, changed, removed samples</li> </ol> <p>Typically, steps 1-3 can be run directly in the database. Analytical databases such as ClickHouse or Snowflake can efficiently handle these operations.</p> <p>The Python pipeline then processes only the delta</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes provenance_by_field and identifies changes\n    diff = store.resolve_update(MyFeature)\n\n    # Process only changed samples\n</code></pre> <p>The <code>diff</code> object has attributes for new upstream samples, samples with new versions, and samples that have been removed from upstream metadata.</p> <p>This approach avoids expensive recomputation when nothing changed, while ensuring correctness when dependencies update.</p>"},{"location":"learn/feature-definitions/","title":"Feature System","text":"<p>Metaxy has a declarative (defined statically at class level), expressive, flexible feature system. It has been inspired by Software-Defined Assets in Dagster.</p> <p>Features represent tabular metadata, typically containing references to external multi-modal data such as files, images, or videos. But it can be just pure metadata as well.</p> <p>I will highlight data and metadata with bold so it really stands out.</p> <p>Metaxy is responsible for providing correct metadata to users. During incremental processing, Metaxy will automatically resolve added, changed and deleted metadata rows and calculate the right sample versions for them. Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage (it's a good idea to inject the sample version into the data sample identifier). Metaxy is designed to be used with systems that do not overwrite existing metadata (Metaxy only appends metadata) and therefore data as well (while we cannot enforce that since the user is responsible for writing the data, it's easily achievable by including the sample version into the data sample identifier).</p> <p>I hope we can stop using bold for data and metadata from now on, hopefully we've made our point.</p> <p>Include sample version in your data path</p> <p>Include the sample version in your data path to ensure strong consistency guarantees. I mean it. Really do it!</p> <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly). Features are bound to a specific Metaxy project, but can be moved between projects over time. Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p>"},{"location":"learn/feature-definitions/#feature-specs","title":"Feature Specs","text":"<p>Before we can define a <code>Feature</code>, we must first create a <code>FeatureSpec</code> object. But before we get to an example, it's necessary to understand the concept of ID columns. Metaxy must know how to uniquely identify feature samples and join metadata tables, therefore, you need to attach one or more ID columns to your <code>FeatureSpec</code>. Very often these ID columns would stay the same across many feature specs, therefore it makes a lot of sense to define them on a shared base class.</p> <p>Some boilerplate with typing is involved (this is typically a good thing):</p> <pre><code>from typing import TypeAlias\n\nfrom metaxy import BaseFeatureSpec\n\n\nVideoIds: TypeAlias = tuple[str]\n\n\nclass VideoFeatureSpec(BaseFeatureSpec):\n    id_columns: VideoIds = (\"video_id\",)\n</code></pre> <p><code>BaseFeatureSpec</code> is a Pydantic model, so all normal Pydantic features apply.</p> <p>With our <code>VideoFeatureSpec</code> in place, we can proceed to defining features that would be using it.</p>"},{"location":"learn/feature-definitions/#feature-definitions","title":"Feature Definitions","text":"<p>Metaxy provides a <code>BaseFeature</code> class that can be extended to make user-defined features. It's a Pydantic model as well. User-defined <code>BaseFeature</code> classes must have fields matching ID columns of the <code>FeatureSpec</code> they are using.</p> <p>With respect to the same DRY principle, we can define a shared base class for features that use the <code>VideoFeatureSpec</code>.</p> <pre><code>from metaxy import BaseFeature\n\n\nclass BaseVideoFeature(\n    BaseFeature, spec=None\n):  # spec=None is important to tell Metaxy this is a base class\n    video_id: str\n</code></pre> <p>Now we are finally ready to define an actual feature.</p> <pre><code>class VideoFeature(BaseVideoFeature, spec=VideoFeatureSpec(key=\"/raw/video\")):\n    path: str\n</code></pre> <p>That's it! That's a root feature, it doesn't have any dependencies. Easy.</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    BaseVideoFeature,\n    spec=VideoFeatureSpec(key=\"/processed/transcript\", deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> <p>Hurray! You get the idea.</p>"},{"location":"learn/feature-definitions/#field-level-dependencies","title":"Field-Level Dependencies","text":"<p>A core (I'll be straight: a killer) feature of Metaxy is the concept of field-level dependencies. These are used to define dependencies between logical fields of features.</p> <p>A field is not to be confused with metadata column (Pydantic fields). Fields are completely independent from them.</p> <p>Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy.</p> <p>Fields refer to data and are logical -- users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>Video</code> feature -- an <code>.mp4</code> file -- may have <code>frames</code> and <code>audio</code> fields.</p> <p>Downstream features can depend on specific fields of upstream features. This enables fine-grained control over field provenance, avoiding unnecessary reprocessing.</p> <p>At this point, careful readers have probably noticed that the <code>Transcript</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express that with Metaxy:</p> <pre><code>from metaxy import FieldDep, FieldSpec\n\nvideo_spec = VideoFeatureSpec(key=\"/raw/video\", fields=[\"audio\", \"frames\"])\n\n\nclass VideoFeature(BaseVideoFeature, spec=video_spec):\n    path: str\n\n\ntranscript_spec = TranscriptFeatureSpec(\n    key=\"/raw/transcript\",\n    fields=[\n        FieldSpec(\n            key=\"text\",\n            deps=[FieldDep(feature=VideoFeature.spec().key, fields=[\"audio\"])],\n        )\n    ],\n)\n\n\nclass TranscriptFeature(BaseTranscriptFeature, spec=transcript_spec):\n    path: str\n</code></pre> <p>Voil\u00e0!</p> <p>Use boilerplate-free API</p> <p>Metaxy allows passing simplified types to some of the models like <code>FeatureSpec</code> or <code>FeatureKey</code>. See syntactic sugar for more details.</p> <p>The Data Versioning docs explain more about how Metaxy calculates versions for different components of a feature graph.</p>"},{"location":"learn/feature-definitions/#attaching-user-defined-metadata","title":"Attaching user-defined metadata","text":"<p>Users can attach arbitrary JSON-like metadata dictionary to feature specs, typically used for declaring ownership, providing information to third-party tooling, or documentation purposes. This metadata does not influence graph topology or the versioning system.</p>"},{"location":"learn/feature-definitions/#fully-qualified-field-key","title":"Fully Qualified Field Key","text":"<p>A fully qualified field key (FQFK) is an identifier that uniquely identifies a field within the whole feature graph. It consists of the feature key and the field key, separated by a colon, for example: <code>/raw/video:frames</code>, <code>/raw/video:audio/english</code>.</p>"},{"location":"learn/feature-discovery/","title":"Feature Discovery","text":"<p>Metaxy provides automatic feature discovery through Python's entrypoint system. This enables modular architecture patterns essential for scaling Metaxy projects.</p>"},{"location":"learn/feature-discovery/#why-feature-discovery","title":"Why Feature Discovery?","text":"<p>Manual feature registration doesn't scale. As your system grows, you need:</p> <ul> <li>Plugin architectures - Third-party teams contribute features without modifying core code</li> <li>Feature collections - Package and distribute related features as installable units</li> <li>Monorepo support - Discover features across multiple packages in a monorepo</li> <li>Internal packages - Share features between projects via private package registries</li> </ul> <p>Feature discovery solves these problems through automatic registration at import time.</p>"},{"location":"learn/feature-discovery/#package-entry-points","title":"Package Entry Points","text":"<p>The most powerful discovery mechanism uses Python's standard entry point system via a well-known <code>\"metaxy.project\"</code> entrypoint group in the package metadata.</p>"},{"location":"learn/feature-discovery/#creating-a-feature-plugin","title":"Creating a Feature Plugin","text":"<p>Structure your feature package:</p> <pre><code>my-video-features/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 my_video_features/\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 detection.py\n        \u2514\u2500\u2500 transcription.py\n</code></pre> <p>Declare entry points in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-video-features\"\nversion = \"1.0.0\"\ndependencies = [\"metaxy\"]\n\n[project.entry-points.\"metaxy.project\"]\nmy-video-features = \"my_video_features\"\n</code></pre> <p>The entry point name is your project name. The value can be either:</p> <ul> <li>Function syntax (<code>module:function</code>) - Points to a callable function that will be invoked to load features. Useful when you need conditional loading or setup logic.</li> <li>Module syntax (<code>module</code>) - Points directly to a module containing Feature definitions. Simply importing the module registers the features.</li> </ul> <p>One Entry Point Per Package</p> <p>Each package can only declare one entry point in the <code>metaxy.project</code> group, since <code>metaxy.toml</code> only supports a single <code>project</code> field.</p> <p>To organize features into logical groups within a package, use submodules and import them from your entry point function.</p>"},{"location":"learn/feature-discovery/#installing-and-using-feature-plugins","title":"Installing and Using Feature Plugins","text":"<p>Install the package:</p> <pre><code>pip install my-video-features\n# Or in a monorepo:\npip install -e ./packages/my-video-features\n</code></pre> <p>UV Package Manager: Entry Point Changes</p> <p>If you're using <code>uv</code> and modify entry points in <code>pyproject.toml</code>, <code>uv sync</code> will not recreate the editable package metadata. You must explicitly reinstall:</p> <pre><code>uv sync --reinstall-package my-video-features my-video-features\n</code></pre>"},{"location":"learn/feature-discovery/#monorepo-patterns","title":"Monorepo Patterns","text":"<p>In monorepos, use entry points to manage feature collections across teams:</p>"},{"location":"learn/feature-discovery/#team-owned-feature-packages","title":"Team-Owned Feature Packages","text":"<pre><code>monorepo/\n\u251c\u2500\u2500 packages/\n\u2502   \u251c\u2500\u2500 core-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u251c\u2500\u2500 ml-features/\n\u2502   \u2502   \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2502   \u2514\u2500\u2500 experimental-features/\n\u2502       \u2514\u2500\u2500 pyproject.toml  # [project.entry-points.\"metaxy.features\"]\n\u2514\u2500\u2500 apps/\n    \u2514\u2500\u2500 main-pipeline/\n        \u2514\u2500\u2500 pyproject.toml  # depends on feature packages\n</code></pre> <p>Each team maintains their features independently:</p> <pre><code># packages/ml-features/pyproject.toml\n[project.entry-points.\"metaxy.project\"]\nml-features = \"ml_features.load\"\n</code></pre> <pre><code># packages/core-features/pyproject.toml\n[project.entry-points.\"metaxy.project\"]\ncore-features = \"core_features.load\"\n</code></pre> <p>The main application imports features from all installed packages, and each feature automatically knows its project based on the entry point.</p>"},{"location":"learn/feature-discovery/#config-based-discovery","title":"Config-Based Discovery","text":"<p>For simpler use cases that don't require distribution, you can specify module paths directly in configuration:</p> metaxy.tomlpyproject.toml <pre><code>project = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre>"},{"location":"learn/feature-discovery/#best-practices","title":"Best Practices","text":"<ol> <li>Use entry points for distribution - Any features intended for reuse should use entry points</li> <li>Version your feature packages - Use semantic versioning for feature collections</li> <li>Test in isolation - Load feature packages into test graphs to verify behavior</li> </ol> <p>The entry point system transforms feature management from a manual process to an automatic, scalable system that grows with your organization.</p>"},{"location":"learn/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: MetadataStore.</p> <p>Metadata stores expose methods for reading, writing, deleting metadata, and the most important one: resolve_update for receiving a metadata increment. Metaxy intentionally does not support mutating metadata in-place for performance reasons. Deletes are not required during normal operations, but they are still supported since users would want to eventually delete stale metadata and data.</p> <p>Metadata reads/writes are not guaranteed to be ACID: Metaxy is designed to interact with analytical databases which lack ACID guarantees by definition and design (for performance reasons). However, Metaxy guarantees to never attempt to retrieve the same sample version twice, so as long as users do not write it twice (or have deduplication configured inside the metadata store) we should be all good.</p> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store. This includes joining upstream features, hashing their versions, and filtering out samples that have already been processed.</p> <p>There are 3 cases where this is done in-memory instead (with the help of polars-hash):</p> <ol> <li>The metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</li> <li>The user explicitly requested to keep the computations in-memory (<code>MetadataStore(..., prefer_native=False)</code>)</li> <li>When having to use a fallback store to retrieve one of the parent features.</li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p> <p>Learn about configuring metadata stores here</p>"},{"location":"learn/metadata-stores/#fallback-stores","title":"Fallback Stores","text":"<p>Fallback stores are a powerful feature that allow stores to read feature metadata from other stores (only if it's missing in the primary store). This is very useful for development, as production data can be retrieved immediately without populating the development environment. This is especially useful for ephemeral environments such as branch/preview deployments (typically created by CI/CD for pull requests) or integration testing environments.</p>"},{"location":"learn/metadata-stores/#project-write-validation","title":"Project Write Validation","text":"<p>By default, <code>MetadataStore</code> raises a <code>ValueError</code> when attempting to write to a project that doesn't match the expected project from <code>MetaxyConfig.get().project</code>.</p> <p>For legitimate cross-project operations (such as migrations that need to update features across multiple projects), an escape hatch is provided via the <code>allow_cross_project_writes()</code> context manager:</p> <pre><code># Normal operation - writes are validated against expected project\nwith store:\n    store.write_metadata(feature_from_my_project, metadata)  # OK\n    store.write_metadata(feature_from_other_project, metadata)  # Raises ValueError\n\n# Migration scenario - temporarily allow cross-project writes\nwith store:\n    with store.allow_cross_project_writes():\n        store.write_metadata(feature_from_project_a, metadata_a)  # OK\n        store.write_metadata(feature_from_project_b, metadata_b)  # OK\n</code></pre>"},{"location":"learn/syntactic-sugar/","title":"Syntactic Sugar","text":""},{"location":"learn/syntactic-sugar/#type-coercion-for-input-types","title":"Type Coercion For Input Types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent feature keys, their fields, and the dependencies between them.</p> <p>To avoid boilerplate, Metaxy also has syntactic sugar for construction of these classes. Different ways to provide them are automatically coerced into canonical internal models. This is fully typed and only affects constructor arguments, so accessing attributes on Metaxy models will always return only the canonical types.</p> <p>Some examples:</p> <pre><code>from metaxy import FeatureKey\n\nkey = FeatureKey(\"prefix/feature\")\nkey = FeatureKey([\"prefix\", \"feature\"])\nkey = FeatureKey(\"prefix\", \"feature\")\nsame_key = FeatureKey(key)\n</code></pre> <p>Metaxy really loves you, the user!</p>"},{"location":"learn/syntactic-sugar/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> accept:</p> <ul> <li>String format: <code>FeatureKey(\"prefix/feature\")</code></li> <li>Sequence format: <code>FeatureKey([\"prefix\", \"feature\"])</code></li> <li>Variadic format: <code>FeatureKey(\"prefix\", \"feature\")</code></li> <li>Same type: <code>FeatureKey(another_feature_key)</code> -- for full Inception mode</li> </ul> <p>All formats produce equivalent keys, internally represented as a sequence of parts.</p>"},{"location":"learn/syntactic-sugar/#featurespec","title":"<code>FeatureSpec</code>","text":""},{"location":"learn/syntactic-sugar/#fields","title":"Fields","text":"<p>FieldSpec can be passed to FeatureSpec as a string that represents the field key:</p> <pre><code>spec = FeatureSpec(\n    ..., fields=[\"my/field\", FieldSpec(key=\"field/with/version\", code_version=\"v1.2.3\")]\n)\n</code></pre>"},{"location":"learn/testing/","title":"Testing Metaxy Features","text":"<p>This guide covers patterns for testing your features when using Metaxy.</p>"},{"location":"learn/testing/#graph-isolation","title":"Graph Isolation","text":"<p>By default, Metaxy uses a single global feature graph where all features register themselves automatically. During testing, you might want to construct your own, clean and isolated graphs.</p>"},{"location":"learn/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>@pytest.fixture(autouse=True)\ndef graph():\n    with FeatureGraph().use():\n        yield graph\n\n\ndef test_my_feature(graph: FeatureGraph):\n    class MyFeature(Feature, spec=...):\n        pass\n\n    # Test operations here\n\n    # inspect the graph object if needed\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one. Multiple graphs can exist at the same time, but only one will be used for feature registration.</p>"},{"location":"learn/testing/#graph-context-management","title":"Graph Context Management","text":"<p>The active graph uses context variables to support multiple graphs:</p> <pre><code># Default global graph (used in production)\ngraph = FeatureGraph()\n\n# Get active graph\nactive = FeatureGraph.get_active()\n\n# Use custom graph temporarily\nwith custom_graph.use():\n    # All operations use custom_graph\n    pass\n</code></pre> <p>This enables:</p> <ul> <li>Isolated testing: Each test gets its own feature registry</li> <li>Migration testing: Load historical graphs for migration scenarios</li> <li>Multi-environment testing: Test different feature configurations</li> </ul>"},{"location":"learn/testing/#testing-metadata-store-operations","title":"Testing Metadata Store Operations","text":""},{"location":"learn/testing/#context-manager-pattern","title":"Context Manager Pattern","text":"<p>Stores must be used as context managers to ensure proper resource cleanup:</p> <pre><code>def test_metadata_operations():\n    with InMemoryMetadataStore() as store:\n        # Create test data\n        df = pl.DataFrame(\n            {\n                \"sample_uid\": [1, 2, 3],\n                \"provenance_by_field\": {...},\n                \"feature_version\": \"abc123\",\n            }\n        )\n\n        # Write metadata\n        store.write_metadata(MyFeature, df)\n\n        # Read and verify\n        result = store.read_metadata(MyFeature)\n        assert len(result) == 3\n</code></pre>"},{"location":"learn/testing/#testing-with-different-backends","title":"Testing with Different Backends","text":"<p>Use parametrized tests to verify behavior across backends:</p> <pre><code>import pytest\n\n\n@pytest.mark.parametrize(\n    \"store_cls\",\n    [\n        InMemoryMetadataStore,\n        DuckDBMetadataStore,\n    ],\n)\ndef test_store_behavior(store_cls, tmp_path):\n    # Use tmp_path for file-based stores\n    store_kwargs = {}\n    if store_cls != InMemoryMetadataStore:\n        store_kwargs[\"path\"] = tmp_path / \"test.db\"\n\n    with store_cls(**store_kwargs) as store:\n        # Test your feature operations\n        pass\n</code></pre>"},{"location":"learn/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing AUTO_CREATE_TABLES Warnings","text":"<p>When testing with <code>auto_create_tables=True</code>, Metaxy emits warnings to remind you not to use this in production. These warnings are important for production safety, but can clutter test output.</p> <p>To suppress these warnings in your test suite, use pytest's <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but pytest filters it from test output.</p> <p>Testing the Warning Itself</p> <p>If you need to verify that the warning is actually emitted, use <code>pytest.warns()</code>:</p> <pre><code>import pytest\n\n\ndef test_auto_create_tables_warning():\n    with pytest.warns(\n        UserWarning, match=r\"AUTO_CREATE_TABLES is enabled.*do not use in production\"\n    ):\n        with DuckDBMetadataStore(\":memory:\", auto_create_tables=True) as store:\n            pass  # Warning is emitted and captured\n</code></pre> <p>This works even with <code>filterwarnings</code> configured, because <code>pytest.warns()</code> explicitly captures and verifies the warning.</p>"},{"location":"learn/testing/#testing-custom-alignment","title":"Testing Custom Alignment","text":"<p>If your feature overrides <code>load_input()</code> for custom alignment, test it thoroughly:</p> <pre><code>def test_custom_alignment():\n    # Prepare test data\n    current = pl.DataFrame({\"sample_uid\": [1, 2, 3], \"custom_field\": [\"a\", \"b\", \"c\"]})\n\n    upstream = {\n        \"video_feature\": pl.DataFrame(\n            {\"sample_uid\": [2, 3, 4], \"provenance_by_field\": {...}}\n        )\n    }\n\n    # Test alignment logic\n    result = MyFeature.load_input(current, upstream)\n\n    # Verify behavior\n    assert set(result[\"sample_uid\"].to_list()) == {2, 3}  # Inner join\n    assert \"custom_field\" in result.columns  # Custom fields preserved\n</code></pre>"},{"location":"learn/testing/#testing-feature-dependencies","title":"Testing Feature Dependencies","text":"<p>Verify that dependencies are correctly defined:</p> <pre><code>def test_feature_dependencies():\n    test_graph = FeatureGraph()\n\n    with test_graph.use():\n        # Define upstream feature\n        class UpstreamFeature(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"upstream\"]),\n                fields=[FieldSpec(key=FieldKey([\"data\"]), code_version=\"1\")],\n            ),\n        ):\n            pass\n\n        # Define downstream feature with dependency\n        class DownstreamFeature(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"downstream\"]),\n                deps=[FeatureDep(feature=FeatureKey([\"upstream\"]))],\n                fields=[\n                    FieldSpec(\n                        key=FieldKey([\"processed\"]),\n                        code_version=\"1\",\n                        deps=[\n                            FieldDep(\n                                feature=FeatureKey([\"upstream\"]),\n                                fields=[FieldKey([\"data\"])],\n                            )\n                        ],\n                    )\n                ],\n            ),\n        ):\n            pass\n\n        # Verify graph structure\n        assert len(test_graph.features_by_key) == 2\n        assert UpstreamFeature in test_graph.get_downstream(DownstreamFeature)\n</code></pre>"},{"location":"learn/testing/#testing-migrations","title":"Testing Migrations","text":""},{"location":"learn/testing/#simulating-feature-changes","title":"Simulating Feature Changes","text":"<p>Test how your features behave when definitions change:</p> <pre><code>def test_migration_scenario():\n    # Initial version\n    graph_v1 = FeatureGraph()\n    with graph_v1.use():\n\n        class MyFeatureV1(\n            Feature,\n            spec=FeatureSpec(\n                key=FeatureKey([\"my_feature\"]),\n                fields=[FieldSpec(key=FieldKey([\"field1\"]), code_version=\"1\")],\n            ),\n        ):\n            pass\n\n    # Record initial state\n    with InMemoryMetadataStore() as store:\n        store.record_feature_graph_snapshot(graph_v1)\n\n        # Modified version\n        graph_v2 = FeatureGraph()\n        with graph_v2.use():\n\n            class MyFeatureV2(\n                Feature,\n                spec=FeatureSpec(\n                    key=FeatureKey([\"my_feature\"]),\n                    fields=[FieldSpec(key=FieldKey([\"field1\"]), code_version=\"2\")],\n                ),\n            ):\n                pass\n\n        # Verify version change detected\n        with graph_v2.use():\n            changes = store.detect_feature_changes(MyFeatureV2)\n            assert changes is not None\n</code></pre>"},{"location":"learn/testing/#testing-migration-idempotency","title":"Testing Migration Idempotency","text":"<p>Ensure migrations can be safely re-run:</p> <pre><code>def test_migration_idempotency():\n    with InMemoryMetadataStore() as store:\n        # Apply migration twice\n        apply_migration(store, migration)\n        result1 = store.read_metadata(MyFeature)\n\n        apply_migration(store, migration)  # Should be no-op\n        result2 = store.read_metadata(MyFeature)\n\n        # Results should be identical\n        assert result1.equals(result2)\n</code></pre>"},{"location":"learn/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Always use isolated graphs - Never rely on the global graph in tests</li> <li>Use context managers - Ensure proper cleanup of stores and resources</li> <li>Test across backends - Verify features work with different metadata stores</li> <li>Test edge cases - Empty data, missing dependencies, version conflicts</li> <li>Mock external dependencies - Isolate tests from external services</li> <li>Verify determinism - Feature versions should be consistent across runs</li> </ol>"},{"location":"learn/integrations/sqlmodel/","title":"SQLModel Integration","text":"<p>The SQLModel integration enables Metaxy features to function as both metadata-tracked features and SQLAlchemy ORM models.</p> <p>This integration combines Metaxy's versioning and dependency tracking with SQLModel's database mapping and query capabilities.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores. The benefits of using SQLModel are mostly in the ability to use migration systems such as Alembic that can ensure schema consistency with Metaxy features, and provide the tools for schema evolution as the features change over time.</p>"},{"location":"learn/integrations/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install metaxy[sqlmodel]\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#basic-usage","title":"Basic Usage","text":"<p>The integration has to be enabled in the configuration file:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_ENABLE=true\n</code></pre> <p>This will expose Metaxy's system tables to SQLAlchemy.</p> <p>First, as always with Metaxy features, we would have to define our ID columns:</p> <pre><code>from metaxy import BaseFeatureSpec\nfrom metaxy.ext.sqlmodel import BaseSQLModelFeature\n\n\nclass SampleFeatureSpec(BaseFeatureSpec):\n    id_columns: tuple[str] = \"sample_id\"\n\n\nclass SampleFeature(BaseSQLModelFeature, table=False, spec=None):\n    sample_id: str\n</code></pre> <p>Note that ID columns cannot be server-generated, so the cannot include a Primary Key.</p> <p>Now we can define feature class that inherits from <code>SampleFeature</code> and specify both Metaxy's <code>spec</code> parameter and SQLModel's <code>table=True</code> parameter:</p> <pre><code>from metaxy import FeatureKey, FieldSpec, FieldKey\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    SampleFeature,\n    table=True,\n    spec=SampleFeatureSpec(\n        key=FeatureKey([\"video\"]),\n        # Root feature with no dependencies\n        fields=[\n            FieldSpec(key=FieldKey([\"frames\"]), code_version=\"1\"),\n            FieldSpec(key=FieldKey([\"duration\"]), code_version=\"1\"),\n        ],\n    ),\n):\n    # User-defined metadata columns\n    path: str\n    duration: float\n</code></pre> <p>This class serves dual purposes:</p> <ul> <li>Metaxy feature: Tracks feature version, field versions, and dependencies</li> <li>SQLModel table: Maps to database schema with ORM functionality</li> </ul> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\"])</code>, the table name becomes <code>\"video\"</code>. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in Metaxy's configuration.</p>"},{"location":"learn/integrations/sqlmodel/#system-managed-columns","title":"System-Managed Columns","text":"<p>Metaxy's metadata store automatically manages versioning columns:</p> <ul> <li><code>provenance_by_field</code>: Struct column mapping field keys to hashes</li> <li><code>feature_version</code>: Hash of feature specification</li> <li><code>snapshot_version</code>: Hash of entire graph state</li> </ul> <p>These columns need not be defined in your SQLModel class. The metadata store injects them during write and read operations.</p>"},{"location":"learn/integrations/sqlmodel/#id-columns","title":"ID Columns","text":"<p>ID columns must exist before database insertion</p> <p>ID columns are used for joins between features, so their values must exist before insertion into the database. This means you cannot use server-generated values (autoincrement, sequences, server_default) for ID columns.</p> <pre><code>Metaxy validates against autoincrement primary keys but cannot detect all server-generated patterns. Ensure your ID columns use client-provided values.\n</code></pre> <p>Example:</p> <pre><code># \u2705 Good: Client-generated ID columns\nclass UserActivity(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"user\", \"activity\"]),\n        id_columns=[\"user_id\", \"session_id\"],  # Client provides these\n        ...\n    ),\n):\n    user_id: str = Field(primary_key=True)  # Client-generated\n    session_id: str = Field(primary_key=True)  # Client-generated\n    created_at: str = Field(sa_column_kwargs={\"server_default\": \"NOW()\"})  # OK - not an ID column\n\n# \u274c Bad: Autoincrement ID column\nclass BadFeature(\n    SQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"bad\"]),\n        id_columns=[\"id\"],  # This is listed as an ID column\n        ...\n    ),\n):\n    id: int = Field(primary_key=True, sa_column_kwargs={\"autoincrement\": True})  # Will raise error\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#loading-features-and-populating-metadata","title":"Loading Features and Populating Metadata","text":"<p>When using metaxy.init_metaxy to discover and import feature modules, all <code>SQLModelFeature</code> classes are automatically registered in SQLModel's metadata:</p> <pre><code>from metaxy import init_metaxy, init_metaxy\nfrom sqlmodel import SQLModel\n\n# Load all features from configured entrypoints\ngraph = init_metaxy()\n\n# All SQLModelFeature tables are now registered in SQLModel.metadata\n# This metadata can be used with Alembic for migrations\nprint(f\"Tables registered: {list(SQLModel.metadata.tables.keys())}\")\n</code></pre> <p>This is particularly useful when:</p> <ul> <li>Generating Alembic migrations that need to discover all tables</li> <li>Setting up database connections that require the complete schema</li> <li>Using SQLModel's <code>create_all()</code> for development/testing (Metaxy's <code>auto_create_tables</code> setting should be preferred over <code>create_all()</code>)</li> </ul> <p>Migration Generation</p> <p>After calling <code>init_metaxy</code>, you can use Alembic to automatically detect all your SQLModelFeature tables and generate migration scripts.</p>"},{"location":"learn/integrations/sqlmodel/#configuration","title":"Configuration","text":"<p>Configure automatic table naming behavior:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\ninfer_db_table_names = true  # Default\n</code></pre> <pre><code>export METAXY_EXT_SQLMODEL_INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#database-migrations-with-alembic","title":"Database Migrations with Alembic","text":"<p>Metaxy provides SQLModel definitions for its system tables that integrate with Alembic for database migrations. This allows you to version control schema changes alongside your application code. Note that you might want to keep separate migrations per each DB-backed <code>MetadataStore</code> used with Metaxy.</p>"},{"location":"learn/integrations/sqlmodel/#separate-migration-management","title":"Separate Migration Management","text":"<p>Metaxy system tables and user application tables should be managed in separate Alembic migration directories. This separation provides critical safety guarantees:</p> <p>System Table Isolation: Metaxy system tables (<code>metaxy-system__feature_versions</code>, <code>metaxy-system__migration_events</code>) have schemas managed by the framework. User migrations cannot accidentally modify these internal structures.</p> <p>Independent Evolution: Metaxy can evolve its system table schemas independently through framework updates without conflicts with user migrations.</p> <p>Failure Isolation: User migration failures remain isolated from metaxy's internal state tracking. A failed user migration leaves system tables intact for debugging and recovery.</p> <p>Clear Audit Trail: Separate migration histories make it trivial to distinguish framework schema changes from application schema changes. This clarity is essential during rollbacks and incident investigation.</p>"},{"location":"learn/integrations/sqlmodel/#setup","title":"Setup","text":"<p>Enable SQLModel system tables in your metaxy configuration and set up two Alembic directories:</p> <pre><code># Standard structure\nproject/\n\u251c\u2500\u2500 alembic/              # User application migrations\n\u2502   \u251c\u2500\u2500 versions/\n\u2502   \u2514\u2500\u2500 env.py\n\u251c\u2500\u2500 .metaxy/\n\u2502   \u2514\u2500\u2500 alembic-system/   # Metaxy system table migrations\n\u2502       \u251c\u2500\u2500 versions/\n\u2502       \u2514\u2500\u2500 env.py\n\u2514\u2500\u2500 metaxy.toml\n</code></pre> <p>Initialize both Alembic directories:</p> <pre><code># Initialize user migrations\nalembic init alembic\n\n# Initialize metaxy system migrations\nalembic init .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#metaxy-system-tables-configuration","title":"Metaxy System Tables Configuration","text":"<p>Configure <code>.metaxy/alembic-system/env.py</code> to manage only metaxy system tables:</p> <pre><code># typical Alembic boilerplate\nfrom metaxy.ext.alembic import get_metaxy_metadata\n\nmetaxy_system_metadata = get_metaxy_metadata()\n\n# metaxy_system_metadata has system tables\n\n# continue with alembic boilerplate\n</code></pre> <p>Configure <code>.metaxy/alembic-system/alembic.ini</code> with your database URL:</p> <pre><code>[alembic]\nscript_location = .metaxy/alembic-system\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#user-application-tables-configuration","title":"User Application Tables Configuration","text":"<p>Configure <code>alembic/env.py</code> to manage user tables, excluding metaxy system tables:</p> <pre><code># standard Alembic boilerplate\nfrom sqlmodel import SQLModel\nfrom metaxy import init_metaxy\n\ninit_metaxy()\n\n# SQLModel.metadata now has user-defined Metaxy tables\n\n\n# continue with alembic boilerplate\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#migration-workflow","title":"Migration Workflow","text":"<p>Generate and apply migrations separately for each concern:</p> <pre><code># 1. Create metaxy system tables (run once during initial setup)\nalembic -c .metaxy/alembic-system/alembic.ini revision --autogenerate -m \"create metaxy system tables\"\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head\n\n# 2. Create and apply user table migrations\nalembic revision --autogenerate -m \"add video feature table\"\nalembic upgrade head\n\n# 3. When modifying user tables, only user migrations change\nalembic revision --autogenerate -m \"add processing timestamp\"\nalembic upgrade head\n</code></pre> <p>When deploying to production, always apply system table migrations before user migrations:</p> <pre><code># Production deployment order\nalembic -c .metaxy/alembic-system/alembic.ini upgrade head  # System tables first\nalembic upgrade head                                 # Then user tables\n</code></pre>"},{"location":"learn/integrations/sqlmodel/#disabling-sqlmodel-system-tables","title":"Disabling SQLModel System Tables","text":"<p>If required, disable SQLModel system tables in <code>metaxy.toml</code>:</p> <pre><code>[ext.sqlmodel]\nenabled = true\nsystem_tables = false\n</code></pre>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Metaxy - Feature Metadata Management</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>migrations</code><ul> <li><code>generate</code></li> <li><code>apply</code></li> <li><code>status</code></li> <li><code>list</code></li> <li><code>explain</code></li> <li><code>describe</code></li> </ul> </li> <li><code>graph</code><ul> <li><code>push</code></li> <li><code>history</code></li> <li><code>describe</code></li> <li><code>render</code></li> </ul> </li> <li><code>graph-diff</code><ul> <li><code>render</code></li> </ul> </li> <li><code>list</code><ul> <li><code>features</code></li> </ul> </li> <li><code>metadata</code><ul> <li><code>copy</code></li> <li><code>drop</code></li> </ul> </li> </ul> <p>Usage:</p> <pre><code>$ metaxy COMMAND\n</code></pre> <p>Arguments:</p> <p>Options:</p> <ul> <li><code>--config-file</code>: Global option. Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Global option. Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Global option. Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: --no-all-projects]</li> </ul> <p>Commands:</p> <ul> <li><code>graph</code>: Manage feature graphs</li> <li><code>graph-diff</code>: Compare and visualize graph snapshots</li> <li><code>list</code>: List Metaxy entities</li> <li><code>metadata</code>: Manage Metaxy metadata</li> <li><code>migrations</code>: Metadata migration commands</li> <li><code>shell</code>: Start interactive shell.</li> </ul>"},{"location":"reference/cli/#metaxy-shell","title":"<code>metaxy shell</code>","text":"<p>Start interactive shell.</p> <p>Usage:</p> <pre><code>$ shell\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations","title":"<code>metaxy migrations</code>","text":"<p>Metadata migration commands</p> <p>Usage:</p> <pre><code>$ migrations COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>apply</code>: Apply migration(s) from YAML files.</li> <li><code>describe</code>: Show verbose description of migration(s).</li> <li><code>explain</code>: Show detailed diff for a migration.</li> <li><code>generate</code>: Generate migration from detected feature changes.</li> <li><code>list</code>: List all migrations in chain order as defined in code.</li> <li><code>status</code>: Show migration chain and execution status.</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-generate","title":"<code>metaxy migrations generate</code>","text":"<p>Generate migration from detected feature changes.</p> <p>Compares the latest snapshot in the store (or specified from_snapshot) with the current active graph to detect changes.</p> <p>The migration is recorded in the system tables (not a YAML file).</p> <p>Usage:</p> <pre><code>$ metaxy migrations generate --op LIST[STR] [OPTIONS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>--OP</code>: Operation class path to use (can be repeated). Example: metaxy.migrations.ops.DataVersionReconciliation  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--name</code>: Migration name (creates {timestamp}_{name} ID)</li> <li><code>--store</code>: Store name (defaults to default)</li> <li><code>--from-snapshot</code>: Compare from this historical snapshot version (defaults to latest)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-apply","title":"<code>metaxy migrations apply</code>","text":"<p>Apply migration(s) from YAML files.</p> <p>Reads migration definitions from .metaxy/migrations/ directory (git). Follows parent chain to ensure correct order.  Tracks execution state in database (events).</p> <p>Usage:</p> <pre><code>$ metaxy migrations apply [OPTIONS] [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to apply (applies all unapplied if not specified)</li> <li><code>STORE, --store</code>: Metadata store to use.</li> <li><code>--dry-run, --no-dry-run</code>: Preview changes without executing  [default: --no-dry-run]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-status","title":"<code>metaxy migrations status</code>","text":"<p>Show migration chain and execution status.</p> <p>Reads migration definitions from YAML files (git). Shows execution status from database events. Displays the parent  chain in order.</p> <p>Usage:</p> <pre><code>$ metaxy migrations status\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-list","title":"<code>metaxy migrations list</code>","text":"<p>List all migrations in chain order as defined in code.</p> <p>Displays a simple table showing migration ID, creation time, and operations.</p> <p>Usage:</p> <pre><code>$ metaxy migrations list\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-explain","title":"<code>metaxy migrations explain</code>","text":"<p>Show detailed diff for a migration.</p> <p>Reads migration from YAML file. Computes and displays the GraphDiff between the two snapshots on-demand.</p> <p>Usage:</p> <pre><code>$ metaxy migrations explain [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to explain (explains latest if not specified)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-describe","title":"<code>metaxy migrations describe</code>","text":"<p>Show verbose description of migration(s).</p> <p>Displays detailed information about what the migration will do: - Migration metadata (ID, parent, snapshots, created  timestamp) - Operations to execute - Affected features with row counts - Execution status if already run</p> <p>Usage:</p> <pre><code>$ metaxy migrations describe [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>MIGRATION-IDS, --migration-ids, --empty-migration-ids</code>: Migration IDs to describe (default: all migrations in order)  [default: []]</li> <li><code>STORE, --store</code>: Metadata store to use.</li> </ul>"},{"location":"reference/cli/#metaxy-graph","title":"<code>metaxy graph</code>","text":"<p>Manage feature graphs</p> <p>Usage:</p> <pre><code>$ graph COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>describe</code>: Describe a graph snapshot.</li> <li><code>history</code>: Show history of recorded graph snapshots.</li> <li><code>push</code>: Record all feature versions (push graph snapshot).</li> <li><code>render</code>: Render feature graph visualization.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-push","title":"<code>metaxy graph push</code>","text":"<p>Record all feature versions (push graph snapshot).</p> <p>Records all features in the active graph to the metadata store with a deterministic snapshot version. This should be run after deploying new feature definitions.</p> <p>Usage:</p> <pre><code>$ metaxy graph push [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"<code>metaxy graph history</code>","text":"<p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing snapshot versions, when they were recorded, and  feature counts.</p> <p>Usage:</p> <pre><code>$ metaxy graph history [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-describe","title":"<code>metaxy graph describe</code>","text":"<p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph  depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no  dependents) - Project breakdown (if multi-project)</p> <p>Usage:</p> <pre><code>$ metaxy graph describe [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"<code>metaxy graph render</code>","text":"<p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:</p> <pre><code>graph (default): Hierarchical tree view  cards: Panel/card-based view with dependency edges\n</code></pre> <p>\u2022 mermaid: Mermaid flowchart markup  \u2022 graphviz: Graphviz DOT format</p> <p>Usage:</p> <pre><code>$ metaxy graph render [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>SHOW-FIELDS, --show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>SHOW-FEATURE-VERSIONS, --show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>SHOW-FIELD-VERSIONS, --show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>SHOW-CODE-VERSIONS, --show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>SHOW-SNAPSHOT-VERSION, --show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>HASH-LENGTH, --hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>DIRECTION, --direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>FEATURE, --feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>UP, --up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>DOWN, --down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>PROJECT, --project</code>: Filter nodes by project (show only features from this project)</li> <li><code>SHOW-PROJECTS, --show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> <li><code>-f, --format</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>-t, --type</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to render (default: current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>MINIMAL, --minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>VERBOSE, --verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> </ul>"},{"location":"reference/cli/#metaxy-graph-diff","title":"<code>metaxy graph-diff</code>","text":"<p>Compare and visualize graph snapshots</p> <p>Usage:</p> <pre><code>$ graph-diff COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>render</code>: Render merged graph visualization comparing two snapshots.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-diff-render","title":"<code>metaxy graph-diff render</code>","text":"<p>Render merged graph visualization comparing two snapshots.</p> <p>Shows all features color-coded by status (added/removed/changed/unchanged). Uses the unified rendering system - same  renderers as 'metaxy graph render'.</p> <p>Special snapshot literals: - \"latest\": Most recent snapshot in the store - \"current\": Current graph state from code</p> <p>Output formats: - terminal: Hierarchical tree view (default) - cards: Panel/card-based view - mermaid: Mermaid flowchart diagram - graphviz: Graphviz DOT format</p> <p>Usage:</p> <pre><code>$ metaxy graph-diff render FROM-SNAPSHOT [ARGS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>FROM-SNAPSHOT</code>: First snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>TO-SNAPSHOT, --to-snapshot</code>: Second snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [default: current]</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-f, --format</code>: Output format: terminal, cards, mermaid, graphviz, json, or yaml  [choices: terminal, cards, mermaid, graphviz, json, yaml] [default: terminal]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>SHOW-FIELDS, --show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>SHOW-FEATURE-VERSIONS, --show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>SHOW-FIELD-VERSIONS, --show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>SHOW-CODE-VERSIONS, --show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>SHOW-SNAPSHOT-VERSION, --show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>HASH-LENGTH, --hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>DIRECTION, --direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>FEATURE, --feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>UP, --up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>DOWN, --down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>PROJECT, --project</code>: Filter nodes by project (show only features from this project)</li> <li><code>SHOW-PROJECTS, --show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> <li><code>MINIMAL, --minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>VERBOSE, --verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> </ul>"},{"location":"reference/cli/#metaxy-list","title":"<code>metaxy list</code>","text":"<p>List Metaxy entities</p> <p>Usage:</p> <pre><code>$ list COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>features</code>: List Metaxy features.</li> </ul>"},{"location":"reference/cli/#metaxy-list-features","title":"<code>metaxy list features</code>","text":"<p>List Metaxy features.</p> <p>Usage:</p> <pre><code>$ metaxy list features\n</code></pre>"},{"location":"reference/cli/#metaxy-metadata","title":"<code>metaxy metadata</code>","text":"<p>Manage Metaxy metadata</p> <p>Usage:</p> <pre><code>$ metadata COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>copy</code>: Copy metadata between stores.</li> <li><code>drop</code>: Drop metadata from a store.</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-copy","title":"<code>metaxy metadata copy</code>","text":"<p>Copy metadata between stores.</p> <p>Copies metadata for specified features from one store to another, optionally using a historical version. Useful for: -  Migrating data between environments - Backfilling metadata - Copying specific feature versions</p> <p>Incremental Mode (default):     By default, performs an anti-join on sample_uid to skip rows that already exist in the destination for the same  snapshot_version. This prevents duplicate writes.  Disabling incremental (--no-incremental) may improve performance  when: - The destination store is empty or has no overlap with source - The destination store has eventual deduplication</p> <p>Usage:</p> <pre><code>$ metaxy metadata copy FROM TO [ARGS]\n</code></pre> <p>Arguments:</p> <ul> <li><code>FROM</code>: Source store name (must be configured in metaxy.toml)  [required]</li> <li><code>TO</code>: Destination store name (must be configured in metaxy.toml)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to copy (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses  --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Copy all features from source store  [default: --no-all-features]</li> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to copy (defaults to latest in source store). The snapshot_version is preserved in the destination.</li> <li><code>INCREMENTAL, --incremental, --no-incremental</code>: Use incremental copy (compare provenance_by_field to skip existing rows). Disable for better performance if destination  is empty or uses deduplication.  [default: --incremental]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-drop","title":"<code>metaxy metadata drop</code>","text":"<p>Drop metadata from a store.</p> <p>Removes metadata for specified features from the store. This is a destructive operation and requires --confirm flag.</p> <p>Useful for: - Cleaning up test data - Re-computing feature metadata from scratch - Removing obsolete features</p> <p>Usage:</p> <pre><code>$ metaxy metadata drop [ARGS]\n</code></pre> <p>Options:</p> <ul> <li><code>STORE, --store</code>: Store name to drop metadata from (defaults to configured default store)</li> <li><code>FEATURE, --feature, --empty-feature</code>: Feature key to drop (e.g., 'my_feature' or 'group/my_feature'). Can be repeated multiple times. If not specified, uses  --all-features.</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Drop metadata for all features in the store  [default: --no-all-features]</li> <li><code>CONFIRM, --confirm, --no-confirm</code>: Confirm the drop operation (required to prevent accidental deletion)  [default: --no-confirm]</li> </ul>"},{"location":"reference/cli/#examples","title":"Examples","text":""},{"location":"reference/cli/#recording-a-graph-snapshot","title":"Recording a graph snapshot","text":"<pre><code># Push the current feature graph to the metadata store\nmetaxy graph push\n</code></pre> <p>The recommendation is to run this command in your CD pipeline.</p>"},{"location":"reference/cli/#generating-and-applying-migrations","title":"Generating and applying migrations","text":"<pre><code># Generate a migration for detected changes\nmetaxy migrations generate --op metaxy.migrations.ops.DataVersionReconciliation\n\n# Apply pending migrations\nmetaxy migrations apply\n</code></pre>"},{"location":"reference/cli/#visualizing-the-feature-graph","title":"Visualizing the feature graph","text":"<pre><code># Render as terminal tree view\nmetaxy graph render\n\n# Render as Mermaid diagram\nmetaxy graph render --format mermaid\n\n# Compare two snapshots\nmetaxy graph-diff render &lt;snapshot-id&gt; current --format mermaid\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Metaxy can be configured using TOML configuration files or environment variables.</p>"},{"location":"reference/configuration/#default-configuration","title":"Default Configuration","text":"<p>Here is the complete default configuration with all available options:</p> !metaxy.tomlpyproject.toml <pre><code># Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy]\n# Default metadata store to use\nstore = \"dev\"\n\n# Optional: Named store configurations\n# stores = {}\n\n# Directory where migration files are stored\nmigrations_dir = \".metaxy/migrations\"\n\n# Optional: List of Python module paths to load for feature discovery\n# entrypoints = []\n\n# Graph rendering theme for CLI visualization\ntheme = \"default\"\n\n# Optional: Truncate hash values to this length (minimum 8 characters). None = no truncation.\n# hash_truncation_length = null\n\n# Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.\nauto_create_tables = false\n\n# Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)\nproject = \"default\"\n\n[tool.metaxy.ext.sqlmodel]\n# Whether to enable the plugin.\nenable = false\n\n# Whether to automatically use `FeatureKey.table_name` for sqlalchemy's __tablename__ value.\ninfer_db_table_names = true\n\n# Whether to use SQLModel definitions for system tables (for Alembic migrations).\nsystem_tables = true\n</code></pre>"},{"location":"reference/configuration/#configuration-fields","title":"Configuration Fields","text":"<p>Each field can be set via TOML configuration or environment variables.</p>"},{"location":"reference/configuration/#store","title":"<code>store</code>","text":"<p>Default metadata store to use</p> <p>Type: <code>str</code>  | Default: <code>\"dev\"</code></p> !metaxy.tomlpyproject.toml <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORE=dev\n</code></pre>"},{"location":"reference/configuration/#stores","title":"<code>stores</code>","text":"<p>Named store configurations</p> <p>Type: dict[str, metaxy.config.StoreConfig]</p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# stores = {}\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# stores = {}\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_STORES=...\n</code></pre>"},{"location":"reference/configuration/#migrations_dir","title":"<code>migrations_dir</code>","text":"<p>Directory where migration files are stored</p> <p>Type: <code>str</code>  | Default: <code>\".metaxy/migrations\"</code></p> !metaxy.tomlpyproject.toml <pre><code>migrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>[tool.metaxy]\nmigrations_dir = \".metaxy/migrations\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_MIGRATIONS_DIR=.metaxy/migrations\n</code></pre>"},{"location":"reference/configuration/#entrypoints","title":"<code>entrypoints</code>","text":"<p>List of Python module paths to load for feature discovery</p> <p>Type: <code>list[str]</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# entrypoints = []\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_ENTRYPOINTS=...\n</code></pre>"},{"location":"reference/configuration/#theme","title":"<code>theme</code>","text":"<p>Graph rendering theme for CLI visualization</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>theme = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\ntheme = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_THEME=default\n</code></pre>"},{"location":"reference/configuration/#hash_truncation_length","title":"<code>hash_truncation_length</code>","text":"<p>Truncate hash values to this length (minimum 8 characters). None = no truncation.</p> <p>Type: <code>int | None</code></p> !metaxy.tomlpyproject.toml <pre><code># Optional\n# hash_truncation_length = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# hash_truncation_length = null\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=...\n</code></pre>"},{"location":"reference/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre>"},{"location":"reference/configuration/#project","title":"<code>project</code>","text":"<p>Project name for metadata isolation. Used to scope system tables and operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid identifiers (alphanumeric, underscores, hyphens) and cannot contain forward slashes (/) or double underscores (__)</p> <p>Type: <code>str</code>  | Default: <code>\"default\"</code></p> !metaxy.tomlpyproject.toml <pre><code>project = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"default\"\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_PROJECT=default\n</code></pre>"},{"location":"reference/configuration/#ext-sqlmodel-configuration","title":"Ext &gt; Sqlmodel Configuration","text":""},{"location":"reference/configuration/#extsqlmodelenable","title":"<code>ext.sqlmodel.enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code>  | Default: <code>False</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__ENABLE=false\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelinfer_db_table_names","title":"<code>ext.sqlmodel.infer_db_table_names</code>","text":"<p>Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's tablename value.</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninfer_db_table_names = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__INFER_DB_TABLE_NAMES=true\n</code></pre>"},{"location":"reference/configuration/#extsqlmodelsystem_tables","title":"<code>ext.sqlmodel.system_tables</code>","text":"<p>Whether to use SQLModel definitions for system tables (for Alembic migrations).</p> <p>Type: <code>bool</code>  | Default: <code>True</code></p> !metaxy.tomlpyproject.toml <pre><code>[ext.sqlmodel]\nsystem_tables = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nsystem_tables = true\n</code></pre> <p>Environment Variable:</p> <pre><code>export METAXY_EXT__SQLMODEL__SYSTEM_TABLES=true\n</code></pre>"},{"location":"reference/configuration/#configuration-types","title":"Configuration Types","text":""},{"location":"reference/configuration/#storeconfig","title":"StoreConfig","text":"<p>Configuration for a single metadata store backend.</p> <p>Fields:</p> <ul> <li><code>type</code> (str): Full import path to the store class</li> <li><code>config</code> (dict[str, Any]): Store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#extconfig","title":"ExtConfig","text":"<p>Configuration for Metaxy integrations with third-party tools.</p> <p>Fields:</p> <ul> <li><code>sqlmodel</code> (SQLModelConfig): SQLModel integration configuration</li> </ul>"},{"location":"reference/configuration/#sqlmodelconfig","title":"SQLModelConfig","text":"<p>Configuration for SQLModel integration.</p> <p>Fields:</p> <ul> <li><code>enable</code> (bool): Whether to enable the plugin (default: <code>false</code>)</li> <li><code>infer_db_table_names</code> (bool): Whether to automatically use <code>FeatureKey.table_name</code> for sqlalchemy's <code>__tablename__</code> value (default: <code>true</code>)</li> <li><code>system_tables</code> (bool): Whether to use SQLModel definitions for system tables (default: <code>true</code>)</li> </ul>"},{"location":"reference/configuration/#store-configuration","title":"Store Configuration","text":"<p>The <code>stores</code> field configures metadata store backends. Each store is defined by:</p> <ul> <li><code>type</code>: Full import path to the store class (e.g., <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code>)</li> <li><code>config</code>: Dictionary of store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#example-multiple-stores-with-fallback-chain","title":"Example: Multiple Stores with Fallback Chain","text":"!metaxy.tomlpyproject.toml <pre><code># Default store to use\nstore = \"dev\"\n\n# Development store (in-memory) with fallback to production\n[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n# Production store\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n\n[tool.metaxy.stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n[tool.metaxy.stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre>"},{"location":"reference/configuration/#available-store-types","title":"Available Store Types","text":"Store Type Import Path Description DuckDB <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code> File-based or in-memory DuckDB backend ClickHouse <code>metaxy.metadata_store.clickhouse.ClickHouseMetadataStore</code> ClickHouse database backend In-Memory <code>metaxy.metadata_store.memory.InMemoryMetadataStore</code> In-memory backend for testing"},{"location":"reference/configuration/#getting-a-store-instance","title":"Getting a Store Instance","text":"<pre><code>from metaxy.config import MetaxyConfig\n\nconfig = MetaxyConfig.load()\n\n# Get the default store\nwith config.get_store() as store:\n    # Use store\n    pass\n\n# Get a specific store by name\nwith config.get_store(\"prod\") as store:\n    # Use store\n    pass\n</code></pre>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol> <p>This allows you to override file-based configuration with environment variables, which is useful for CI/CD pipelines and different deployment environments.</p>"},{"location":"reference/configuration/#loading-configuration","title":"Loading Configuration","text":""},{"location":"reference/configuration/#auto-discovery","title":"Auto-Discovery","text":"<pre><code>from metaxy.config import MetaxyConfig\n\n# Auto-discover config file in current or parent directories\nconfig = MetaxyConfig.load()\n</code></pre>"},{"location":"reference/configuration/#explicit-file","title":"Explicit File","text":"<pre><code># Load from specific file\nconfig = MetaxyConfig.load(\"path/to/metaxy.toml\")\n</code></pre>"},{"location":"reference/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code># Create configuration programmatically\nconfig = MetaxyConfig(\n    store=\"prod\",\n    migrations_dir=\".migrations\",\n)\n</code></pre>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#metaxy","title":"<code>metaxy</code>","text":"<p>The top-level <code>metaxy</code> module provides the main public API for Metaxy.</p>"},{"location":"reference/api/#initialization","title":"Initialization","text":""},{"location":"reference/api/#metaxy.init_metaxy","title":"init_metaxy","text":"<pre><code>init_metaxy(config_file: Path | None = None, search_parents: bool = True) -&gt; MetaxyConfig\n</code></pre> <p>Main user-facing initialization function for Metaxy. It loads the configuration and features.</p> <p>Features are discovered from installed Python packages metadata.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. Defaults to None.</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to search parent directories for configuration files. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code> (              <code>MetaxyConfig</code> )          \u2013            <p>The initialized Metaxy configuration.</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>def init_metaxy(\n    config_file: Path | None = None, search_parents: bool = True\n) -&gt; MetaxyConfig:\n    \"\"\"Main user-facing initialization function for Metaxy. It loads the configuration and features.\n\n    Features are [discovered](../../learn/feature-discovery.md) from installed Python packages metadata.\n\n    Args:\n        config_file (Path | None, optional): Path to the configuration file. Defaults to None.\n        search_parents (bool, optional): Whether to search parent directories for configuration files. Defaults to True.\n\n    Returns:\n        MetaxyConfig: The initialized Metaxy configuration.\n    \"\"\"\n    cfg = MetaxyConfig.load(\n        config_file=config_file,\n        search_parents=search_parents,\n    )\n    load_features(cfg.entrypoints)\n    return cfg\n</code></pre>"},{"location":"reference/api/#metadata-stores","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata through the MetadaStore interface.</p>"},{"location":"reference/api/#dependency-specification","title":"Dependency Specification","text":"<p>Metaxy has a declarative feature specification system that allows users to express dependencies between their features and their versioned fields.</p>"},{"location":"reference/api/config/","title":"Configuration","text":"<p>This is the Python SDK for Metaxy's configuration. See config file reference to learn how to configure Metaxy via <code>TOML</code> files.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig","title":"MetaxyConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> <p>The default store is <code>\"dev\"</code>; <code>METAXY_STORE</code> can be used to override it.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.plugins","title":"plugins  <code>property</code>","text":"<pre><code>plugins: list[str]\n</code></pre> <p>Returns all enabled plugin names from ext configuration.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_project","title":"validate_project  <code>classmethod</code>","text":"<pre><code>validate_project(v: str) -&gt; str\n</code></pre> <p>Validate project name follows naming rules.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"project\")\n@classmethod\ndef validate_project(cls, v: str) -&gt; str:\n    \"\"\"Validate project name follows naming rules.\"\"\"\n    if not v:\n        raise ValueError(\"project name cannot be empty\")\n    if \"/\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain forward slashes (/). \"\n            f\"Forward slashes are reserved for FeatureKey separation\"\n        )\n    if \"__\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain double underscores (__). \"\n            f\"Double underscores are reserved for table name generation\"\n        )\n    import re\n\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", v):\n        raise ValueError(\n            f\"project name '{v}' must contain only alphanumeric characters, underscores, and hyphens\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_hash_truncation_length","title":"validate_hash_truncation_length  <code>classmethod</code>","text":"<pre><code>validate_hash_truncation_length(v: int | None) -&gt; int | None\n</code></pre> <p>Validate hash truncation length is at least 8 if set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"hash_truncation_length\")\n@classmethod\ndef validate_hash_truncation_length(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate hash truncation length is at least 8 if set.\"\"\"\n    if v is not None and v &lt; 8:\n        raise ValueError(\n            f\"hash_truncation_length must be at least 8 characters, got {v}\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.settings_customise_sources","title":"settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(settings_cls: type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources: init \u2192 env \u2192 TOML.</p> <p>Priority (first wins): 1. Init arguments 2. Environment variables 3. TOML file</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources: init \u2192 env \u2192 TOML.\n\n    Priority (first wins):\n    1. Init arguments\n    2. Environment variables\n    3. TOML file\n    \"\"\"\n    toml_settings = TomlConfigSettingsSource(settings_cls)\n    return (init_settings, env_settings, toml_settings)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get","title":"get  <code>classmethod</code>","text":"<pre><code>get() -&gt; MetaxyConfig\n</code></pre> <p>Get the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get(cls) -&gt; \"MetaxyConfig\":\n    \"\"\"Get the current Metaxy configuration.\"\"\"\n    cfg = _metaxy_config.get()\n    if cfg is None:\n        warnings.warn(\n            UserWarning(\n                \"Global Metaxy configuration not initialized. It can be set with MetaxyConfig.set(config) typically after loading it from a toml file. Returning default configuration (with environment variables and other pydantic settings sources resolved, project='default').\"\n            )\n        )\n        return cls(project=\"default\")\n    else:\n        return cfg\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.set","title":"set  <code>classmethod</code>","text":"<pre><code>set(config: Self | None) -&gt; None\n</code></pre> <p>Set the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef set(cls, config: Self | None) -&gt; None:\n    \"\"\"Set the current Metaxy configuration.\"\"\"\n    _metaxy_config.set(config)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.is_set","title":"is_set  <code>classmethod</code>","text":"<pre><code>is_set() -&gt; bool\n</code></pre> <p>Check if the current Metaxy configuration is set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef is_set(cls) -&gt; bool:\n    \"\"\"Check if the current Metaxy configuration is set.\"\"\"\n    return _metaxy_config.get() is not None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.reset","title":"reset  <code>classmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the current Metaxy configuration to None.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset the current Metaxy configuration to None.\"\"\"\n    _metaxy_config.set(None)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(config_file: str | Path | None = None, *, search_parents: bool = True, auto_discovery_start: Path | None = None) -&gt; MetaxyConfig\n</code></pre> <p>Load config with auto-discovery and parent directory search.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional config file path (overrides auto-discovery)</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Search parent directories for config file (default: True)</p> </li> <li> <code>auto_discovery_start</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to start search from (defaults to cwd)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>Loaded config (TOML + env vars merged)</p> </li> </ul> Example <pre><code># Auto-discover with parent search\nconfig = MetaxyConfig.load()\n\n# Explicit file\nconfig = MetaxyConfig.load(\"custom.toml\")\n\n# Auto-discover without parent search\nconfig = MetaxyConfig.load(search_parents=False)\n\n# Auto-discover from a specific directory\nconfig = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; \"MetaxyConfig\":\n    \"\"\"Load config with auto-discovery and parent directory search.\n\n    Args:\n        config_file: Optional config file path (overrides auto-discovery)\n        search_parents: Search parent directories for config file (default: True)\n        auto_discovery_start: Directory to start search from (defaults to cwd)\n\n    Returns:\n        Loaded config (TOML + env vars merged)\n\n    Example:\n        ```py\n        # Auto-discover with parent search\n        config = MetaxyConfig.load()\n\n        # Explicit file\n        config = MetaxyConfig.load(\"custom.toml\")\n\n        # Auto-discover without parent search\n        config = MetaxyConfig.load(search_parents=False)\n\n        # Auto-discover from a specific directory\n        config = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n        ```\n    \"\"\"\n    # Search for config file if not explicitly provided\n    if config_file is None and search_parents:\n        config_file = cls._discover_config_with_parents(auto_discovery_start)\n\n    # For explicit file, temporarily patch the TomlConfigSettingsSource\n    # to use that file, then use normal instantiation\n    # This ensures env vars still work\n\n    if config_file:\n        # Create a custom settings source class for this file\n        toml_path = Path(config_file)\n\n        class CustomTomlSource(TomlConfigSettingsSource):\n            def __init__(self, settings_cls: type[BaseSettings]):\n                # Skip auto-discovery, use explicit file\n                super(TomlConfigSettingsSource, self).__init__(settings_cls)\n                self.toml_file = toml_path\n                self.toml_data = self._load_toml()\n\n        # Customize sources to use custom TOML file\n        original_method = cls.settings_customise_sources\n\n        @classmethod  # type: ignore[misc]\n        def custom_sources(\n            cls_inner,\n            settings_cls,\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n        ):\n            toml_settings = CustomTomlSource(settings_cls)\n            return (init_settings, env_settings, toml_settings)\n\n        # Temporarily replace method\n        cls.settings_customise_sources = custom_sources  # type: ignore[assignment]\n        config = cls()\n        cls.settings_customise_sources = original_method  # type: ignore[method-assign]\n    else:\n        # Use default sources (auto-discovery + env vars)\n        config = cls()\n\n    cls.set(config)\n\n    return config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_store","title":"get_store","text":"<pre><code>get_store(name: str | None = None) -&gt; MetadataStore\n</code></pre> <p>Instantiate metadata store by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Store name (uses config.store if None)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore</code>           \u2013            <p>Instantiated metadata store</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store name not found in config, or if fallback stores have different hash algorithms than the parent store</p> </li> <li> <code>ImportError</code>             \u2013            <p>If store class cannot be imported</p> </li> </ul> Example <pre><code>config = MetaxyConfig.load()\nstore = config.get_store(\"prod\")\n\n# Use default store\nstore = config.get_store()\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>def get_store(self, name: str | None = None) -&gt; \"MetadataStore\":\n    \"\"\"Instantiate metadata store by name.\n\n    Args:\n        name: Store name (uses config.store if None)\n\n    Returns:\n        Instantiated metadata store\n\n    Raises:\n        ValueError: If store name not found in config, or if fallback stores\n            have different hash algorithms than the parent store\n        ImportError: If store class cannot be imported\n\n    Example:\n        ```py\n        config = MetaxyConfig.load()\n        store = config.get_store(\"prod\")\n\n        # Use default store\n        store = config.get_store()\n        ```\n    \"\"\"\n    from metaxy.data_versioning.hash_algorithms import HashAlgorithm\n\n    if len(self.stores) == 0:\n        raise ValueError(\n            \"No Metaxy stores available. They should be configured in metaxy.toml|pyproject.toml or via environment variables.\"\n        )\n\n    name = name or self.store\n\n    if name not in self.stores:\n        raise ValueError(\n            f\"Store '{name}' not found in config. \"\n            f\"Available stores: {list(self.stores.keys())}\"\n        )\n\n    store_config = self.stores[name]\n\n    # Import store class\n    store_class = self._import_class(store_config.type)\n\n    # Extract configuration\n    config_copy = store_config.config.copy()\n    fallback_store_names = config_copy.pop(\"fallback_stores\", [])\n\n    # Get hash_algorithm from config (if specified) and convert to enum\n    configured_hash_algorithm = config_copy.get(\"hash_algorithm\")\n    if configured_hash_algorithm is not None:\n        # Convert string to enum if needed\n        if isinstance(configured_hash_algorithm, str):\n            configured_hash_algorithm = HashAlgorithm(configured_hash_algorithm)\n            config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n    else:\n        # Use default\n        configured_hash_algorithm = HashAlgorithm.XXHASH64\n        config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n\n    # Get hash_truncation_length from global config (unless overridden in store config)\n    if (\n        \"hash_truncation_length\" not in config_copy\n        and self.hash_truncation_length is not None\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"hash_truncation_length\"] = self.hash_truncation_length\n\n    # Get auto_create_tables from global config (unless overridden in store config)\n    if (\n        \"auto_create_tables\" not in config_copy\n        and self.auto_create_tables is not None\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"auto_create_tables\"] = self.auto_create_tables\n\n    # Build fallback stores recursively\n    fallback_stores = []\n    for fallback_name in fallback_store_names:\n        fallback_store = self.get_store(fallback_name)\n        fallback_stores.append(fallback_store)\n\n    # Instantiate store with config + fallback_stores\n    store = store_class(\n        fallback_stores=fallback_stores,\n        **config_copy,\n    )\n\n    # Verify the store actually uses the hash algorithm we configured\n    # (in case a store subclass overrides the default or ignores the parameter)\n    if store.hash_algorithm != configured_hash_algorithm:\n        raise ValueError(\n            f\"Store '{name}' ({store_class.__name__}) was configured with \"\n            f\"hash_algorithm='{configured_hash_algorithm.value}' but is using \"\n            f\"'{store.hash_algorithm.value}'. The store class may have overridden \"\n            f\"the hash algorithm. All stores must use the same hash algorithm.\"\n        )\n\n    return store\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig","title":"StoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for a single metadata store.</p> Example <pre><code>config = StoreConfig(\n    type=\"metaxy_delta.DeltaMetadataStore\",\n    config={\n        \"table_uri\": \"s3://bucket/metadata\",\n        \"region\": \"us-west-2\",\n        \"fallback_stores\": [\"prod\"],\n    }\n)\n</code></pre>"},{"location":"reference/api/types/","title":"Types","text":"<p>A few types used in Metaxy here and there.</p>"},{"location":"reference/api/types/#metaxy.data_versioning.diff.LazyIncrement","title":"LazyIncrement","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of resolving an incremental update with lazy Narwhals LazyFrames.</p> <p>Attributes:</p> <ul> <li> <code>added</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>New samples that appear upstream and haven't been processed yet.</p> <p>Columns: <code>[*user_defined_columns, \"provenance_by_field\"]</code></p> </li> <li> <code>changed</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples with new field provenance that should be re-processed.</p> <p>Columns: <code>[*user_defined_columns, \"provenance_by_field\"]</code></p> </li> <li> <code>removed</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples that have been previously processed but have been removed from upstream since that.</p> <p>Columns: <code>[*id_columns, \"provenance_by_field\"]</code></p> </li> </ul> Note <p><code>added</code> and <code>changed</code> contain all the user-defined columns, but <code>removed</code> only contains the ID columns.</p>"},{"location":"reference/api/types/#metaxy.data_versioning.diff.LazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.data_versioning.diff.LazyIncrement.collect","title":"collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>Materialize all lazy frames to create a Increment.</p> <p>Returns:</p> <ul> <li> <code>Increment</code>           \u2013            <p>Increment with all frames materialized to eager DataFrames.</p> </li> </ul> Source code in <code>src/metaxy/data_versioning/diff/base.py</code> <pre><code>def collect(self) -&gt; \"Increment\":\n    \"\"\"Materialize all lazy frames to create a Increment.\n\n    Returns:\n        Increment with all frames materialized to eager DataFrames.\n    \"\"\"\n    return Increment(\n        added=self.added.collect(),\n        changed=self.changed.collect(),\n        removed=self.removed.collect(),\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.data_versioning.diff.Increment","title":"Increment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of resolving an incremental update with eager Narwhals DataFrames.</p> <p>Contains materialized Narwhals DataFrames.</p> <p>Users can convert to their preferred format: - Polars: result.added.to_native()</p> <p>Attributes:</p> <ul> <li> <code>added</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>New samples that appear upstream and haven't been processed yet.</p> <p>Columns: <code>[*user_defined_columns, \"provenance_by_field\"]</code></p> </li> <li> <code>changed</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>Samples with new field provenance that should be re-processed.</p> <p>Columns: <code>[*user_defined_columns, \"provenance_by_field\"]</code></p> </li> <li> <code>removed</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>Samples that have been previously processed but have been removed from upstream since that.</p> <p>Columns: <code>[*id_columns, \"provenance_by_field\"]</code></p> </li> </ul> Note <p><code>added</code> and <code>changed</code> contain all the user-defined columns, but <code>removed</code> only contains the ID columns.</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm","title":"HashAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Supported hash algorithms for field provenance calculation.</p> <p>These algorithms are chosen for: - Speed (non-cryptographic hashes preferred) - Cross-database availability - Good collision resistance for field provenance calculation</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH64","title":"XXHASH64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH64 = 'xxhash64'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH32","title":"XXHASH32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH32 = 'xxhash32'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.WYHASH","title":"WYHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WYHASH = 'wyhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.SHA256","title":"SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'sha256'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.MD5","title":"MD5  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MD5 = 'md5'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.FARMHASH","title":"FARMHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FARMHASH = 'farmhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.models.types.SnapshotPushResult","title":"SnapshotPushResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of recording a feature graph snapshot.</p> <p>Attributes:</p> <ul> <li> <code>snapshot_version</code>               (<code>str</code>)           \u2013            <p>The deterministic hash of the graph snapshot</p> </li> <li> <code>already_recorded</code>               (<code>bool</code>)           \u2013            <p>True if computational changes were already recorded</p> </li> <li> <code>metadata_changed</code>               (<code>bool</code>)           \u2013            <p>True if metadata-only changes were detected</p> </li> <li> <code>features_with_spec_changes</code>               (<code>list[str]</code>)           \u2013            <p>List of feature keys with spec version changes</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.IDColumns","title":"IDColumns  <code>module-attribute</code>","text":"<pre><code>IDColumns: TypeAlias = Sequence[str]\n</code></pre>"},{"location":"reference/api/definitions/","title":"Definitions","text":"<p>Metaxy's dependency specification system allows users to express dependencies between their features and their fields.</p>"},{"location":"reference/api/definitions/#featurespec","title":"<code>FeatureSpec</code>","text":"<p>FeatureSpec is the core of Metaxy's dependency specification system: it stores all the information about the parents, field mappings, and other metadata associated with a feature.</p>"},{"location":"reference/api/definitions/#feature","title":"Feature","text":"<p>A feature in Metaxy is used to model user-defined metadata. It must have a <code>FeatureSpec</code> instance associated with it. A <code>Feature</code> class is typically associated with a single table in the MetadataStore.</p>"},{"location":"reference/api/definitions/#fieldspec","title":"FieldSpec","text":"<p>A [field] in Metaxy is a logical slices of the data represented by feature metadata. Users are free to define their own fields as is suitable for them.</p> <p>Dependencies between fields are modeled with FieldDep and can be automatic (via field mappings) or explicitly set by users.</p>"},{"location":"reference/api/definitions/#graph","title":"Graph","text":"<p>All features live on a FeatureGraph object. The users don't typically interact with it outside of advanced use cases.</p>"},{"location":"reference/api/definitions/feature-spec/","title":"Feature Spec","text":"<p>Feature specs act as source of truth for all metadata related to features: their dependencies, fields, code versions, and so on.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec","title":"BaseFeatureSpec  <code>pydantic-model</code>","text":"<pre><code>BaseFeatureSpec(key: str, *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>BaseFeatureSpec(key: Sequence[str], *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>BaseFeatureSpec(key: FeatureKey, *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <pre><code>BaseFeatureSpec(key: Any = None, **data: Any)\n</code></pre> <p>               Bases: <code>_BaseFeatureSpec</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream features and all their fields.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on all upstream fields\\n    &gt;&gt;&gt; AllFieldsMapping()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.all()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    type: Always \\\"DEFAULT\\\" for discriminated union serialization\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\\n\\nExamples:\\n    &gt;&gt;&gt; # Exact match only (default)\\n    &gt;&gt;&gt; DefaultFieldsMapping()\\n\\n    &gt;&gt;&gt; # Enable suffix matching\\n    &gt;&gt;&gt; DefaultFieldsMapping(match_suffix=True)\\n\\n    &gt;&gt;&gt; # Exclude specific fields from being auto-mapped\\n    &gt;&gt;&gt; DefaultFieldsMapping(exclude_fields=[FieldKey([\\\"metadata\\\"])])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        or FeatureKey instance.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        }\n      },\n      \"required\": [\n        \"feature\",\n        \"fields_mapping\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"JsonValue\": {},\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\\n\\nArguments:\\n    type: Always \\\"SPECIFIC\\\" for discriminated union serialization\\n    mapping: Mapping of downstream field keys to their corresponding upstream field keys.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on specific upstream fields\\n    &gt;&gt;&gt; SpecificFieldsMapping({\\\"\\\"downstream\\\": {\\\"upstream_1\\\", \\\"upstream_2\\\"}})\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.specific({\\\"field1\\\", \\\"field2\\\"})\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FeatureDep\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Deps\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/JsonValue\"\n      },\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    },\n    \"id_columns\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"id_columns\"\n  ],\n  \"title\": \"BaseFeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FeatureKey</code>)             </li> <li> <code>deps</code>                 (<code>list[FeatureDep] | None</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldSpec]</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, JsonValue]</code>)             </li> <li> <code>id_columns</code>                 (<code>SkipValidation[IDColumns]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_unique_field_keys</code> </li> <li> <code>validate_id_columns</code> </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(self, key: Any = None, **data: Any) -&gt; None:\n    if key is not None:\n        super().__init__(key=key, **data)\n    else:\n        super().__init__(**data)\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.metadata","title":"metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, JsonValue]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.code_version","title":"code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.feature_spec_version","title":"feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]))],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.table_name","title":"table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.validate_unique_field_keys","title":"validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; BaseFeatureSpec\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; BaseFeatureSpec:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(\n                f\"Duplicate field key found: {field.key}. \"\n                f\"All fields must have unique keys.\"\n            )\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.BaseFeatureSpec.validate_id_columns","title":"validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; BaseFeatureSpec\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; BaseFeatureSpec:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\n            \"id_columns must be non-empty if specified. Use None for default.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec","title":"FeatureSpec  <code>pydantic-model</code>","text":"<pre><code>FeatureSpec(key: str, *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>FeatureSpec(key: Sequence[str], *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>FeatureSpec(key: FeatureKey, *, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, id_columns: IDColumns | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <pre><code>FeatureSpec(key: Any = None, **data: Any)\n</code></pre> <p>               Bases: <code>BaseFeatureSpec</code></p> <p>A default concrete implementation of BaseFeatureSpec that has a <code>sample_uid</code> ID column.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream features and all their fields.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on all upstream fields\\n    &gt;&gt;&gt; AllFieldsMapping()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.all()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    type: Always \\\"DEFAULT\\\" for discriminated union serialization\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\\n\\nExamples:\\n    &gt;&gt;&gt; # Exact match only (default)\\n    &gt;&gt;&gt; DefaultFieldsMapping()\\n\\n    &gt;&gt;&gt; # Enable suffix matching\\n    &gt;&gt;&gt; DefaultFieldsMapping(match_suffix=True)\\n\\n    &gt;&gt;&gt; # Exclude specific fields from being auto-mapped\\n    &gt;&gt;&gt; DefaultFieldsMapping(exclude_fields=[FieldKey([\\\"metadata\\\"])])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        or FeatureKey instance.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        }\n      },\n      \"required\": [\n        \"feature\",\n        \"fields_mapping\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"JsonValue\": {},\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\\n\\nArguments:\\n    type: Always \\\"SPECIFIC\\\" for discriminated union serialization\\n    mapping: Mapping of downstream field keys to their corresponding upstream field keys.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on specific upstream fields\\n    &gt;&gt;&gt; SpecificFieldsMapping({\\\"\\\"downstream\\\": {\\\"upstream_1\\\", \\\"upstream_2\\\"}})\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.specific({\\\"field1\\\", \\\"field2\\\"})\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"A default concrete implementation of BaseFeatureSpec that has a `sample_uid` ID column.\",\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FeatureDep\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Deps\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/JsonValue\"\n      },\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    },\n    \"id_columns\": {\n      \"default\": [\n        \"sample_uid\"\n      ],\n      \"description\": \"List of columns that uniquely identify a row. They will be used by Metaxy in joins.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"key\"\n  ],\n  \"title\": \"FeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FeatureKey</code>)             </li> <li> <code>deps</code>                 (<code>list[FeatureDep] | None</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldSpec]</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, JsonValue]</code>)             </li> <li> <code>id_columns</code>                 (<code>SkipValidation[IDColumns]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_unique_field_keys</code> </li> <li> <code>validate_id_columns</code> </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(self, key: Any = None, **data: Any) -&gt; None:\n    if key is not None:\n        super().__init__(key=key, **data)\n    else:\n        super().__init__(**data)\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.metadata","title":"metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, JsonValue]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.code_version","title":"code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.feature_spec_version","title":"feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]))],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.id_columns","title":"id_columns  <code>pydantic-field</code>","text":"<pre><code>id_columns: SkipValidation[IDColumns] = ('sample_uid',)\n</code></pre> <p>List of columns that uniquely identify a row. They will be used by Metaxy in joins.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.table_name","title":"table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_unique_field_keys","title":"validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; BaseFeatureSpec\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; BaseFeatureSpec:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(\n                f\"Duplicate field key found: {field.key}. \"\n                f\"All fields must have unique keys.\"\n            )\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_id_columns","title":"validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; BaseFeatureSpec\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; BaseFeatureSpec:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\n            \"id_columns must be non-empty if specified. Use None for default.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#feature-dependencies","title":"Feature Dependencies","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep","title":"FeatureDep  <code>pydantic-model</code>","text":"<pre><code>FeatureDep(*, feature: str, columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre><pre><code>FeatureDep(*, feature: Sequence[str], columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre><pre><code>FeatureDep(*, feature: FeatureKey, columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre><pre><code>FeatureDep(*, feature: FeatureSpecProtocol, columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre><pre><code>FeatureDep(*, feature: type[BaseFeature], columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None)\n</code></pre> <pre><code>FeatureDep(*, feature: CoercibleToFeatureKey | FeatureSpecProtocol | type[BaseFeature], columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature dependency specification with optional column selection and renaming.</p> <p>Attributes:</p> <ul> <li> <code>key</code>           \u2013            <p>The feature key to depend on. Accepts string (\"a/b/c\"), list ([\"a\", \"b\", \"c\"]), or FeatureKey instance.</p> </li> <li> <code>columns</code>               (<code>tuple[str, ...] | None</code>)           \u2013            <p>Optional tuple of column names to select from upstream feature. - None (default): Keep all columns from upstream - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.) - Tuple of names: Keep only specified columns (plus system columns)</p> </li> <li> <code>rename</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Optional mapping of old column names to new names. Applied after column selection.</p> </li> <li> <code>fields_mapping</code>               (<code>FieldsMapping</code>)           \u2013            <p>Optional field mapping configuration for automatic field dependency resolution. When provided, fields without explicit deps will automatically map to matching upstream fields. Defaults to using <code>[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]</code>.</p> </li> </ul> <p>Examples:</p> <pre><code># Keep all columns with default field mapping\nFeatureDep(feature=\"upstream\")\n\n# Keep all columns with suffix matching\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.default(match_suffix=True))\n\n# Keep all columns with all fields mapping\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.all())\n\n# Keep only specific columns\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\")\n)\n\n# Rename columns to avoid conflicts\nFeatureDep(\n    feature=\"upstream/feature\",\n    rename={\"old_name\": \"new_name\"}\n)\n\n# Select and rename\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\"),\n    rename={\"col1\": \"upstream_col1\"}\n)\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream features and all their fields.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on all upstream fields\\n    &gt;&gt;&gt; AllFieldsMapping()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.all()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    type: Always \\\"DEFAULT\\\" for discriminated union serialization\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\\n\\nExamples:\\n    &gt;&gt;&gt; # Exact match only (default)\\n    &gt;&gt;&gt; DefaultFieldsMapping()\\n\\n    &gt;&gt;&gt; # Enable suffix matching\\n    &gt;&gt;&gt; DefaultFieldsMapping(match_suffix=True)\\n\\n    &gt;&gt;&gt; # Exclude specific fields from being auto-mapped\\n    &gt;&gt;&gt; DefaultFieldsMapping(exclude_fields=[FieldKey([\\\"metadata\\\"])])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\\n\\nArguments:\\n    type: Always \\\"SPECIFIC\\\" for discriminated union serialization\\n    mapping: Mapping of downstream field keys to their corresponding upstream field keys.\\n\\nExamples:\\n    &gt;&gt;&gt; # Explicitly depend on specific upstream fields\\n    &gt;&gt;&gt; SpecificFieldsMapping({\\\"\\\"downstream\\\": {\\\"upstream_1\\\", \\\"upstream_2\\\"}})\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; FieldsMapping.specific({\\\"field1\\\", \\\"field2\\\"})\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        or FeatureKey instance.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n    ```\",\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Columns\"\n    },\n    \"rename\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Rename\"\n    },\n    \"fields_mapping\": {\n      \"$ref\": \"#/$defs/FieldsMapping\"\n    }\n  },\n  \"required\": [\n    \"feature\",\n    \"fields_mapping\"\n  ],\n  \"title\": \"FeatureDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>columns</code>                 (<code>tuple[str, ...] | None</code>)             </li> <li> <code>rename</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>fields_mapping</code>                 (<code>FieldsMapping</code>)             </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(\n    self,\n    *,\n    feature: CoercibleToFeatureKey | FeatureSpecProtocol | type[BaseFeature],\n    columns: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    # Handle different key types with proper type checking\n    resolved_key: FeatureKey\n\n    # Check if it's a BaseFeatureSpec instance (using Protocol)\n    if isinstance(feature, FeatureSpecProtocol):\n        resolved_key = feature.key\n    # Check if it's a Feature class (using Protocol for runtime check)\n    elif isinstance(feature, type) and hasattr(feature, \"spec\"):\n        resolved_key = feature.spec().key\n    # Check if it's already a FeatureKey\n    elif isinstance(feature, FeatureKey):\n        resolved_key = feature\n    else:\n        # Must be a CoercibleToFeatureKey (str or list of str)\n        resolved_key = FeatureKeyAdapter.validate_python(feature)\n\n    super().__init__(\n        feature=resolved_key,\n        columns=columns,\n        rename=rename,\n        fields_mapping=fields_mapping or FieldsMapping.default(),\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.table_name","title":"table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.feature.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature/","title":"Feature","text":"<p><code>BaseFeature</code> is the most important class in Metaxy.</p> <p>Users can extend this class to define their features.</p> <p>! \"Code Version Access\"     Retrieve a feature's code version from its spec: <code>MyFeature.spec().code_version</code>.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey","title":"FeatureKey  <code>pydantic-model</code>","text":"<pre><code>FeatureKey(key: str)\n</code></pre><pre><code>FeatureKey(key: Sequence[str])\n</code></pre><pre><code>FeatureKey(key: Self)\n</code></pre><pre><code>FeatureKey(*parts: str)\n</code></pre><pre><code>FeatureKey(*, parts: Sequence[str])\n</code></pre> <pre><code>FeatureKey(*args: str | _CoercibleToKey | Self, **kwargs: Any)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Feature key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Examples:</p> <pre><code>FeatureKey(\"a/b/c\")  # String format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey([\"a\", \"b\", \"c\"])  # List format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(FeatureKey([\"a\", \"b\", \"c\"]))  # FeatureKey copy\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(\"a\", \"b\", \"c\")  # Variadic format\n# FeatureKey(parts=['a', 'b', 'c'])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n  \"properties\": {\n    \"parts\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Parts\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"parts\"\n  ],\n  \"title\": \"FeatureKey\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>parts</code>                 (<code>tuple[str, ...]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_input</code> </li> <li> <code>_validate_parts_content</code>                 \u2192                   <code>parts</code> </li> </ul> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(self, *args: str | _CoercibleToKey | Self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize FeatureKey from various input types.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.to_string","title":"to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__le__","title":"__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__contains__","title":"__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__reversed__","title":"__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__get_validators__","title":"__get_validators__  <code>classmethod</code>","text":"<pre><code>__get_validators__()\n</code></pre> <p>Pydantic validator for when used as a field type.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef __get_validators__(cls):\n    \"\"\"Pydantic validator for when used as a field type.\"\"\"\n    yield cls.validate\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.validate","title":"validate  <code>classmethod</code>","text":"<pre><code>validate(value: Any) -&gt; FeatureKey\n</code></pre> <p>Convert various inputs to FeatureKey.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef validate(cls, value: Any) -&gt; FeatureKey:\n    \"\"\"Convert various inputs to FeatureKey.\"\"\"\n    if isinstance(value, cls):\n        return value\n    return cls(value)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.model_dump","title":"model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureKey.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature","title":"BaseFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"properties\": {},\n  \"title\": \"BaseFeature\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.table_name","title":"table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_version","title":"feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_spec_version","title":"feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_tracking_version","title":"feature_tracking_version  <code>classmethod</code>","text":"<pre><code>feature_tracking_version() -&gt; str\n</code></pre> <p>Get hash combining feature spec version and project.</p> <p>This version is used in system tables to track when features move between projects or when their specifications change. It combines: - feature_spec_version: Complete feature specification hash - project: The project this feature belongs to</p> <p>This allows the migration system to detect when a feature moves from one project to another, triggering appropriate migrations.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of feature_spec_version + project</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(...)):\n    pass\nMyFeature.feature_tracking_version()  # Combines spec + project\n# 'abc789...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_tracking_version(cls) -&gt; str:\n    \"\"\"Get hash combining feature spec version and project.\n\n    This version is used in system tables to track when features move between projects\n    or when their specifications change. It combines:\n    - feature_spec_version: Complete feature specification hash\n    - project: The project this feature belongs to\n\n    This allows the migration system to detect when a feature moves from one project\n    to another, triggering appropriate migrations.\n\n    Returns:\n        SHA256 hex digest of feature_spec_version + project\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(...)):\n            pass\n        MyFeature.feature_tracking_version()  # Combines spec + project\n        # 'abc789...'\n        ```\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(cls.feature_spec_version().encode())\n    hasher.update(cls.project.encode())\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.provenance_by_field","title":"provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.load_input","title":"load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: UpstreamJoiner, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>UpstreamJoiner</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: \"UpstreamJoiner\",\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.resolve_data_version_diff","title":"resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: MetadataDiffResolver, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>MetadataDiffResolver</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: \"MetadataDiffResolver\",\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.data_versioning.diff import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature","title":"Feature  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFeature</code></p> <p>A default specialization of BaseFeature that uses a <code>sample_uid</code> ID column.</p> Show JSON schema: <pre><code>{\n  \"description\": \"A default specialization of BaseFeature that uses a `sample_uid` ID column.\",\n  \"properties\": {},\n  \"title\": \"Feature\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.Feature.table_name","title":"table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.feature_version","title":"feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.feature_spec_version","title":"feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.feature_tracking_version","title":"feature_tracking_version  <code>classmethod</code>","text":"<pre><code>feature_tracking_version() -&gt; str\n</code></pre> <p>Get hash combining feature spec version and project.</p> <p>This version is used in system tables to track when features move between projects or when their specifications change. It combines: - feature_spec_version: Complete feature specification hash - project: The project this feature belongs to</p> <p>This allows the migration system to detect when a feature moves from one project to another, triggering appropriate migrations.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of feature_spec_version + project</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(...)):\n    pass\nMyFeature.feature_tracking_version()  # Combines spec + project\n# 'abc789...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_tracking_version(cls) -&gt; str:\n    \"\"\"Get hash combining feature spec version and project.\n\n    This version is used in system tables to track when features move between projects\n    or when their specifications change. It combines:\n    - feature_spec_version: Complete feature specification hash\n    - project: The project this feature belongs to\n\n    This allows the migration system to detect when a feature moves from one project\n    to another, triggering appropriate migrations.\n\n    Returns:\n        SHA256 hex digest of feature_spec_version + project\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(...)):\n            pass\n        MyFeature.feature_tracking_version()  # Combines spec + project\n        # 'abc789...'\n        ```\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(cls.feature_spec_version().encode())\n    hasher.update(cls.project.encode())\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.provenance_by_field","title":"provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.load_input","title":"load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: UpstreamJoiner, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>UpstreamJoiner</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: \"UpstreamJoiner\",\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.Feature.resolve_data_version_diff","title":"resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: MetadataDiffResolver, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>MetadataDiffResolver</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: \"MetadataDiffResolver\",\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.data_versioning.diff import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.get_feature_by_key","title":"get_feature_by_key","text":"<pre><code>get_feature_by_key(key: FeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key from the active graph.</p> <p>Convenience function that retrieves Metaxy feature class from the currently active feature graph. Can be useful when receiving a feature key from storage or across process boundaries.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>from metaxy import get_feature_by_key, FeatureKey\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = get_feature_by_key(parent_key)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(key: \"FeatureKey\") -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key from the active graph.\n\n    Convenience function that retrieves Metaxy feature class from the currently active [feature graph][metaxy.FeatureGraph]. Can be useful when receiving a feature key from storage or across process boundaries.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        from metaxy import get_feature_by_key, FeatureKey\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = get_feature_by_key(parent_key)\n        ```\n    \"\"\"\n    graph = FeatureGraph.get_active()\n    return graph.get_feature_by_key(key)\n</code></pre>"},{"location":"reference/api/definitions/field/","title":"Field","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey","title":"FieldKey  <code>pydantic-model</code>","text":"<pre><code>FieldKey(*args: str | _CoercibleToKey | Self, **kwargs: Any)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Examples:</p> <pre><code>FieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(\"a\", \"b\", \"c\")  # Variadic format\n# FieldKey(parts=['a', 'b', 'c'])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n  \"properties\": {\n    \"parts\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Parts\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"parts\"\n  ],\n  \"title\": \"FieldKey\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>parts</code>                 (<code>tuple[str, ...]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_input</code> </li> <li> <code>_validate_parts_content</code>                 \u2192                   <code>parts</code> </li> </ul> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(self, *args: str | _CoercibleToKey | Self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize FieldKey from various input types.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_string","title":"to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__le__","title":"__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__contains__","title":"__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__reversed__","title":"__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__get_validators__","title":"__get_validators__  <code>classmethod</code>","text":"<pre><code>__get_validators__()\n</code></pre> <p>Pydantic validator for when used as a field type.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef __get_validators__(cls):\n    \"\"\"Pydantic validator for when used as a field type.\"\"\"\n    yield cls.validate\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.validate","title":"validate  <code>classmethod</code>","text":"<pre><code>validate(value: Any) -&gt; FieldKey\n</code></pre> <p>Convert various inputs to FieldKey.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>@classmethod\ndef validate(cls, value: Any) -&gt; FieldKey:\n    \"\"\"Convert various inputs to FieldKey.\"\"\"\n    if isinstance(value, cls):\n        return value\n    return cls(value)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.model_dump","title":"model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__hash__","title":"__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__eq__","title":"__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec","title":"FieldSpec  <code>pydantic-model</code>","text":"<pre><code>FieldSpec(key: CoercibleToFieldKey, code_version: str = DEFAULT_CODE_VERSION, deps: SpecialFieldDep | list[FieldDep] | None = None, *args, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FieldKey\"\n    },\n    \"code_version\": {\n      \"default\": \"__metaxy_initial__\",\n      \"title\": \"Code Version\",\n      \"type\": \"string\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/SpecialFieldDep\"\n        },\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldDep\"\n          },\n          \"type\": \"array\"\n        }\n      ],\n      \"title\": \"Deps\"\n    }\n  },\n  \"title\": \"FieldSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FieldKey</code>)             </li> <li> <code>code_version</code>                 (<code>str</code>)             </li> <li> <code>deps</code>                 (<code>SpecialFieldDep | list[FieldDep]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    key: CoercibleToFieldKey,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n    *args,\n    **kwargs: Any,\n) -&gt; None:\n    validated_key = FieldKeyAdapter.validate_python(key)\n\n    # Handle None deps - use empty list as default\n    if deps is None:\n        deps = []\n\n    super().__init__(\n        key=validated_key,\n        code_version=code_version,\n        deps=deps,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec.__get_pydantic_core_schema__","title":"__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(source_type, handler)\n</code></pre> <p>Add custom validator to coerce strings to FieldSpec.</p> Source code in <code>src/metaxy/models/field.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler):\n    \"\"\"Add custom validator to coerce strings to FieldSpec.\"\"\"\n    from pydantic_core import core_schema\n\n    # Get the default schema\n    python_schema = handler(source_type)\n\n    # Wrap it with a before validator that converts strings\n    return core_schema.no_info_before_validator_function(\n        _validate_field_spec_from_string,\n        python_schema,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldDep","title":"FieldDep  <code>pydantic-model</code>","text":"<pre><code>FieldDep(feature: CoercibleToFeatureKey | FeatureSpec | type[Feature], fields: list[CoercibleToFieldKey] | Literal[ALL] = ALL, *args, **kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FeatureKey\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExamples:\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(\\\"a\\\", \\\"b\\\", \\\"c\\\")  # Variadic format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"properties\": {\n        \"parts\": {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Parts\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"parts\"\n      ],\n      \"title\": \"FieldKey\",\n      \"type\": \"object\"\n    }\n  },\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"const\": \"__METAXY_ALL_DEP__\",\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"__METAXY_ALL_DEP__\",\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FieldDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldKey] | Literal[ALL]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    feature: \"CoercibleToFeatureKey | FeatureSpec | type[Feature]\",\n    fields: list[CoercibleToFieldKey]\n    | Literal[SpecialFieldDep.ALL] = SpecialFieldDep.ALL,\n    *args,\n    **kwargs,\n):\n    from metaxy.models.feature import Feature\n    from metaxy.models.feature_spec import FeatureSpec\n\n    if isinstance(feature, FeatureSpec):\n        feature_key = feature.key\n    elif isinstance(feature, type) and issubclass(feature, Feature):\n        feature_key = feature.spec().key\n    else:\n        feature_key = FeatureKeyAdapter.validate_python(feature)\n\n    assert isinstance(feature_key, FeatureKey)\n\n    if isinstance(fields, list):\n        validated_fields: Any = TypeAdapter(list[FieldKey]).validate_python(fields)\n    else:\n        validated_fields = fields  # Keep the enum value as-is\n\n    super().__init__(feature=feature_key, fields=validated_fields, *args, **kwargs)\n</code></pre>"},{"location":"reference/api/definitions/graph/","title":"Feature Graph","text":"<p><code>FeatureGraph</code> is a global \"God\" object that holds all the features loaded by Metaxy via the feature discovery mechanism.</p> <p>Users may interact with <code>FeatureGraph</code> when writing custom migrations, otherwise they are not exposed to it.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph","title":"FeatureGraph","text":"<pre><code>FeatureGraph()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def __init__(self):\n    self.features_by_key: dict[FeatureKey, type[BaseFeature]] = {}\n    self.feature_specs_by_key: dict[FeatureKey, BaseFeatureSpecWithIDColumns] = {}\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.snapshot_version","title":"snapshot_version  <code>property</code>","text":"<pre><code>snapshot_version: str\n</code></pre> <p>Generate a snapshot version representing the current topology + versions of the feature graph</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-functions","title":"Functions","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature","title":"add_feature","text":"<pre><code>add_feature(feature: type[BaseFeature]) -&gt; None\n</code></pre> <p>Add a feature to the graph.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to register</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a feature with the same key is already registered        or if duplicate column names would result from renaming operations</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature(self, feature: type[\"BaseFeature\"]) -&gt; None:\n    \"\"\"Add a feature to the graph.\n\n    Args:\n        feature: Feature class to register\n\n    Raises:\n        ValueError: If a feature with the same key is already registered\n                   or if duplicate column names would result from renaming operations\n    \"\"\"\n    if feature.spec().key in self.features_by_key:\n        existing = self.features_by_key[feature.spec().key]\n        raise ValueError(\n            f\"Feature with key {feature.spec().key.to_string()} already registered. \"\n            f\"Existing: {existing.__name__}, New: {feature.__name__}. \"\n            f\"Each feature key must be unique within a graph.\"\n        )\n\n    # Validate that there are no duplicate column names across dependencies after renaming\n    if feature.spec().deps:\n        self._validate_no_duplicate_columns(feature.spec())\n\n    self.features_by_key[feature.spec().key] = feature\n    self.feature_specs_by_key[feature.spec().key] = feature.spec()\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.remove_feature","title":"remove_feature","text":"<pre><code>remove_feature(key: FeatureKey) -&gt; None\n</code></pre> <p>Remove a feature from the graph.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to remove</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def remove_feature(self, key: FeatureKey) -&gt; None:\n    \"\"\"Remove a feature from the graph.\n\n    Args:\n        key: Feature key to remove\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    if key not in self.features_by_key:\n        raise KeyError(\n            f\"No feature with key {key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.features_by_key.keys()]}\"\n        )\n\n    del self.features_by_key[key]\n    del self.feature_specs_by_key[key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_by_key","title":"get_feature_by_key","text":"<pre><code>get_feature_by_key(key: FeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>graph = FeatureGraph.get_active()\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = graph.get_feature_by_key(parent_key)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(self, key: FeatureKey) -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        graph = FeatureGraph.get_active()\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = graph.get_feature_by_key(parent_key)\n        ```\n    \"\"\"\n    if key not in self.features_by_key:\n        raise KeyError(\n            f\"No feature with key {key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.features_by_key.keys()]}\"\n        )\n    return self.features_by_key[key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version_by_field","title":"get_feature_version_by_field","text":"<pre><code>get_feature_version_by_field(key: FeatureKey) -&gt; dict[str, str]\n</code></pre> <p>Computes the field provenance map for a feature.</p> <p>Hash together field provenance entries with the feature code version.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>dict[str, str]: The provenance hash for each field in the feature plan. Keys are field names as strings.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version_by_field(self, key: FeatureKey) -&gt; dict[str, str]:\n    \"\"\"Computes the field provenance map for a feature.\n\n    Hash together field provenance entries with the feature code version.\n\n    Returns:\n        dict[str, str]: The provenance hash for each field in the feature plan.\n            Keys are field names as strings.\n    \"\"\"\n    res = {}\n\n    plan = self.get_feature_plan(key)\n\n    for k, v in plan.feature.fields_by_key.items():\n        res[k.to_string()] = self.get_field_version(\n            FQFieldKey(field=k, feature=key)\n        )\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version","title":"get_feature_version","text":"<pre><code>get_feature_version(key: FeatureKey) -&gt; str\n</code></pre> <p>Computes the feature version as a single string</p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version(self, key: FeatureKey) -&gt; str:\n    \"\"\"Computes the feature version as a single string\"\"\"\n    hasher = hashlib.sha256()\n    provenance_by_field = self.get_feature_version_by_field(key)\n    for field_key in sorted(provenance_by_field):\n        hasher.update(field_key.encode())\n        hasher.update(provenance_by_field[field_key].encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_downstream_features","title":"get_downstream_features","text":"<pre><code>get_downstream_features(sources: list[FeatureKey]) -&gt; list[FeatureKey]\n</code></pre> <p>Get all features downstream of sources, topologically sorted.</p> <p>Performs a depth-first traversal of the dependency graph to find all features that transitively depend on any of the source features.</p> <p>Parameters:</p> <ul> <li> <code>sources</code>               (<code>list[FeatureKey]</code>)           \u2013            <p>List of source feature keys</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of downstream feature keys in topological order (dependencies first).</p> </li> <li> <code>list[FeatureKey]</code>           \u2013            <p>Does not include the source features themselves.</p> </li> </ul> Example <pre><code># DAG: A -&gt; B -&gt; D\n#      A -&gt; C -&gt; D\ngraph.get_downstream_features([FeatureKey([\"A\"])])\n# [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_downstream_features(self, sources: list[FeatureKey]) -&gt; list[FeatureKey]:\n    \"\"\"Get all features downstream of sources, topologically sorted.\n\n    Performs a depth-first traversal of the dependency graph to find all\n    features that transitively depend on any of the source features.\n\n    Args:\n        sources: List of source feature keys\n\n    Returns:\n        List of downstream feature keys in topological order (dependencies first).\n        Does not include the source features themselves.\n\n    Example:\n        ```py\n        # DAG: A -&gt; B -&gt; D\n        #      A -&gt; C -&gt; D\n        graph.get_downstream_features([FeatureKey([\"A\"])])\n        # [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n        ```\n    \"\"\"\n    source_set = set(sources)\n    visited = set()\n    post_order = []  # Reverse topological order\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS traversal.\"\"\"\n        if key in visited:\n            return\n        visited.add(key)\n\n        # Find all features that depend on this one\n        for feature_key, feature_spec in self.feature_specs_by_key.items():\n            if feature_spec.deps:\n                for dep in feature_spec.deps:\n                    if dep.feature == key:\n                        # This feature depends on 'key', so visit it\n                        visit(feature_key)\n\n        post_order.append(key)\n\n    # Visit all sources\n    for source in sources:\n        visit(source)\n\n    # Remove sources from result, reverse to get topological order\n    result = [k for k in reversed(post_order) if k not in source_set]\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.to_snapshot","title":"to_snapshot","text":"<pre><code>to_snapshot() -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Serialize graph to snapshot format.</p> <p>Returns a dict mapping feature_key (string) to feature data dict, including the import path of the Feature class for reconstruction.</p> <p>Returns:</p> <ul> <li> <code>dict[str, dict[str, Any]]</code>           \u2013            <p>Dict of feature_key -&gt; { feature_spec: dict, feature_version: str, feature_spec_version: str, feature_tracking_version: str, feature_class_path: str, project: str</p> </li> <li> <code>dict[str, dict[str, Any]]</code>           \u2013            <p>}</p> </li> </ul> Example <pre><code>snapshot = graph.to_snapshot()\nsnapshot[\"video_processing\"][\"feature_version\"]\n# 'abc12345'\nsnapshot[\"video_processing\"][\"feature_spec_version\"]\n# 'def67890'\nsnapshot[\"video_processing\"][\"feature_tracking_version\"]\n# 'xyz98765'\nsnapshot[\"video_processing\"][\"feature_class_path\"]\n# 'myapp.features.video.VideoProcessing'\nsnapshot[\"video_processing\"][\"project\"]\n# 'myapp'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def to_snapshot(self) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Serialize graph to snapshot format.\n\n    Returns a dict mapping feature_key (string) to feature data dict,\n    including the import path of the Feature class for reconstruction.\n\n    Returns:\n        Dict of feature_key -&gt; {\n            feature_spec: dict,\n            feature_version: str,\n            feature_spec_version: str,\n            feature_tracking_version: str,\n            feature_class_path: str,\n            project: str\n        }\n\n    Example:\n        ```py\n        snapshot = graph.to_snapshot()\n        snapshot[\"video_processing\"][\"feature_version\"]\n        # 'abc12345'\n        snapshot[\"video_processing\"][\"feature_spec_version\"]\n        # 'def67890'\n        snapshot[\"video_processing\"][\"feature_tracking_version\"]\n        # 'xyz98765'\n        snapshot[\"video_processing\"][\"feature_class_path\"]\n        # 'myapp.features.video.VideoProcessing'\n        snapshot[\"video_processing\"][\"project\"]\n        # 'myapp'\n        ```\n    \"\"\"\n    snapshot = {}\n\n    for feature_key, feature_cls in self.features_by_key.items():\n        feature_key_str = feature_key.to_string()\n        feature_spec_dict = feature_cls.spec().model_dump(mode=\"json\")  # type: ignore[attr-defined]\n        feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n        feature_spec_version = feature_cls.spec().feature_spec_version  # type: ignore[attr-defined]\n        feature_tracking_version = feature_cls.feature_tracking_version()  # type: ignore[attr-defined]\n        project = feature_cls.project  # type: ignore[attr-defined]\n\n        # Get class import path (module.ClassName)\n        class_path = f\"{feature_cls.__module__}.{feature_cls.__name__}\"\n\n        snapshot[feature_key_str] = {\n            \"feature_spec\": feature_spec_dict,\n            \"feature_version\": feature_version,\n            \"feature_spec_version\": feature_spec_version,\n            \"feature_tracking_version\": feature_tracking_version,\n            \"feature_class_path\": class_path,\n            \"project\": project,\n        }\n\n    return snapshot\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.from_snapshot","title":"from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(snapshot_data: dict[str, dict[str, Any]], *, class_path_overrides: dict[str, str] | None = None, force_reload: bool = False) -&gt; FeatureGraph\n</code></pre> <p>Reconstruct graph from snapshot by importing Feature classes.</p> <p>Strictly requires Feature classes to exist at their recorded import paths. This ensures custom methods (like load_input) are available.</p> <p>If a feature has been moved/renamed, use class_path_overrides to specify the new location.</p> <p>Parameters:</p> <ul> <li> <code>snapshot_data</code>               (<code>dict[str, dict[str, Any]]</code>)           \u2013            <p>Dict of feature_key -&gt; dict containing feature_spec (dict), feature_class_path (str), and other fields as returned by to_snapshot() or loaded from DB</p> </li> <li> <code>class_path_overrides</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dict mapping feature_key to new class path                  for features that have been moved/renamed</p> </li> <li> <code>force_reload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, reload modules from disk to get current code state.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>New FeatureGraph with historical features</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If feature class cannot be imported at recorded path</p> </li> </ul> Example <pre><code># Load snapshot from metadata store\nhistorical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n# With override for moved feature\nhistorical_graph = FeatureGraph.from_snapshot(\n    snapshot_data,\n    class_path_overrides={\n        \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n    }\n)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef from_snapshot(\n    cls,\n    snapshot_data: dict[str, dict[str, Any]],\n    *,\n    class_path_overrides: dict[str, str] | None = None,\n    force_reload: bool = False,\n) -&gt; \"FeatureGraph\":\n    \"\"\"Reconstruct graph from snapshot by importing Feature classes.\n\n    Strictly requires Feature classes to exist at their recorded import paths.\n    This ensures custom methods (like load_input) are available.\n\n    If a feature has been moved/renamed, use class_path_overrides to specify\n    the new location.\n\n    Args:\n        snapshot_data: Dict of feature_key -&gt; dict containing\n            feature_spec (dict), feature_class_path (str), and other fields\n            as returned by to_snapshot() or loaded from DB\n        class_path_overrides: Optional dict mapping feature_key to new class path\n                             for features that have been moved/renamed\n        force_reload: If True, reload modules from disk to get current code state.\n\n    Returns:\n        New FeatureGraph with historical features\n\n    Raises:\n        ImportError: If feature class cannot be imported at recorded path\n\n    Example:\n        ```py\n        # Load snapshot from metadata store\n        historical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n        # With override for moved feature\n        historical_graph = FeatureGraph.from_snapshot(\n            snapshot_data,\n            class_path_overrides={\n                \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n            }\n        )\n        ```\n    \"\"\"\n    import importlib\n    import sys\n\n    graph = cls()\n    class_path_overrides = class_path_overrides or {}\n\n    # If force_reload, collect all module paths first to remove ALL features\n    # from those modules before reloading (modules can have multiple features)\n    modules_to_reload = set()\n    if force_reload:\n        for feature_key_str, feature_data in snapshot_data.items():\n            class_path = class_path_overrides.get(\n                feature_key_str\n            ) or feature_data.get(\"feature_class_path\")\n            if class_path:\n                module_path, _ = class_path.rsplit(\".\", 1)\n                if module_path in sys.modules:\n                    modules_to_reload.add(module_path)\n\n    # Use context manager to temporarily set the new graph as active\n    # This ensures imported Feature classes register to the new graph, not the current one\n    with graph.use():\n        for feature_key_str, feature_data in snapshot_data.items():\n            # Parse FeatureSpec for validation\n            feature_spec_dict = feature_data[\"feature_spec\"]\n            FeatureSpec.model_validate(feature_spec_dict)\n\n            # Get class path (check overrides first)\n            if feature_key_str in class_path_overrides:\n                class_path = class_path_overrides[feature_key_str]\n            else:\n                class_path = feature_data.get(\"feature_class_path\")\n                if not class_path:\n                    raise ValueError(\n                        f\"Feature '{feature_key_str}' has no feature_class_path in snapshot. \"\n                        f\"Cannot reconstruct historical graph.\"\n                    )\n\n            # Import the class\n            try:\n                module_path, class_name = class_path.rsplit(\".\", 1)\n\n                # Force reload module from disk if requested\n                # This is critical for migration detection - when code changes,\n                # we need fresh imports to detect the changes\n                if force_reload and module_path in modules_to_reload:\n                    # Before first reload of this module, remove ALL features from this module\n                    # (a module can define multiple features)\n                    if module_path in modules_to_reload:\n                        # Find all features from this module in snapshot and remove them\n                        for fk_str, fd in snapshot_data.items():\n                            fcp = class_path_overrides.get(fk_str) or fd.get(\n                                \"feature_class_path\"\n                            )\n                            if fcp and fcp.rsplit(\".\", 1)[0] == module_path:\n                                fspec_dict = fd[\"feature_spec\"]\n                                fspec = FeatureSpec.model_validate(fspec_dict)\n                                if fspec.key in graph.features_by_key:\n                                    graph.remove_feature(fspec.key)\n\n                        # Mark module as processed so we don't remove features again\n                        modules_to_reload.discard(module_path)\n\n                    module = importlib.reload(sys.modules[module_path])\n                else:\n                    module = __import__(module_path, fromlist=[class_name])\n\n                feature_cls = getattr(module, class_name)\n            except (ImportError, AttributeError) as e:\n                raise ImportError(\n                    f\"Cannot import Feature class '{class_path}' for feature graph reconstruction from snapshot. \"\n                    f\"Feature '{feature_key_str}' is required to reconstruct the graph, but the class \"\n                    f\"cannot be found at the recorded import path. \"\n                ) from e\n\n            # Validate the imported class matches the stored spec\n            if not hasattr(feature_cls, \"spec\"):\n                raise TypeError(\n                    f\"Imported class '{class_path}' is not a valid Feature class \"\n                    f\"(missing 'spec' attribute)\"\n                )\n\n            # Register the imported feature to this graph if not already present\n            # If the module was imported for the first time, the metaclass already registered it\n            # If the module was previously imported, we need to manually register it\n            if feature_cls.spec().key not in graph.features_by_key:\n                graph.add_feature(feature_cls)\n\n    return graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_active","title":"get_active  <code>classmethod</code>","text":"<pre><code>get_active() -&gt; FeatureGraph\n</code></pre> <p>Get the currently active graph.</p> <p>Returns the graph from the context variable if set, otherwise returns the default global graph.</p> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>Active FeatureGraph instance</p> </li> </ul> Example <pre><code># Normal usage - returns default graph\nreg = FeatureGraph.get_active()\n\n# With custom graph in context\nwith my_graph.use():\n    reg = FeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef get_active(cls) -&gt; \"FeatureGraph\":\n    \"\"\"Get the currently active graph.\n\n    Returns the graph from the context variable if set, otherwise returns\n    the default global graph.\n\n    Returns:\n        Active FeatureGraph instance\n\n    Example:\n        ```py\n        # Normal usage - returns default graph\n        reg = FeatureGraph.get_active()\n\n        # With custom graph in context\n        with my_graph.use():\n            reg = FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    return _active_graph.get() or graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.set_active","title":"set_active  <code>classmethod</code>","text":"<pre><code>set_active(reg: FeatureGraph) -&gt; None\n</code></pre> <p>Set the active graph for the current context.</p> <p>This sets the context variable that will be returned by get_active(). Typically used in application setup code or test fixtures.</p> <p>Parameters:</p> <ul> <li> <code>reg</code>               (<code>FeatureGraph</code>)           \u2013            <p>FeatureGraph to activate</p> </li> </ul> Example <pre><code># In application setup\nmy_graph = FeatureGraph()\nFeatureGraph.set_active(my_graph)\n\n# Now all operations use my_graph\nFeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef set_active(cls, reg: \"FeatureGraph\") -&gt; None:\n    \"\"\"Set the active graph for the current context.\n\n    This sets the context variable that will be returned by get_active().\n    Typically used in application setup code or test fixtures.\n\n    Args:\n        reg: FeatureGraph to activate\n\n    Example:\n        ```py\n        # In application setup\n        my_graph = FeatureGraph()\n        FeatureGraph.set_active(my_graph)\n\n        # Now all operations use my_graph\n        FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    _active_graph.set(reg)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.use","title":"use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Context manager to temporarily use this graph as active.</p> <p>This is the recommended way to use custom registries, especially in tests. The graph is automatically restored when the context exits.</p> <p>Yields:</p> <ul> <li> <code>FeatureGraph</code> (              <code>Self</code> )          \u2013            <p>This graph instance</p> </li> </ul> Example <pre><code>test_graph = FeatureGraph()\n\nwith test_graph.use():\n    # All operations use test_graph\n    class TestFeature(Feature, spec=...):\n        pass\n\n# Outside context, back to previous graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Context manager to temporarily use this graph as active.\n\n    This is the recommended way to use custom registries, especially in tests.\n    The graph is automatically restored when the context exits.\n\n    Yields:\n        FeatureGraph: This graph instance\n\n    Example:\n        ```py\n        test_graph = FeatureGraph()\n\n        with test_graph.use():\n            # All operations use test_graph\n            class TestFeature(Feature, spec=...):\n                pass\n\n        # Outside context, back to previous graph\n        ```\n    \"\"\"\n    token = _active_graph.set(self)\n    try:\n        yield self\n    finally:\n        _active_graph.reset(token)\n</code></pre>"},{"location":"reference/api/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata behind an interface called MetadaStore.</p> <p>Users can extend this class to implement support for arbitrary metadata storage such as databases, lakehouse formats, or really any kind of external system.</p> <p>Metaxy has built-in support for the following metadata store types:</p>"},{"location":"reference/api/metadata-stores/#databases","title":"Databases","text":"<p>See IbisMetadataStore.</p>"},{"location":"reference/api/metadata-stores/#in-memory","title":"In-memory","text":"<p>See InMemoryMetadataStore.</p>"},{"location":"reference/api/metadata-stores/#metadata-store-interface","title":"Metadata Store Interface","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore","title":"MetadataStore","text":"<pre><code>MetadataStore(*, hash_algorithm: HashAlgorithm | None = None, hash_truncation_length: int | None = None, prefer_native: bool = True, fallback_stores: list[MetadataStore] | None = None, auto_create_tables: bool | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for metadata storage backends.</p> <p>Supports: - Append-only metadata storage patterns</p> <ul> <li> <p>Composable fallback store chains for development and testing purposes</p> </li> <li> <p>Backend-specific computation optimizations</p> </li> </ul> Context Manager <p>Stores must be used as context managers for resource management.</p> <p>Parameters:</p> <ul> <li> <code>hash_algorithm</code>               (<code>HashAlgorithm | None</code>, default:                   <code>None</code> )           \u2013            <p>Hash algorithm to use for field provenance. Default: None (uses default algorithm for this store type)</p> </li> <li> <code>hash_truncation_length</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Length to truncate hashes to (minimum 8). Default: None (uses global setting or no truncation)</p> </li> <li> <code>prefer_native</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, prefer native field provenance calculations when possible. If False, always use Polars components. Default: True</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. Used when upstream features are not in this store.</p> </li> <li> <code>auto_create_tables</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, automatically create tables when opening the store. If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var). If False, never auto-create tables. WARNING: Auto-create is intended for development/testing only. Do not use in production. Use proper database migration tools like Alembic for production deployments. Default: None (reads from global config, falls back to False for safety)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If fallback stores use different hash algorithms or truncation lengths</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    hash_algorithm: HashAlgorithm | None = None,\n    hash_truncation_length: int | None = None,\n    prefer_native: bool = True,\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_create_tables: bool | None = None,\n):\n    \"\"\"\n    Initialize metadata store.\n\n    Args:\n        hash_algorithm: Hash algorithm to use for field provenance.\n            Default: None (uses default algorithm for this store type)\n        hash_truncation_length: Length to truncate hashes to (minimum 8).\n            Default: None (uses global setting or no truncation)\n        prefer_native: If True, prefer native field provenance calculations when possible.\n            If False, always use Polars components. Default: True\n        fallback_stores: Ordered list of read-only fallback stores.\n            Used when upstream features are not in this store.\n        auto_create_tables: If True, automatically create tables when opening the store.\n            If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var).\n            If False, never auto-create tables.\n            WARNING: Auto-create is intended for development/testing only. Do not use in production.\n            Use proper database migration tools like Alembic for production deployments.\n            Default: None (reads from global config, falls back to False for safety)\n\n    Raises:\n        ValueError: If fallback stores use different hash algorithms or truncation lengths\n    \"\"\"\n    # Initialize state early so properties can check it\n    self._is_open = False\n    self._context_depth = 0\n    self._prefer_native = prefer_native\n    self._allow_cross_project_writes = False\n\n    # Resolve auto_create_tables from global config if not explicitly provided\n    if auto_create_tables is None:\n        from metaxy.config import MetaxyConfig\n\n        self.auto_create_tables = MetaxyConfig.get().auto_create_tables\n    else:\n        self.auto_create_tables = auto_create_tables\n\n    # Use store's default algorithm if not specified\n    if hash_algorithm is None:\n        hash_algorithm = self._get_default_hash_algorithm()\n\n    self.hash_algorithm = hash_algorithm\n\n    # Set hash truncation length - use explicit value or get from global setting\n    if hash_truncation_length is not None:\n        # Validate minimum length\n        if hash_truncation_length &lt; 8:\n            raise ValueError(\n                f\"hash_truncation_length must be at least 8 characters, got {hash_truncation_length}\"\n            )\n        self.hash_truncation_length = hash_truncation_length\n    else:\n        # Use global setting if available\n        from metaxy.utils.hashing import get_hash_truncation_length\n\n        self.hash_truncation_length = get_hash_truncation_length()\n\n    self.fallback_stores = fallback_stores or []\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.open","title":"open  <code>abstractmethod</code>","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open/initialize the store for operations.</p> <p>Called by enter. Subclasses implement connection setup here. Can be called manually but context manager usage is recommended.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef open(self) -&gt; None:\n    \"\"\"Open/initialize the store for operations.\n\n    Called by __enter__. Subclasses implement connection setup here.\n    Can be called manually but context manager usage is recommended.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.close","title":"close  <code>abstractmethod</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close/cleanup the store.</p> <p>Called by exit. Subclasses implement connection cleanup here. Can be called manually but context manager usage is recommended.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close/cleanup the store.\n\n    Called by __exit__. Subclasses implement connection cleanup here.\n    Can be called manually but context manager usage is recommended.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    # Track nesting depth\n    self._context_depth += 1\n\n    # Only open on first enter\n    if self._context_depth == 1:\n        # Warn if auto_create_tables is enabled (and store wants warnings)\n        if self.auto_create_tables and self._should_warn_auto_create_tables:\n            import warnings\n\n            warnings.warn(\n                f\"AUTO_CREATE_TABLES is enabled for {self.display()} - \"\n                \"do not use in production! \"\n                \"Use proper database migration tools like Alembic for production deployments.\",\n                UserWarning,\n                stacklevel=3,  # stacklevel=3 to point to user's 'with store:' line\n            )\n\n        self.open()\n        self._is_open = True\n\n        # Validate after opening (when all components are ready)\n        self._validate_after_open()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit context manager.\"\"\"\n    # Decrement depth\n    self._context_depth -= 1\n\n    # Only close when fully exited\n    if self._context_depth == 0:\n        self._is_open = False\n        self.close()\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Check if this store can support the algorithm\n    # Try native field provenance calculations first (if supported), then Polars\n    supported_algorithms = []\n\n    if self._supports_native_components():\n        try:\n            _, calculator, _ = self._create_native_components()\n            supported_algorithms = calculator.supported_algorithms\n        except Exception:\n            # If native field provenance calculations fail, fall back to Polars\n            pass\n\n    # If no native support or prefer_native=False, use Polars\n    if not supported_algorithms:\n        polars_calc = PolarsProvenanceByFieldCalculator()\n        supported_algorithms = polars_calc.supported_algorithms\n\n    if self.hash_algorithm not in supported_algorithms:\n        from metaxy.metadata_store.exceptions import (\n            HashAlgorithmNotSupportedError,\n        )\n\n        raise HashAlgorithmNotSupportedError(\n            f\"Hash algorithm {self.hash_algorithm} not supported by {self.__class__.__name__}. \"\n            f\"Supported: {supported_algorithms}\"\n        )\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: DataFrame[Any] | DataFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (immutable, append-only).</p> <p>Automatically adds 'feature_version' column from current code state, unless the DataFrame already contains one (useful for migrations).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>DataFrame[Any] | DataFrame</code>)           \u2013            <p>Narwhals DataFrame or Polars DataFrame containing metadata. Must have 'provenance_by_field' column of type Struct with fields matching feature's fields. May optionally contain 'feature_version' column (for migrations).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Always writes to current store, never to fallback stores.</li> <li>If df already contains 'feature_version' column, it will be used   as-is (no replacement). This allows migrations to write historical   versions. A warning is issued unless suppressed via context manager.</li> <li>Project validation is performed unless disabled via allow_cross_project_writes()</li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: nw.DataFrame[Any] | pl.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (immutable, append-only).\n\n    Automatically adds 'feature_version' column from current code state,\n    unless the DataFrame already contains one (useful for migrations).\n\n    Args:\n        feature: Feature to write metadata for\n        df: Narwhals DataFrame or Polars DataFrame containing metadata.\n            Must have 'provenance_by_field' column of type Struct with fields matching feature's fields.\n            May optionally contain 'feature_version' column (for migrations).\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Always writes to current store, never to fallback stores.\n        - If df already contains 'feature_version' column, it will be used\n          as-is (no replacement). This allows migrations to write historical\n          versions. A warning is issued unless suppressed via context manager.\n        - Project validation is performed unless disabled via allow_cross_project_writes()\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Narwhals to Polars if needed\n    if isinstance(df, nw.DataFrame):\n        df = df.to_polars()\n    # nw.DataFrame also matches as DataFrame in some contexts, ensure it's Polars\n    if not isinstance(df, pl.DataFrame):\n        # Must be some other type - shouldn't happen but handle defensively\n        if hasattr(df, \"to_polars\"):\n            df = df.to_polars()\n        elif hasattr(df, \"to_pandas\"):\n            df = pl.from_pandas(df.to_pandas())\n        else:\n            raise TypeError(f\"Cannot convert {type(df)} to Polars DataFrame\")\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df)\n        self._write_metadata_impl(feature_key, df)\n        return\n\n    # For regular features: add feature_version and snapshot_version, validate, and write\n    # Check if feature_version and snapshot_version already exist in DataFrame\n    if \"feature_version\" in df.columns and \"snapshot_version\" in df.columns:\n        # DataFrame already has feature_version and snapshot_version - use as-is\n        # This is intended for migrations writing historical versions\n        # Issue a warning unless we're in a suppression context\n        if not _suppress_feature_version_warning.get():\n            import warnings\n\n            warnings.warn(\n                f\"Writing metadata for {feature_key.to_string()} with existing \"\n                f\"feature_version and snapshot_version columns. This is intended for migrations only. \"\n                f\"Normal code should let write_metadata() add the current versions automatically.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Get current feature version and snapshot_version from code and add them\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            current_feature_version = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            feature_cls = graph.features_by_key[feature_key]\n            current_feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n\n        # Get snapshot_version from active graph\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n        current_snapshot_version = graph.snapshot_version\n\n        df = df.with_columns(\n            [\n                pl.lit(current_feature_version).alias(\"feature_version\"),\n                pl.lit(current_snapshot_version).alias(\"snapshot_version\"),\n            ]\n        )\n\n    # Validate schema\n    self._validate_schema(df)\n\n    # Rename provenance_by_field -&gt; metaxy_provenance_by_field for database storage\n    # (Python code uses provenance_by_field, database uses metaxy_provenance_by_field)\n    df = df.rename({\"provenance_by_field\": \"metaxy_provenance_by_field\"})\n\n    # Write metadata\n    self._write_metadata_impl(feature_key, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.record_feature_graph_snapshot","title":"record_feature_graph_snapshot","text":"<pre><code>record_feature_graph_snapshot() -&gt; SnapshotPushResult\n</code></pre> <p>Record all features in graph with a graph snapshot version.</p> <p>This should be called during CD (Continuous Deployment) to record what feature versions are being deployed. Typically invoked via <code>metaxy graph push</code>.</p> <p>Records all features in the graph with the same snapshot_version, representing a consistent state of the entire feature graph based on code definitions.</p> <p>The snapshot_version is a deterministic hash of all feature_version hashes in the graph, making it idempotent - calling multiple times with the same feature definitions produces the same snapshot_version.</p> <p>This method detects three scenarios: 1. New snapshot (computational changes): No existing rows with this snapshot_version 2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version 3. No changes: Snapshot exists with identical feature_spec_versions for all features</p> <p>Returns: SnapshotPushResult</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def record_feature_graph_snapshot(self) -&gt; SnapshotPushResult:\n    \"\"\"Record all features in graph with a graph snapshot version.\n\n    This should be called during CD (Continuous Deployment) to record what\n    feature versions are being deployed. Typically invoked via `metaxy graph push`.\n\n    Records all features in the graph with the same snapshot_version, representing\n    a consistent state of the entire feature graph based on code definitions.\n\n    The snapshot_version is a deterministic hash of all feature_version hashes\n    in the graph, making it idempotent - calling multiple times with the\n    same feature definitions produces the same snapshot_version.\n\n    This method detects three scenarios:\n    1. New snapshot (computational changes): No existing rows with this snapshot_version\n    2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version\n    3. No changes: Snapshot exists with identical feature_spec_versions for all features\n\n    Returns: SnapshotPushResult\n    \"\"\"\n\n    from metaxy.models.feature import FeatureGraph\n\n    graph = FeatureGraph.get_active()\n\n    # Use to_snapshot() to get the snapshot dict\n    snapshot_dict = graph.to_snapshot()\n\n    # Generate deterministic snapshot_version from graph\n    snapshot_version = graph.snapshot_version\n\n    # Read existing feature versions once\n    try:\n        existing_versions_lazy = self.read_metadata_in_store(FEATURE_VERSIONS_KEY)\n        # Materialize to Polars for iteration\n        existing_versions = (\n            existing_versions_lazy.collect().to_polars()\n            if existing_versions_lazy is not None\n            else None\n        )\n    except Exception:\n        # Table doesn't exist yet\n        existing_versions = None\n\n    # Get project from any feature in the graph (all should have the same project)\n    # Default to empty string if no features in graph\n    if graph.features_by_key:\n        # Get first feature's project\n        first_feature = next(iter(graph.features_by_key.values()))\n        project_name = first_feature.project  # type: ignore[attr-defined]\n    else:\n        project_name = \"\"\n\n    # Check if this exact snapshot already exists for this project\n    snapshot_already_exists = False\n    existing_spec_versions: dict[str, str] = {}\n\n    if existing_versions is not None:\n        # Check if project column exists (it may not in old tables)\n        if \"project\" in existing_versions.columns:\n            snapshot_rows = existing_versions.filter(\n                (pl.col(\"snapshot_version\") == snapshot_version)\n                &amp; (pl.col(\"project\") == project_name)\n            )\n        else:\n            # Old table without project column - just check snapshot_version\n            snapshot_rows = existing_versions.filter(\n                pl.col(\"snapshot_version\") == snapshot_version\n            )\n        snapshot_already_exists = snapshot_rows.height &gt; 0\n\n        if snapshot_already_exists:\n            # Check if feature_spec_version column exists (backward compatibility)\n            # Old records (before issue #77) won't have this column\n            has_spec_version = \"feature_spec_version\" in snapshot_rows.columns\n\n            if has_spec_version:\n                # Build dict of existing feature_key -&gt; feature_spec_version\n                for row in snapshot_rows.iter_rows(named=True):\n                    existing_spec_versions[row[\"feature_key\"]] = row[\n                        \"feature_spec_version\"\n                    ]\n            # If no spec_version column, existing_spec_versions remains empty\n            # This means we'll treat it as \"no metadata changes\" (conservative approach)\n\n    # Scenario 1: New snapshot (no existing rows)\n    if not snapshot_already_exists:\n        # Build records from snapshot_dict\n        records = []\n        for feature_key_str in sorted(snapshot_dict.keys()):\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            # Always record all features for this snapshot (don't skip based on feature_version alone)\n            # Each snapshot must be complete to support migration detection\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write all new records at once\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=False,\n            metadata_changed=False,\n            features_with_spec_changes=[],\n        )\n\n    # Scenario 2 &amp; 3: Snapshot exists - check for metadata changes\n    features_with_spec_changes = []\n\n    for feature_key_str, feature_data in snapshot_dict.items():\n        current_spec_version = feature_data[\"feature_spec_version\"]\n        existing_spec_version = existing_spec_versions.get(feature_key_str)\n\n        if existing_spec_version != current_spec_version:\n            features_with_spec_changes.append(feature_key_str)\n\n    # If metadata changed, append new rows for affected features\n    if features_with_spec_changes:\n        records = []\n        for feature_key_str in features_with_spec_changes:\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write updated records (append-only)\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=True,\n            metadata_changed=True,\n            features_with_spec_changes=features_with_spec_changes,\n        )\n\n    # Scenario 3: No changes at all\n    return SnapshotPushResult(\n        snapshot_version=snapshot_version,\n        already_recorded=True,\n        metadata_changed=False,\n        features_with_spec_changes=[],\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata_in_store","title":"read_metadata_in_store  <code>abstractmethod</code>","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from THIS store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied natively in store)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions for this specific feature.</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if feature not found in the store</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from THIS store only (no fallback).\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Filter by specific feature_version (applied natively in store)\n        filters: List of Narwhals filter expressions for this specific feature.\n        columns: Subset of columns to return\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if feature not found in the store\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return rows with current feature_version (default: True for safety)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]\n        columns: Subset of columns to return\n        allow_fallback: If True, check fallback stores on local miss\n        current_only: If True, only return rows with current feature_version\n            (default: True for safety)\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Determine which feature_version to use\n    feature_version_filter = feature_version\n    if current_only and not is_system_table:\n        # Get current feature_version\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            feature_version_filter = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            # Only try to get from graph if feature_key exists in graph\n            # This allows reading system tables or external features not in current graph\n            if feature_key in graph.features_by_key:\n                feature_cls = graph.features_by_key[feature_key]\n                feature_version_filter = feature_cls.feature_version()  # type: ignore[attr-defined]\n            else:\n                # Feature not in graph - skip feature_version filtering\n                feature_version_filter = None\n\n    # Map column names: provenance_by_field -&gt; metaxy_provenance_by_field for DB query\n    db_columns = None\n    if columns is not None:\n        db_columns = [\n            \"metaxy_provenance_by_field\" if col == \"provenance_by_field\" else col\n            for col in columns\n        ]\n\n    # Try local first with filters\n    lazy_frame = self.read_metadata_in_store(\n        feature,\n        feature_version=feature_version_filter,\n        filters=filters,  # Pass filters directly\n        columns=db_columns,  # Use mapped column names\n    )\n\n    if lazy_frame is not None:\n        # Rename metaxy_provenance_by_field -&gt; provenance_by_field for Python code\n        # (Database uses metaxy_provenance_by_field, Python code uses provenance_by_field)\n        if \"metaxy_provenance_by_field\" in lazy_frame.collect_schema().names():\n            lazy_frame = lazy_frame.rename(\n                {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n            )\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,  # Pass through filters directly\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,  # Pass through current_only\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    # Check local\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if check_fallback:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.list_features","title":"list_features","text":"<pre><code>list_features(*, include_fallback: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>List all features in store.</p> <p>Parameters:</p> <ul> <li> <code>include_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include features from fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of FeatureKey objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def list_features(self, *, include_fallback: bool = False) -&gt; list[FeatureKey]:\n    \"\"\"\n    List all features in store.\n\n    Args:\n        include_fallback: If True, include features from fallback stores\n\n    Returns:\n        List of FeatureKey objects\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    features = self._list_features_local()\n\n    if include_fallback:\n        for store in self.fallback_stores:\n            features.extend(store.list_features(include_fallback=True))\n\n    # Deduplicate\n    seen = set()\n    unique_features = []\n    for feature in features:\n        key_str = feature.to_string()\n        if key_str not in seen:\n            seen.add(key_str)\n            unique_features.append(feature)\n\n    return unique_features\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.display","title":"display  <code>abstractmethod</code>","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return a human-readable display string for this store.</p> <p>Used in warnings, logs, and CLI output to identify the store.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef display(self) -&gt; str:\n    \"\"\"Return a human-readable display string for this store.\n\n    Used in warnings, logs, and CLI output to identify the store.\n\n    Returns:\n        Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_graph_snapshots","title":"read_graph_snapshots","text":"<pre><code>read_graph_snapshots(project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read recorded graph snapshots from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, returns snapshots from all projects.</p> </li> </ul> <p>Returns a DataFrame with columns: - snapshot_version: Unique identifier for each graph snapshot - recorded_at: Timestamp when the snapshot was recorded - feature_count: Number of features in this snapshot</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with snapshot information, sorted by recorded_at descending</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Example <pre><code>with store:\n    # Get snapshots for a specific project\n    snapshots = store.read_graph_snapshots(project=\"my_project\")\n    latest_snapshot = snapshots[\"snapshot_version\"][0]\n    print(f\"Latest snapshot: {latest_snapshot}\")\n\n    # Get snapshots across all projects\n    all_snapshots = store.read_graph_snapshots()\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_graph_snapshots(self, project: str | None = None) -&gt; pl.DataFrame:\n    \"\"\"Read recorded graph snapshots from the feature_versions system table.\n\n    Args:\n        project: Project name to filter by. If None, returns snapshots from all projects.\n\n    Returns a DataFrame with columns:\n    - snapshot_version: Unique identifier for each graph snapshot\n    - recorded_at: Timestamp when the snapshot was recorded\n    - feature_count: Number of features in this snapshot\n\n    Returns:\n        Polars DataFrame with snapshot information, sorted by recorded_at descending\n\n    Raises:\n        StoreNotOpenError: If store is not open\n\n    Example:\n        ```py\n        with store:\n            # Get snapshots for a specific project\n            snapshots = store.read_graph_snapshots(project=\"my_project\")\n            latest_snapshot = snapshots[\"snapshot_version\"][0]\n            print(f\"Latest snapshot: {latest_snapshot}\")\n\n            # Get snapshots across all projects\n            all_snapshots = store.read_graph_snapshots()\n        ```\n    \"\"\"\n    self._check_open()\n\n    # Build filters based on project parameter\n    filters = None\n    if project is not None:\n        import narwhals as nw\n\n        filters = [nw.col(\"project\") == project]\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No snapshots recorded yet\n        return pl.DataFrame(\n            schema={\n                \"snapshot_version\": pl.String,\n                \"recorded_at\": pl.Datetime(\"us\"),\n                \"feature_count\": pl.UInt32,\n            }\n        )\n\n    versions_df = versions_lazy.collect().to_polars()\n\n    # Group by snapshot_version and get earliest recorded_at and count\n    snapshots = (\n        versions_df.group_by(\"snapshot_version\")\n        .agg(\n            [\n                pl.col(\"recorded_at\").min().alias(\"recorded_at\"),\n                pl.col(\"feature_key\").count().alias(\"feature_count\"),\n            ]\n        )\n        .sort(\"recorded_at\", descending=True)\n    )\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_features","title":"read_features","text":"<pre><code>read_features(*, current: bool = True, snapshot_version: str | None = None, project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read feature version information from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>current</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return features from the current code snapshot.      If False, must provide snapshot_version.</p> </li> <li> <code>snapshot_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific snapshot version to filter by. Required if current=False.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_key: Feature identifier</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_version: Version hash of the feature</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>recorded_at: When this version was recorded</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_spec: JSON serialized feature specification</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_class_path: Python import path to the feature class</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>snapshot_version: Graph snapshot this feature belongs to</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If current=False but no snapshot_version provided</p> </li> </ul> <p>Examples:</p> <pre><code># Get features from current code\nwith store:\n    features = store.read_features(current=True)\n    print(f\"Current graph has {len(features)} features\")\n</code></pre> <pre><code># Get features from a specific snapshot\nwith store:\n    features = store.read_features(current=False, snapshot_version=\"abc123\")\n    for row in features.iter_rows(named=True):\n        print(f\"{row['feature_key']}: {row['feature_version']}\")\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_features(\n    self,\n    *,\n    current: bool = True,\n    snapshot_version: str | None = None,\n    project: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Read feature version information from the feature_versions system table.\n\n    Args:\n        current: If True, only return features from the current code snapshot.\n                 If False, must provide snapshot_version.\n        snapshot_version: Specific snapshot version to filter by. Required if current=False.\n        project: Project name to filter by. Defaults to None.\n\n    Returns:\n        Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:\n        - feature_key: Feature identifier\n        - feature_version: Version hash of the feature\n        - recorded_at: When this version was recorded\n        - feature_spec: JSON serialized feature specification\n        - feature_class_path: Python import path to the feature class\n        - snapshot_version: Graph snapshot this feature belongs to\n\n    Raises:\n        StoreNotOpenError: If store is not open\n        ValueError: If current=False but no snapshot_version provided\n\n    Examples:\n        ```py\n        # Get features from current code\n        with store:\n            features = store.read_features(current=True)\n            print(f\"Current graph has {len(features)} features\")\n        ```\n\n        ```py\n        # Get features from a specific snapshot\n        with store:\n            features = store.read_features(current=False, snapshot_version=\"abc123\")\n            for row in features.iter_rows(named=True):\n                print(f\"{row['feature_key']}: {row['feature_version']}\")\n        ```\n    \"\"\"\n    self._check_open()\n\n    if not current and snapshot_version is None:\n        raise ValueError(\"Must provide snapshot_version when current=False\")\n\n    if current:\n        # Get current snapshot from active graph\n        graph = FeatureGraph.get_active()\n        snapshot_version = graph.snapshot_version\n\n    filters = [nw.col(\"snapshot_version\") == snapshot_version]\n    if project is not None:\n        filters.append(nw.col(\"project\") == project)\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No features recorded yet\n        return pl.DataFrame(schema=FEATURE_VERSIONS_SCHEMA)\n\n    # Filter by snapshot_version\n    versions_df = versions_lazy.collect().to_polars()\n\n    return versions_df\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\"Destination store must be opened (use context manager)\")\n\n    # Automatically handle source store context manager\n    should_close_source = not from_store._is_open\n    if should_close_source:\n        from_store.__enter__()\n\n    try:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n    finally:\n        if should_close_source:\n            from_store.__exit__(None, None, None)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_upstream_metadata","title":"read_upstream_metadata","text":"<pre><code>read_upstream_metadata(feature: FeatureKey | type[BaseFeature], field: FieldKey | None = None, *, filters: Mapping[str, Sequence[Expr]] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; dict[str, LazyFrame[Any]]\n</code></pre> <p>Read all upstream dependencies for a feature/field.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature whose dependencies to load</p> </li> <li> <code>field</code>               (<code>FieldKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific field (if None, loads all deps for feature)</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only read current feature_version for upstream</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.</p> </li> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Each LazyFrame has a 'provenance_by_field' column (Struct).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DependencyError</code>             \u2013            <p>If required upstream feature is missing</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_upstream_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    field: FieldKey | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; dict[str, nw.LazyFrame[Any]]:\n    \"\"\"\n    Read all upstream dependencies for a feature/field.\n\n    Args:\n        feature: Feature whose dependencies to load\n        field: Specific field (if None, loads all deps for feature)\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}\n        allow_fallback: Whether to check fallback stores\n        current_only: If True, only read current feature_version for upstream\n\n    Returns:\n        Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.\n        Each LazyFrame has a 'provenance_by_field' column (Struct).\n\n    Raises:\n        DependencyError: If required upstream feature is missing\n    \"\"\"\n    plan = self._resolve_feature_plan(feature)\n\n    # Get all upstream features we need\n    upstream_features = set()\n\n    if field is None:\n        # All fields' dependencies\n        for cont in plan.feature.fields:\n            upstream_features.update(self._get_field_dependencies(plan, cont.key))\n    else:\n        # Specific field's dependencies\n        upstream_features.update(self._get_field_dependencies(plan, field))\n\n    # Load metadata for each upstream feature\n    # Use the feature's graph to look up upstream feature classes\n    if isinstance(feature, FeatureKey):\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n    else:\n        graph = feature.graph\n\n    upstream_metadata = {}\n    for upstream_fq_key in upstream_features:\n        upstream_feature_key = upstream_fq_key.feature\n\n        # Extract filters for this specific upstream feature\n        upstream_filters = None\n        if filters:\n            upstream_key_str = upstream_feature_key.to_string()\n            if upstream_key_str in filters:\n                upstream_filters = filters[upstream_key_str]\n\n        try:\n            # Look up the Feature class from the graph and pass it to read_metadata\n            # This way we use the bound graph instead of relying on active context\n            upstream_feature_cls = graph.features_by_key[upstream_feature_key]\n            lazy_frame = self.read_metadata(\n                upstream_feature_cls,\n                filters=upstream_filters,  # Pass extracted filters (Sequence or None)\n                allow_fallback=allow_fallback,\n                current_only=current_only,  # Pass through current_only\n            )\n            # Use string key for dict\n            upstream_metadata[upstream_feature_key.to_string()] = lazy_frame\n        except FeatureNotFoundError as e:\n            raise DependencyError(\n                f\"Missing upstream feature {upstream_feature_key.to_string()} \"\n                f\"required by {plan.feature.key.to_string()}\"\n            ) from e\n\n    return upstream_metadata\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>DataFrame[Any] | LazyFrame[Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>\"provenance_by_field\"</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>\"provenance_by_field\"</code> from, so users must provide samples with manually computed <code>\"provenance_by_field\"</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied when reading upstream metadata to filter samples at the source. Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.data_versioning.diff.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.data_versioning.diff.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `\"provenance_by_field\"` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `\"provenance_by_field\"` from, so users\n            must provide samples with manually computed `\"provenance_by_field\"` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied when reading upstream metadata to filter samples at the source.\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.data_versioning.diff.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.data_versioning.diff.Increment][] with eager Narwhals DataFrames.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    plan = feature.graph.get_feature_plan(feature.spec().key)\n\n    # Escape hatch: if samples provided, use them directly (skip join/calculation)\n    if samples is not None:\n        import logging\n\n        import polars as pl\n\n        logger = logging.getLogger(__name__)\n\n        # Convert samples to lazy if needed\n        if isinstance(samples, nw.LazyFrame):\n            samples_lazy = samples\n        elif isinstance(samples, nw.DataFrame):\n            samples_lazy = samples.lazy()\n        else:\n            samples_lazy = nw.from_native(samples).lazy()\n\n        # Check if samples are Polars-backed (common case for escape hatch)\n        samples_native = samples_lazy.to_native()\n        is_polars_samples = isinstance(samples_native, (pl.DataFrame, pl.LazyFrame))\n\n        if is_polars_samples and self._supports_native_components():\n            # User provided Polars samples but store uses native (SQL) backend\n            # Need to materialize current metadata to Polars for compatibility\n            logger.warning(\n                f\"Feature {feature.spec().key}: samples parameter is Polars-backed but store uses native SQL backend. \"\n                f\"Materializing current metadata to Polars for diff comparison. \"\n                f\"For better performance, consider using samples with backend matching the store's backend.\"\n            )\n            # Get current metadata and materialize to Polars\n            current_lazy_native = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            if current_lazy_native is not None:\n                # Rename metaxy_provenance_by_field -&gt; provenance_by_field before converting\n                if (\n                    \"metaxy_provenance_by_field\"\n                    in current_lazy_native.collect_schema().names()\n                ):\n                    current_lazy_native = current_lazy_native.rename(\n                        {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                    )\n                # Convert to Polars using Narwhals' built-in method\n                current_lazy = nw.from_native(\n                    current_lazy_native.collect().to_polars().lazy()\n                )\n            else:\n                current_lazy = None\n        else:\n            # Same backend or no conversion needed - direct read\n            current_lazy = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            # Rename metaxy_provenance_by_field -&gt; provenance_by_field\n            if (\n                current_lazy is not None\n                and \"metaxy_provenance_by_field\"\n                in current_lazy.collect_schema().names()\n            ):\n                current_lazy = current_lazy.rename(\n                    {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                )\n\n        # Use diff resolver to compare samples with current\n        from metaxy.data_versioning.diff.narwhals import NarwhalsDiffResolver\n\n        diff_resolver = NarwhalsDiffResolver()\n\n        lazy_result = diff_resolver.find_changes(\n            target_provenance=samples_lazy,\n            current_metadata=current_lazy,\n            id_columns=feature.spec().id_columns,  # Get ID columns from feature spec\n        )\n\n        return lazy_result if lazy else lazy_result.collect()\n\n    # Root features without samples: error (samples required)\n    if not plan.deps:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and provenance_by_field columns. \"\n            f\"Root features require manual provenance_by_field computation.\"\n        )\n\n    # Non-root features without samples: automatic upstream loading\n    # Check where upstream data lives\n    upstream_location = self._check_upstream_location(feature)\n\n    if upstream_location == \"all_local\":\n        # All upstream in this store - use native field provenance calculations\n        return self._resolve_update_native(feature, filters=filters, lazy=lazy)\n    else:\n        # Some upstream in fallback stores - use Polars components\n        return self._resolve_update_polars(feature, filters=filters, lazy=lazy)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/","title":"Base Metadata Store","text":"<p>This one is mostly useful for testing.</p>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore","title":"InMemoryMetadataStore","text":"<pre><code>InMemoryMetadataStore(**kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>In-memory metadata store using dict-based storage.</p> <p>Features: - Simple dict storage: {FeatureKey: pl.DataFrame} - Fast for testing and prototyping - No persistence (data lost when process exits) - Schema validation on write - Uses Polars components for all operations</p> <p>Limitations: - Not suitable for production - Data lost on process exit - No concurrency support across processes - Memory-bound (all data in RAM)</p> Notes <p>Uses Narwhals LazyFrames (nw.LazyFrame) for all operations</p> Components <p>Components are created on-demand in resolve_update(). Uses Polars internally but exposes Narwhals interface. Only supports Polars components (no native backend).</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialize in-memory store.\n\n    Args:\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n    \"\"\"\n    # Use tuple as key (hashable) instead of string to avoid parsing issues\n    self._storage: dict[tuple[str, ...], pl.DataFrame] = {}\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    # Track nesting depth\n    self._context_depth += 1\n\n    # Only open on first enter\n    if self._context_depth == 1:\n        # Warn if auto_create_tables is enabled (and store wants warnings)\n        if self.auto_create_tables and self._should_warn_auto_create_tables:\n            import warnings\n\n            warnings.warn(\n                f\"AUTO_CREATE_TABLES is enabled for {self.display()} - \"\n                \"do not use in production! \"\n                \"Use proper database migration tools like Alembic for production deployments.\",\n                UserWarning,\n                stacklevel=3,  # stacklevel=3 to point to user's 'with store:' line\n            )\n\n        self.open()\n        self._is_open = True\n\n        # Validate after opening (when all components are ready)\n        self._validate_after_open()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit context manager.\"\"\"\n    # Decrement depth\n    self._context_depth -= 1\n\n    # Only close when fully exited\n    if self._context_depth == 0:\n        self._is_open = False\n        self.close()\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Check if this store can support the algorithm\n    # Try native field provenance calculations first (if supported), then Polars\n    supported_algorithms = []\n\n    if self._supports_native_components():\n        try:\n            _, calculator, _ = self._create_native_components()\n            supported_algorithms = calculator.supported_algorithms\n        except Exception:\n            # If native field provenance calculations fail, fall back to Polars\n            pass\n\n    # If no native support or prefer_native=False, use Polars\n    if not supported_algorithms:\n        polars_calc = PolarsProvenanceByFieldCalculator()\n        supported_algorithms = polars_calc.supported_algorithms\n\n    if self.hash_algorithm not in supported_algorithms:\n        from metaxy.metadata_store.exceptions import (\n            HashAlgorithmNotSupportedError,\n        )\n\n        raise HashAlgorithmNotSupportedError(\n            f\"Hash algorithm {self.hash_algorithm} not supported by {self.__class__.__name__}. \"\n            f\"Supported: {supported_algorithms}\"\n        )\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: DataFrame[Any] | DataFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (immutable, append-only).</p> <p>Automatically adds 'feature_version' column from current code state, unless the DataFrame already contains one (useful for migrations).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>DataFrame[Any] | DataFrame</code>)           \u2013            <p>Narwhals DataFrame or Polars DataFrame containing metadata. Must have 'provenance_by_field' column of type Struct with fields matching feature's fields. May optionally contain 'feature_version' column (for migrations).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Always writes to current store, never to fallback stores.</li> <li>If df already contains 'feature_version' column, it will be used   as-is (no replacement). This allows migrations to write historical   versions. A warning is issued unless suppressed via context manager.</li> <li>Project validation is performed unless disabled via allow_cross_project_writes()</li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: nw.DataFrame[Any] | pl.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (immutable, append-only).\n\n    Automatically adds 'feature_version' column from current code state,\n    unless the DataFrame already contains one (useful for migrations).\n\n    Args:\n        feature: Feature to write metadata for\n        df: Narwhals DataFrame or Polars DataFrame containing metadata.\n            Must have 'provenance_by_field' column of type Struct with fields matching feature's fields.\n            May optionally contain 'feature_version' column (for migrations).\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Always writes to current store, never to fallback stores.\n        - If df already contains 'feature_version' column, it will be used\n          as-is (no replacement). This allows migrations to write historical\n          versions. A warning is issued unless suppressed via context manager.\n        - Project validation is performed unless disabled via allow_cross_project_writes()\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Narwhals to Polars if needed\n    if isinstance(df, nw.DataFrame):\n        df = df.to_polars()\n    # nw.DataFrame also matches as DataFrame in some contexts, ensure it's Polars\n    if not isinstance(df, pl.DataFrame):\n        # Must be some other type - shouldn't happen but handle defensively\n        if hasattr(df, \"to_polars\"):\n            df = df.to_polars()\n        elif hasattr(df, \"to_pandas\"):\n            df = pl.from_pandas(df.to_pandas())\n        else:\n            raise TypeError(f\"Cannot convert {type(df)} to Polars DataFrame\")\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df)\n        self._write_metadata_impl(feature_key, df)\n        return\n\n    # For regular features: add feature_version and snapshot_version, validate, and write\n    # Check if feature_version and snapshot_version already exist in DataFrame\n    if \"feature_version\" in df.columns and \"snapshot_version\" in df.columns:\n        # DataFrame already has feature_version and snapshot_version - use as-is\n        # This is intended for migrations writing historical versions\n        # Issue a warning unless we're in a suppression context\n        if not _suppress_feature_version_warning.get():\n            import warnings\n\n            warnings.warn(\n                f\"Writing metadata for {feature_key.to_string()} with existing \"\n                f\"feature_version and snapshot_version columns. This is intended for migrations only. \"\n                f\"Normal code should let write_metadata() add the current versions automatically.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Get current feature version and snapshot_version from code and add them\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            current_feature_version = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            feature_cls = graph.features_by_key[feature_key]\n            current_feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n\n        # Get snapshot_version from active graph\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n        current_snapshot_version = graph.snapshot_version\n\n        df = df.with_columns(\n            [\n                pl.lit(current_feature_version).alias(\"feature_version\"),\n                pl.lit(current_snapshot_version).alias(\"snapshot_version\"),\n            ]\n        )\n\n    # Validate schema\n    self._validate_schema(df)\n\n    # Rename provenance_by_field -&gt; metaxy_provenance_by_field for database storage\n    # (Python code uses provenance_by_field, database uses metaxy_provenance_by_field)\n    df = df.rename({\"provenance_by_field\": \"metaxy_provenance_by_field\"})\n\n    # Write metadata\n    self._write_metadata_impl(feature_key, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.record_feature_graph_snapshot","title":"record_feature_graph_snapshot","text":"<pre><code>record_feature_graph_snapshot() -&gt; SnapshotPushResult\n</code></pre> <p>Record all features in graph with a graph snapshot version.</p> <p>This should be called during CD (Continuous Deployment) to record what feature versions are being deployed. Typically invoked via <code>metaxy graph push</code>.</p> <p>Records all features in the graph with the same snapshot_version, representing a consistent state of the entire feature graph based on code definitions.</p> <p>The snapshot_version is a deterministic hash of all feature_version hashes in the graph, making it idempotent - calling multiple times with the same feature definitions produces the same snapshot_version.</p> <p>This method detects three scenarios: 1. New snapshot (computational changes): No existing rows with this snapshot_version 2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version 3. No changes: Snapshot exists with identical feature_spec_versions for all features</p> <p>Returns: SnapshotPushResult</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def record_feature_graph_snapshot(self) -&gt; SnapshotPushResult:\n    \"\"\"Record all features in graph with a graph snapshot version.\n\n    This should be called during CD (Continuous Deployment) to record what\n    feature versions are being deployed. Typically invoked via `metaxy graph push`.\n\n    Records all features in the graph with the same snapshot_version, representing\n    a consistent state of the entire feature graph based on code definitions.\n\n    The snapshot_version is a deterministic hash of all feature_version hashes\n    in the graph, making it idempotent - calling multiple times with the\n    same feature definitions produces the same snapshot_version.\n\n    This method detects three scenarios:\n    1. New snapshot (computational changes): No existing rows with this snapshot_version\n    2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version\n    3. No changes: Snapshot exists with identical feature_spec_versions for all features\n\n    Returns: SnapshotPushResult\n    \"\"\"\n\n    from metaxy.models.feature import FeatureGraph\n\n    graph = FeatureGraph.get_active()\n\n    # Use to_snapshot() to get the snapshot dict\n    snapshot_dict = graph.to_snapshot()\n\n    # Generate deterministic snapshot_version from graph\n    snapshot_version = graph.snapshot_version\n\n    # Read existing feature versions once\n    try:\n        existing_versions_lazy = self.read_metadata_in_store(FEATURE_VERSIONS_KEY)\n        # Materialize to Polars for iteration\n        existing_versions = (\n            existing_versions_lazy.collect().to_polars()\n            if existing_versions_lazy is not None\n            else None\n        )\n    except Exception:\n        # Table doesn't exist yet\n        existing_versions = None\n\n    # Get project from any feature in the graph (all should have the same project)\n    # Default to empty string if no features in graph\n    if graph.features_by_key:\n        # Get first feature's project\n        first_feature = next(iter(graph.features_by_key.values()))\n        project_name = first_feature.project  # type: ignore[attr-defined]\n    else:\n        project_name = \"\"\n\n    # Check if this exact snapshot already exists for this project\n    snapshot_already_exists = False\n    existing_spec_versions: dict[str, str] = {}\n\n    if existing_versions is not None:\n        # Check if project column exists (it may not in old tables)\n        if \"project\" in existing_versions.columns:\n            snapshot_rows = existing_versions.filter(\n                (pl.col(\"snapshot_version\") == snapshot_version)\n                &amp; (pl.col(\"project\") == project_name)\n            )\n        else:\n            # Old table without project column - just check snapshot_version\n            snapshot_rows = existing_versions.filter(\n                pl.col(\"snapshot_version\") == snapshot_version\n            )\n        snapshot_already_exists = snapshot_rows.height &gt; 0\n\n        if snapshot_already_exists:\n            # Check if feature_spec_version column exists (backward compatibility)\n            # Old records (before issue #77) won't have this column\n            has_spec_version = \"feature_spec_version\" in snapshot_rows.columns\n\n            if has_spec_version:\n                # Build dict of existing feature_key -&gt; feature_spec_version\n                for row in snapshot_rows.iter_rows(named=True):\n                    existing_spec_versions[row[\"feature_key\"]] = row[\n                        \"feature_spec_version\"\n                    ]\n            # If no spec_version column, existing_spec_versions remains empty\n            # This means we'll treat it as \"no metadata changes\" (conservative approach)\n\n    # Scenario 1: New snapshot (no existing rows)\n    if not snapshot_already_exists:\n        # Build records from snapshot_dict\n        records = []\n        for feature_key_str in sorted(snapshot_dict.keys()):\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            # Always record all features for this snapshot (don't skip based on feature_version alone)\n            # Each snapshot must be complete to support migration detection\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write all new records at once\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=False,\n            metadata_changed=False,\n            features_with_spec_changes=[],\n        )\n\n    # Scenario 2 &amp; 3: Snapshot exists - check for metadata changes\n    features_with_spec_changes = []\n\n    for feature_key_str, feature_data in snapshot_dict.items():\n        current_spec_version = feature_data[\"feature_spec_version\"]\n        existing_spec_version = existing_spec_versions.get(feature_key_str)\n\n        if existing_spec_version != current_spec_version:\n            features_with_spec_changes.append(feature_key_str)\n\n    # If metadata changed, append new rows for affected features\n    if features_with_spec_changes:\n        records = []\n        for feature_key_str in features_with_spec_changes:\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write updated records (append-only)\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=True,\n            metadata_changed=True,\n            features_with_spec_changes=features_with_spec_changes,\n        )\n\n    # Scenario 3: No changes at all\n    return SnapshotPushResult(\n        snapshot_version=snapshot_version,\n        already_recorded=True,\n        metadata_changed=False,\n        features_with_spec_changes=[],\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return rows with current feature_version (default: True for safety)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]\n        columns: Subset of columns to return\n        allow_fallback: If True, check fallback stores on local miss\n        current_only: If True, only return rows with current feature_version\n            (default: True for safety)\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Determine which feature_version to use\n    feature_version_filter = feature_version\n    if current_only and not is_system_table:\n        # Get current feature_version\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            feature_version_filter = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            # Only try to get from graph if feature_key exists in graph\n            # This allows reading system tables or external features not in current graph\n            if feature_key in graph.features_by_key:\n                feature_cls = graph.features_by_key[feature_key]\n                feature_version_filter = feature_cls.feature_version()  # type: ignore[attr-defined]\n            else:\n                # Feature not in graph - skip feature_version filtering\n                feature_version_filter = None\n\n    # Map column names: provenance_by_field -&gt; metaxy_provenance_by_field for DB query\n    db_columns = None\n    if columns is not None:\n        db_columns = [\n            \"metaxy_provenance_by_field\" if col == \"provenance_by_field\" else col\n            for col in columns\n        ]\n\n    # Try local first with filters\n    lazy_frame = self.read_metadata_in_store(\n        feature,\n        feature_version=feature_version_filter,\n        filters=filters,  # Pass filters directly\n        columns=db_columns,  # Use mapped column names\n    )\n\n    if lazy_frame is not None:\n        # Rename metaxy_provenance_by_field -&gt; provenance_by_field for Python code\n        # (Database uses metaxy_provenance_by_field, Python code uses provenance_by_field)\n        if \"metaxy_provenance_by_field\" in lazy_frame.collect_schema().names():\n            lazy_frame = lazy_frame.rename(\n                {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n            )\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,  # Pass through filters directly\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,  # Pass through current_only\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    # Check local\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if check_fallback:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.list_features","title":"list_features","text":"<pre><code>list_features(*, include_fallback: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>List all features in store.</p> <p>Parameters:</p> <ul> <li> <code>include_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include features from fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of FeatureKey objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def list_features(self, *, include_fallback: bool = False) -&gt; list[FeatureKey]:\n    \"\"\"\n    List all features in store.\n\n    Args:\n        include_fallback: If True, include features from fallback stores\n\n    Returns:\n        List of FeatureKey objects\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    features = self._list_features_local()\n\n    if include_fallback:\n        for store in self.fallback_stores:\n            features.extend(store.list_features(include_fallback=True))\n\n    # Deduplicate\n    seen = set()\n    unique_features = []\n    for feature in features:\n        key_str = feature.to_string()\n        if key_str not in seen:\n            seen.add(key_str)\n            unique_features.append(feature)\n\n    return unique_features\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_graph_snapshots","title":"read_graph_snapshots","text":"<pre><code>read_graph_snapshots(project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read recorded graph snapshots from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, returns snapshots from all projects.</p> </li> </ul> <p>Returns a DataFrame with columns: - snapshot_version: Unique identifier for each graph snapshot - recorded_at: Timestamp when the snapshot was recorded - feature_count: Number of features in this snapshot</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with snapshot information, sorted by recorded_at descending</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Example <pre><code>with store:\n    # Get snapshots for a specific project\n    snapshots = store.read_graph_snapshots(project=\"my_project\")\n    latest_snapshot = snapshots[\"snapshot_version\"][0]\n    print(f\"Latest snapshot: {latest_snapshot}\")\n\n    # Get snapshots across all projects\n    all_snapshots = store.read_graph_snapshots()\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_graph_snapshots(self, project: str | None = None) -&gt; pl.DataFrame:\n    \"\"\"Read recorded graph snapshots from the feature_versions system table.\n\n    Args:\n        project: Project name to filter by. If None, returns snapshots from all projects.\n\n    Returns a DataFrame with columns:\n    - snapshot_version: Unique identifier for each graph snapshot\n    - recorded_at: Timestamp when the snapshot was recorded\n    - feature_count: Number of features in this snapshot\n\n    Returns:\n        Polars DataFrame with snapshot information, sorted by recorded_at descending\n\n    Raises:\n        StoreNotOpenError: If store is not open\n\n    Example:\n        ```py\n        with store:\n            # Get snapshots for a specific project\n            snapshots = store.read_graph_snapshots(project=\"my_project\")\n            latest_snapshot = snapshots[\"snapshot_version\"][0]\n            print(f\"Latest snapshot: {latest_snapshot}\")\n\n            # Get snapshots across all projects\n            all_snapshots = store.read_graph_snapshots()\n        ```\n    \"\"\"\n    self._check_open()\n\n    # Build filters based on project parameter\n    filters = None\n    if project is not None:\n        import narwhals as nw\n\n        filters = [nw.col(\"project\") == project]\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No snapshots recorded yet\n        return pl.DataFrame(\n            schema={\n                \"snapshot_version\": pl.String,\n                \"recorded_at\": pl.Datetime(\"us\"),\n                \"feature_count\": pl.UInt32,\n            }\n        )\n\n    versions_df = versions_lazy.collect().to_polars()\n\n    # Group by snapshot_version and get earliest recorded_at and count\n    snapshots = (\n        versions_df.group_by(\"snapshot_version\")\n        .agg(\n            [\n                pl.col(\"recorded_at\").min().alias(\"recorded_at\"),\n                pl.col(\"feature_key\").count().alias(\"feature_count\"),\n            ]\n        )\n        .sort(\"recorded_at\", descending=True)\n    )\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_features","title":"read_features","text":"<pre><code>read_features(*, current: bool = True, snapshot_version: str | None = None, project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read feature version information from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>current</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return features from the current code snapshot.      If False, must provide snapshot_version.</p> </li> <li> <code>snapshot_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific snapshot version to filter by. Required if current=False.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_key: Feature identifier</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_version: Version hash of the feature</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>recorded_at: When this version was recorded</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_spec: JSON serialized feature specification</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_class_path: Python import path to the feature class</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>snapshot_version: Graph snapshot this feature belongs to</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If current=False but no snapshot_version provided</p> </li> </ul> <p>Examples:</p> <pre><code># Get features from current code\nwith store:\n    features = store.read_features(current=True)\n    print(f\"Current graph has {len(features)} features\")\n</code></pre> <pre><code># Get features from a specific snapshot\nwith store:\n    features = store.read_features(current=False, snapshot_version=\"abc123\")\n    for row in features.iter_rows(named=True):\n        print(f\"{row['feature_key']}: {row['feature_version']}\")\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_features(\n    self,\n    *,\n    current: bool = True,\n    snapshot_version: str | None = None,\n    project: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Read feature version information from the feature_versions system table.\n\n    Args:\n        current: If True, only return features from the current code snapshot.\n                 If False, must provide snapshot_version.\n        snapshot_version: Specific snapshot version to filter by. Required if current=False.\n        project: Project name to filter by. Defaults to None.\n\n    Returns:\n        Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:\n        - feature_key: Feature identifier\n        - feature_version: Version hash of the feature\n        - recorded_at: When this version was recorded\n        - feature_spec: JSON serialized feature specification\n        - feature_class_path: Python import path to the feature class\n        - snapshot_version: Graph snapshot this feature belongs to\n\n    Raises:\n        StoreNotOpenError: If store is not open\n        ValueError: If current=False but no snapshot_version provided\n\n    Examples:\n        ```py\n        # Get features from current code\n        with store:\n            features = store.read_features(current=True)\n            print(f\"Current graph has {len(features)} features\")\n        ```\n\n        ```py\n        # Get features from a specific snapshot\n        with store:\n            features = store.read_features(current=False, snapshot_version=\"abc123\")\n            for row in features.iter_rows(named=True):\n                print(f\"{row['feature_key']}: {row['feature_version']}\")\n        ```\n    \"\"\"\n    self._check_open()\n\n    if not current and snapshot_version is None:\n        raise ValueError(\"Must provide snapshot_version when current=False\")\n\n    if current:\n        # Get current snapshot from active graph\n        graph = FeatureGraph.get_active()\n        snapshot_version = graph.snapshot_version\n\n    filters = [nw.col(\"snapshot_version\") == snapshot_version]\n    if project is not None:\n        filters.append(nw.col(\"project\") == project)\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No features recorded yet\n        return pl.DataFrame(schema=FEATURE_VERSIONS_SCHEMA)\n\n    # Filter by snapshot_version\n    versions_df = versions_lazy.collect().to_polars()\n\n    return versions_df\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\"Destination store must be opened (use context manager)\")\n\n    # Automatically handle source store context manager\n    should_close_source = not from_store._is_open\n    if should_close_source:\n        from_store.__enter__()\n\n    try:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n    finally:\n        if should_close_source:\n            from_store.__exit__(None, None, None)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_upstream_metadata","title":"read_upstream_metadata","text":"<pre><code>read_upstream_metadata(feature: FeatureKey | type[BaseFeature], field: FieldKey | None = None, *, filters: Mapping[str, Sequence[Expr]] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; dict[str, LazyFrame[Any]]\n</code></pre> <p>Read all upstream dependencies for a feature/field.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature whose dependencies to load</p> </li> <li> <code>field</code>               (<code>FieldKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific field (if None, loads all deps for feature)</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only read current feature_version for upstream</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.</p> </li> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Each LazyFrame has a 'provenance_by_field' column (Struct).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DependencyError</code>             \u2013            <p>If required upstream feature is missing</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_upstream_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    field: FieldKey | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; dict[str, nw.LazyFrame[Any]]:\n    \"\"\"\n    Read all upstream dependencies for a feature/field.\n\n    Args:\n        feature: Feature whose dependencies to load\n        field: Specific field (if None, loads all deps for feature)\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}\n        allow_fallback: Whether to check fallback stores\n        current_only: If True, only read current feature_version for upstream\n\n    Returns:\n        Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.\n        Each LazyFrame has a 'provenance_by_field' column (Struct).\n\n    Raises:\n        DependencyError: If required upstream feature is missing\n    \"\"\"\n    plan = self._resolve_feature_plan(feature)\n\n    # Get all upstream features we need\n    upstream_features = set()\n\n    if field is None:\n        # All fields' dependencies\n        for cont in plan.feature.fields:\n            upstream_features.update(self._get_field_dependencies(plan, cont.key))\n    else:\n        # Specific field's dependencies\n        upstream_features.update(self._get_field_dependencies(plan, field))\n\n    # Load metadata for each upstream feature\n    # Use the feature's graph to look up upstream feature classes\n    if isinstance(feature, FeatureKey):\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n    else:\n        graph = feature.graph\n\n    upstream_metadata = {}\n    for upstream_fq_key in upstream_features:\n        upstream_feature_key = upstream_fq_key.feature\n\n        # Extract filters for this specific upstream feature\n        upstream_filters = None\n        if filters:\n            upstream_key_str = upstream_feature_key.to_string()\n            if upstream_key_str in filters:\n                upstream_filters = filters[upstream_key_str]\n\n        try:\n            # Look up the Feature class from the graph and pass it to read_metadata\n            # This way we use the bound graph instead of relying on active context\n            upstream_feature_cls = graph.features_by_key[upstream_feature_key]\n            lazy_frame = self.read_metadata(\n                upstream_feature_cls,\n                filters=upstream_filters,  # Pass extracted filters (Sequence or None)\n                allow_fallback=allow_fallback,\n                current_only=current_only,  # Pass through current_only\n            )\n            # Use string key for dict\n            upstream_metadata[upstream_feature_key.to_string()] = lazy_frame\n        except FeatureNotFoundError as e:\n            raise DependencyError(\n                f\"Missing upstream feature {upstream_feature_key.to_string()} \"\n                f\"required by {plan.feature.key.to_string()}\"\n            ) from e\n\n    return upstream_metadata\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>DataFrame[Any] | LazyFrame[Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>\"provenance_by_field\"</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>\"provenance_by_field\"</code> from, so users must provide samples with manually computed <code>\"provenance_by_field\"</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied when reading upstream metadata to filter samples at the source. Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.data_versioning.diff.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.data_versioning.diff.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `\"provenance_by_field\"` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `\"provenance_by_field\"` from, so users\n            must provide samples with manually computed `\"provenance_by_field\"` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied when reading upstream metadata to filter samples at the source.\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.data_versioning.diff.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.data_versioning.diff.Increment][] with eager Narwhals DataFrames.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    plan = feature.graph.get_feature_plan(feature.spec().key)\n\n    # Escape hatch: if samples provided, use them directly (skip join/calculation)\n    if samples is not None:\n        import logging\n\n        import polars as pl\n\n        logger = logging.getLogger(__name__)\n\n        # Convert samples to lazy if needed\n        if isinstance(samples, nw.LazyFrame):\n            samples_lazy = samples\n        elif isinstance(samples, nw.DataFrame):\n            samples_lazy = samples.lazy()\n        else:\n            samples_lazy = nw.from_native(samples).lazy()\n\n        # Check if samples are Polars-backed (common case for escape hatch)\n        samples_native = samples_lazy.to_native()\n        is_polars_samples = isinstance(samples_native, (pl.DataFrame, pl.LazyFrame))\n\n        if is_polars_samples and self._supports_native_components():\n            # User provided Polars samples but store uses native (SQL) backend\n            # Need to materialize current metadata to Polars for compatibility\n            logger.warning(\n                f\"Feature {feature.spec().key}: samples parameter is Polars-backed but store uses native SQL backend. \"\n                f\"Materializing current metadata to Polars for diff comparison. \"\n                f\"For better performance, consider using samples with backend matching the store's backend.\"\n            )\n            # Get current metadata and materialize to Polars\n            current_lazy_native = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            if current_lazy_native is not None:\n                # Rename metaxy_provenance_by_field -&gt; provenance_by_field before converting\n                if (\n                    \"metaxy_provenance_by_field\"\n                    in current_lazy_native.collect_schema().names()\n                ):\n                    current_lazy_native = current_lazy_native.rename(\n                        {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                    )\n                # Convert to Polars using Narwhals' built-in method\n                current_lazy = nw.from_native(\n                    current_lazy_native.collect().to_polars().lazy()\n                )\n            else:\n                current_lazy = None\n        else:\n            # Same backend or no conversion needed - direct read\n            current_lazy = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            # Rename metaxy_provenance_by_field -&gt; provenance_by_field\n            if (\n                current_lazy is not None\n                and \"metaxy_provenance_by_field\"\n                in current_lazy.collect_schema().names()\n            ):\n                current_lazy = current_lazy.rename(\n                    {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                )\n\n        # Use diff resolver to compare samples with current\n        from metaxy.data_versioning.diff.narwhals import NarwhalsDiffResolver\n\n        diff_resolver = NarwhalsDiffResolver()\n\n        lazy_result = diff_resolver.find_changes(\n            target_provenance=samples_lazy,\n            current_metadata=current_lazy,\n            id_columns=feature.spec().id_columns,  # Get ID columns from feature spec\n        )\n\n        return lazy_result if lazy else lazy_result.collect()\n\n    # Root features without samples: error (samples required)\n    if not plan.deps:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and provenance_by_field columns. \"\n            f\"Root features require manual provenance_by_field computation.\"\n        )\n\n    # Non-root features without samples: automatic upstream loading\n    # Check where upstream data lives\n    upstream_location = self._check_upstream_location(feature)\n\n    if upstream_location == \"all_local\":\n        # All upstream in this store - use native field provenance calculations\n        return self._resolve_update_native(feature, filters=filters, lazy=lazy)\n    else:\n        # Some upstream in fallback stores - use Polars components\n        return self._resolve_update_polars(feature, filters=filters, lazy=lazy)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version\n        filters: List of Narwhals filter expressions\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    storage_key = self._get_storage_key(feature_key)\n\n    if storage_key not in self._storage:\n        return None\n\n    # Start with lazy Polars DataFrame, wrap with Narwhals\n    df_lazy = self._storage[storage_key].lazy()\n    nw_lazy = nw.from_native(df_lazy)\n\n    # Apply feature_version filter\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(nw.col(\"feature_version\") == feature_version)\n\n    # Apply generic Narwhals filters\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Check if result would be empty (we need to check the underlying frame)\n    # For now, return the lazy frame - emptiness check happens when materializing\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.clear","title":"clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all metadata from store.</p> <p>Useful for testing.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all metadata from store.\n\n    Useful for testing.\n    \"\"\"\n    self._storage.clear()\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open the in-memory store.</p> <p>For InMemoryMetadataStore, this is a no-op since no external resources need initialization. The auto_create_tables setting has no effect for in-memory stores (no tables to create).</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"Open the in-memory store.\n\n    For InMemoryMetadataStore, this is a no-op since no external\n    resources need initialization. The auto_create_tables setting\n    has no effect for in-memory stores (no tables to create).\n    \"\"\"\n    # No resources to initialize for in-memory storage\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the in-memory store.</p> <p>For InMemoryMetadataStore, this is a no-op since no external resources need cleanup.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the in-memory store.\n\n    For InMemoryMetadataStore, this is a no-op since no external\n    resources need cleanup.\n    \"\"\"\n    pass  # No resources to cleanup for in-memory storage\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    num_features = len(self._storage)\n    num_fallbacks = len(self.fallback_stores)\n    return (\n        f\"InMemoryMetadataStore(\"\n        f\"features={num_features}, \"\n        f\"fallback_stores={num_fallbacks})\"\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    if self._is_open:\n        num_features = len(self._storage)\n        return f\"InMemoryMetadataStore(features={num_features})\"\n    else:\n        return \"InMemoryMetadataStore()\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/","title":"Database Metadata Stores","text":"<p>Metadata stores that are backed by databases are implemented with Ibis.</p> <p>Users can extend IbisMetadataStore to work with databases that are not natively supported by Metaxy.</p>"},{"location":"reference/api/metadata-stores/ibis/#ibis-metadata-store","title":"Ibis Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore","title":"IbisMetadataStore","text":"<pre><code>IbisMetadataStore(connection_string: str | None = None, *, backend: str | None = None, connection_params: dict[str, Any] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>Generic SQL metadata store using Ibis.</p> <p>Supports any Ibis backend that supports struct types: - DuckDB: Fast local analytical database - PostgreSQL: Production-grade RDBMS - MySQL: Popular RDBMS - ClickHouse: High-performance analytical database - And other backends with struct support</p> <p>Note: Backends without native struct support (e.g., SQLite) are NOT supported. The provenance_by_field field requires struct type support for proper storage.</p> <p>Storage layout: - Each feature gets its own table: {namespace}__{feature_name} - System tables: __metaxy__feature_versions, __metaxy__migrations - Uses Ibis for cross-database compatibility</p> <p>Note: Uses MD5 hash by default for cross-database compatibility. DuckDBMetadataStore overrides this with dynamic algorithm detection. For other backends, override the calculator instance variable with backend-specific implementations.</p> Example <pre><code># ClickHouse\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# PostgreSQL\nstore = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\n\n# DuckDB (use DuckDBMetadataStore instead for better hash support)\nstore = IbisMetadataStore(\"duckdb:///metadata.db\")\n\nwith store:\n    store.write_metadata(MyFeature, df)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis connection string (e.g., \"clickhouse://host:9000/db\") If provided, backend and connection_params are ignored.</p> </li> <li> <code>backend</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\") Used with connection_params for more control.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Backend-specific connection parameters e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor backend is provided</p> </li> <li> <code>ImportError</code>             \u2013            <p>If Ibis or required backend driver not installed</p> </li> </ul> Example <pre><code># Using connection string\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# Using backend + params\nstore = IbisMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\"host\": \"localhost\", \"port\": 9000}\n    )\n</code></pre> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize Ibis metadata store.\n\n    Args:\n        connection_string: Ibis connection string (e.g., \"clickhouse://host:9000/db\")\n            If provided, backend and connection_params are ignored.\n        backend: Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\")\n            Used with connection_params for more control.\n        connection_params: Backend-specific connection parameters\n            e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n\n    Raises:\n        ValueError: If neither connection_string nor backend is provided\n        ImportError: If Ibis or required backend driver not installed\n\n    Example:\n        ```py\n        # Using connection string\n        store = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n        # Using backend + params\n        store = IbisMetadataStore(\n            backend=\"clickhouse\",\n            connection_params={\"host\": \"localhost\", \"port\": 9000}\n            )\n        ```\n    \"\"\"\n    try:\n        import ibis\n\n        self._ibis = ibis\n    except ImportError as e:\n        raise ImportError(\n            \"Ibis is required for IbisMetadataStore. \"\n            \"Install with: pip install ibis-framework[BACKEND] \"\n            \"where BACKEND is one of: duckdb, postgres, clickhouse, mysql, etc.\"\n        ) from e\n\n    if connection_string is None and backend is None:\n        raise ValueError(\n            \"Must provide either connection_string or backend. \"\n            \"Example: connection_string='clickhouse://host:9000/db' \"\n            \"or backend='clickhouse' with connection_params\"\n        )\n\n    self.connection_string = connection_string\n    self.backend = backend\n    self.connection_params = connection_params or {}\n    self._conn: ibis.BaseBackend | None = None\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open connection to database via Ibis.</p> <p>Subclasses should override this to add backend-specific initialization (e.g., loading extensions) and should call super().open() first.</p> <p>If auto_create_tables is enabled, creates system tables.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"Open connection to database via Ibis.\n\n    Subclasses should override this to add backend-specific initialization\n    (e.g., loading extensions) and should call super().open() first.\n\n    If auto_create_tables is enabled, creates system tables.\n    \"\"\"\n    if self.connection_string:\n        # Use connection string\n        self._conn = self._ibis.connect(self.connection_string)\n    else:\n        # Use backend + params\n        # Get backend-specific connect function\n        assert self.backend is not None, (\n            \"backend must be set if connection_string is None\"\n        )\n        backend_module = getattr(self._ibis, self.backend)\n        self._conn = backend_module.connect(**self.connection_params)\n\n    # Auto-create system tables if enabled (warning is handled in base class)\n    if self.auto_create_tables:\n        self._create_system_tables()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the Ibis connection.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the Ibis connection.\"\"\"\n    if self._conn is not None:\n        # Ibis connections may not have explicit close method\n        # but setting to None releases resources\n        self._conn = None\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = feature_key.table_name\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(nw.col(\"feature_version\") == feature_version)\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    if self._is_open:\n        num_features = len(self._list_features_local())\n        return f\"IbisMetadataStore(backend={backend_info}, features={num_features})\"\n    else:\n        return f\"IbisMetadataStore(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    # Track nesting depth\n    self._context_depth += 1\n\n    # Only open on first enter\n    if self._context_depth == 1:\n        # Warn if auto_create_tables is enabled (and store wants warnings)\n        if self.auto_create_tables and self._should_warn_auto_create_tables:\n            import warnings\n\n            warnings.warn(\n                f\"AUTO_CREATE_TABLES is enabled for {self.display()} - \"\n                \"do not use in production! \"\n                \"Use proper database migration tools like Alembic for production deployments.\",\n                UserWarning,\n                stacklevel=3,  # stacklevel=3 to point to user's 'with store:' line\n            )\n\n        self.open()\n        self._is_open = True\n\n        # Validate after opening (when all components are ready)\n        self._validate_after_open()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit context manager.\"\"\"\n    # Decrement depth\n    self._context_depth -= 1\n\n    # Only close when fully exited\n    if self._context_depth == 0:\n        self._is_open = False\n        self.close()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Check if this store can support the algorithm\n    # Try native field provenance calculations first (if supported), then Polars\n    supported_algorithms = []\n\n    if self._supports_native_components():\n        try:\n            _, calculator, _ = self._create_native_components()\n            supported_algorithms = calculator.supported_algorithms\n        except Exception:\n            # If native field provenance calculations fail, fall back to Polars\n            pass\n\n    # If no native support or prefer_native=False, use Polars\n    if not supported_algorithms:\n        polars_calc = PolarsProvenanceByFieldCalculator()\n        supported_algorithms = polars_calc.supported_algorithms\n\n    if self.hash_algorithm not in supported_algorithms:\n        from metaxy.metadata_store.exceptions import (\n            HashAlgorithmNotSupportedError,\n        )\n\n        raise HashAlgorithmNotSupportedError(\n            f\"Hash algorithm {self.hash_algorithm} not supported by {self.__class__.__name__}. \"\n            f\"Supported: {supported_algorithms}\"\n        )\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: DataFrame[Any] | DataFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (immutable, append-only).</p> <p>Automatically adds 'feature_version' column from current code state, unless the DataFrame already contains one (useful for migrations).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>DataFrame[Any] | DataFrame</code>)           \u2013            <p>Narwhals DataFrame or Polars DataFrame containing metadata. Must have 'provenance_by_field' column of type Struct with fields matching feature's fields. May optionally contain 'feature_version' column (for migrations).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Always writes to current store, never to fallback stores.</li> <li>If df already contains 'feature_version' column, it will be used   as-is (no replacement). This allows migrations to write historical   versions. A warning is issued unless suppressed via context manager.</li> <li>Project validation is performed unless disabled via allow_cross_project_writes()</li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: nw.DataFrame[Any] | pl.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (immutable, append-only).\n\n    Automatically adds 'feature_version' column from current code state,\n    unless the DataFrame already contains one (useful for migrations).\n\n    Args:\n        feature: Feature to write metadata for\n        df: Narwhals DataFrame or Polars DataFrame containing metadata.\n            Must have 'provenance_by_field' column of type Struct with fields matching feature's fields.\n            May optionally contain 'feature_version' column (for migrations).\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Always writes to current store, never to fallback stores.\n        - If df already contains 'feature_version' column, it will be used\n          as-is (no replacement). This allows migrations to write historical\n          versions. A warning is issued unless suppressed via context manager.\n        - Project validation is performed unless disabled via allow_cross_project_writes()\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Narwhals to Polars if needed\n    if isinstance(df, nw.DataFrame):\n        df = df.to_polars()\n    # nw.DataFrame also matches as DataFrame in some contexts, ensure it's Polars\n    if not isinstance(df, pl.DataFrame):\n        # Must be some other type - shouldn't happen but handle defensively\n        if hasattr(df, \"to_polars\"):\n            df = df.to_polars()\n        elif hasattr(df, \"to_pandas\"):\n            df = pl.from_pandas(df.to_pandas())\n        else:\n            raise TypeError(f\"Cannot convert {type(df)} to Polars DataFrame\")\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df)\n        self._write_metadata_impl(feature_key, df)\n        return\n\n    # For regular features: add feature_version and snapshot_version, validate, and write\n    # Check if feature_version and snapshot_version already exist in DataFrame\n    if \"feature_version\" in df.columns and \"snapshot_version\" in df.columns:\n        # DataFrame already has feature_version and snapshot_version - use as-is\n        # This is intended for migrations writing historical versions\n        # Issue a warning unless we're in a suppression context\n        if not _suppress_feature_version_warning.get():\n            import warnings\n\n            warnings.warn(\n                f\"Writing metadata for {feature_key.to_string()} with existing \"\n                f\"feature_version and snapshot_version columns. This is intended for migrations only. \"\n                f\"Normal code should let write_metadata() add the current versions automatically.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Get current feature version and snapshot_version from code and add them\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            current_feature_version = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            feature_cls = graph.features_by_key[feature_key]\n            current_feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n\n        # Get snapshot_version from active graph\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n        current_snapshot_version = graph.snapshot_version\n\n        df = df.with_columns(\n            [\n                pl.lit(current_feature_version).alias(\"feature_version\"),\n                pl.lit(current_snapshot_version).alias(\"snapshot_version\"),\n            ]\n        )\n\n    # Validate schema\n    self._validate_schema(df)\n\n    # Rename provenance_by_field -&gt; metaxy_provenance_by_field for database storage\n    # (Python code uses provenance_by_field, database uses metaxy_provenance_by_field)\n    df = df.rename({\"provenance_by_field\": \"metaxy_provenance_by_field\"})\n\n    # Write metadata\n    self._write_metadata_impl(feature_key, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.record_feature_graph_snapshot","title":"record_feature_graph_snapshot","text":"<pre><code>record_feature_graph_snapshot() -&gt; SnapshotPushResult\n</code></pre> <p>Record all features in graph with a graph snapshot version.</p> <p>This should be called during CD (Continuous Deployment) to record what feature versions are being deployed. Typically invoked via <code>metaxy graph push</code>.</p> <p>Records all features in the graph with the same snapshot_version, representing a consistent state of the entire feature graph based on code definitions.</p> <p>The snapshot_version is a deterministic hash of all feature_version hashes in the graph, making it idempotent - calling multiple times with the same feature definitions produces the same snapshot_version.</p> <p>This method detects three scenarios: 1. New snapshot (computational changes): No existing rows with this snapshot_version 2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version 3. No changes: Snapshot exists with identical feature_spec_versions for all features</p> <p>Returns: SnapshotPushResult</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def record_feature_graph_snapshot(self) -&gt; SnapshotPushResult:\n    \"\"\"Record all features in graph with a graph snapshot version.\n\n    This should be called during CD (Continuous Deployment) to record what\n    feature versions are being deployed. Typically invoked via `metaxy graph push`.\n\n    Records all features in the graph with the same snapshot_version, representing\n    a consistent state of the entire feature graph based on code definitions.\n\n    The snapshot_version is a deterministic hash of all feature_version hashes\n    in the graph, making it idempotent - calling multiple times with the\n    same feature definitions produces the same snapshot_version.\n\n    This method detects three scenarios:\n    1. New snapshot (computational changes): No existing rows with this snapshot_version\n    2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version\n    3. No changes: Snapshot exists with identical feature_spec_versions for all features\n\n    Returns: SnapshotPushResult\n    \"\"\"\n\n    from metaxy.models.feature import FeatureGraph\n\n    graph = FeatureGraph.get_active()\n\n    # Use to_snapshot() to get the snapshot dict\n    snapshot_dict = graph.to_snapshot()\n\n    # Generate deterministic snapshot_version from graph\n    snapshot_version = graph.snapshot_version\n\n    # Read existing feature versions once\n    try:\n        existing_versions_lazy = self.read_metadata_in_store(FEATURE_VERSIONS_KEY)\n        # Materialize to Polars for iteration\n        existing_versions = (\n            existing_versions_lazy.collect().to_polars()\n            if existing_versions_lazy is not None\n            else None\n        )\n    except Exception:\n        # Table doesn't exist yet\n        existing_versions = None\n\n    # Get project from any feature in the graph (all should have the same project)\n    # Default to empty string if no features in graph\n    if graph.features_by_key:\n        # Get first feature's project\n        first_feature = next(iter(graph.features_by_key.values()))\n        project_name = first_feature.project  # type: ignore[attr-defined]\n    else:\n        project_name = \"\"\n\n    # Check if this exact snapshot already exists for this project\n    snapshot_already_exists = False\n    existing_spec_versions: dict[str, str] = {}\n\n    if existing_versions is not None:\n        # Check if project column exists (it may not in old tables)\n        if \"project\" in existing_versions.columns:\n            snapshot_rows = existing_versions.filter(\n                (pl.col(\"snapshot_version\") == snapshot_version)\n                &amp; (pl.col(\"project\") == project_name)\n            )\n        else:\n            # Old table without project column - just check snapshot_version\n            snapshot_rows = existing_versions.filter(\n                pl.col(\"snapshot_version\") == snapshot_version\n            )\n        snapshot_already_exists = snapshot_rows.height &gt; 0\n\n        if snapshot_already_exists:\n            # Check if feature_spec_version column exists (backward compatibility)\n            # Old records (before issue #77) won't have this column\n            has_spec_version = \"feature_spec_version\" in snapshot_rows.columns\n\n            if has_spec_version:\n                # Build dict of existing feature_key -&gt; feature_spec_version\n                for row in snapshot_rows.iter_rows(named=True):\n                    existing_spec_versions[row[\"feature_key\"]] = row[\n                        \"feature_spec_version\"\n                    ]\n            # If no spec_version column, existing_spec_versions remains empty\n            # This means we'll treat it as \"no metadata changes\" (conservative approach)\n\n    # Scenario 1: New snapshot (no existing rows)\n    if not snapshot_already_exists:\n        # Build records from snapshot_dict\n        records = []\n        for feature_key_str in sorted(snapshot_dict.keys()):\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            # Always record all features for this snapshot (don't skip based on feature_version alone)\n            # Each snapshot must be complete to support migration detection\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write all new records at once\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=False,\n            metadata_changed=False,\n            features_with_spec_changes=[],\n        )\n\n    # Scenario 2 &amp; 3: Snapshot exists - check for metadata changes\n    features_with_spec_changes = []\n\n    for feature_key_str, feature_data in snapshot_dict.items():\n        current_spec_version = feature_data[\"feature_spec_version\"]\n        existing_spec_version = existing_spec_versions.get(feature_key_str)\n\n        if existing_spec_version != current_spec_version:\n            features_with_spec_changes.append(feature_key_str)\n\n    # If metadata changed, append new rows for affected features\n    if features_with_spec_changes:\n        records = []\n        for feature_key_str in features_with_spec_changes:\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write updated records (append-only)\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=True,\n            metadata_changed=True,\n            features_with_spec_changes=features_with_spec_changes,\n        )\n\n    # Scenario 3: No changes at all\n    return SnapshotPushResult(\n        snapshot_version=snapshot_version,\n        already_recorded=True,\n        metadata_changed=False,\n        features_with_spec_changes=[],\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return rows with current feature_version (default: True for safety)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]\n        columns: Subset of columns to return\n        allow_fallback: If True, check fallback stores on local miss\n        current_only: If True, only return rows with current feature_version\n            (default: True for safety)\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Determine which feature_version to use\n    feature_version_filter = feature_version\n    if current_only and not is_system_table:\n        # Get current feature_version\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            feature_version_filter = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            # Only try to get from graph if feature_key exists in graph\n            # This allows reading system tables or external features not in current graph\n            if feature_key in graph.features_by_key:\n                feature_cls = graph.features_by_key[feature_key]\n                feature_version_filter = feature_cls.feature_version()  # type: ignore[attr-defined]\n            else:\n                # Feature not in graph - skip feature_version filtering\n                feature_version_filter = None\n\n    # Map column names: provenance_by_field -&gt; metaxy_provenance_by_field for DB query\n    db_columns = None\n    if columns is not None:\n        db_columns = [\n            \"metaxy_provenance_by_field\" if col == \"provenance_by_field\" else col\n            for col in columns\n        ]\n\n    # Try local first with filters\n    lazy_frame = self.read_metadata_in_store(\n        feature,\n        feature_version=feature_version_filter,\n        filters=filters,  # Pass filters directly\n        columns=db_columns,  # Use mapped column names\n    )\n\n    if lazy_frame is not None:\n        # Rename metaxy_provenance_by_field -&gt; provenance_by_field for Python code\n        # (Database uses metaxy_provenance_by_field, Python code uses provenance_by_field)\n        if \"metaxy_provenance_by_field\" in lazy_frame.collect_schema().names():\n            lazy_frame = lazy_frame.rename(\n                {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n            )\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,  # Pass through filters directly\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,  # Pass through current_only\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    # Check local\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if check_fallback:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.list_features","title":"list_features","text":"<pre><code>list_features(*, include_fallback: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>List all features in store.</p> <p>Parameters:</p> <ul> <li> <code>include_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include features from fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of FeatureKey objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def list_features(self, *, include_fallback: bool = False) -&gt; list[FeatureKey]:\n    \"\"\"\n    List all features in store.\n\n    Args:\n        include_fallback: If True, include features from fallback stores\n\n    Returns:\n        List of FeatureKey objects\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    features = self._list_features_local()\n\n    if include_fallback:\n        for store in self.fallback_stores:\n            features.extend(store.list_features(include_fallback=True))\n\n    # Deduplicate\n    seen = set()\n    unique_features = []\n    for feature in features:\n        key_str = feature.to_string()\n        if key_str not in seen:\n            seen.add(key_str)\n            unique_features.append(feature)\n\n    return unique_features\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_graph_snapshots","title":"read_graph_snapshots","text":"<pre><code>read_graph_snapshots(project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read recorded graph snapshots from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, returns snapshots from all projects.</p> </li> </ul> <p>Returns a DataFrame with columns: - snapshot_version: Unique identifier for each graph snapshot - recorded_at: Timestamp when the snapshot was recorded - feature_count: Number of features in this snapshot</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with snapshot information, sorted by recorded_at descending</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Example <pre><code>with store:\n    # Get snapshots for a specific project\n    snapshots = store.read_graph_snapshots(project=\"my_project\")\n    latest_snapshot = snapshots[\"snapshot_version\"][0]\n    print(f\"Latest snapshot: {latest_snapshot}\")\n\n    # Get snapshots across all projects\n    all_snapshots = store.read_graph_snapshots()\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_graph_snapshots(self, project: str | None = None) -&gt; pl.DataFrame:\n    \"\"\"Read recorded graph snapshots from the feature_versions system table.\n\n    Args:\n        project: Project name to filter by. If None, returns snapshots from all projects.\n\n    Returns a DataFrame with columns:\n    - snapshot_version: Unique identifier for each graph snapshot\n    - recorded_at: Timestamp when the snapshot was recorded\n    - feature_count: Number of features in this snapshot\n\n    Returns:\n        Polars DataFrame with snapshot information, sorted by recorded_at descending\n\n    Raises:\n        StoreNotOpenError: If store is not open\n\n    Example:\n        ```py\n        with store:\n            # Get snapshots for a specific project\n            snapshots = store.read_graph_snapshots(project=\"my_project\")\n            latest_snapshot = snapshots[\"snapshot_version\"][0]\n            print(f\"Latest snapshot: {latest_snapshot}\")\n\n            # Get snapshots across all projects\n            all_snapshots = store.read_graph_snapshots()\n        ```\n    \"\"\"\n    self._check_open()\n\n    # Build filters based on project parameter\n    filters = None\n    if project is not None:\n        import narwhals as nw\n\n        filters = [nw.col(\"project\") == project]\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No snapshots recorded yet\n        return pl.DataFrame(\n            schema={\n                \"snapshot_version\": pl.String,\n                \"recorded_at\": pl.Datetime(\"us\"),\n                \"feature_count\": pl.UInt32,\n            }\n        )\n\n    versions_df = versions_lazy.collect().to_polars()\n\n    # Group by snapshot_version and get earliest recorded_at and count\n    snapshots = (\n        versions_df.group_by(\"snapshot_version\")\n        .agg(\n            [\n                pl.col(\"recorded_at\").min().alias(\"recorded_at\"),\n                pl.col(\"feature_key\").count().alias(\"feature_count\"),\n            ]\n        )\n        .sort(\"recorded_at\", descending=True)\n    )\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_features","title":"read_features","text":"<pre><code>read_features(*, current: bool = True, snapshot_version: str | None = None, project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read feature version information from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>current</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return features from the current code snapshot.      If False, must provide snapshot_version.</p> </li> <li> <code>snapshot_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific snapshot version to filter by. Required if current=False.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_key: Feature identifier</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_version: Version hash of the feature</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>recorded_at: When this version was recorded</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_spec: JSON serialized feature specification</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_class_path: Python import path to the feature class</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>snapshot_version: Graph snapshot this feature belongs to</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If current=False but no snapshot_version provided</p> </li> </ul> <p>Examples:</p> <pre><code># Get features from current code\nwith store:\n    features = store.read_features(current=True)\n    print(f\"Current graph has {len(features)} features\")\n</code></pre> <pre><code># Get features from a specific snapshot\nwith store:\n    features = store.read_features(current=False, snapshot_version=\"abc123\")\n    for row in features.iter_rows(named=True):\n        print(f\"{row['feature_key']}: {row['feature_version']}\")\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_features(\n    self,\n    *,\n    current: bool = True,\n    snapshot_version: str | None = None,\n    project: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Read feature version information from the feature_versions system table.\n\n    Args:\n        current: If True, only return features from the current code snapshot.\n                 If False, must provide snapshot_version.\n        snapshot_version: Specific snapshot version to filter by. Required if current=False.\n        project: Project name to filter by. Defaults to None.\n\n    Returns:\n        Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:\n        - feature_key: Feature identifier\n        - feature_version: Version hash of the feature\n        - recorded_at: When this version was recorded\n        - feature_spec: JSON serialized feature specification\n        - feature_class_path: Python import path to the feature class\n        - snapshot_version: Graph snapshot this feature belongs to\n\n    Raises:\n        StoreNotOpenError: If store is not open\n        ValueError: If current=False but no snapshot_version provided\n\n    Examples:\n        ```py\n        # Get features from current code\n        with store:\n            features = store.read_features(current=True)\n            print(f\"Current graph has {len(features)} features\")\n        ```\n\n        ```py\n        # Get features from a specific snapshot\n        with store:\n            features = store.read_features(current=False, snapshot_version=\"abc123\")\n            for row in features.iter_rows(named=True):\n                print(f\"{row['feature_key']}: {row['feature_version']}\")\n        ```\n    \"\"\"\n    self._check_open()\n\n    if not current and snapshot_version is None:\n        raise ValueError(\"Must provide snapshot_version when current=False\")\n\n    if current:\n        # Get current snapshot from active graph\n        graph = FeatureGraph.get_active()\n        snapshot_version = graph.snapshot_version\n\n    filters = [nw.col(\"snapshot_version\") == snapshot_version]\n    if project is not None:\n        filters.append(nw.col(\"project\") == project)\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No features recorded yet\n        return pl.DataFrame(schema=FEATURE_VERSIONS_SCHEMA)\n\n    # Filter by snapshot_version\n    versions_df = versions_lazy.collect().to_polars()\n\n    return versions_df\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\"Destination store must be opened (use context manager)\")\n\n    # Automatically handle source store context manager\n    should_close_source = not from_store._is_open\n    if should_close_source:\n        from_store.__enter__()\n\n    try:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n    finally:\n        if should_close_source:\n            from_store.__exit__(None, None, None)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.read_upstream_metadata","title":"read_upstream_metadata","text":"<pre><code>read_upstream_metadata(feature: FeatureKey | type[BaseFeature], field: FieldKey | None = None, *, filters: Mapping[str, Sequence[Expr]] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; dict[str, LazyFrame[Any]]\n</code></pre> <p>Read all upstream dependencies for a feature/field.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature whose dependencies to load</p> </li> <li> <code>field</code>               (<code>FieldKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific field (if None, loads all deps for feature)</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only read current feature_version for upstream</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.</p> </li> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Each LazyFrame has a 'provenance_by_field' column (Struct).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DependencyError</code>             \u2013            <p>If required upstream feature is missing</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_upstream_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    field: FieldKey | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; dict[str, nw.LazyFrame[Any]]:\n    \"\"\"\n    Read all upstream dependencies for a feature/field.\n\n    Args:\n        feature: Feature whose dependencies to load\n        field: Specific field (if None, loads all deps for feature)\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}\n        allow_fallback: Whether to check fallback stores\n        current_only: If True, only read current feature_version for upstream\n\n    Returns:\n        Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.\n        Each LazyFrame has a 'provenance_by_field' column (Struct).\n\n    Raises:\n        DependencyError: If required upstream feature is missing\n    \"\"\"\n    plan = self._resolve_feature_plan(feature)\n\n    # Get all upstream features we need\n    upstream_features = set()\n\n    if field is None:\n        # All fields' dependencies\n        for cont in plan.feature.fields:\n            upstream_features.update(self._get_field_dependencies(plan, cont.key))\n    else:\n        # Specific field's dependencies\n        upstream_features.update(self._get_field_dependencies(plan, field))\n\n    # Load metadata for each upstream feature\n    # Use the feature's graph to look up upstream feature classes\n    if isinstance(feature, FeatureKey):\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n    else:\n        graph = feature.graph\n\n    upstream_metadata = {}\n    for upstream_fq_key in upstream_features:\n        upstream_feature_key = upstream_fq_key.feature\n\n        # Extract filters for this specific upstream feature\n        upstream_filters = None\n        if filters:\n            upstream_key_str = upstream_feature_key.to_string()\n            if upstream_key_str in filters:\n                upstream_filters = filters[upstream_key_str]\n\n        try:\n            # Look up the Feature class from the graph and pass it to read_metadata\n            # This way we use the bound graph instead of relying on active context\n            upstream_feature_cls = graph.features_by_key[upstream_feature_key]\n            lazy_frame = self.read_metadata(\n                upstream_feature_cls,\n                filters=upstream_filters,  # Pass extracted filters (Sequence or None)\n                allow_fallback=allow_fallback,\n                current_only=current_only,  # Pass through current_only\n            )\n            # Use string key for dict\n            upstream_metadata[upstream_feature_key.to_string()] = lazy_frame\n        except FeatureNotFoundError as e:\n            raise DependencyError(\n                f\"Missing upstream feature {upstream_feature_key.to_string()} \"\n                f\"required by {plan.feature.key.to_string()}\"\n            ) from e\n\n    return upstream_metadata\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>DataFrame[Any] | LazyFrame[Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>\"provenance_by_field\"</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>\"provenance_by_field\"</code> from, so users must provide samples with manually computed <code>\"provenance_by_field\"</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied when reading upstream metadata to filter samples at the source. Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.data_versioning.diff.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.data_versioning.diff.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `\"provenance_by_field\"` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `\"provenance_by_field\"` from, so users\n            must provide samples with manually computed `\"provenance_by_field\"` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied when reading upstream metadata to filter samples at the source.\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.data_versioning.diff.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.data_versioning.diff.Increment][] with eager Narwhals DataFrames.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    plan = feature.graph.get_feature_plan(feature.spec().key)\n\n    # Escape hatch: if samples provided, use them directly (skip join/calculation)\n    if samples is not None:\n        import logging\n\n        import polars as pl\n\n        logger = logging.getLogger(__name__)\n\n        # Convert samples to lazy if needed\n        if isinstance(samples, nw.LazyFrame):\n            samples_lazy = samples\n        elif isinstance(samples, nw.DataFrame):\n            samples_lazy = samples.lazy()\n        else:\n            samples_lazy = nw.from_native(samples).lazy()\n\n        # Check if samples are Polars-backed (common case for escape hatch)\n        samples_native = samples_lazy.to_native()\n        is_polars_samples = isinstance(samples_native, (pl.DataFrame, pl.LazyFrame))\n\n        if is_polars_samples and self._supports_native_components():\n            # User provided Polars samples but store uses native (SQL) backend\n            # Need to materialize current metadata to Polars for compatibility\n            logger.warning(\n                f\"Feature {feature.spec().key}: samples parameter is Polars-backed but store uses native SQL backend. \"\n                f\"Materializing current metadata to Polars for diff comparison. \"\n                f\"For better performance, consider using samples with backend matching the store's backend.\"\n            )\n            # Get current metadata and materialize to Polars\n            current_lazy_native = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            if current_lazy_native is not None:\n                # Rename metaxy_provenance_by_field -&gt; provenance_by_field before converting\n                if (\n                    \"metaxy_provenance_by_field\"\n                    in current_lazy_native.collect_schema().names()\n                ):\n                    current_lazy_native = current_lazy_native.rename(\n                        {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                    )\n                # Convert to Polars using Narwhals' built-in method\n                current_lazy = nw.from_native(\n                    current_lazy_native.collect().to_polars().lazy()\n                )\n            else:\n                current_lazy = None\n        else:\n            # Same backend or no conversion needed - direct read\n            current_lazy = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            # Rename metaxy_provenance_by_field -&gt; provenance_by_field\n            if (\n                current_lazy is not None\n                and \"metaxy_provenance_by_field\"\n                in current_lazy.collect_schema().names()\n            ):\n                current_lazy = current_lazy.rename(\n                    {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                )\n\n        # Use diff resolver to compare samples with current\n        from metaxy.data_versioning.diff.narwhals import NarwhalsDiffResolver\n\n        diff_resolver = NarwhalsDiffResolver()\n\n        lazy_result = diff_resolver.find_changes(\n            target_provenance=samples_lazy,\n            current_metadata=current_lazy,\n            id_columns=feature.spec().id_columns,  # Get ID columns from feature spec\n        )\n\n        return lazy_result if lazy else lazy_result.collect()\n\n    # Root features without samples: error (samples required)\n    if not plan.deps:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and provenance_by_field columns. \"\n            f\"Root features require manual provenance_by_field computation.\"\n        )\n\n    # Non-root features without samples: automatic upstream loading\n    # Check where upstream data lives\n    upstream_location = self._check_upstream_location(feature)\n\n    if upstream_location == \"all_local\":\n        # All upstream in this store - use native field provenance calculations\n        return self._resolve_update_native(feature, filters=filters, lazy=lazy)\n    else:\n        # Some upstream in fallback stores - use Polars components\n        return self._resolve_update_polars(feature, filters=filters, lazy=lazy)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/","title":"Clickhouse Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","title":"ClickHouseMetadataStore","text":"<pre><code>ClickHouseMetadataStore(connection_string: str | None = None, *, connection_params: dict[str, Any] | None = None, fallback_stores: list[MetadataStore] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>ClickHouse metadata storeusing Ibis backend.</p> Connection Parameters <pre><code>store = ClickHouseMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\n        \"host\": \"localhost\",\n        \"port\": 9000,\n        \"database\": \"default\",\n        \"user\": \"default\",\n        \"password\": \"\"\n    },\n    hash_algorithm=HashAlgorithm.XXHASH64\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ClickHouse connection string.</p> <p>Format: <code>clickhouse://[user[:password]@]host[:port]/database[?param=value]</code></p> <p>Examples:     <pre><code>- \"clickhouse://localhost:9000/default\"\n- \"clickhouse://user:pass@host:9000/db\"\n- \"clickhouse://host:9000/db?secure=true\"\n</code></pre></p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Alternative to connection_string, specify params as dict:</p> <ul> <li> <p>host: Server host</p> </li> <li> <p>port: Server port (default: <code>9000</code>)</p> </li> <li> <p>database: Database name</p> </li> <li> <p>user: Username</p> </li> <li> <p>password: Password</p> </li> <li> <p>secure: Use secure connection (default: <code>False</code>)</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-clickhouse not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor connection_params provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/clickhouse.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [ClickHouse](https://clickhouse.com/) metadata store.\n\n    Args:\n        connection_string: ClickHouse connection string.\n\n            Format: `clickhouse://[user[:password]@]host[:port]/database[?param=value]`\n\n            Examples:\n                ```\n                - \"clickhouse://localhost:9000/default\"\n                - \"clickhouse://user:pass@host:9000/db\"\n                - \"clickhouse://host:9000/db?secure=true\"\n                ```\n\n        connection_params: Alternative to connection_string, specify params as dict:\n\n            - host: Server host\n\n            - port: Server port (default: `9000`)\n\n            - database: Database name\n\n            - user: Username\n\n            - password: Password\n\n            - secure: Use secure connection (default: `False`)\n\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Raises:\n        ImportError: If ibis-clickhouse not installed\n        ValueError: If neither connection_string nor connection_params provided\n    \"\"\"\n    if connection_string is None and connection_params is None:\n        raise ValueError(\n            \"Must provide either connection_string or connection_params. \"\n            \"Example: connection_string='clickhouse://localhost:9000/default'\"\n        )\n\n    # Initialize Ibis store with ClickHouse backend\n    super().__init__(\n        connection_string=connection_string,\n        backend=\"clickhouse\" if connection_string is None else None,\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open connection to database via Ibis.</p> <p>Subclasses should override this to add backend-specific initialization (e.g., loading extensions) and should call super().open() first.</p> <p>If auto_create_tables is enabled, creates system tables.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"Open connection to database via Ibis.\n\n    Subclasses should override this to add backend-specific initialization\n    (e.g., loading extensions) and should call super().open() first.\n\n    If auto_create_tables is enabled, creates system tables.\n    \"\"\"\n    if self.connection_string:\n        # Use connection string\n        self._conn = self._ibis.connect(self.connection_string)\n    else:\n        # Use backend + params\n        # Get backend-specific connect function\n        assert self.backend is not None, (\n            \"backend must be set if connection_string is None\"\n        )\n        backend_module = getattr(self._ibis, self.backend)\n        self._conn = backend_module.connect(**self.connection_params)\n\n    # Auto-create system tables if enabled (warning is handled in base class)\n    if self.auto_create_tables:\n        self._create_system_tables()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the Ibis connection.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the Ibis connection.\"\"\"\n    if self._conn is not None:\n        # Ibis connections may not have explicit close method\n        # but setting to None releases resources\n        self._conn = None\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    # Track nesting depth\n    self._context_depth += 1\n\n    # Only open on first enter\n    if self._context_depth == 1:\n        # Warn if auto_create_tables is enabled (and store wants warnings)\n        if self.auto_create_tables and self._should_warn_auto_create_tables:\n            import warnings\n\n            warnings.warn(\n                f\"AUTO_CREATE_TABLES is enabled for {self.display()} - \"\n                \"do not use in production! \"\n                \"Use proper database migration tools like Alembic for production deployments.\",\n                UserWarning,\n                stacklevel=3,  # stacklevel=3 to point to user's 'with store:' line\n            )\n\n        self.open()\n        self._is_open = True\n\n        # Validate after opening (when all components are ready)\n        self._validate_after_open()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit context manager.\"\"\"\n    # Decrement depth\n    self._context_depth -= 1\n\n    # Only close when fully exited\n    if self._context_depth == 0:\n        self._is_open = False\n        self.close()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Check if this store can support the algorithm\n    # Try native field provenance calculations first (if supported), then Polars\n    supported_algorithms = []\n\n    if self._supports_native_components():\n        try:\n            _, calculator, _ = self._create_native_components()\n            supported_algorithms = calculator.supported_algorithms\n        except Exception:\n            # If native field provenance calculations fail, fall back to Polars\n            pass\n\n    # If no native support or prefer_native=False, use Polars\n    if not supported_algorithms:\n        polars_calc = PolarsProvenanceByFieldCalculator()\n        supported_algorithms = polars_calc.supported_algorithms\n\n    if self.hash_algorithm not in supported_algorithms:\n        from metaxy.metadata_store.exceptions import (\n            HashAlgorithmNotSupportedError,\n        )\n\n        raise HashAlgorithmNotSupportedError(\n            f\"Hash algorithm {self.hash_algorithm} not supported by {self.__class__.__name__}. \"\n            f\"Supported: {supported_algorithms}\"\n        )\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: DataFrame[Any] | DataFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (immutable, append-only).</p> <p>Automatically adds 'feature_version' column from current code state, unless the DataFrame already contains one (useful for migrations).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>DataFrame[Any] | DataFrame</code>)           \u2013            <p>Narwhals DataFrame or Polars DataFrame containing metadata. Must have 'provenance_by_field' column of type Struct with fields matching feature's fields. May optionally contain 'feature_version' column (for migrations).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Always writes to current store, never to fallback stores.</li> <li>If df already contains 'feature_version' column, it will be used   as-is (no replacement). This allows migrations to write historical   versions. A warning is issued unless suppressed via context manager.</li> <li>Project validation is performed unless disabled via allow_cross_project_writes()</li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: nw.DataFrame[Any] | pl.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (immutable, append-only).\n\n    Automatically adds 'feature_version' column from current code state,\n    unless the DataFrame already contains one (useful for migrations).\n\n    Args:\n        feature: Feature to write metadata for\n        df: Narwhals DataFrame or Polars DataFrame containing metadata.\n            Must have 'provenance_by_field' column of type Struct with fields matching feature's fields.\n            May optionally contain 'feature_version' column (for migrations).\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Always writes to current store, never to fallback stores.\n        - If df already contains 'feature_version' column, it will be used\n          as-is (no replacement). This allows migrations to write historical\n          versions. A warning is issued unless suppressed via context manager.\n        - Project validation is performed unless disabled via allow_cross_project_writes()\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Narwhals to Polars if needed\n    if isinstance(df, nw.DataFrame):\n        df = df.to_polars()\n    # nw.DataFrame also matches as DataFrame in some contexts, ensure it's Polars\n    if not isinstance(df, pl.DataFrame):\n        # Must be some other type - shouldn't happen but handle defensively\n        if hasattr(df, \"to_polars\"):\n            df = df.to_polars()\n        elif hasattr(df, \"to_pandas\"):\n            df = pl.from_pandas(df.to_pandas())\n        else:\n            raise TypeError(f\"Cannot convert {type(df)} to Polars DataFrame\")\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df)\n        self._write_metadata_impl(feature_key, df)\n        return\n\n    # For regular features: add feature_version and snapshot_version, validate, and write\n    # Check if feature_version and snapshot_version already exist in DataFrame\n    if \"feature_version\" in df.columns and \"snapshot_version\" in df.columns:\n        # DataFrame already has feature_version and snapshot_version - use as-is\n        # This is intended for migrations writing historical versions\n        # Issue a warning unless we're in a suppression context\n        if not _suppress_feature_version_warning.get():\n            import warnings\n\n            warnings.warn(\n                f\"Writing metadata for {feature_key.to_string()} with existing \"\n                f\"feature_version and snapshot_version columns. This is intended for migrations only. \"\n                f\"Normal code should let write_metadata() add the current versions automatically.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Get current feature version and snapshot_version from code and add them\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            current_feature_version = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            feature_cls = graph.features_by_key[feature_key]\n            current_feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n\n        # Get snapshot_version from active graph\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n        current_snapshot_version = graph.snapshot_version\n\n        df = df.with_columns(\n            [\n                pl.lit(current_feature_version).alias(\"feature_version\"),\n                pl.lit(current_snapshot_version).alias(\"snapshot_version\"),\n            ]\n        )\n\n    # Validate schema\n    self._validate_schema(df)\n\n    # Rename provenance_by_field -&gt; metaxy_provenance_by_field for database storage\n    # (Python code uses provenance_by_field, database uses metaxy_provenance_by_field)\n    df = df.rename({\"provenance_by_field\": \"metaxy_provenance_by_field\"})\n\n    # Write metadata\n    self._write_metadata_impl(feature_key, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.record_feature_graph_snapshot","title":"record_feature_graph_snapshot","text":"<pre><code>record_feature_graph_snapshot() -&gt; SnapshotPushResult\n</code></pre> <p>Record all features in graph with a graph snapshot version.</p> <p>This should be called during CD (Continuous Deployment) to record what feature versions are being deployed. Typically invoked via <code>metaxy graph push</code>.</p> <p>Records all features in the graph with the same snapshot_version, representing a consistent state of the entire feature graph based on code definitions.</p> <p>The snapshot_version is a deterministic hash of all feature_version hashes in the graph, making it idempotent - calling multiple times with the same feature definitions produces the same snapshot_version.</p> <p>This method detects three scenarios: 1. New snapshot (computational changes): No existing rows with this snapshot_version 2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version 3. No changes: Snapshot exists with identical feature_spec_versions for all features</p> <p>Returns: SnapshotPushResult</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def record_feature_graph_snapshot(self) -&gt; SnapshotPushResult:\n    \"\"\"Record all features in graph with a graph snapshot version.\n\n    This should be called during CD (Continuous Deployment) to record what\n    feature versions are being deployed. Typically invoked via `metaxy graph push`.\n\n    Records all features in the graph with the same snapshot_version, representing\n    a consistent state of the entire feature graph based on code definitions.\n\n    The snapshot_version is a deterministic hash of all feature_version hashes\n    in the graph, making it idempotent - calling multiple times with the\n    same feature definitions produces the same snapshot_version.\n\n    This method detects three scenarios:\n    1. New snapshot (computational changes): No existing rows with this snapshot_version\n    2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version\n    3. No changes: Snapshot exists with identical feature_spec_versions for all features\n\n    Returns: SnapshotPushResult\n    \"\"\"\n\n    from metaxy.models.feature import FeatureGraph\n\n    graph = FeatureGraph.get_active()\n\n    # Use to_snapshot() to get the snapshot dict\n    snapshot_dict = graph.to_snapshot()\n\n    # Generate deterministic snapshot_version from graph\n    snapshot_version = graph.snapshot_version\n\n    # Read existing feature versions once\n    try:\n        existing_versions_lazy = self.read_metadata_in_store(FEATURE_VERSIONS_KEY)\n        # Materialize to Polars for iteration\n        existing_versions = (\n            existing_versions_lazy.collect().to_polars()\n            if existing_versions_lazy is not None\n            else None\n        )\n    except Exception:\n        # Table doesn't exist yet\n        existing_versions = None\n\n    # Get project from any feature in the graph (all should have the same project)\n    # Default to empty string if no features in graph\n    if graph.features_by_key:\n        # Get first feature's project\n        first_feature = next(iter(graph.features_by_key.values()))\n        project_name = first_feature.project  # type: ignore[attr-defined]\n    else:\n        project_name = \"\"\n\n    # Check if this exact snapshot already exists for this project\n    snapshot_already_exists = False\n    existing_spec_versions: dict[str, str] = {}\n\n    if existing_versions is not None:\n        # Check if project column exists (it may not in old tables)\n        if \"project\" in existing_versions.columns:\n            snapshot_rows = existing_versions.filter(\n                (pl.col(\"snapshot_version\") == snapshot_version)\n                &amp; (pl.col(\"project\") == project_name)\n            )\n        else:\n            # Old table without project column - just check snapshot_version\n            snapshot_rows = existing_versions.filter(\n                pl.col(\"snapshot_version\") == snapshot_version\n            )\n        snapshot_already_exists = snapshot_rows.height &gt; 0\n\n        if snapshot_already_exists:\n            # Check if feature_spec_version column exists (backward compatibility)\n            # Old records (before issue #77) won't have this column\n            has_spec_version = \"feature_spec_version\" in snapshot_rows.columns\n\n            if has_spec_version:\n                # Build dict of existing feature_key -&gt; feature_spec_version\n                for row in snapshot_rows.iter_rows(named=True):\n                    existing_spec_versions[row[\"feature_key\"]] = row[\n                        \"feature_spec_version\"\n                    ]\n            # If no spec_version column, existing_spec_versions remains empty\n            # This means we'll treat it as \"no metadata changes\" (conservative approach)\n\n    # Scenario 1: New snapshot (no existing rows)\n    if not snapshot_already_exists:\n        # Build records from snapshot_dict\n        records = []\n        for feature_key_str in sorted(snapshot_dict.keys()):\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            # Always record all features for this snapshot (don't skip based on feature_version alone)\n            # Each snapshot must be complete to support migration detection\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write all new records at once\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=False,\n            metadata_changed=False,\n            features_with_spec_changes=[],\n        )\n\n    # Scenario 2 &amp; 3: Snapshot exists - check for metadata changes\n    features_with_spec_changes = []\n\n    for feature_key_str, feature_data in snapshot_dict.items():\n        current_spec_version = feature_data[\"feature_spec_version\"]\n        existing_spec_version = existing_spec_versions.get(feature_key_str)\n\n        if existing_spec_version != current_spec_version:\n            features_with_spec_changes.append(feature_key_str)\n\n    # If metadata changed, append new rows for affected features\n    if features_with_spec_changes:\n        records = []\n        for feature_key_str in features_with_spec_changes:\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write updated records (append-only)\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=True,\n            metadata_changed=True,\n            features_with_spec_changes=features_with_spec_changes,\n        )\n\n    # Scenario 3: No changes at all\n    return SnapshotPushResult(\n        snapshot_version=snapshot_version,\n        already_recorded=True,\n        metadata_changed=False,\n        features_with_spec_changes=[],\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = feature_key.table_name\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(nw.col(\"feature_version\") == feature_version)\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return rows with current feature_version (default: True for safety)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]\n        columns: Subset of columns to return\n        allow_fallback: If True, check fallback stores on local miss\n        current_only: If True, only return rows with current feature_version\n            (default: True for safety)\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Determine which feature_version to use\n    feature_version_filter = feature_version\n    if current_only and not is_system_table:\n        # Get current feature_version\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            feature_version_filter = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            # Only try to get from graph if feature_key exists in graph\n            # This allows reading system tables or external features not in current graph\n            if feature_key in graph.features_by_key:\n                feature_cls = graph.features_by_key[feature_key]\n                feature_version_filter = feature_cls.feature_version()  # type: ignore[attr-defined]\n            else:\n                # Feature not in graph - skip feature_version filtering\n                feature_version_filter = None\n\n    # Map column names: provenance_by_field -&gt; metaxy_provenance_by_field for DB query\n    db_columns = None\n    if columns is not None:\n        db_columns = [\n            \"metaxy_provenance_by_field\" if col == \"provenance_by_field\" else col\n            for col in columns\n        ]\n\n    # Try local first with filters\n    lazy_frame = self.read_metadata_in_store(\n        feature,\n        feature_version=feature_version_filter,\n        filters=filters,  # Pass filters directly\n        columns=db_columns,  # Use mapped column names\n    )\n\n    if lazy_frame is not None:\n        # Rename metaxy_provenance_by_field -&gt; provenance_by_field for Python code\n        # (Database uses metaxy_provenance_by_field, Python code uses provenance_by_field)\n        if \"metaxy_provenance_by_field\" in lazy_frame.collect_schema().names():\n            lazy_frame = lazy_frame.rename(\n                {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n            )\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,  # Pass through filters directly\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,  # Pass through current_only\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    # Check local\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if check_fallback:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.list_features","title":"list_features","text":"<pre><code>list_features(*, include_fallback: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>List all features in store.</p> <p>Parameters:</p> <ul> <li> <code>include_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include features from fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of FeatureKey objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def list_features(self, *, include_fallback: bool = False) -&gt; list[FeatureKey]:\n    \"\"\"\n    List all features in store.\n\n    Args:\n        include_fallback: If True, include features from fallback stores\n\n    Returns:\n        List of FeatureKey objects\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    features = self._list_features_local()\n\n    if include_fallback:\n        for store in self.fallback_stores:\n            features.extend(store.list_features(include_fallback=True))\n\n    # Deduplicate\n    seen = set()\n    unique_features = []\n    for feature in features:\n        key_str = feature.to_string()\n        if key_str not in seen:\n            seen.add(key_str)\n            unique_features.append(feature)\n\n    return unique_features\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    if self._is_open:\n        num_features = len(self._list_features_local())\n        return f\"IbisMetadataStore(backend={backend_info}, features={num_features})\"\n    else:\n        return f\"IbisMetadataStore(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_graph_snapshots","title":"read_graph_snapshots","text":"<pre><code>read_graph_snapshots(project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read recorded graph snapshots from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, returns snapshots from all projects.</p> </li> </ul> <p>Returns a DataFrame with columns: - snapshot_version: Unique identifier for each graph snapshot - recorded_at: Timestamp when the snapshot was recorded - feature_count: Number of features in this snapshot</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with snapshot information, sorted by recorded_at descending</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Example <pre><code>with store:\n    # Get snapshots for a specific project\n    snapshots = store.read_graph_snapshots(project=\"my_project\")\n    latest_snapshot = snapshots[\"snapshot_version\"][0]\n    print(f\"Latest snapshot: {latest_snapshot}\")\n\n    # Get snapshots across all projects\n    all_snapshots = store.read_graph_snapshots()\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_graph_snapshots(self, project: str | None = None) -&gt; pl.DataFrame:\n    \"\"\"Read recorded graph snapshots from the feature_versions system table.\n\n    Args:\n        project: Project name to filter by. If None, returns snapshots from all projects.\n\n    Returns a DataFrame with columns:\n    - snapshot_version: Unique identifier for each graph snapshot\n    - recorded_at: Timestamp when the snapshot was recorded\n    - feature_count: Number of features in this snapshot\n\n    Returns:\n        Polars DataFrame with snapshot information, sorted by recorded_at descending\n\n    Raises:\n        StoreNotOpenError: If store is not open\n\n    Example:\n        ```py\n        with store:\n            # Get snapshots for a specific project\n            snapshots = store.read_graph_snapshots(project=\"my_project\")\n            latest_snapshot = snapshots[\"snapshot_version\"][0]\n            print(f\"Latest snapshot: {latest_snapshot}\")\n\n            # Get snapshots across all projects\n            all_snapshots = store.read_graph_snapshots()\n        ```\n    \"\"\"\n    self._check_open()\n\n    # Build filters based on project parameter\n    filters = None\n    if project is not None:\n        import narwhals as nw\n\n        filters = [nw.col(\"project\") == project]\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No snapshots recorded yet\n        return pl.DataFrame(\n            schema={\n                \"snapshot_version\": pl.String,\n                \"recorded_at\": pl.Datetime(\"us\"),\n                \"feature_count\": pl.UInt32,\n            }\n        )\n\n    versions_df = versions_lazy.collect().to_polars()\n\n    # Group by snapshot_version and get earliest recorded_at and count\n    snapshots = (\n        versions_df.group_by(\"snapshot_version\")\n        .agg(\n            [\n                pl.col(\"recorded_at\").min().alias(\"recorded_at\"),\n                pl.col(\"feature_key\").count().alias(\"feature_count\"),\n            ]\n        )\n        .sort(\"recorded_at\", descending=True)\n    )\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_features","title":"read_features","text":"<pre><code>read_features(*, current: bool = True, snapshot_version: str | None = None, project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read feature version information from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>current</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return features from the current code snapshot.      If False, must provide snapshot_version.</p> </li> <li> <code>snapshot_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific snapshot version to filter by. Required if current=False.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_key: Feature identifier</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_version: Version hash of the feature</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>recorded_at: When this version was recorded</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_spec: JSON serialized feature specification</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_class_path: Python import path to the feature class</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>snapshot_version: Graph snapshot this feature belongs to</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If current=False but no snapshot_version provided</p> </li> </ul> <p>Examples:</p> <pre><code># Get features from current code\nwith store:\n    features = store.read_features(current=True)\n    print(f\"Current graph has {len(features)} features\")\n</code></pre> <pre><code># Get features from a specific snapshot\nwith store:\n    features = store.read_features(current=False, snapshot_version=\"abc123\")\n    for row in features.iter_rows(named=True):\n        print(f\"{row['feature_key']}: {row['feature_version']}\")\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_features(\n    self,\n    *,\n    current: bool = True,\n    snapshot_version: str | None = None,\n    project: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Read feature version information from the feature_versions system table.\n\n    Args:\n        current: If True, only return features from the current code snapshot.\n                 If False, must provide snapshot_version.\n        snapshot_version: Specific snapshot version to filter by. Required if current=False.\n        project: Project name to filter by. Defaults to None.\n\n    Returns:\n        Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:\n        - feature_key: Feature identifier\n        - feature_version: Version hash of the feature\n        - recorded_at: When this version was recorded\n        - feature_spec: JSON serialized feature specification\n        - feature_class_path: Python import path to the feature class\n        - snapshot_version: Graph snapshot this feature belongs to\n\n    Raises:\n        StoreNotOpenError: If store is not open\n        ValueError: If current=False but no snapshot_version provided\n\n    Examples:\n        ```py\n        # Get features from current code\n        with store:\n            features = store.read_features(current=True)\n            print(f\"Current graph has {len(features)} features\")\n        ```\n\n        ```py\n        # Get features from a specific snapshot\n        with store:\n            features = store.read_features(current=False, snapshot_version=\"abc123\")\n            for row in features.iter_rows(named=True):\n                print(f\"{row['feature_key']}: {row['feature_version']}\")\n        ```\n    \"\"\"\n    self._check_open()\n\n    if not current and snapshot_version is None:\n        raise ValueError(\"Must provide snapshot_version when current=False\")\n\n    if current:\n        # Get current snapshot from active graph\n        graph = FeatureGraph.get_active()\n        snapshot_version = graph.snapshot_version\n\n    filters = [nw.col(\"snapshot_version\") == snapshot_version]\n    if project is not None:\n        filters.append(nw.col(\"project\") == project)\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No features recorded yet\n        return pl.DataFrame(schema=FEATURE_VERSIONS_SCHEMA)\n\n    # Filter by snapshot_version\n    versions_df = versions_lazy.collect().to_polars()\n\n    return versions_df\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\"Destination store must be opened (use context manager)\")\n\n    # Automatically handle source store context manager\n    should_close_source = not from_store._is_open\n    if should_close_source:\n        from_store.__enter__()\n\n    try:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n    finally:\n        if should_close_source:\n            from_store.__exit__(None, None, None)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.read_upstream_metadata","title":"read_upstream_metadata","text":"<pre><code>read_upstream_metadata(feature: FeatureKey | type[BaseFeature], field: FieldKey | None = None, *, filters: Mapping[str, Sequence[Expr]] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; dict[str, LazyFrame[Any]]\n</code></pre> <p>Read all upstream dependencies for a feature/field.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature whose dependencies to load</p> </li> <li> <code>field</code>               (<code>FieldKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific field (if None, loads all deps for feature)</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only read current feature_version for upstream</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.</p> </li> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Each LazyFrame has a 'provenance_by_field' column (Struct).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DependencyError</code>             \u2013            <p>If required upstream feature is missing</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_upstream_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    field: FieldKey | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; dict[str, nw.LazyFrame[Any]]:\n    \"\"\"\n    Read all upstream dependencies for a feature/field.\n\n    Args:\n        feature: Feature whose dependencies to load\n        field: Specific field (if None, loads all deps for feature)\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}\n        allow_fallback: Whether to check fallback stores\n        current_only: If True, only read current feature_version for upstream\n\n    Returns:\n        Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.\n        Each LazyFrame has a 'provenance_by_field' column (Struct).\n\n    Raises:\n        DependencyError: If required upstream feature is missing\n    \"\"\"\n    plan = self._resolve_feature_plan(feature)\n\n    # Get all upstream features we need\n    upstream_features = set()\n\n    if field is None:\n        # All fields' dependencies\n        for cont in plan.feature.fields:\n            upstream_features.update(self._get_field_dependencies(plan, cont.key))\n    else:\n        # Specific field's dependencies\n        upstream_features.update(self._get_field_dependencies(plan, field))\n\n    # Load metadata for each upstream feature\n    # Use the feature's graph to look up upstream feature classes\n    if isinstance(feature, FeatureKey):\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n    else:\n        graph = feature.graph\n\n    upstream_metadata = {}\n    for upstream_fq_key in upstream_features:\n        upstream_feature_key = upstream_fq_key.feature\n\n        # Extract filters for this specific upstream feature\n        upstream_filters = None\n        if filters:\n            upstream_key_str = upstream_feature_key.to_string()\n            if upstream_key_str in filters:\n                upstream_filters = filters[upstream_key_str]\n\n        try:\n            # Look up the Feature class from the graph and pass it to read_metadata\n            # This way we use the bound graph instead of relying on active context\n            upstream_feature_cls = graph.features_by_key[upstream_feature_key]\n            lazy_frame = self.read_metadata(\n                upstream_feature_cls,\n                filters=upstream_filters,  # Pass extracted filters (Sequence or None)\n                allow_fallback=allow_fallback,\n                current_only=current_only,  # Pass through current_only\n            )\n            # Use string key for dict\n            upstream_metadata[upstream_feature_key.to_string()] = lazy_frame\n        except FeatureNotFoundError as e:\n            raise DependencyError(\n                f\"Missing upstream feature {upstream_feature_key.to_string()} \"\n                f\"required by {plan.feature.key.to_string()}\"\n            ) from e\n\n    return upstream_metadata\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/clickhouse/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>DataFrame[Any] | LazyFrame[Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>\"provenance_by_field\"</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>\"provenance_by_field\"</code> from, so users must provide samples with manually computed <code>\"provenance_by_field\"</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied when reading upstream metadata to filter samples at the source. Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.data_versioning.diff.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.data_versioning.diff.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `\"provenance_by_field\"` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `\"provenance_by_field\"` from, so users\n            must provide samples with manually computed `\"provenance_by_field\"` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied when reading upstream metadata to filter samples at the source.\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.data_versioning.diff.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.data_versioning.diff.Increment][] with eager Narwhals DataFrames.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    plan = feature.graph.get_feature_plan(feature.spec().key)\n\n    # Escape hatch: if samples provided, use them directly (skip join/calculation)\n    if samples is not None:\n        import logging\n\n        import polars as pl\n\n        logger = logging.getLogger(__name__)\n\n        # Convert samples to lazy if needed\n        if isinstance(samples, nw.LazyFrame):\n            samples_lazy = samples\n        elif isinstance(samples, nw.DataFrame):\n            samples_lazy = samples.lazy()\n        else:\n            samples_lazy = nw.from_native(samples).lazy()\n\n        # Check if samples are Polars-backed (common case for escape hatch)\n        samples_native = samples_lazy.to_native()\n        is_polars_samples = isinstance(samples_native, (pl.DataFrame, pl.LazyFrame))\n\n        if is_polars_samples and self._supports_native_components():\n            # User provided Polars samples but store uses native (SQL) backend\n            # Need to materialize current metadata to Polars for compatibility\n            logger.warning(\n                f\"Feature {feature.spec().key}: samples parameter is Polars-backed but store uses native SQL backend. \"\n                f\"Materializing current metadata to Polars for diff comparison. \"\n                f\"For better performance, consider using samples with backend matching the store's backend.\"\n            )\n            # Get current metadata and materialize to Polars\n            current_lazy_native = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            if current_lazy_native is not None:\n                # Rename metaxy_provenance_by_field -&gt; provenance_by_field before converting\n                if (\n                    \"metaxy_provenance_by_field\"\n                    in current_lazy_native.collect_schema().names()\n                ):\n                    current_lazy_native = current_lazy_native.rename(\n                        {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                    )\n                # Convert to Polars using Narwhals' built-in method\n                current_lazy = nw.from_native(\n                    current_lazy_native.collect().to_polars().lazy()\n                )\n            else:\n                current_lazy = None\n        else:\n            # Same backend or no conversion needed - direct read\n            current_lazy = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            # Rename metaxy_provenance_by_field -&gt; provenance_by_field\n            if (\n                current_lazy is not None\n                and \"metaxy_provenance_by_field\"\n                in current_lazy.collect_schema().names()\n            ):\n                current_lazy = current_lazy.rename(\n                    {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                )\n\n        # Use diff resolver to compare samples with current\n        from metaxy.data_versioning.diff.narwhals import NarwhalsDiffResolver\n\n        diff_resolver = NarwhalsDiffResolver()\n\n        lazy_result = diff_resolver.find_changes(\n            target_provenance=samples_lazy,\n            current_metadata=current_lazy,\n            id_columns=feature.spec().id_columns,  # Get ID columns from feature spec\n        )\n\n        return lazy_result if lazy else lazy_result.collect()\n\n    # Root features without samples: error (samples required)\n    if not plan.deps:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and provenance_by_field columns. \"\n            f\"Root features require manual provenance_by_field computation.\"\n        )\n\n    # Non-root features without samples: automatic upstream loading\n    # Check where upstream data lives\n    upstream_location = self._check_upstream_location(feature)\n\n    if upstream_location == \"all_local\":\n        # All upstream in this store - use native field provenance calculations\n        return self._resolve_update_native(feature, filters=filters, lazy=lazy)\n    else:\n        # Some upstream in fallback stores - use Polars components\n        return self._resolve_update_polars(feature, filters=filters, lazy=lazy)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/","title":"DuckDB Metadata Store","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore","title":"DuckDBMetadataStore","text":"<pre><code>DuckDBMetadataStore(database: str | Path, *, config: dict[str, str] | None = None, extensions: Sequence[ExtensionInput] | None = None, fallback_stores: list[MetadataStore] | None = None, ducklake: DuckLakeConfigInput | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>DuckDB metadata store using Ibis backend.</p> Local File <pre><code>store = DuckDBMetadataStore(\"metadata.db\")\n</code></pre> In-memory database <pre><code># In-memory database\nstore = DuckDBMetadataStore(\":memory:\")\n</code></pre> MotherDuck <pre><code># MotherDuck\nstore = DuckDBMetadataStore(\"md:my_database\")\n</code></pre> With extensions <pre><code># With extensions\nstore = DuckDBMetadataStore(\n    \"metadata.db\",\n    hash_algorithm=HashAlgorithm.XXHASH64,\n    extensions=[\"hashfuncs\"]\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>database</code>               (<code>str | Path</code>)           \u2013            <p>Database connection string or path. - File path: <code>\"metadata.db\"</code> or <code>Path(\"metadata.db\")</code></p> <ul> <li> <p>In-memory: <code>\":memory:\"</code></p> </li> <li> <p>MotherDuck: <code>\"md:my_database\"</code> or <code>\"md:my_database?motherduck_token=...\"</code></p> </li> <li> <p>S3: <code>\"s3://bucket/path/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>HTTPS: <code>\"https://example.com/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>Any valid DuckDB connection string</p> </li> </ul> </li> <li> <code>config</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})</p> </li> <li> <code>extensions</code>               (<code>Sequence[ExtensionInput] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of DuckDB extensions to install and load on open. Supports strings (community repo), mapping-like objects with <code>name</code>/<code>repository</code> keys, or metaxy.metadata_store.duckdb.ExtensionSpec instances.</p> </li> </ul> Optional DuckLake attachment configuration. Provide either a <p>mapping with 'metadata_backend' and 'storage_backend' entries or a DuckLakeAttachmentConfig instance. When supplied, the DuckDB connection is configured to ATTACH the DuckLake catalog after open(). fallback_stores: Ordered list of read-only fallback stores.</p> <p>**kwargs: Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> Warning <p>Parent directories are NOT created automatically. Ensure paths exist before initializing the store.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def __init__(\n    self,\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[ExtensionInput] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    ducklake: DuckLakeConfigInput | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize [DuckDB](https://duckdb.org/) metadata store.\n\n    Args:\n        database: Database connection string or path.\n            - File path: `\"metadata.db\"` or `Path(\"metadata.db\")`\n\n            - In-memory: `\":memory:\"`\n\n            - MotherDuck: `\"md:my_database\"` or `\"md:my_database?motherduck_token=...\"`\n\n            - S3: `\"s3://bucket/path/database.duckdb\"` (read-only via ATTACH)\n\n            - HTTPS: `\"https://example.com/database.duckdb\"` (read-only via ATTACH)\n\n            - Any valid DuckDB connection string\n\n        config: Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})\n        extensions: List of DuckDB extensions to install and load on open.\n            Supports strings (community repo), mapping-like objects with\n            ``name``/``repository`` keys, or [metaxy.metadata_store.duckdb.ExtensionSpec][] instances.\n\n    ducklake: Optional DuckLake attachment configuration. Provide either a\n        mapping with 'metadata_backend' and 'storage_backend' entries or a\n        DuckLakeAttachmentConfig instance. When supplied, the DuckDB\n        connection is configured to ATTACH the DuckLake catalog after open().\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Warning:\n        Parent directories are NOT created automatically. Ensure paths exist\n        before initializing the store.\n    \"\"\"\n    database_str = str(database)\n\n    # Build connection params for Ibis DuckDB backend\n    # Ibis DuckDB backend accepts config params directly (not nested under 'config')\n    connection_params = {\"database\": database_str}\n    if config:\n        connection_params.update(config)\n\n    self.database = database_str\n    base_extensions: list[NormalisedExtension] = _normalise_extensions(\n        extensions or []\n    )\n\n    self._ducklake_config: DuckLakeAttachmentConfig | None = None\n    self._ducklake_attachment: DuckLakeAttachmentManager | None = None\n    if ducklake is not None:\n        attachment_config, manager = build_ducklake_attachment(ducklake)\n        ensure_extensions_with_plugins(base_extensions, attachment_config.plugins)\n        self._ducklake_config = attachment_config\n        self._ducklake_attachment = manager\n\n    self.extensions = base_extensions\n\n    # Auto-add hashfuncs extension if not present (needed for default XXHASH64)\n    extension_names: list[str] = []\n    for ext in self.extensions:\n        if isinstance(ext, str):\n            extension_names.append(ext)\n        elif isinstance(ext, ExtensionSpec):\n            extension_names.append(ext.name)\n        else:\n            # After _normalise_extensions, this should not happen\n            # But keep defensive check for type safety\n            raise TypeError(\n                f\"Extension must be str or ExtensionSpec after normalization; got {type(ext)}\"\n            )\n    if \"hashfuncs\" not in extension_names:\n        self.extensions.append(\"hashfuncs\")\n\n    # Initialize Ibis store with DuckDB backend\n    super().__init__(\n        backend=\"duckdb\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment","title":"ducklake_attachment  <code>property</code>","text":"<pre><code>ducklake_attachment: DuckLakeAttachmentManager\n</code></pre> <p>DuckLake attachment manager (raises if not configured).</p>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment_config","title":"ducklake_attachment_config  <code>property</code>","text":"<pre><code>ducklake_attachment_config: DuckLakeAttachmentConfig\n</code></pre> <p>DuckLake attachment configuration (raises if not configured).</p>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ibis_conn","title":"ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.open","title":"open","text":"<pre><code>open() -&gt; None\n</code></pre> <p>Open DuckDB connection and configure optional DuckLake attachment.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"Open DuckDB connection and configure optional DuckLake attachment.\"\"\"\n    super().open()\n    if self._ducklake_attachment is not None:\n        try:\n            duckdb_conn = self._duckdb_raw_connection()\n            self._ducklake_attachment.configure(duckdb_conn)\n        except Exception:\n            # Ensure connection is closed if DuckLake configuration fails\n            super().close()\n            raise\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.preview_ducklake_sql","title":"preview_ducklake_sql","text":"<pre><code>preview_ducklake_sql() -&gt; list[str]\n</code></pre> <p>Return DuckLake attachment SQL if configured.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def preview_ducklake_sql(self) -&gt; list[str]:\n    \"\"\"Return DuckLake attachment SQL if configured.\"\"\"\n    return self.ducklake_attachment.preview_sql()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the Ibis connection.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the Ibis connection.\"\"\"\n    if self._conn is not None:\n        # Ibis connections may not have explicit close method\n        # but setting to None releases resources\n        self._conn = None\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager.\"\"\"\n    # Track nesting depth\n    self._context_depth += 1\n\n    # Only open on first enter\n    if self._context_depth == 1:\n        # Warn if auto_create_tables is enabled (and store wants warnings)\n        if self.auto_create_tables and self._should_warn_auto_create_tables:\n            import warnings\n\n            warnings.warn(\n                f\"AUTO_CREATE_TABLES is enabled for {self.display()} - \"\n                \"do not use in production! \"\n                \"Use proper database migration tools like Alembic for production deployments.\",\n                UserWarning,\n                stacklevel=3,  # stacklevel=3 to point to user's 'with store:' line\n            )\n\n        self.open()\n        self._is_open = True\n\n        # Validate after opening (when all components are ready)\n        self._validate_after_open()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb) -&gt; None\n</code></pre> <p>Exit context manager.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Exit context manager.\"\"\"\n    # Decrement depth\n    self._context_depth -= 1\n\n    # Only close when fully exited\n    if self._context_depth == 0:\n        self._is_open = False\n        self.close()\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.validate_hash_algorithm","title":"validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Check if this store can support the algorithm\n    # Try native field provenance calculations first (if supported), then Polars\n    supported_algorithms = []\n\n    if self._supports_native_components():\n        try:\n            _, calculator, _ = self._create_native_components()\n            supported_algorithms = calculator.supported_algorithms\n        except Exception:\n            # If native field provenance calculations fail, fall back to Polars\n            pass\n\n    # If no native support or prefer_native=False, use Polars\n    if not supported_algorithms:\n        polars_calc = PolarsProvenanceByFieldCalculator()\n        supported_algorithms = polars_calc.supported_algorithms\n\n    if self.hash_algorithm not in supported_algorithms:\n        from metaxy.metadata_store.exceptions import (\n            HashAlgorithmNotSupportedError,\n        )\n\n        raise HashAlgorithmNotSupportedError(\n            f\"Hash algorithm {self.hash_algorithm} not supported by {self.__class__.__name__}. \"\n            f\"Supported: {supported_algorithms}\"\n        )\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.allow_cross_project_writes","title":"allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.write_metadata","title":"write_metadata","text":"<pre><code>write_metadata(feature: FeatureKey | type[BaseFeature], df: DataFrame[Any] | DataFrame) -&gt; None\n</code></pre> <p>Write metadata for a feature (immutable, append-only).</p> <p>Automatically adds 'feature_version' column from current code state, unless the DataFrame already contains one (useful for migrations).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>DataFrame[Any] | DataFrame</code>)           \u2013            <p>Narwhals DataFrame or Polars DataFrame containing metadata. Must have 'provenance_by_field' column of type Struct with fields matching feature's fields. May optionally contain 'feature_version' column (for migrations).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Always writes to current store, never to fallback stores.</li> <li>If df already contains 'feature_version' column, it will be used   as-is (no replacement). This allows migrations to write historical   versions. A warning is issued unless suppressed via context manager.</li> <li>Project validation is performed unless disabled via allow_cross_project_writes()</li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    df: nw.DataFrame[Any] | pl.DataFrame,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (immutable, append-only).\n\n    Automatically adds 'feature_version' column from current code state,\n    unless the DataFrame already contains one (useful for migrations).\n\n    Args:\n        feature: Feature to write metadata for\n        df: Narwhals DataFrame or Polars DataFrame containing metadata.\n            Must have 'provenance_by_field' column of type Struct with fields matching feature's fields.\n            May optionally contain 'feature_version' column (for migrations).\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Always writes to current store, never to fallback stores.\n        - If df already contains 'feature_version' column, it will be used\n          as-is (no replacement). This allows migrations to write historical\n          versions. A warning is issued unless suppressed via context manager.\n        - Project validation is performed unless disabled via allow_cross_project_writes()\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Narwhals to Polars if needed\n    if isinstance(df, nw.DataFrame):\n        df = df.to_polars()\n    # nw.DataFrame also matches as DataFrame in some contexts, ensure it's Polars\n    if not isinstance(df, pl.DataFrame):\n        # Must be some other type - shouldn't happen but handle defensively\n        if hasattr(df, \"to_polars\"):\n            df = df.to_polars()\n        elif hasattr(df, \"to_pandas\"):\n            df = pl.from_pandas(df.to_pandas())\n        else:\n            raise TypeError(f\"Cannot convert {type(df)} to Polars DataFrame\")\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df)\n        self._write_metadata_impl(feature_key, df)\n        return\n\n    # For regular features: add feature_version and snapshot_version, validate, and write\n    # Check if feature_version and snapshot_version already exist in DataFrame\n    if \"feature_version\" in df.columns and \"snapshot_version\" in df.columns:\n        # DataFrame already has feature_version and snapshot_version - use as-is\n        # This is intended for migrations writing historical versions\n        # Issue a warning unless we're in a suppression context\n        if not _suppress_feature_version_warning.get():\n            import warnings\n\n            warnings.warn(\n                f\"Writing metadata for {feature_key.to_string()} with existing \"\n                f\"feature_version and snapshot_version columns. This is intended for migrations only. \"\n                f\"Normal code should let write_metadata() add the current versions automatically.\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        # Get current feature version and snapshot_version from code and add them\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            current_feature_version = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            feature_cls = graph.features_by_key[feature_key]\n            current_feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n\n        # Get snapshot_version from active graph\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n        current_snapshot_version = graph.snapshot_version\n\n        df = df.with_columns(\n            [\n                pl.lit(current_feature_version).alias(\"feature_version\"),\n                pl.lit(current_snapshot_version).alias(\"snapshot_version\"),\n            ]\n        )\n\n    # Validate schema\n    self._validate_schema(df)\n\n    # Rename provenance_by_field -&gt; metaxy_provenance_by_field for database storage\n    # (Python code uses provenance_by_field, database uses metaxy_provenance_by_field)\n    df = df.rename({\"provenance_by_field\": \"metaxy_provenance_by_field\"})\n\n    # Write metadata\n    self._write_metadata_impl(feature_key, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.drop_feature_metadata","title":"drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: FeatureKey | type[BaseFeature]) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: FeatureKey | type[BaseFeature]) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.record_feature_graph_snapshot","title":"record_feature_graph_snapshot","text":"<pre><code>record_feature_graph_snapshot() -&gt; SnapshotPushResult\n</code></pre> <p>Record all features in graph with a graph snapshot version.</p> <p>This should be called during CD (Continuous Deployment) to record what feature versions are being deployed. Typically invoked via <code>metaxy graph push</code>.</p> <p>Records all features in the graph with the same snapshot_version, representing a consistent state of the entire feature graph based on code definitions.</p> <p>The snapshot_version is a deterministic hash of all feature_version hashes in the graph, making it idempotent - calling multiple times with the same feature definitions produces the same snapshot_version.</p> <p>This method detects three scenarios: 1. New snapshot (computational changes): No existing rows with this snapshot_version 2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version 3. No changes: Snapshot exists with identical feature_spec_versions for all features</p> <p>Returns: SnapshotPushResult</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def record_feature_graph_snapshot(self) -&gt; SnapshotPushResult:\n    \"\"\"Record all features in graph with a graph snapshot version.\n\n    This should be called during CD (Continuous Deployment) to record what\n    feature versions are being deployed. Typically invoked via `metaxy graph push`.\n\n    Records all features in the graph with the same snapshot_version, representing\n    a consistent state of the entire feature graph based on code definitions.\n\n    The snapshot_version is a deterministic hash of all feature_version hashes\n    in the graph, making it idempotent - calling multiple times with the\n    same feature definitions produces the same snapshot_version.\n\n    This method detects three scenarios:\n    1. New snapshot (computational changes): No existing rows with this snapshot_version\n    2. Metadata-only changes: Snapshot exists but some features have different feature_spec_version\n    3. No changes: Snapshot exists with identical feature_spec_versions for all features\n\n    Returns: SnapshotPushResult\n    \"\"\"\n\n    from metaxy.models.feature import FeatureGraph\n\n    graph = FeatureGraph.get_active()\n\n    # Use to_snapshot() to get the snapshot dict\n    snapshot_dict = graph.to_snapshot()\n\n    # Generate deterministic snapshot_version from graph\n    snapshot_version = graph.snapshot_version\n\n    # Read existing feature versions once\n    try:\n        existing_versions_lazy = self.read_metadata_in_store(FEATURE_VERSIONS_KEY)\n        # Materialize to Polars for iteration\n        existing_versions = (\n            existing_versions_lazy.collect().to_polars()\n            if existing_versions_lazy is not None\n            else None\n        )\n    except Exception:\n        # Table doesn't exist yet\n        existing_versions = None\n\n    # Get project from any feature in the graph (all should have the same project)\n    # Default to empty string if no features in graph\n    if graph.features_by_key:\n        # Get first feature's project\n        first_feature = next(iter(graph.features_by_key.values()))\n        project_name = first_feature.project  # type: ignore[attr-defined]\n    else:\n        project_name = \"\"\n\n    # Check if this exact snapshot already exists for this project\n    snapshot_already_exists = False\n    existing_spec_versions: dict[str, str] = {}\n\n    if existing_versions is not None:\n        # Check if project column exists (it may not in old tables)\n        if \"project\" in existing_versions.columns:\n            snapshot_rows = existing_versions.filter(\n                (pl.col(\"snapshot_version\") == snapshot_version)\n                &amp; (pl.col(\"project\") == project_name)\n            )\n        else:\n            # Old table without project column - just check snapshot_version\n            snapshot_rows = existing_versions.filter(\n                pl.col(\"snapshot_version\") == snapshot_version\n            )\n        snapshot_already_exists = snapshot_rows.height &gt; 0\n\n        if snapshot_already_exists:\n            # Check if feature_spec_version column exists (backward compatibility)\n            # Old records (before issue #77) won't have this column\n            has_spec_version = \"feature_spec_version\" in snapshot_rows.columns\n\n            if has_spec_version:\n                # Build dict of existing feature_key -&gt; feature_spec_version\n                for row in snapshot_rows.iter_rows(named=True):\n                    existing_spec_versions[row[\"feature_key\"]] = row[\n                        \"feature_spec_version\"\n                    ]\n            # If no spec_version column, existing_spec_versions remains empty\n            # This means we'll treat it as \"no metadata changes\" (conservative approach)\n\n    # Scenario 1: New snapshot (no existing rows)\n    if not snapshot_already_exists:\n        # Build records from snapshot_dict\n        records = []\n        for feature_key_str in sorted(snapshot_dict.keys()):\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            # Always record all features for this snapshot (don't skip based on feature_version alone)\n            # Each snapshot must be complete to support migration detection\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write all new records at once\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=False,\n            metadata_changed=False,\n            features_with_spec_changes=[],\n        )\n\n    # Scenario 2 &amp; 3: Snapshot exists - check for metadata changes\n    features_with_spec_changes = []\n\n    for feature_key_str, feature_data in snapshot_dict.items():\n        current_spec_version = feature_data[\"feature_spec_version\"]\n        existing_spec_version = existing_spec_versions.get(feature_key_str)\n\n        if existing_spec_version != current_spec_version:\n            features_with_spec_changes.append(feature_key_str)\n\n    # If metadata changed, append new rows for affected features\n    if features_with_spec_changes:\n        records = []\n        for feature_key_str in features_with_spec_changes:\n            feature_data = snapshot_dict[feature_key_str]\n\n            # Serialize complete BaseFeatureSpec\n            feature_spec_json = json.dumps(feature_data[\"feature_spec\"])\n\n            records.append(\n                {\n                    \"project\": project_name,\n                    \"feature_key\": feature_key_str,\n                    \"feature_version\": feature_data[\"feature_version\"],\n                    \"feature_spec_version\": feature_data[\"feature_spec_version\"],\n                    \"feature_tracking_version\": feature_data[\n                        \"feature_tracking_version\"\n                    ],\n                    \"recorded_at\": datetime.now(timezone.utc),\n                    \"feature_spec\": feature_spec_json,\n                    \"feature_class_path\": feature_data[\"feature_class_path\"],\n                    \"snapshot_version\": snapshot_version,\n                }\n            )\n\n        # Bulk write updated records (append-only)\n        if records:\n            version_records = pl.DataFrame(\n                records,\n                schema=FEATURE_VERSIONS_SCHEMA,\n            )\n            self._write_metadata_impl(FEATURE_VERSIONS_KEY, version_records)\n\n        return SnapshotPushResult(\n            snapshot_version=snapshot_version,\n            already_recorded=True,\n            metadata_changed=True,\n            features_with_spec_changes=features_with_spec_changes,\n        )\n\n    # Scenario 3: No changes at all\n    return SnapshotPushResult(\n        snapshot_version=snapshot_version,\n        already_recorded=True,\n        metadata_changed=False,\n        features_with_spec_changes=[],\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_metadata_in_store","title":"read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = feature_key.table_name\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(nw.col(\"feature_version\") == feature_version)\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_metadata","title":"read_metadata","text":"<pre><code>read_metadata(feature: FeatureKey | type[BaseFeature], *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return rows with current feature_version (default: True for safety)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: [nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]\n        columns: Subset of columns to return\n        allow_fallback: If True, check fallback stores on local miss\n        current_only: If True, only return rows with current feature_version\n            (default: True for safety)\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        ValueError: If both feature_version and current_only=True are provided\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Determine which feature_version to use\n    feature_version_filter = feature_version\n    if current_only and not is_system_table:\n        # Get current feature_version\n        if isinstance(feature, type) and issubclass(feature, BaseFeature):\n            feature_version_filter = feature.feature_version()  # type: ignore[attr-defined]\n        else:\n            from metaxy.models.feature import FeatureGraph\n\n            graph = FeatureGraph.get_active()\n            # Only try to get from graph if feature_key exists in graph\n            # This allows reading system tables or external features not in current graph\n            if feature_key in graph.features_by_key:\n                feature_cls = graph.features_by_key[feature_key]\n                feature_version_filter = feature_cls.feature_version()  # type: ignore[attr-defined]\n            else:\n                # Feature not in graph - skip feature_version filtering\n                feature_version_filter = None\n\n    # Map column names: provenance_by_field -&gt; metaxy_provenance_by_field for DB query\n    db_columns = None\n    if columns is not None:\n        db_columns = [\n            \"metaxy_provenance_by_field\" if col == \"provenance_by_field\" else col\n            for col in columns\n        ]\n\n    # Try local first with filters\n    lazy_frame = self.read_metadata_in_store(\n        feature,\n        feature_version=feature_version_filter,\n        filters=filters,  # Pass filters directly\n        columns=db_columns,  # Use mapped column names\n    )\n\n    if lazy_frame is not None:\n        # Rename metaxy_provenance_by_field -&gt; provenance_by_field for Python code\n        # (Database uses metaxy_provenance_by_field, Python code uses provenance_by_field)\n        if \"metaxy_provenance_by_field\" in lazy_frame.collect_schema().names():\n            lazy_frame = lazy_frame.rename(\n                {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n            )\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,  # Pass through filters directly\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,  # Pass through current_only\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.has_feature","title":"has_feature","text":"<pre><code>has_feature(feature: FeatureKey | type[BaseFeature], *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    # Check local\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if check_fallback:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.list_features","title":"list_features","text":"<pre><code>list_features(*, include_fallback: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>List all features in store.</p> <p>Parameters:</p> <ul> <li> <code>include_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include features from fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of FeatureKey objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def list_features(self, *, include_fallback: bool = False) -&gt; list[FeatureKey]:\n    \"\"\"\n    List all features in store.\n\n    Args:\n        include_fallback: If True, include features from fallback stores\n\n    Returns:\n        List of FeatureKey objects\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    features = self._list_features_local()\n\n    if include_fallback:\n        for store in self.fallback_stores:\n            features.extend(store.list_features(include_fallback=True))\n\n    # Deduplicate\n    seen = set()\n    unique_features = []\n    for feature in features:\n        key_str = feature.to_string()\n        if key_str not in seen:\n            seen.add(key_str)\n            unique_features.append(feature)\n\n    return unique_features\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.display","title":"display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    backend_info = self.connection_string or f\"{self.backend}\"\n    if self._is_open:\n        num_features = len(self._list_features_local())\n        return f\"IbisMetadataStore(backend={backend_info}, features={num_features})\"\n    else:\n        return f\"IbisMetadataStore(backend={backend_info})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_graph_snapshots","title":"read_graph_snapshots","text":"<pre><code>read_graph_snapshots(project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read recorded graph snapshots from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, returns snapshots from all projects.</p> </li> </ul> <p>Returns a DataFrame with columns: - snapshot_version: Unique identifier for each graph snapshot - recorded_at: Timestamp when the snapshot was recorded - feature_count: Number of features in this snapshot</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with snapshot information, sorted by recorded_at descending</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Example <pre><code>with store:\n    # Get snapshots for a specific project\n    snapshots = store.read_graph_snapshots(project=\"my_project\")\n    latest_snapshot = snapshots[\"snapshot_version\"][0]\n    print(f\"Latest snapshot: {latest_snapshot}\")\n\n    # Get snapshots across all projects\n    all_snapshots = store.read_graph_snapshots()\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_graph_snapshots(self, project: str | None = None) -&gt; pl.DataFrame:\n    \"\"\"Read recorded graph snapshots from the feature_versions system table.\n\n    Args:\n        project: Project name to filter by. If None, returns snapshots from all projects.\n\n    Returns a DataFrame with columns:\n    - snapshot_version: Unique identifier for each graph snapshot\n    - recorded_at: Timestamp when the snapshot was recorded\n    - feature_count: Number of features in this snapshot\n\n    Returns:\n        Polars DataFrame with snapshot information, sorted by recorded_at descending\n\n    Raises:\n        StoreNotOpenError: If store is not open\n\n    Example:\n        ```py\n        with store:\n            # Get snapshots for a specific project\n            snapshots = store.read_graph_snapshots(project=\"my_project\")\n            latest_snapshot = snapshots[\"snapshot_version\"][0]\n            print(f\"Latest snapshot: {latest_snapshot}\")\n\n            # Get snapshots across all projects\n            all_snapshots = store.read_graph_snapshots()\n        ```\n    \"\"\"\n    self._check_open()\n\n    # Build filters based on project parameter\n    filters = None\n    if project is not None:\n        import narwhals as nw\n\n        filters = [nw.col(\"project\") == project]\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No snapshots recorded yet\n        return pl.DataFrame(\n            schema={\n                \"snapshot_version\": pl.String,\n                \"recorded_at\": pl.Datetime(\"us\"),\n                \"feature_count\": pl.UInt32,\n            }\n        )\n\n    versions_df = versions_lazy.collect().to_polars()\n\n    # Group by snapshot_version and get earliest recorded_at and count\n    snapshots = (\n        versions_df.group_by(\"snapshot_version\")\n        .agg(\n            [\n                pl.col(\"recorded_at\").min().alias(\"recorded_at\"),\n                pl.col(\"feature_key\").count().alias(\"feature_count\"),\n            ]\n        )\n        .sort(\"recorded_at\", descending=True)\n    )\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_features","title":"read_features","text":"<pre><code>read_features(*, current: bool = True, snapshot_version: str | None = None, project: str | None = None) -&gt; DataFrame\n</code></pre> <p>Read feature version information from the feature_versions system table.</p> <p>Parameters:</p> <ul> <li> <code>current</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only return features from the current code snapshot.      If False, must provide snapshot_version.</p> </li> <li> <code>snapshot_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific snapshot version to filter by. Required if current=False.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:</p> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_key: Feature identifier</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_version: Version hash of the feature</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>recorded_at: When this version was recorded</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_spec: JSON serialized feature specification</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>feature_class_path: Python import path to the feature class</li> </ul> </li> <li> <code>DataFrame</code>           \u2013            <ul> <li>snapshot_version: Graph snapshot this feature belongs to</li> </ul> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If current=False but no snapshot_version provided</p> </li> </ul> <p>Examples:</p> <pre><code># Get features from current code\nwith store:\n    features = store.read_features(current=True)\n    print(f\"Current graph has {len(features)} features\")\n</code></pre> <pre><code># Get features from a specific snapshot\nwith store:\n    features = store.read_features(current=False, snapshot_version=\"abc123\")\n    for row in features.iter_rows(named=True):\n        print(f\"{row['feature_key']}: {row['feature_version']}\")\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_features(\n    self,\n    *,\n    current: bool = True,\n    snapshot_version: str | None = None,\n    project: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Read feature version information from the feature_versions system table.\n\n    Args:\n        current: If True, only return features from the current code snapshot.\n                 If False, must provide snapshot_version.\n        snapshot_version: Specific snapshot version to filter by. Required if current=False.\n        project: Project name to filter by. Defaults to None.\n\n    Returns:\n        Polars DataFrame with columns from FEATURE_VERSIONS_SCHEMA:\n        - feature_key: Feature identifier\n        - feature_version: Version hash of the feature\n        - recorded_at: When this version was recorded\n        - feature_spec: JSON serialized feature specification\n        - feature_class_path: Python import path to the feature class\n        - snapshot_version: Graph snapshot this feature belongs to\n\n    Raises:\n        StoreNotOpenError: If store is not open\n        ValueError: If current=False but no snapshot_version provided\n\n    Examples:\n        ```py\n        # Get features from current code\n        with store:\n            features = store.read_features(current=True)\n            print(f\"Current graph has {len(features)} features\")\n        ```\n\n        ```py\n        # Get features from a specific snapshot\n        with store:\n            features = store.read_features(current=False, snapshot_version=\"abc123\")\n            for row in features.iter_rows(named=True):\n                print(f\"{row['feature_key']}: {row['feature_version']}\")\n        ```\n    \"\"\"\n    self._check_open()\n\n    if not current and snapshot_version is None:\n        raise ValueError(\"Must provide snapshot_version when current=False\")\n\n    if current:\n        # Get current snapshot from active graph\n        graph = FeatureGraph.get_active()\n        snapshot_version = graph.snapshot_version\n\n    filters = [nw.col(\"snapshot_version\") == snapshot_version]\n    if project is not None:\n        filters.append(nw.col(\"project\") == project)\n\n    versions_lazy = self.read_metadata_in_store(\n        FEATURE_VERSIONS_KEY, filters=filters\n    )\n    if versions_lazy is None:\n        # No features recorded yet\n        return pl.DataFrame(schema=FEATURE_VERSIONS_SCHEMA)\n\n    # Filter by snapshot_version\n    versions_df = versions_lazy.collect().to_polars()\n\n    return versions_df\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.copy_metadata","title":"copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[FeatureKey | type[BaseFeature]] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[FeatureKey | type[BaseFeature]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[FeatureKey | type[BaseFeature]] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\"Destination store must be opened (use context manager)\")\n\n    # Automatically handle source store context manager\n    should_close_source = not from_store._is_open\n    if should_close_source:\n        from_store.__enter__()\n\n    try:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n    finally:\n        if should_close_source:\n            from_store.__exit__(None, None, None)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.read_upstream_metadata","title":"read_upstream_metadata","text":"<pre><code>read_upstream_metadata(feature: FeatureKey | type[BaseFeature], field: FieldKey | None = None, *, filters: Mapping[str, Sequence[Expr]] | None = None, allow_fallback: bool = True, current_only: bool = True) -&gt; dict[str, LazyFrame[Any]]\n</code></pre> <p>Read all upstream dependencies for a feature/field.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>FeatureKey | type[BaseFeature]</code>)           \u2013            <p>Feature whose dependencies to load</p> </li> <li> <code>field</code>               (<code>FieldKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Specific field (if None, loads all deps for feature)</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only read current feature_version for upstream</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.</p> </li> <li> <code>dict[str, LazyFrame[Any]]</code>           \u2013            <p>Each LazyFrame has a 'provenance_by_field' column (Struct).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>DependencyError</code>             \u2013            <p>If required upstream feature is missing</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_upstream_metadata(\n    self,\n    feature: FeatureKey | type[BaseFeature],\n    field: FieldKey | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n) -&gt; dict[str, nw.LazyFrame[Any]]:\n    \"\"\"\n    Read all upstream dependencies for a feature/field.\n\n    Args:\n        feature: Feature whose dependencies to load\n        field: Specific field (if None, loads all deps for feature)\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Example: {\"upstream/feature1\": [nw.col(\"x\") &gt; 10], \"upstream/feature2\": [...]}\n        allow_fallback: Whether to check fallback stores\n        current_only: If True, only read current feature_version for upstream\n\n    Returns:\n        Dict mapping upstream feature keys (as strings) to Narwhals LazyFrames.\n        Each LazyFrame has a 'provenance_by_field' column (Struct).\n\n    Raises:\n        DependencyError: If required upstream feature is missing\n    \"\"\"\n    plan = self._resolve_feature_plan(feature)\n\n    # Get all upstream features we need\n    upstream_features = set()\n\n    if field is None:\n        # All fields' dependencies\n        for cont in plan.feature.fields:\n            upstream_features.update(self._get_field_dependencies(plan, cont.key))\n    else:\n        # Specific field's dependencies\n        upstream_features.update(self._get_field_dependencies(plan, field))\n\n    # Load metadata for each upstream feature\n    # Use the feature's graph to look up upstream feature classes\n    if isinstance(feature, FeatureKey):\n        from metaxy.models.feature import FeatureGraph\n\n        graph = FeatureGraph.get_active()\n    else:\n        graph = feature.graph\n\n    upstream_metadata = {}\n    for upstream_fq_key in upstream_features:\n        upstream_feature_key = upstream_fq_key.feature\n\n        # Extract filters for this specific upstream feature\n        upstream_filters = None\n        if filters:\n            upstream_key_str = upstream_feature_key.to_string()\n            if upstream_key_str in filters:\n                upstream_filters = filters[upstream_key_str]\n\n        try:\n            # Look up the Feature class from the graph and pass it to read_metadata\n            # This way we use the bound graph instead of relying on active context\n            upstream_feature_cls = graph.features_by_key[upstream_feature_key]\n            lazy_frame = self.read_metadata(\n                upstream_feature_cls,\n                filters=upstream_filters,  # Pass extracted filters (Sequence or None)\n                allow_fallback=allow_fallback,\n                current_only=current_only,  # Pass through current_only\n            )\n            # Use string key for dict\n            upstream_metadata[upstream_feature_key.to_string()] = lazy_frame\n        except FeatureNotFoundError as e:\n            raise DependencyError(\n                f\"Missing upstream feature {upstream_feature_key.to_string()} \"\n                f\"required by {plan.feature.key.to_string()}\"\n            ) from e\n\n    return upstream_metadata\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.resolve_update","title":"resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[False] = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: Literal[True], **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: DataFrame[Any] | LazyFrame[Any] | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, lazy: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>DataFrame[Any] | LazyFrame[Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pre-computed DataFrame with ID columns and <code>\"provenance_by_field\"</code> column. When provided, <code>MetadataStore</code> skips upstream loading, joining, and field provenance calculation.</p> <p>Required for root features (features with no upstream dependencies). Root features don't have upstream to calculate <code>\"provenance_by_field\"</code> from, so users must provide samples with manually computed <code>\"provenance_by_field\"</code> column.</p> <p>For non-root features, use this when you want to bypass the automatic upstream loading and field provenance calculation.</p> <p>Examples:</p> <ul> <li> <p>Loading upstream from custom sources</p> </li> <li> <p>Pre-computing field provenances with custom logic</p> </li> <li> <p>Testing specific scenarios</p> </li> </ul> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to lists of Narwhals filter expressions. Applied when reading upstream metadata to filter samples at the source. Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return metaxy.data_versioning.diff.LazyIncrement with lazy Narwhals LazyFrames. If <code>False</code>, return metaxy.data_versioning.diff.Increment with eager Narwhals DataFrames.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> DataFrame has been provided when resolving an update for a root feature.</p> </li> </ul> <p>Examples:</p> <pre><code># Root feature - samples required\nsamples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> <pre><code># Non-root feature - automatic (normal usage)\nresult = store.resolve_update(DownstreamFeature)\n</code></pre> <pre><code># Non-root feature - with escape hatch (advanced)\ncustom_samples = compute_custom_field_provenance(...)\nresult = store.resolve_update(DownstreamFeature, samples=custom_samples)\n</code></pre> Note <p>Users can then process only added/changed and call write_metadata().</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    lazy: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: Pre-computed DataFrame with ID columns\n            and `\"provenance_by_field\"` column. When provided, `MetadataStore` skips upstream loading, joining,\n            and field provenance calculation.\n\n            **Required for root features** (features with no upstream dependencies).\n            Root features don't have upstream to calculate `\"provenance_by_field\"` from, so users\n            must provide samples with manually computed `\"provenance_by_field\"` column.\n\n            For non-root features, use this when you\n            want to bypass the automatic upstream loading and field provenance calculation.\n\n            Examples:\n\n            - Loading upstream from custom sources\n\n            - Pre-computing field provenances with custom logic\n\n            - Testing specific scenarios\n\n            Setting this parameter during normal operations is not required.\n\n        filters: Dict mapping feature keys (as strings) to lists of Narwhals filter expressions.\n            Applied when reading upstream metadata to filter samples at the source.\n            Example: {\"upstream/feature\": [nw.col(\"x\") &gt; 10], ...}\n        lazy: If `True`, return [metaxy.data_versioning.diff.LazyIncrement][] with lazy Narwhals LazyFrames.\n            If `False`, return [metaxy.data_versioning.diff.Increment][] with eager Narwhals DataFrames.\n        **kwargs: Backend-specific parameters\n\n    Raises:\n        ValueError: If no `samples` DataFrame has been provided when resolving an update for a root feature.\n\n    Examples:\n        ```py\n        # Root feature - samples required\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n\n        ```py\n        # Non-root feature - automatic (normal usage)\n        result = store.resolve_update(DownstreamFeature)\n        ```\n\n        ```py\n        # Non-root feature - with escape hatch (advanced)\n        custom_samples = compute_custom_field_provenance(...)\n        result = store.resolve_update(DownstreamFeature, samples=custom_samples)\n        ```\n\n    Note:\n        Users can then process only added/changed and call write_metadata().\n    \"\"\"\n    import narwhals as nw\n\n    plan = feature.graph.get_feature_plan(feature.spec().key)\n\n    # Escape hatch: if samples provided, use them directly (skip join/calculation)\n    if samples is not None:\n        import logging\n\n        import polars as pl\n\n        logger = logging.getLogger(__name__)\n\n        # Convert samples to lazy if needed\n        if isinstance(samples, nw.LazyFrame):\n            samples_lazy = samples\n        elif isinstance(samples, nw.DataFrame):\n            samples_lazy = samples.lazy()\n        else:\n            samples_lazy = nw.from_native(samples).lazy()\n\n        # Check if samples are Polars-backed (common case for escape hatch)\n        samples_native = samples_lazy.to_native()\n        is_polars_samples = isinstance(samples_native, (pl.DataFrame, pl.LazyFrame))\n\n        if is_polars_samples and self._supports_native_components():\n            # User provided Polars samples but store uses native (SQL) backend\n            # Need to materialize current metadata to Polars for compatibility\n            logger.warning(\n                f\"Feature {feature.spec().key}: samples parameter is Polars-backed but store uses native SQL backend. \"\n                f\"Materializing current metadata to Polars for diff comparison. \"\n                f\"For better performance, consider using samples with backend matching the store's backend.\"\n            )\n            # Get current metadata and materialize to Polars\n            current_lazy_native = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            if current_lazy_native is not None:\n                # Rename metaxy_provenance_by_field -&gt; provenance_by_field before converting\n                if (\n                    \"metaxy_provenance_by_field\"\n                    in current_lazy_native.collect_schema().names()\n                ):\n                    current_lazy_native = current_lazy_native.rename(\n                        {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                    )\n                # Convert to Polars using Narwhals' built-in method\n                current_lazy = nw.from_native(\n                    current_lazy_native.collect().to_polars().lazy()\n                )\n            else:\n                current_lazy = None\n        else:\n            # Same backend or no conversion needed - direct read\n            current_lazy = self.read_metadata_in_store(\n                feature, feature_version=feature.feature_version()\n            )\n            # Rename metaxy_provenance_by_field -&gt; provenance_by_field\n            if (\n                current_lazy is not None\n                and \"metaxy_provenance_by_field\"\n                in current_lazy.collect_schema().names()\n            ):\n                current_lazy = current_lazy.rename(\n                    {\"metaxy_provenance_by_field\": \"provenance_by_field\"}\n                )\n\n        # Use diff resolver to compare samples with current\n        from metaxy.data_versioning.diff.narwhals import NarwhalsDiffResolver\n\n        diff_resolver = NarwhalsDiffResolver()\n\n        lazy_result = diff_resolver.find_changes(\n            target_provenance=samples_lazy,\n            current_metadata=current_lazy,\n            id_columns=feature.spec().id_columns,  # Get ID columns from feature spec\n        )\n\n        return lazy_result if lazy else lazy_result.collect()\n\n    # Root features without samples: error (samples required)\n    if not plan.deps:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and provenance_by_field columns. \"\n            f\"Root features require manual provenance_by_field computation.\"\n        )\n\n    # Non-root features without samples: automatic upstream loading\n    # Check where upstream data lives\n    upstream_location = self._check_upstream_location(feature)\n\n    if upstream_location == \"all_local\":\n        # All upstream in this store - use native field provenance calculations\n        return self._resolve_update_native(feature, filters=filters, lazy=lazy)\n    else:\n        # Some upstream in fallback stores - use Polars components\n        return self._resolve_update_polars(feature, filters=filters, lazy=lazy)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.ExtensionSpec","title":"ExtensionSpec  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB extension specification accepted by DuckDBMetadataStore.</p> <p>Supports additional keys for forward compatibility.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\\n\\nSupports additional keys for forward compatibility.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"repository\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Repository\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"ExtensionSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>repository</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store.duckdb.DuckLakeConfigInput","title":"DuckLakeConfigInput  <code>module-attribute</code>","text":"<pre><code>DuckLakeConfigInput = DuckLakeAttachmentConfig | Mapping[str, Any]\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig","title":"DuckLakeAttachmentConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration payload used to attach DuckLake to a DuckDB connection.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"Configuration payload used to attach DuckLake to a DuckDB connection.\",\n  \"properties\": {\n    \"metadata_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Metadata Backend\",\n      \"type\": \"object\"\n    },\n    \"storage_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Storage Backend\",\n      \"type\": \"object\"\n    },\n    \"alias\": {\n      \"default\": \"ducklake\",\n      \"title\": \"Alias\",\n      \"type\": \"string\"\n    },\n    \"plugins\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Plugins\",\n      \"type\": \"array\"\n    },\n    \"attach_options\": {\n      \"additionalProperties\": true,\n      \"title\": \"Attach Options\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"metadata_backend\",\n    \"storage_backend\"\n  ],\n  \"title\": \"DuckLakeAttachmentConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metadata_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>storage_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>alias</code>                 (<code>str</code>)             </li> <li> <code>plugins</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>attach_options</code>                 (<code>dict[str, Any]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_coerce_backends</code>                 \u2192                   <code>metadata_backend</code>,                   <code>storage_backend</code> </li> <li> <code>_coerce_alias</code>                 \u2192                   <code>alias</code> </li> <li> <code>_coerce_plugins</code>                 \u2192                   <code>plugins</code> </li> <li> <code>_coerce_attach_options</code>                 \u2192                   <code>attach_options</code> </li> </ul>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.metadata_sql_parts","title":"metadata_sql_parts","text":"<pre><code>metadata_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed metadata SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef metadata_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed metadata SQL components for DuckLake attachments.\"\"\"\n    return resolve_metadata_backend(self.metadata_backend, self.alias)\n</code></pre>"},{"location":"reference/api/metadata-stores/ibis/duckdb/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.storage_sql_parts","title":"storage_sql_parts","text":"<pre><code>storage_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed storage SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef storage_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed storage SQL components for DuckLake attachments.\"\"\"\n    return resolve_storage_backend(self.storage_backend, self.alias)\n</code></pre>"}]}