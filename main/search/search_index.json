{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"Metaxy"},{"location":"#metaxy","title":"Metaxy","text":"<p>Metaxy is a pluggable metadata layer for building multimodal Data and ML pipelines. Metaxy manages and tracks metadata across complex computational graphs and provides sample and sub-sample versioning, allowing the codebase to evolve over time without friction.</p>"},{"location":"#the-problem-sample-level-versioning","title":"The problem: sample-level versioning","text":"<p>Info</p> <p>Data, ML and AI workloads processing large amounts of images, videos, audios, or texts (1) can be very expensive to run. In contrast to traditional data engineering, re-running the whole pipeline on changes is no longer an option. Therefore, it becomes crucially important to correctly implement incremental processing, sample-level versioning and prunable updates.</p> <ol> <li>or really any kind of data</li> </ol> <p>These workloads evolve all the time, with new data being shipped, bugfixes or algorithm changes introduced, and new features added to the pipeline. This means the pipeline has to be re-computed frequently, but at the same time it's important to avoid unnecessary recomputations for individual data samples.</p> <p>Info</p> <p>Unnecessary recomputations can waste dozens of thousands of dollars on compute, and battling sample-level orchestration complexity can cost even more in engineering efforts.</p> Data vs Metadata Clarifications <p>Metaxy features represent tabular metadata, typically containing references to external multimodal data such as files, images, or videos.</p> Subject Description Data The actual multimodal data itself, such as images, audio files, video files, text documents, and other raw content that your pipelines process and transform. Metadata Information about the data, typically including references to where data is stored (e.g., object store keys), plus additional descriptive entries such as video length, file size, format, version, and other attributes. <p>Metaxy does not interact with data and is not responsible for its content. As an edge case, Metaxy may also manage pure metadata tables that do not reference any external data.</p> <p>In contrast to what one might expect, spinning up a thousand compute nodes is a much easier task with established solutions, while sample-level versioning remains a challenging problem (1).</p> <ol> <li>it is hard to overestimate the amount of pain @danielgafni has endured before building Metaxy</li> </ol>"},{"location":"#the-solution","title":"The solution","text":"<p>Until recently, a general solution for this problem did not exist, but not anymore  !</p> <p>Metaxy allows creating and updating feature definitions which can independently version different fields of the same data sample and express granular field-level lineage.</p> <p>Just Use Metaxy</p> <p>Metaxy has quite a few superpowers:</p> <ul> <li>Cache every single sample in the data pipeline. Millions of cache keys can be calculated in under a second (1). Benefit from prunable partial updates.</li> <li>Freedom from storage lock-in. Swap storage backends in development and production environments without breaking a sweat (2).</li> <li>Metaxy is pluggable, declarative, composable and extensible (3): use it to build custom integrations and workflows, benefit from emergent capabilities that enable tooling, visualizations and optimizations you didn't even plan for.</li> </ul> <ol> <li>Our experience at Anam with ClickHouse</li> <li>For example, develop against DeltaLake and scale production with ClickHouse without code changes.</li> <li>See our official integrations here</li> </ol> <p>And now the killer feature:</p> <p>Super Granular Data Versioning</p> <p>The feature that makes Metaxy really stand out is the ability to identify prunable partial data updates (1) and skip unnecessary downstream computations. At the moment of writing, Metaxy is the only available tool that tackles these problems.</p> <ol> <li>which are very common in multimodal pipelines</li> </ol> <p>Read The Pitch to be impressed even more.</p> <p>All of this is possible thanks to (1) Narwhals, Ibis, and a few clever tricks.</p> <ol> <li>we really do stand on the shoulders of giants</li> </ol>"},{"location":"#reliability","title":"Reliability","text":"<p>Metaxy was designed to handle large amounts of big metadata in distributed environments, makes very few assumptions about usage patterns and is non-invasive to the rest of the data pipeline.</p> <p>Metaxy is fanatically tested across all supported metadata stores, Python versions and platforms <sup>1</sup>. We guarantee versioning consistency across the supported metadata stores.</p> <p>We have been dogfooding Metaxy at Anam since December 2025. We are running it in production with ClickHouse, Dagster, and Ray (1), and it's powering all our pipelines that prepare training data for our video generation models.</p> <ol> <li>and integrations with these tools are probably the most complete at the moment</li> </ol> <p>That being said, Metaxy is still an early project, so while the core functionality is rock solid, some rough edges with other parts of Metaxy are expected.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install Metaxy from PyPI:</p> <pre><code>uv add metaxy\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Tip</p> <p>Itching to get your hands dirty? Head to Quickstart.</p>"},{"location":"#whats-next","title":"What's Next?","text":"<p>Here are a few more useful links:</p> <ul> <li>Read the Metaxy Pitch</li> <li>Learn about Metaxy design choices</li> <li>Learn more about Metaxy concepts</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Invoke <code>mx</code> CLI from your terminal</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul>"},{"location":"#blog-posts","title":"Blog Posts","text":"<ul> <li>Announcement post by Anam</li> <li>Dagster + Metaxy by Dagster labs</li> </ul> <ol> <li> <p>The CLI is not tested on Windows yet.\u00a0\u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable user-facing Metaxy changes are to be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning, except for experimental features.</p> <p>While Metaxy's core functionality and versioning engine are stable and quite complete, the CLI and some of the more advanced APIs and integrations are considered experimental.</p> <p>Abstract</p> <p>stable features are guaranteed to follow SemVer and won't receive breaking changes in between minor releases. Such changes will be announced with a deprecation warning. A feature is considered stable if it's documented and doesn't have an <code>\"Experimental\"</code> badge.</p> <p>experimental features may be changed or removed at any time without deprecation warnings. They may be documented and in this case must display an <code>\"Experimental\"</code> warning badge:</p> <p>Experimental</p> <p>This functionality is experimental.</p>"},{"location":"changelog/#010","title":"0.1.0","text":"<p>The first public Metaxy release !</p> <p>This release should be considered an alpha release: it is ready for production use and has been dogfooded internally at Anam.ai, but we don't have any community feedback yet.</p>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li> <p>Feature Definitions and related models</p> </li> <li> <p>Metaxy versioning engine and storage layout</p> </li> <li> <p>MetadataStore API</p> </li> <li> <p>Integrations: DeltaLake, Dagster, Ray, ClickHouse, MCP, Claude</p> </li> </ul>"},{"location":"changelog/#experimental","title":"Experimental","text":"<ul> <li> <p>CLI</p> </li> <li> <p><code>metaxy.lock</code>-based workflow for multi-environment setups</p> </li> <li> <p>Integrations: DuckDB, BigQuery, LanceDB, SQLAlchemy, SQLModel</p> </li> </ul>"},{"location":"examples/","title":"Metaxy Examples","text":"<p>Here you can find complete example Metaxy projects. Each example is a self-contained Python project that demonstrates a specific feature or use case of Metaxy.</p> <p>Verified</p> <p>Examples are continuously tested in CI and are guaranteed to work.</p> <ul> <li>Aggregation</li> <li>Basic Example</li> <li>DuckLake</li> <li>Expansion</li> </ul>"},{"location":"examples/aggregation/","title":"Aggregation","text":""},{"location":"examples/aggregation/#overview","title":"Overview","text":"<p> View Source on GitHub</p> <p>This example demonstrates how to implement aggregation (<code>N:1</code>) relationships with Metaxy. In such relationships multiple parent samples produce a single child sample.</p> <p>These relationships can be modeled with LineageRelationship.aggregation lineage type.</p> <p>We will use a speaker embedding pipeline as an example, where multiple audio recordings from the same speaker are aggregated to compute a single speaker embedding.</p>"},{"location":"examples/aggregation/#the-pipeline","title":"The Pipeline","text":"<p>Let's define a pipeline with two features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart LR\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    audio[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;audio&lt;/b&gt;&lt;br/&gt;3dac67c8&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- default (80200592)&lt;/div&gt;\"]\n    speaker_embedding[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;speaker/embedding&lt;/b&gt;&lt;br/&gt;a485fa5b&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- embedding (27a15391)&lt;/div&gt;\"]\n    audio --&gt; speaker_embedding\n</code></pre>"},{"location":"examples/aggregation/#defining-features-audio","title":"Defining features: <code>\"audio\"</code>","text":"<p>Each audio recording has an <code>audio_id</code> (unique identifier) and a <code>speaker_id</code> (which speaker it belongs to). Multiple audio recordings can belong to the same speaker.</p> src/example_aggregation/features.py<pre><code>import metaxy as mx\n\n\nclass Audio(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"audio\",\n        id_columns=[\"audio_id\"],\n        fields=[\"default\"],\n    ),\n):\n    \"\"\"Audio recordings of different speakers.\"\"\"\n\n    audio_id: str\n    speaker_id: str\n    duration_seconds: float\n    path: str\n</code></pre>"},{"location":"examples/aggregation/#defining-features-speakerembedding","title":"Defining features: <code>\"speaker/embedding\"</code>","text":"<p><code>\"speaker/embedding\"</code> aggregates all audio recordings from a speaker into a single embedding. The key configuration is the <code>lineage</code> parameter which tells Metaxy that multiple <code>\"audio\"</code> records with the same <code>speaker_id</code> are aggregated into one <code>\"speaker/embedding\"</code>.</p> src/example_aggregation/features.py<pre><code>class SpeakerEmbedding(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"speaker/embedding\",\n        id_columns=[\"speaker_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=Audio,\n                lineage=mx.LineageRelationship.aggregation(on=[\"speaker_id\"]),\n            )\n        ],\n        fields=[\n            mx.FieldSpec(key=\"embedding\", code_version=\"1\"),\n        ],\n    ),\n):\n    \"\"\"Speaker embedding aggregated from all their audio recordings.\n\n    This demonstrates N:1 aggregation lineage where multiple audio recordings\n    from the same speaker are aggregated into a single speaker embedding.\n    \"\"\"\n\n    speaker_id: str\n    n_dim: int\n    path: str\n</code></pre> <p>The <code>LineageRelationship.aggregation(on=[\"speaker_id\"])</code> declaration is the key part. It tells Metaxy:</p> <ol> <li>Multiple <code>\"audio\"</code> rows are aggregated into one <code>\"speaker/embedding\"</code> row</li> <li>The aggregation is keyed on <code>speaker_id</code> - all audio with the same speaker_id contributes to one embedding</li> <li>When any audio for a speaker changes, the aggregated provenance changes, triggering recomputation of that speaker's embedding</li> </ol>"},{"location":"examples/aggregation/#getting-started","title":"Getting Started","text":"<p>Install the example's dependencies:</p> <pre><code>uv sync\n</code></pre>"},{"location":"examples/aggregation/#walkthrough","title":"Walkthrough","text":"<p>Here is the pipeline code that processes audio and computes speaker embeddings:</p> <code>pipeline.py</code> pipeline.py<pre><code>import metaxy as mx\nimport polars as pl\n\nfrom example_aggregation.features import Audio, SpeakerEmbedding\n\n# Audio samples: 2 speakers with 2 recordings each\nAUDIO_SAMPLES = pl.DataFrame(\n    [\n        {\n            \"audio_id\": \"a1\",\n            \"speaker_id\": \"s1\",\n            \"duration_seconds\": 30.5,\n            \"path\": \"audio/s1_recording1.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a1_v1\"},\n        },\n        {\n            \"audio_id\": \"a2\",\n            \"speaker_id\": \"s1\",\n            \"duration_seconds\": 45.2,\n            \"path\": \"audio/s1_recording2.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a2_v1\"},\n        },\n        {\n            \"audio_id\": \"a3\",\n            \"speaker_id\": \"s2\",\n            \"duration_seconds\": 60.0,\n            \"path\": \"audio/s2_recording1.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a3_v1\"},\n        },\n        {\n            \"audio_id\": \"a4\",\n            \"speaker_id\": \"s2\",\n            \"duration_seconds\": 35.8,\n            \"path\": \"audio/s2_recording2.wav\",\n            \"metaxy_provenance_by_field\": {\"default\": \"a4_v1\"},\n        },\n    ]\n)\n\n\ndef main():\n    cfg = mx.init()\n    store = cfg.get_store(\"dev\")\n\n    # Step 1: Write audio metadata\n    with store:\n        increment = store.resolve_update(Audio, samples=AUDIO_SAMPLES)\n        if len(increment.new) &gt; 0:\n            print(f\"Found {len(increment.new)} new audio recordings\")\n            store.write(Audio, increment.new)\n        elif len(increment.stale) &gt; 0:\n            print(f\"Found {len(increment.stale)} changed audio recordings\")\n            store.write(Audio, increment.stale)\n        else:\n            print(\"No new or changed audio recordings\")\n\n    # Step 2: Compute speaker embeddings\n    with store:\n        increment = store.resolve_update(SpeakerEmbedding)\n\n        added_df = increment.new.to_polars()\n        changed_df = increment.stale.to_polars()\n\n        speakers_to_process = (\n            pl.concat([added_df, changed_df])\n            .select(\"speaker_id\")\n            .unique()\n            .sort(\"speaker_id\")\n            .to_series()\n            .to_list()\n        )\n\n        print(\n            f\"Found {len(speakers_to_process)} speakers that need embedding computation\"\n        )\n\n        if speakers_to_process:\n            embedding_data = []\n\n            for speaker_id in speakers_to_process:\n                speaker_rows = pl.concat([added_df, changed_df]).filter(\n                    pl.col(\"speaker_id\") == speaker_id\n                )\n\n                provenance_by_field = speaker_rows[\"metaxy_provenance_by_field\"][0]\n                provenance = speaker_rows[\"metaxy_provenance\"][0]\n\n                n_audio = len(speaker_rows)\n                print(\n                    f\"  Computing embedding for speaker {speaker_id} from {n_audio} audio recordings\"\n                )\n\n                embedding_data.append(\n                    {\n                        \"speaker_id\": speaker_id,\n                        \"n_dim\": 512,\n                        \"path\": f\"embeddings/{speaker_id}.npy\",\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            embedding_df = pl.DataFrame(embedding_data)\n            print(f\"Writing embeddings for {len(embedding_data)} speakers\")\n            store.write(SpeakerEmbedding, embedding_df)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/aggregation/#step-1-initial-run","title":"Step 1: Initial Run","text":"<p>Run the pipeline to create audio recordings and speaker embeddings:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 4 new audio recordings\nFound 2 speakers that need embedding computation\n  Computing embedding for speaker s1 from 2 audio recordings\n  Computing embedding for speaker s2 from 2 audio recordings\nWriting embeddings for 2 speakers\n</code></pre> <p>All features have been materialized:</p> <ul> <li>4 audio recordings (2 per speaker)</li> <li>2 speaker embeddings (one per speaker)</li> </ul>"},{"location":"examples/aggregation/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>No new or changed audio recordings\nFound 0 speakers that need embedding computation\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/aggregation/#step-3-update-one-audio-recording","title":"Step 3: Update One Audio Recording","text":"<p>Now let's update the provenance of audio <code>a1</code> (belonging to speaker <code>s1</code>):</p> <code>patches/01_update_audio_provenance.patch</code> patches/01_update_audio_provenance.patch<pre><code>--- a/pipeline.py\n+++ b/pipeline.py\n@@ -22,7 +22,7 @@ AUDIO_SAMPLES = pl.DataFrame(\n             \"speaker_id\": \"s1\",\n             \"duration_seconds\": 30.5,\n             \"path\": \"audio/s1_recording1.wav\",\n-            \"metaxy_provenance_by_field\": {\"default\": \"a1_v1\"},\n+            \"metaxy_provenance_by_field\": {\"default\": \"a1_v2\"},\n         },\n         {\n             \"audio_id\": \"a2\",\n</code></pre> <p>This represents a change to one audio recording (perhaps it was re-processed or updated).</p>"},{"location":"examples/aggregation/#step-4-observe-selective-recomputation","title":"Step 4: Observe Selective Recomputation","text":"<p>Run the pipeline again after the audio change:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 1 changed audio recordings\nFound 1 speakers that need embedding computation\n  Computing embedding for speaker s1 from 2 audio recordings\nWriting embeddings for 1 speakers\n</code></pre> <p>Key observation:</p> <ul> <li>Only speaker <code>s1</code>'s embedding is recomputed (because audio <code>a1</code> belongs to <code>s1</code>)</li> <li>Speaker <code>s2</code>'s embedding is not recomputed (none of their audio changed)</li> </ul> <p>This demonstrates that Metaxy correctly tracks aggregation lineage - when any audio for a speaker changes, only that speaker's embedding needs recomputation.</p>"},{"location":"examples/aggregation/#step-5-add-new-audio","title":"Step 5: Add New Audio","text":"<p>Now let's add a new audio recording for speaker <code>s1</code>:</p> <code>patches/02_add_audio.patch</code> patches/02_add_audio.patch<pre><code>--- a/pipeline.py\n+++ b/pipeline.py\n@@ -45,6 +45,13 @@\n             \"path\": \"audio/s2_recording2.wav\",\n             \"metaxy_provenance_by_field\": {\"default\": \"a4_v1\"},\n         },\n+        {\n+            \"audio_id\": \"a5\",\n+            \"speaker_id\": \"s1\",\n+            \"duration_seconds\": 25.0,\n+            \"path\": \"audio/s1_recording3.wav\",\n+            \"metaxy_provenance_by_field\": {\"default\": \"a5_v1\"},\n+        },\n     ]\n )\n</code></pre>"},{"location":"examples/aggregation/#step-6-observe-aggregation-update","title":"Step 6: Observe Aggregation Update","text":"<p>Run the pipeline again:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 1 new audio recordings\nFound 1 speakers that need embedding computation\n  Computing embedding for speaker s1 from 3 audio recordings\nWriting embeddings for 1 speakers\n</code></pre> <p>Key observation:</p> <ul> <li>Only speaker <code>s1</code>'s embedding is recomputed (the new audio belongs to <code>s1</code>)</li> <li>Speaker <code>s1</code> now has 3 audio recordings (up from 2)</li> <li>Speaker <code>s2</code> remains unchanged</li> </ul>"},{"location":"examples/aggregation/#how-it-works","title":"How It Works","text":"<p>Metaxy uses window functions to compute aggregated provenance without reducing rows. When resolving updates for <code>\"speaker/embedding\"</code>:</p> <ol> <li>All audio rows for the same speaker get identical aggregated provenance values</li> <li>The aggregated provenance is computed from the individual audio provenances</li> <li>When any audio for a speaker changes, the aggregated provenance changes</li> <li>This triggers recomputation of only the affected speaker's embedding</li> </ol>"},{"location":"examples/aggregation/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling aggregation relationships: LineageRelationship.aggregation. Other Metaxy features continue to seamlessly work with aggregation relationships.</p>"},{"location":"examples/aggregation/#related-materials","title":"Related Materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>One-to-Many Expansion (the inverse relationship)</li> </ul>"},{"location":"examples/basic/","title":"Basic Example","text":""},{"location":"examples/basic/#overview","title":"Overview","text":"<p> View Source on GitHub</p> <p>This example demonstrates how Metaxy automatically detects changes in upstream features and triggers recomputation of downstream features. It shows the core value proposition of Metaxy: avoiding unnecessary recomputation while ensuring data consistency.</p> <p>We will build a simple two-feature pipeline where a child feature depends on a parent feature. When the parent's algorithm changes (represented by <code>code_version</code>), the child feature is automatically recomputed.</p>"},{"location":"examples/basic/#the-pipeline","title":"The Pipeline","text":"<p>Let's define a pipeline with two features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart LR\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    examples_parent[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/parent&lt;/b&gt;&lt;br/&gt;7de0f5e8&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- embeddings (05e66510)&lt;/div&gt;\"]\n    examples_child[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/child&lt;/b&gt;&lt;br/&gt;b10ea448&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- predictions (9cd1c608)&lt;/div&gt;\"]\n    examples_parent --&gt; examples_child\n</code></pre>"},{"location":"examples/basic/#defining-features-examplesparent","title":"Defining features: <code>\"examples/parent\"</code>","text":"<p>The parent feature represents raw embeddings computed from source data. It has a single field <code>embeddings</code> with a <code>code_version</code> that tracks the algorithm version.</p> src/example_basic/features.py<pre><code>import metaxy as mx\n\n\nclass ParentFeature(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"examples/parent\",\n        fields=[\n            mx.FieldSpec(\n                key=\"embeddings\",\n                code_version=\"1\",\n            ),\n        ],\n        id_columns=(\"sample_uid\",),\n    ),\n):\n    \"\"\"Parent feature that generates embeddings from raw data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"examples/basic/#defining-features-exampleschild","title":"Defining features: <code>\"examples/child\"</code>","text":"<p>The child feature depends on the parent and produces predictions. The key configuration is the <code>FeatureDep</code> which declares that <code>\"examples/child\"</code> depends on <code>\"examples/parent\"</code>.</p> src/example_basic/features.py<pre><code>class ChildFeature(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"examples/child\",\n        deps=[ParentFeature],\n        fields=[\"predictions\"],\n        id_columns=(\"sample_uid\",),\n    ),\n):\n    \"\"\"Child feature that uses parent embeddings to generate predictions.\"\"\"\n\n    pass\n</code></pre> <p>The <code>FeatureDep</code> declaration tells Metaxy:</p> <ol> <li><code>\"examples/child\"</code> depends on <code>\"examples/parent\"</code></li> <li>When the parent's field provenance changes, the child must be recomputed</li> <li>This dependency is tracked automatically, enabling incremental recomputation</li> </ol>"},{"location":"examples/basic/#getting-started","title":"Getting Started","text":"<p>Install the example's dependencies:</p> <pre><code>uv sync\n</code></pre>"},{"location":"examples/basic/#walkthrough","title":"Walkthrough","text":""},{"location":"examples/basic/#step-1-initial-run","title":"Step 1: Initial Run","text":"<p>Run the pipeline to create parent embeddings and child predictions:</p> <pre><code>$ python src/example_basic/pipeline.py\n</code></pre> <pre><code>Graph project_version: 490f2c18\nWritten 3 rows for feature examples/parent\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph project_version: 490f2c18\n\n\ud83d\udcca Computing examples/child...\n  feature_version: b10ea448\nIdentified: 3 new samples, 0 samples with new provenance_by_field\n\u2713 Materialized 3 new samples\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\n\n\u2705 Pipeline complete!\n</code></pre> <p>The pipeline materialized 3 samples for the child feature. Each sample has its provenance tracked.</p>"},{"location":"examples/basic/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>$ python src/example_basic/pipeline.py\n</code></pre> <pre><code>Graph project_version: 490f2c18\nMetadata already exists for feature examples/parent (feature_version: 7de0f5e8...)\nSkipping write to avoid duplicates\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph project_version: 490f2c18\n\n\ud83d\udcca Computing examples/child...\n  feature_version: b10ea448\nIdentified: 0 new samples, 0 samples with new provenance_by_field\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\nNo changes detected (idempotent)\n\n\u2705 Pipeline complete!\n</code></pre> <p>Key observation: No recomputation occurred.</p>"},{"location":"examples/basic/#step-3-update-parent-algorithm","title":"Step 3: Update Parent Algorithm","text":"<p>Now let's simulate an algorithm improvement by changing the parent's <code>code_version</code> from <code>\"1\"</code> to <code>\"2\"</code>:</p> PatchFeature Graph Changes patches/01_update_parent_algorithm.patch<pre><code>--- a/src/example_basic/features.py\n+++ b/src/example_basic/features.py\n@@ -15,7 +15,7 @@ class ParentFeature(\n         fields=[\n             FieldSpec(\n                 key=\"embeddings\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n         ],\n         id_columns=(\"sample_uid\",),\n</code></pre> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    examples_parent[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/parent&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;7de0f5e8&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;68827f3e&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;embeddings&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;05e66510&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;3c8d3e9b&lt;/font&gt;)&lt;/div&gt;\"]\n    examples_child[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;examples/child&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;b10ea448&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e5b92b18&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;predictions&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;9cd1c608&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;7cef6acb&lt;/font&gt;)&lt;/div&gt;\"]\n    examples_parent --&gt; examples_child\n\n\n    style examples_child stroke:#FFAA00,stroke-width:2px\n    style examples_parent stroke:#FFAA00,stroke-width:2px</code></pre> <p>This change means that the existing embeddings and the downstream feature have to be recomputed.</p>"},{"location":"examples/basic/#step-4-observe-automatic-recomputation","title":"Step 4: Observe Automatic Recomputation","text":"<p>Run the pipeline again after the algorithm change:</p> <pre><code>$ python src/example_basic/pipeline.py\n</code></pre> <pre><code>Graph project_version: c423d51a\nWritten 3 rows for feature examples/parent\nPipeline\n============================================================\n\n[1/2] Computing parent feature...\n\n[2/2] Computing child feature...\nGraph project_version: c423d51a\n\n\ud83d\udcca Computing examples/child...\n  feature_version: e5b92b18\nIdentified: 3 new samples, 0 samples with new provenance_by_field\n\u2713 Materialized 3 new samples\n\n\ud83d\udccb Child provenance_by_field:\n  sample_uid=1: {'predictions': '24503967'}\n  sample_uid=2: {'predictions': '24458329'}\n  sample_uid=3: {'predictions': '26963083'}\n\n\n\u2705 Pipeline complete!\n</code></pre> <p>Key observation: The child feature was automatically recomputed because:</p> <ol> <li>The parent's <code>code_version</code> changed from <code>\"1\"</code> to <code>\"2\"</code></li> <li>This changed the parent's <code>metaxy_feature_version</code></li> <li>The child's field dependency on <code>embeddings</code> detected the change</li> <li>All child samples were marked for recomputation</li> </ol>"},{"location":"examples/basic/#how-it-works","title":"How It Works","text":"<p>Metaxy tracks provenance at the field level using:</p> <ol> <li>Field Version: A hash combining the field's <code>code_version</code> and provenances of upstream fields</li> <li>Feature Version: A hash combining the field versions of all fields in the feature</li> <li>Dependency Resolution: When resolving updates, Metaxy computes what the provenance would be and compares it to what's stored</li> </ol> <p>This enables precise, incremental recomputation without re-processing unchanged data.</p>"},{"location":"examples/basic/#conclusion","title":"Conclusion","text":"<p>Metaxy provides automatic change detection and incremental recomputation through:</p> <ul> <li>Feature dependency tracking via <code>FeatureDep</code></li> <li>Algorithm versioning via <code>FieldSpec.code_version</code></li> <li>Provenance-based change detection via <code>MetadataStore.resolve_update</code></li> </ul> <p>This mechanism ensures your pipelines are both efficient and keep relevant data up to date.</p>"},{"location":"examples/basic/#related-materials","title":"Related Materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Data Versioning</li> <li>Relationships</li> </ul>"},{"location":"examples/ducklake/","title":"DuckLake","text":""},{"location":"examples/ducklake/#overview","title":"Overview","text":"<p> View Source on GitHub</p> <p>This example demonstrates how to configure DuckLake with the DuckDB metadata store. DuckLake is an open lakehouse format that separates the metadata catalog (table definitions, schema evolution, and transaction history) from data file storage. This lets you choose independent backends for each layer, for example PostgreSQL for the catalog and S3 for data files.</p> <p>We will set up a DuckLake-backed store via <code>metaxy.toml</code> and preview the SQL statements that DuckLake would execute when attaching to a DuckDB connection.</p>"},{"location":"examples/ducklake/#getting-started","title":"Getting Started","text":"<p>Install the example's dependencies:</p> <pre><code>uv sync\n</code></pre>"},{"location":"examples/ducklake/#configuration","title":"Configuration","text":"<p>DuckLake is configured in <code>metaxy.toml</code> with two parts: a catalog backend (where the catalog is stored) and a storage backend (where data files live).</p> <p>The active configuration below uses SQLite for the metadata catalog and the local filesystem for data storage. Commented-out sections show alternative backends.</p> metaxy.toml<pre><code>project = \"example_ducklake\"\nentrypoints = [\"example_ducklake.demo\"]\nauto_create_tables = true # Enable for development/examples\n\n[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\n\n[stores.dev.config]\ndatabase = \"/tmp/ducklake_demo.db\"\n\n[stores.dev.config.ducklake]\nalias = \"ducklake\"\n\n[stores.dev.config.ducklake.attach_options]\noverride_data_path = true\n\n# -- Metadata backend: SQLite (local, no extra dependencies) -----------------\n[stores.dev.config.ducklake.catalog]\ntype = \"sqlite\"\nuri = \"/tmp/ducklake_meta.db\"\n\n# -- Metadata backend: PostgreSQL ---------------------------------------------\n# [stores.dev.config.ducklake.catalog]\n# type = \"postgres\"\n# secret_name = \"my_pg_secret\"       # Required: name for the DuckDB secret\n# host = \"localhost\"\n# port = 5432\n# database = \"ducklake_meta\"\n# user = \"ducklake\"\n# password = \"changeme\"\n# # Extra parameters forwarded to DuckDB's CREATE SECRET (optional):\n# # secret_parameters = { sslmode = \"require\" }\n# # Omit inline credentials (host, database, user, password) to reference\n# # a pre-existing DuckDB secret named by secret_name.\n\n# -- Metadata backend: MotherDuck (managed, no storage needed) --------\n# [stores.dev.config.ducklake.catalog]\n# type = \"motherduck\"\n# database = \"my_lake\"\n\n# -- Metadata backend: MotherDuck BYOB (bring your own bucket) ----------------\n# Use MotherDuck for the catalog but store data in your own S3 bucket.\n# [stores.dev.config.ducklake.catalog]\n# type = \"motherduck\"\n# database = \"my_ducklake\"\n# region = \"eu-central-1\"\n# [stores.dev.config.ducklake.storage]\n# type = \"s3\"\n# secret_name = \"my_s3_secret\"\n# key_id = \"AKIA...\"\n# secret = \"...\"\n# region = \"eu-central-1\"\n# scope = \"s3://mybucket/\"\n# bucket = \"mybucket\"\n\n# -- Storage backend: local filesystem ----------------------------------------\n[stores.dev.config.ducklake.storage]\ntype = \"local\"\npath = \"/tmp/ducklake_storage\"\n\n# -- Storage backend: S3 -----------------------------------------------------\n# [stores.dev.config.ducklake.storage]\n# type = \"s3\"\n# secret_name = \"my_s3_secret\"       # Required: name for the DuckDB secret\n# bucket = \"my-ducklake-bucket\"\n# prefix = \"data\"\n# region = \"eu-central-1\"\n# # Provide inline credentials to create a secret, or omit them to reference\n# # a pre-existing DuckDB secret named by secret_name.\n# # key_id = \"AKIA...\"\n# # secret = \"secret\"\n# # For credential chain (IAM roles, env vars, etc.) instead of static credentials:\n# # secret_parameters = { provider = \"credential_chain\" }\n# # Extra parameters forwarded to DuckDB's CREATE SECRET (optional):\n# # secret_parameters = { kms_key_id = \"arn:aws:kms:...\" }\n\n# -- Storage backend: Cloudflare R2 ------------------------------------------\n# [stores.dev.config.ducklake.storage]\n# type = \"r2\"\n# secret_name = \"my_r2_secret\"       # Required: name for the DuckDB secret\n# account_id = \"your-cloudflare-account-id\"\n# data_path = \"r2://my-bucket/ducklake/\"\n# # key_id = \"R2_ACCESS_KEY\"\n# # secret = \"R2_SECRET_KEY\"\n\n# -- Storage backend: Google Cloud Storage ------------------------------------\n# [stores.dev.config.ducklake.storage]\n# type = \"gcs\"\n# secret_name = \"my_gcs_secret\"      # Required: name for the DuckDB secret\n# data_path = \"gs://my-bucket/ducklake/\"\n# # key_id = \"HMAC_ACCESS_KEY\"\n# # secret = \"HMAC_SECRET\"\n</code></pre> <p>Available backend combinations:</p> Catalog backend Storage backend DuckDB, SQLite, PostgreSQL local filesystem, S3, Cloudflare R2, Google Cloud Storage MotherDuck managed (no storage backend needed), or BYOB with S3/R2/GCS <p>Tip</p> <p>To use the credential chain (IAM roles, environment variables, etc.) instead of static credentials, set <code>secret_parameters = { provider = \"credential_chain\" }</code> on S3, R2, or GCS storage backends.</p> <p>Note</p> <p>MotherDuck supports a \"Bring Your Own Bucket\" (BYOB) mode where MotherDuck manages the DuckLake catalog while you provide your own S3-compatible storage. Storage secrets are created <code>IN MOTHERDUCK</code> so that MotherDuck compute can access your bucket.</p>"},{"location":"examples/ducklake/#walkthrough","title":"Walkthrough","text":"<p>The demo script initializes the store from configuration and previews the SQL statements that would be executed:</p> src/example_ducklake/demo.py<pre><code>\"\"\"Demonstration of configuring the DuckLake metadata store.\n\nThis example mirrors the Dagster DuckLake resource behaviour while staying\nNarwhals-compatible. No DuckLake installation is required to run this script;\nwe preview the SQL statements that would be executed when attaching DuckLake.\n\"\"\"\n\nimport metaxy as mx\nfrom metaxy.ext.metadata_stores.duckdb import DuckDBMetadataStore\n\n\ndef preview_attachment_sql(store: DuckDBMetadataStore) -&gt; list[str]:\n    \"\"\"Return the SQL statements DuckLake would execute on open().\"\"\"\n    return store.preview_ducklake_sql()\n\n\nif __name__ == \"__main__\":\n    # Initialize metaxy and get config (searches for metaxy.toml)\n    config = mx.init()\n    ducklake_store = config.get_store()\n    assert isinstance(ducklake_store, DuckDBMetadataStore), (\n        \"DuckLake example misconfigured: expected DuckDBMetadataStore.\"\n    )\n    ducklake_store.ducklake_attachment\n    print(\"DuckLake store initialised. Extensions:\", ducklake_store.extensions)\n    print(\"\\nPreview of DuckLake ATTACH SQL:\")\n    for line in preview_attachment_sql(ducklake_store):\n        print(\"  \", line)\n</code></pre> <p>Run the demo:</p> <pre><code>uv run python src/example_ducklake/demo.py\n</code></pre> <p>The output shows the full sequence of SQL statements: creating secrets for the metadata and storage backends, and attaching the DuckLake database.</p>"},{"location":"examples/ducklake/#related-materials","title":"Related Materials","text":"<ul> <li>DuckDB Metadata Store</li> <li><code>DuckLakeConfig</code></li> </ul>"},{"location":"examples/expansion/","title":"Expansion","text":""},{"location":"examples/expansion/#overview","title":"Overview","text":"<p> View Source on GitHub</p> <p>This example demonstrates how to implement expansion (<code>1:N</code>) transformations with Metaxy. In such relationships a single parent sample can map into multiple child samples.</p> <p>These relationships can be modeled with LineageRelationship.expansion lineage type.</p> <p>We will use a hypothetical video chunking pipeline as an example.</p>"},{"location":"examples/expansion/#the-pipeline","title":"The Pipeline","text":"<p>We are going to define a typical video processing pipeline with three features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart LR\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;d842b740&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (7132721c)&lt;br/&gt;- frames (3f6f401c)&lt;/div&gt;\"]\n    video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;7dc3712a&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (1549b1fa)&lt;br/&gt;- frames (df8943d3)&lt;/div&gt;\"]\n    video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;52f055e3&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (0561bd8c)&lt;/div&gt;\"]\n    video_raw --&gt; video_chunk\n    video_chunk --&gt; video_faces\n</code></pre>"},{"location":"examples/expansion/#defining-features-videoraw","title":"Defining features: <code>\"video/raw\"</code>","text":"<p>Each video-like feature in our pipeline is going to have two fields: <code>audio</code> and <code>frames</code>.</p> <p>Let's set the code version of <code>audio</code> to <code>\"1\"</code> in order to change it in the future. <code>frames</code> field will have a default version.</p> src/example_expansion/features.py<pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video/raw\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            \"frames\",\n        ],\n    ),\n):\n    video_id: str\n    path: str  # where the video is stored\n</code></pre>"},{"location":"examples/expansion/#defining-features-videochunk","title":"Defining features: <code>\"video/chunk\"</code>","text":"<p><code>\"video/chunk\"</code> represents a piece of the upstream <code>\"video/raw\"</code> feature. Since each <code>\"video/raw\"</code> sample can be split into multiple chunks, we need to tell Metaxy how to map each chunk to its parent video.</p> src/example_expansion/features.py<pre><code>class VideoChunk(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"chunk\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=Video,\n                lineage=mx.LineageRelationship.expansion(on=[\"video_id\"]),\n            )\n        ],\n        fields=[\"audio\", \"frames\"],\n    ),\n):\n    video_id: str  # points to the parent video\n    video_chunk_id: str\n    path: str  # where the video chunk is stored\n</code></pre> <p>We do not specify custom versions on its fields. Metaxy will automatically assign field-level lineage by matching on field names: <code>\"video/chunk:frames\"</code> depends on <code>\"video/raw:frames\"</code> and <code>\"video/chunk:audio\"</code> depends on <code>\"video/raw:audio\"</code>.</p> <pre><code>flowchart LR\n    subgraph video_raw[\"video/raw\"]\n        video_raw_audio[\"audio\"]\n        video_raw_frames[\"frames\"]\n    end\n    subgraph video_chunk[\"video/chunk\"]\n        video_chunk_audio[\"audio\"]\n        video_chunk_frames[\"frames\"]\n    end\n    video_raw_audio --&gt; video_chunk_audio\n    video_raw_frames --&gt; video_chunk_frames\n    style video_raw stroke:#4C78A8,stroke-width:2px\n    style video_chunk stroke:#4C78A8,stroke-width:2px</code></pre>"},{"location":"examples/expansion/#defining-features-videofaces","title":"Defining features: <code>\"video/faces\"</code>","text":"<p><code>\"video/faces\"</code> processes video chunks and only depends on the <code>frames</code> field. This can be expressed with a <code>FieldDep</code>.</p> src/example_expansion/features.py<pre><code>class FaceRecognition(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"faces\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[VideoChunk],\n        fields=[\n            mx.FieldSpec(\n                key=\"faces\", deps=[mx.FieldDep(feature=VideoChunk, fields=[\"frames\"])]\n            )\n        ],\n    ),\n):\n    video_chunk_id: str\n    num_faces: int  # number of faces detected\n</code></pre> <p><pre><code>flowchart LR\n    subgraph video_chunk[\"video/chunk\"]\n        video_chunk_audio[\"audio\"]\n        video_chunk_frames[\"frames\"]\n    end\n    subgraph video_faces[\"video/faces\"]\n        video_faces_faces[\"faces\"]\n    end\n    video_chunk_frames --&gt; video_faces_faces\n    style video_chunk stroke:#4C78A8,stroke-width:2px\n    style video_faces stroke:#4C78A8,stroke-width:2px</code></pre> This completes the feature definitions. Let's proceed to running the pipeline.</p>"},{"location":"examples/expansion/#getting-started","title":"Getting Started","text":"<p>Install the example's dependencies:</p> <pre><code>uv sync\n</code></pre>"},{"location":"examples/expansion/#walkthrough","title":"Walkthrough","text":"<p>Here is a toy pipeline for computing the feature graph described above:</p> <code>pipeline.py</code> pipeline.py<pre><code>import os\nimport random\n\nimport metaxy as mx\nimport narwhals as nw\nimport polars as pl\n\nfrom example_expansion.features import FaceRecognition, Video, VideoChunk\nfrom example_expansion.utils import split_video_into_chunks\n\n\ndef main():\n    # Set random seed from environment if provided (for deterministic testing)\n    if seed_str := os.environ.get(\"RANDOM_SEED\"):\n        random.seed(int(seed_str))\n    cfg = mx.init()\n    store = cfg.get_store(\"dev\")\n\n    # let's pretend somebody has already created the videos for us\n    samples = pl.DataFrame(\n        {\n            \"video_id\": [1, 2, 3],\n            \"path\": [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"],\n            \"metaxy_provenance_by_field\": [\n                {\"audio\": \"v1\", \"frames\": \"v1\"},\n                {\"audio\": \"v2\", \"frames\": \"v2\"},\n                {\"audio\": \"v3\", \"frames\": \"v3\"},\n            ],\n        }\n    )\n\n    with store:\n        # showcase: resolve incremental update for a root feature\n        increment = store.resolve_update(Video, samples=nw.from_native(samples))\n        if len(increment.new) &gt; 0:\n            print(f\"Found {len(increment.new)} new videos\")\n            store.write(Video, increment.new)\n\n    # Resolve videos that need to be split into chunks\n    with store:\n        increment = store.resolve_update(VideoChunk)\n        # the DataFrame dimensions matches Video (with ID column renamed)\n\n        print(\n            f\"Found {len(increment.new)} new videos and {len(increment.stale)} stale videos that need chunking\"\n        )\n\n        for row_dict in pl.concat(\n            [increment.new.to_polars(), increment.stale.to_polars()]\n        ).iter_rows(named=True):\n            print(f\"Processing video: {row_dict}\")\n            # let's split each video to 3-5 chunks randomly\n\n            video_id = row_dict[\"video_id\"]\n            path = row_dict[\"path\"]\n\n            provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n            provenance = row_dict[\"metaxy_provenance\"]\n\n            # pretend we split the video into chunks\n            chunk_paths = split_video_into_chunks(path)\n\n            # Generate chunk IDs based on the parent video ID\n            chunk_ids = [f\"{video_id}_{i}\" for i in range(len(chunk_paths))]\n\n            # write the chunks to the store\n            # CRUSIAL: all the chunks **must share the same provenance values**\n            chunk_df = pl.DataFrame(\n                {\n                    \"video_id\": [video_id] * len(chunk_paths),\n                    \"video_chunk_id\": chunk_ids,\n                    \"path\": chunk_paths,\n                    \"metaxy_provenance_by_field\": [provenance_by_field]\n                    * len(chunk_paths),\n                    \"metaxy_provenance\": [provenance] * len(chunk_paths),\n                }\n            )\n            print(f\"Writing {len(chunk_paths)} chunks for video {video_id}\")\n            store.write(VideoChunk, nw.from_native(chunk_df))\n\n    # Process face recognition on video chunks\n    with store:\n        increment = store.resolve_update(FaceRecognition)\n        print(\n            f\"Found {len(increment.new)} new video chunks and {len(increment.stale)} stale video chunks that need face recognition\"\n        )\n\n        if len(increment.new) &gt; 0:\n            # simulate face detection on each chunk\n            face_data = []\n            for row_dict in pl.concat(\n                [increment.new.to_polars(), increment.stale.to_polars()]\n            ).iter_rows(named=True):\n                video_chunk_id = row_dict[\"video_chunk_id\"]\n                provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n                provenance = row_dict[\"metaxy_provenance\"]\n\n                # simulate detecting random number of faces\n                num_faces = random.randint(0, 10)\n\n                face_data.append(\n                    {\n                        \"video_chunk_id\": video_chunk_id,\n                        \"num_faces\": num_faces,\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            face_df = pl.DataFrame(face_data)\n            print(f\"Writing face recognition results for {len(face_data)} chunks\")\n            store.write(FaceRecognition, nw.from_native(face_df))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/expansion/#step-1-launch-initial-run","title":"Step 1: Launch Initial Run","text":"<p>Run the pipeline to create videos, chunks, and face recognition results:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 3 new videos\nFound 3 new videos and 0 stale videos that need chunking\nProcessing video: {'video_id': 1, 'path': 'video1.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '23124187', 'frames': '24116928'}, 'metaxy_provenance': '40433844', 'metaxy_data_version': '40433844', 'metaxy_data_version_by_field': {'audio': '23124187', 'frames': '24116928'}}\nWriting 5 chunks for video 1\nProcessing video: {'video_id': 2, 'path': 'video2.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '18611808', 'frames': '18345318'}, 'metaxy_provenance': '12032307', 'metaxy_data_version': '12032307', 'metaxy_data_version_by_field': {'audio': '18611808', 'frames': '18345318'}}\nWriting 3 chunks for video 2\nProcessing video: {'video_id': 3, 'path': 'video3.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '24638180', 'frames': '16645902'}, 'metaxy_provenance': '21310928', 'metaxy_data_version': '21310928', 'metaxy_data_version_by_field': {'audio': '24638180', 'frames': '16645902'}}\nWriting 3 chunks for video 3\nFound 11 new video chunks and 0 stale video chunks that need face recognition\nWriting face recognition results for 11 chunks\n</code></pre> <p>All three features have been materialized. Note that the <code>\"video/chunk\"</code> feature may dynamically create as many samples as needed: Metaxy doesn't need to know anything about this in advance, except the relationship type.</p>"},{"location":"examples/expansion/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 0 new videos and 0 stale videos that need chunking\nFound 0 new video chunks and 0 stale video chunks that need face recognition\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/expansion/#step-3-change-audio-code-version","title":"Step 3: Change Audio Code Version","text":"<p>Now let's bump the code version on the <code>audio</code> field of <code>\"video/raw\"</code> feature:</p> PatchFeature Graph Changes patches/01_update_video_code_version.patch<pre><code>--- a/src/example_expansion/features.py\n+++ b/src/example_expansion/features.py\n@@ -9,6 +9,6 @@ class Video(\n         id_columns=[\"video_id\"],\n         fields=[\n-            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n+            mx.FieldSpec(key=\"audio\", code_version=\"2\"),\n             \"frames\",\n         ],\n     ),\n</code></pre> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart TB\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;d842b740&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;6e5a4ab8&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- frames (3f6f401c)&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;7132721c&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;f8e35296&lt;/font&gt;)&lt;/div&gt;\"]\n    video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;7dc3712a&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;504507a9&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- frames (df8943d3)&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;1549b1fa&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;7948d163&lt;/font&gt;)&lt;/div&gt;\"]\n    video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;52f055e3&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (0561bd8c)&lt;/div&gt;\"]\n    video_raw --&gt; video_chunk\n    video_chunk --&gt; video_faces\n\n\n    style video_chunk stroke:#FFAA00,stroke-width:2px\n    style video_faces stroke:#808080\n    style video_raw stroke:#FFAA00,stroke-width:2px</code></pre> <p>This represents updating the audio processing algorithm, and therefore the audio data, while frame data is kept the same.</p>"},{"location":"examples/expansion/#step-4-observe-field-level-tracking","title":"Step 4: Observe Field-Level Tracking","text":"<p>Run the pipeline again after the code change:</p> <pre><code>$ python pipeline.py\n</code></pre> <pre><code>Found 3 new videos\nFound 3 new videos and 0 stale videos that need chunking\nProcessing video: {'video_id': 1, 'path': 'video1.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '23124187', 'frames': '24116928'}, 'metaxy_provenance': '40433844', 'metaxy_data_version': '40433844', 'metaxy_data_version_by_field': {'audio': '23124187', 'frames': '24116928'}}\nWriting 5 chunks for video 1\nProcessing video: {'video_id': 2, 'path': 'video2.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '18611808', 'frames': '18345318'}, 'metaxy_provenance': '12032307', 'metaxy_data_version': '12032307', 'metaxy_data_version_by_field': {'audio': '18611808', 'frames': '18345318'}}\nWriting 3 chunks for video 2\nProcessing video: {'video_id': 3, 'path': 'video3.mp4', 'metaxy_deleted_at': None, 'metaxy_provenance_by_field': {'audio': '24638180', 'frames': '16645902'}, 'metaxy_provenance': '21310928', 'metaxy_data_version': '21310928', 'metaxy_data_version_by_field': {'audio': '24638180', 'frames': '16645902'}}\nWriting 3 chunks for video 3\nFound 0 new video chunks and 0 stale video chunks that need face recognition\n</code></pre> <p>Key observation:</p> <ul> <li><code>\"video/chunk\"</code> has been recomputed since the <code>audio</code> field on it has been affected by the upstream change</li> <li><code>\"video/faces\"</code> did not require a recompute, because it only depends on the <code>frames</code> field (which did not change)</li> </ul>"},{"location":"examples/expansion/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling expansion relationships: LineageRelationship.expansion. Other Metaxy features such as field-level versioning continue to work seamlessly when declaring expansion relationships.</p>"},{"location":"examples/expansion/#related-materials","title":"Related materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>Fields Mapping</li> </ul>"},{"location":"guide/","title":"User Guide","text":"<p>Welcome to the Metaxy User Guide. Start with the introduction to understand what Metaxy does, then follow the quickstart to get up and running.</p> <ul> <li> <p> Introduction</p> <p>Learn what Metaxy is and why it exists.</p> </li> <li> <p> Quickstart</p> <p>Get started with using Metaxy.</p> </li> <li> <p> Concepts</p> <p>Understand the core concepts and abstractions.</p> </li> <li> <p> Application Lifecycle</p> <p>Development, testing, and deployment patterns.</p> </li> </ul>"},{"location":"guide/qa/","title":"Questions And Answers","text":"<p>Why are feature definitions created via classes?</p> <p>In short: it's handy for some integrations, allows to have more type annotation goodness, but isn't strictly necessary. We will explore other interfaces in the future (see <code>anam-org/metaxy#800</code>).</p> <p>Do you support PostgreSQL?</p> <p>Not at the moment, but we are working on it in <code>anam-org/metaxy#223</code>.</p> <p>Why does the logo mean? Why does it look like this?</p> <p>It's an accident. I wanted it to resemble a galaxy, but it is what it is.</p>"},{"location":"guide/concepts/","title":"Concepts","text":"<p>Metaxy is built around a few core ideas that work together to solve the problem of incremental processing in multimodal pipelines.</p> <p>Feature definitions declare what data you have and how it depends on other data. Metaxy builds a feature graph from these definitions and uses it to track versions at the sample level. When upstream data changes, Metaxy identifies exactly which downstream samples need recomputation and resolves an incremental update. Feature versions are persisted in a metadata store. Feature definitions may define custom metadata columns (such as file path, size, etc.) which are stored alongside the versioning information.</p>"},{"location":"guide/concepts/#core-concepts","title":"Core Concepts","text":"<ul> <li> <p> Metadata Stores</p> <p>Unified interface for storing and retrieving metadata across different storage backends.</p> </li> <li> <p> Feature Definitions</p> <p>Declarative specifications that define your data schema, dependencies, and versioning.</p> </li> <li> <p> Versioning</p> <p>Sample-level version tracking that detects changes and determines what needs recomputation.</p> </li> <li> <p> Feature Discovery</p> <p>Automatic registration and graph building from feature definitions in your codebase.</p> </li> </ul>"},{"location":"guide/concepts/#dependencies-and-lineage","title":"Dependencies and Lineage","text":"<ul> <li> <p> Lineage Relationship</p> <p>How features relate to upstream dependencies: one-to-one, one-to-many, or many-to-one.</p> </li> <li> <p> Optional Dependencies</p> <p>Handle missing upstream data gracefully without blocking downstream processing.</p> </li> <li> <p> Filters</p> <p>Select subsets of samples for processing based on metadata column conditions.</p> </li> </ul>"},{"location":"guide/concepts/#advanced-topics","title":"Advanced Topics","text":"<ul> <li> <p> Deletions</p> <p>Propagating sample deletions through the feature graph correctly.</p> </li> <li> <p> System Columns</p> <p>Reserved columns used internally for versioning and deduplication.</p> </li> <li> <p> Testing</p> <p>Patterns and utilities for testing features in isolation.</p> </li> </ul>"},{"location":"guide/concepts/deletions/","title":"Metadata Deletion","text":"<p><code>MetadataStore.resolve_update</code> can be used to identify orphaned samples that no longer exist upstream. Users may want to delete these samples for the current feature from the metadata store.</p> <p>Metaxy supports two deletion modes: soft deletes that preserve history and hard deletes that permanently remove records. Soft deletes are the default behavior and preferred for most use cases since they maintain audit trails while allowing records to be filtered out from normal queries.</p>"},{"location":"guide/concepts/deletions/#soft-deletes","title":"Soft deletes","text":"<p>Soft deletes mark records as deleted without physically removing them. When you call <code>delete</code>, Metaxy appends a new row with the <code>metaxy_deleted_at</code> system column set to the deletion timestamp. This preserves your full history\u2014nothing is lost, and you can always query for soft-deleted records if needed.</p> <p>By default, <code>read</code> filters out soft-deleted records automatically. You only see active data. Behind the scenes, Metaxy keeps the latest version of each record by coalescing deletion and creation timestamps, so even if you've updated a record multiple times, queries return only the current state.</p> <pre><code>import narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=nw.col(\"status\") == \"pending\",\n    )\n\nwith store:\n    active = store.read(MyFeature)\n    all_rows = store.read(MyFeature, include_soft_deleted=True)\n</code></pre> <p>If you track custom deletion flags in your feature schema, filter them through <code>read</code> filters.</p>"},{"location":"guide/concepts/deletions/#hard-deletes","title":"Hard deletes","text":"<p>Hard deletes permanently remove rows from storage. Use them when you need to physically delete data, such as for compliance requirements or to reclaim space. Pass <code>soft=False</code> to <code>delete</code> and specify which records to remove with a filter expression.</p> <pre><code>import narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=nw.col(\"quality\") &lt; 0.8,\n        soft=False,\n    )\n</code></pre> <p>Not all metadata stores support hard deletes. If your store doesn't support them, you'll get a <code>NotImplementedError</code>.</p>"},{"location":"guide/concepts/deletions/#cli-workflows","title":"CLI workflows","text":"<p>Run cleanups from the command line using <code>metaxy metadata delete</code>:</p> <pre><code># Soft delete by default\nmetaxy metadata delete --feature predictions --filter \"confidence &lt; 0.3\"\n\n# Hard delete\nmetaxy metadata delete --feature predictions --filter \"created_at &lt; '2024-01-01'\" --soft=false\n</code></pre> <p>Learn more in the CLI reference</p>"},{"location":"guide/concepts/filters/","title":"Specifying Filters As Text","text":"<p>There are a few occasions with Metaxy where users may want to define custom filter expressions via text, mainly being CLI arguments or configuration files. For this purpose, Metaxy implements <code>parse_filter_string</code>, which converts SQL-like <code>WHERE</code> clauses into Narwhals filter expressions.</p> <p>The following syntax is supported:</p> <ul> <li> <p>Comparisons: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></p> </li> <li> <p>Logical operators: <code>AND</code>, <code>OR</code>, <code>NOT</code></p> </li> <li> <p>Set membership: <code>IN</code>, <code>NOT IN</code></p> </li> <li> <p>Null checks: <code>IS NULL</code>, <code>IS NOT NULL</code></p> </li> <li> <p>Parentheses for grouping</p> </li> <li> <p>Column references (identifiers or dotted paths)</p> </li> <li> <p>Literals: strings (<code>'value'</code>), numbers, booleans (<code>TRUE</code>/<code>FALSE</code>), and <code>NULL</code></p> </li> <li> <p>Implicit boolean columns (e.g., <code>NOT is_active</code>)</p> </li> </ul> <p>Example</p> <pre><code>import polars as pl\nimport narwhals as nw\nfrom metaxy.models.filter_expression import parse_filter_string\n\n# Create a sample Polars DataFrame\npdf = pl.DataFrame({\"age\": [10, 20, 30], \"status\": [\"active\", \"deleted\", \"active\"]})\ndf = nw.from_native(pdf)\n\n# Parse a SQL WHERE clause into a backend-agnostic Narwhals expression\nexpr = parse_filter_string(\"(age &gt; 25 OR age &lt; 18) AND status != 'deleted'\")\n\nresult = df.filter(expr)\n\nassert result[\"age\"].to_list() == [10, 30]\n</code></pre>"},{"location":"guide/concepts/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: <code>MetadataStore</code>. <code>MetadataStore</code> is implemented to satisfy storage design choices.</p> <p>All operations with metadata stores may reference features as one of the supported syntactic sugar alternatives. In practice, it is typically convenient to either use feature classes or stringified feature keys.</p> <p>Metadata accept Narwhals-compatible dataframes and return Narwhals dataframes. In practice, we have tested Metaxy with Pandas, Polars and Ibis dataframes.</p>"},{"location":"guide/concepts/metadata-stores/#instantiation","title":"Instantiation","text":"<p>There are generally two ways to create a <code>MetadataStore</code>. We are going to demonstrate both with DeltaLake as an example.</p> <ol> <li> <p>Using the Python API directly:</p> <pre><code>from metaxy.ext.metadata_stores.delta import DeltaMetadataStore\n\nstore = DeltaMetadataStore(root_path=\"/path/to/directory\")\n</code></pre> </li> <li> <p>Via Metaxy configuration:</p> <p>First, create a <code>metaxy.toml</code> file:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"/path/to/directory\"\n</code></pre> <p>Now the metadata store can be constructed from a <code>MetaxyConfig</code> instance.</p> Metaxy Already InitializedWith Metaxy Initialization <pre><code>import metaxy as mx\n\nconfig = mx.MetaxyConfig.get()\nstore = config.get_store(\"dev\")\n</code></pre> <p> <pre><code>import metaxy as mx\n\nconfig = mx.init()\nstore = config.get_store(\"dev\")\n</code></pre></p> </li> </ol> <p>Now the <code>store</code> is ready to be used. We'll also assume there is a <code>MyFeature</code> feature class (1) prepared.</p> <ol> <li>with <code>\"my/feature\"</code> key</li> </ol>"},{"location":"guide/concepts/metadata-stores/#writes","title":"Writes","text":"<p>In order to save metadata into a metadata store, you can use the <code>write</code> method:</p> <p>Example</p> <pre><code>with store.open(\"w\"):\n    store.write(MyFeature, df)\n</code></pre> <p>Subsequent writes effectively overwrite the previous metadata, while actually appending to the same table.</p> Flushing Metadata In The Background <p>Usually it's desired to write metadata to the metadata store as soon as it becomes available. This ensures the pipeline can resume processing after a failure and no data is lost. <code>BufferedMetadataWriter</code> can be used to achieve this: it writes metadata in real-time from a background thread.</p>"},{"location":"guide/concepts/metadata-stores/#reads","title":"Reads","text":"<p>Metadata can be retrieved using the <code>read</code> method:</p> <p>Example</p> <pre><code>with store.open(\"w\"):\n    df = store.write(\"my/feature\", df)  # string keys work as well\n\nwith store:\n    df = store.read(\"my/feature\")\n</code></pre> <p>By default, Metaxy drops historical records with the same feature version, which makes the <code>write</code>-<code>read</code> sequence idempotent for an outside observer.</p>"},{"location":"guide/concepts/metadata-stores/#increment-resolution","title":"Increment Resolution","text":"<p>Increments can be computed using the <code>resolve_update</code> method:</p> <p>Example</p> <p> <pre><code>with store.open(\"w\"):\n    inc = store.resolve_update(\"my/feature\")\n</code></pre></p> <p>The returned <code>Increment</code> (or <code>LazyIncrement</code>) holds fresh samples that haven't been processed yet, stale samples which require to be processed again, and orphaned samples which are no longer present in upstream features and may be deleted.</p> <p>Tip</p> <p>Root features (1) require the <code>samples</code> argument to be set as well, since Metaxy would not be able to load upstream metadata automatically.</p> <ol> <li>features that do not have upstream features</li> </ol> <p>It is up to the caller to decide how to handle the processing and potential deletion of orphaned samples.</p> <p>Once processing is complete, the caller is expected to call <code>MetadataStore.write</code> to record metadata about the processed samples.</p> <p>Where are increments computed?</p> <p>Learn more here.</p> <p>How are increments computed?</p> <p>Learn more here.</p>"},{"location":"guide/concepts/metadata-stores/#deletes","title":"Deletes","text":"<p>Metadata stores support deletions, which are not required during normal Metaxy operations (1).</p> <ol> <li>deletions might be necessary when working with expansion linear relationships and re-computing samples without changing the feature version</li> </ol> <p>Here is an example of how a deletion would look like:</p> <pre><code>from datetime import datetime, timedelta, timezone\n\nimport narwhals as nw\n\nwith store.open(\"w\"):\n    store.delete(\n        MyFeature,\n        filters=[nw.col(\"metaxy_created_at\") &lt; datetime.now(timezone.utc) - timedelta(days=30)],\n    )\n</code></pre> <p>Learn more about deletions here.</p>"},{"location":"guide/concepts/metadata-stores/#fallback-stores","title":"Fallback Stores","text":"<p>Metaxy metadata stores can be configured to pull missing metadata from another store. This is very useful for local and testing workflows, because it allows to avoid materializing the entire data pipeline locally. Instead, Metaxy stores can automatically pull missing metadata from production.</p> <p>Example Metaxy configuration:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"${HOME}/.metaxy/dev\"\nfallback_stores = [\"prod\"]\n\n[stores.prod]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"s3://my-prod-bucket/metaxy\"\n</code></pre> <p>Warning</p> <p>Currently, the \"missing metadata\" detection works by checking whether the feature table exists in the store. This works in conjunction with automatic table creation, but doesn't work if empty tables are pre-created by e.g. migration tooling or some kind of CI/CD workflows. This will be improved in the future.</p> <p>Metaxy doesn't mix metadata from different stores: either the entire feature is going to be pulled from the fallback store, or the primary store will be used.</p> <p>Fallback stores can be chained at arbitrary depth.</p>"},{"location":"guide/concepts/metadata-stores/#metadata-store-implementations","title":"Metadata Store Implementations","text":"<p>Metaxy provides ready <code>MetadataStore</code> implementations for popular databases and storage systems.</p>"},{"location":"guide/concepts/projects/","title":"Metaxy Projects","text":"<p>As the data processing pipeline grows, it often becomes necessary to split it into multiple Python projects.</p> <p>Metaxy has a Project system which helps with organizing Metaxy features into separate Python packages.</p>"},{"location":"guide/concepts/projects/#metaxy-project","title":"Metaxy Project","text":"<p>A Metaxy project is a collection of features defined in the same location: typically a Python package. By default, each Metaxy feature definition is assigned a project name based on the top-level Python module where it's defined in.</p> Project Name Inference <p> my_package.my_feature.py<pre><code>import metaxy as mx\n\nclass NewFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"new/feature\", id_columns=[\"id\"])):\n    id: str\n\nassert mx.get_feature_by_key(\"new/feature\").project == \"my_package\"\n</code></pre></p> <p>As long as all feature definitions are located in the same project, users won't need to interact with the project concept when using Metaxy, and a single global project exists implicitly.</p> <p>Once the codebase is split into multiple projects, certain Metaxy operations start to operate at the scope of a specific project. Users then need to explicitly set the current Metaxy project by using the <code>MetaxyConfig.project</code>, typically via the config file:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>project = \"my_package\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my_package\"\n</code></pre> <pre><code>METAXY_PROJECT=my_package\n</code></pre> <p>Some Metaxy CLI commands can only be executed within a specific project: for example, <code>metaxy push</code> must specify (1) the project to be serialized.</p> <ol> <li>or be able to infer</li> </ol>"},{"location":"guide/concepts/projects/#feature-discovery","title":"Feature Discovery","text":"<p>Because features can depend on features from other projects, it becomes necessary to register all the necessary feature definitions from different projects on the same global feature graph at runtime.</p> <p>There are two ways for Metaxy to register a feature definition:</p> <ol> <li> <p>From a Python class that inherits from <code>BaseFeature</code>. The feature definitions is registered in Metaxy as soon as the class is created.</p> </li> <li> <p>From a <code>metaxy.lock</code> file. This is advanced functionality only needed when working with external features. This happens automatically when calling <code>metaxy.init</code> and doesn't require any additional setup.</p> </li> </ol> <p>To assist with step (1) and lift the burden of manually importing all the required features, Metaxy provides two options to automate this process: config entry points and distribution entry points.</p>"},{"location":"guide/concepts/projects/#config-entry-points","title":"Config Entry Points","text":"<p>Module paths with Metaxy features can be specified in the Metaxy config:</p> metaxy.tomlpyproject.toml <pre><code>project = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <p>These entrypoints only take effect in the current Metaxy project.</p>"},{"location":"guide/concepts/projects/#distribution-entry-points","title":"Distribution Entry Points","text":"<p>Metaxy also supports automatically exposing feature definitions to other packages in the same Python environment. This can be achieved by setting <code>\"metaxy.project\"</code> distribution entry point. For example:</p> pyproject.toml<pre><code>[project.entry-points.\"metaxy.project\"]\nmy-key = \"my_package.features\"\n</code></pre> <p>Note</p> <p>Currently the name of the key (<code>my-key</code> in the example above) is not used by Metaxy and is not important.</p>"},{"location":"guide/concepts/syntactic-sugar/","title":"Syntactic Sugar","text":""},{"location":"guide/concepts/syntactic-sugar/#type-coercion-for-input-types","title":"Type Coercion For Input Types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent features, feature keys, their fields, and dependencies between them. But specifying all of these models can be very verbose and cumbersome.</p> <p>Because Metaxy loves its users, we provide syntactic sugar for simplified construction of these models, and various Metaxy APIs typically accept unions of equivalent types. Metaxy coerces them into canonical internal models.</p> <p>This is fully typed and only affects constructor arguments. Attributes on Metaxy objects will always return only the canonical type.</p>"},{"location":"guide/concepts/syntactic-sugar/#features","title":"Features","text":"<p>APIs that require feature references accept the following equivalent objects:</p> <ul> <li> <p>slash-separated strings: <code>\"my/feature\"</code></p> </li> <li> <p>sequence of strings: <code>[\"my\", \"feature\"]</code></p> </li> <li> <p><code>FeatureKey</code>: <code>mx.FeatureKey(\"my/feature\")</code></p> </li> <li> <p><code>FeatureSpec</code>: <code>mx.FeatureSpec(key=\"my/feature\", ...)</code></p> </li> <li> <p>[<code>BaseFeature</code>] types: <code>Myfeature</code>, where <code>Myfeature</code> is a subclass of <code>BaseFeature</code></p> </li> </ul>"},{"location":"guide/concepts/syntactic-sugar/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> can be constructed from:</p> <ul> <li> <p>slash-separated strings: <code>FeatureKey(\"prefix/feature\")</code></p> </li> <li> <p>sequence of strings: <code>FeatureKey([\"prefix\", \"feature\"])</code></p> </li> </ul> <p>Internally they are represented as a sequence of parts.</p>"},{"location":"guide/concepts/syntactic-sugar/#fields","title":"Fields","text":"<p><code>fields</code> argument of <code>FeatureSpec</code> can omit the full <code>FieldsSpec</code>:</p> <pre><code>import metaxy as mx\n\nspec = mx.FeatureSpec(\n    key=\"example/fields\",\n    id_columns=[\"id\"],\n    fields=[\"my/field\", mx.FieldSpec(key=\"field/with/version\", code_version=\"v1.2.3\")],\n)\n</code></pre>"},{"location":"guide/concepts/syntactic-sugar/#fields-mapping","title":"Fields Mapping","text":"<p>Metaxy uses a bunch of common sense heuristics automatically find parent fields by matching on their names. This is enabled by default. For example, using the same field names in upstream and downstream features will automatically create a dependency between these fields:</p> <pre><code>import metaxy as mx\n\n\nclass Parent(mx.BaseFeature, spec=mx.FeatureSpec(key=\"parent/feature\", id_columns=[\"id\"], fields=[\"my_field\"])):\n    id: str\n\n\nclass Child(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"child/feature\", id_columns=[\"id\"], deps=[Parent], fields=[\"my_field\"])\n):\n    id: str\n</code></pre> <p>is equivalent to:</p> <pre><code>import metaxy as mx\n\n\nclass Grandchild(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"grandchild/feature\",\n        id_columns=[\"id\"],\n        deps=[Child],\n        fields=[mx.FieldSpec(key=\"my_field\", deps=[mx.FieldDep(feature=Parent, fields=[\"my_field\"])])],\n    ),\n):\n    id: str\n</code></pre>"},{"location":"guide/concepts/versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static (1) and deterministic.</p> <ol> <li>Versions can be calculated ahead of time (before the data processing job is executed).</li> </ol> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"guide/concepts/versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them. They exist in Python at runtime, and are also serialized to the metadata store when <code>metaxy push</code> is called.</p>"},{"location":"guide/concepts/versioning/#field-level","title":"Field Level","text":"<p>Field Code Version is defined on the field and is provided by the user (defaults to <code>\"__metaxy_initial__\"</code>). Apart from overriding data versions, this is the only input to the versioning system that can be directly modified by the user.</p> <p>Code Version Value</p> <p>The value can be an arbitrary string, but in the future we might implement something around semantic versioning.</p> <p>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</p> Visualization <p></p>"},{"location":"guide/concepts/versioning/#feature-level","title":"Feature Level","text":"<p>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</p> Visualization <p></p> <p>This version is stored as <code>metaxy_feature_version</code> system column.</p> <p>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</p>"},{"location":"guide/concepts/versioning/#project-level","title":"Project Level","text":"<p>Project Version: is computed from the Feature Versions of all features in the Metaxy project.</p> Visualization <p></p> How is project version used? <p>This value is used to uniquely encode versioned feature graph topology. <code>metaxy push</code> CLI can be used to keep track of previous versions of the feature graph, enabling features such as data version reconciliation migrations.</p> <p>This version is stored as <code>metaxy_project_version</code> system column.</p>"},{"location":"guide/concepts/versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to be computed. They are stored separately for each row in the feature table.</p>"},{"location":"guide/concepts/versioning/#provenance","title":"Provenance","text":"<p>Provenance By Field is computed from the upstream Provenance By Field (with respect to defined field-level lineage and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</p> id metaxy_provenance_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <p>Sample Provenance is derived from the Provenance By Field by simply hashing it.</p> <p>Computing this value is the goal of the entire versioning engine. It's a string value that only changes when versions of the specific upstream fields the sample depends on change. It acts as source of truth for resolving incremental updates for feature metadata.</p> <p>Most of the time <code>metaxy_provenance_by_field</code> and <code>metaxy_provenance</code> are used for the final data version columns as is, except when the user wants to override the latter. These final versions are then recursively used to compute downstream provenances.</p> Visualization <p></p> <p>These versions are stored as <code>metaxy_provenance_by_field</code> and <code>metaxy_provenance</code> system columns.</p>"},{"location":"guide/concepts/versioning/#data-version","title":"Data Version","text":"<p>Users can override the computed sample-level versions (<code>metaxy_provenance_by_field</code>) by setting <code>metaxy_data_version_by_field</code> on their metadata, effectively providing a Data Version for the sample. This can be used for preventing unnecessary downstream updates, if the computed sample stays the same even after upstream data has changed. <code>metaxy_data_version_by_field</code> is then used to compute <code>metaxy_data_version</code> by hashing all the fields together.</p> <p>For example, the data version can be calculated by running <code>sha256</code> over the file, or a perceptual hashing method for images and videos.</p> <p>This customization only affects how downstream increments are calculated, as the data version cannot be known until the feature is computed.</p> <p>These versions are stored as <code>metaxy_data_version_by_field</code> and <code>metaxy_data_version</code> system columns.</p>"},{"location":"guide/concepts/versioning/#provenance-vs-data-version","title":"Provenance Vs Data Version","text":"<p>To summarize, <code>metaxy_provenance</code> and <code>metaxy_provenance_by_field</code> are used to determine whether the current feature has to be updated. Usually they are used for <code>metaxy_data_version</code> and <code>metaxy_data_version_by_field</code>, but the user can override this. These columns in turn are used to calculate provenances for downstream features.</p>"},{"location":"guide/concepts/versioning/#example-partial-data-updates","title":"Example: Partial Data Updates","text":"<p>This example makes use of Metaxy's syntactic sugar.</p> <p>Consider a video processing pipeline with these features:</p> <p><pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            mx.FieldSpec(key=\"frames\", code_version=\"1\"),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    video_id: str\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/crop\",\n        id_columns=[\"video_id\"],\n        deps=[Video],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),  # (1)!\n            mx.FieldSpec(key=\"frames\", code_version=\"1\"),  # (2)!\n        ],\n    ),\n):\n    video_id: str  # ID column\n\n\nclass FaceDetection(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/face_detection\",\n        id_columns=[\"video_id\"],\n        deps=[Crop],\n        fields=[\n            mx.FieldSpec(\n                key=\"faces\",\n                code_version=\"1\",\n                deps=[mx.FieldDep(feature=Crop, fields=[\"frames\"])],\n            ),\n        ],\n    ),\n):\n    video_id: str\n\n\nclass SpeechToText(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"example/stt\",\n        id_columns=[\"video_id\"],\n        deps=[Video],\n        fields=[\n            mx.FieldSpec(\n                key=\"transcription\",\n                code_version=\"1\",\n                deps=[mx.FieldDep(feature=Video, fields=[\"audio\"])],\n            ),\n        ],\n    ),\n):\n    video_id: str\n</code></pre></p> <ol> <li> <p>This <code>audio</code> field automatically depends on the <code>audio</code> field of the <code>\"example/video\"</code> feature, because their names match.</p> </li> <li> <p>This <code>frames</code> field automatically depends on the <code>frames</code> field of the <code>\"example/video\"</code> feature, because their names match.</p> </li> </ol> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart LR\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;c2ac395f&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (22742381)&lt;br/&gt;- frames (794116a9)&lt;/div&gt;\"]\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;34d75856&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- audio (4c726c4b)&lt;br/&gt;- frames (2419e09d)&lt;/div&gt;\"]\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;f1526ee0&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (006efeef)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;d953dea4&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- transcription (3ec3826d)&lt;/div&gt;\"]\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt\n</code></pre>"},{"location":"guide/concepts/versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>\"example/video\"</code> feature changes (1):</p> <ol> <li>\u0437erhaps, something like denoising has been applied</li> </ol> <code>patches/01_update_audio_version.patch</code> patches/01_update_audio_version.patch<pre><code>--- a/src/example_overview/features.py\n+++ b/src/example_overview/features.py\n@@ -14,7 +14,7 @@ class Video(\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n             FieldSpec(\n                 key=\"frames\",\n</code></pre> <p>Here is how the change affects feature and field versions through the feature graph:</p> <pre><code>---\ntitle: Feature Graph Changes\n---\nflowchart LR\n    %% Snapshot version: none\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;c2ac395f&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;2faffb98&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- frames (794116a9)&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;22742381&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;09c8398b&lt;/font&gt;)&lt;/div&gt;\"]\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;34d75856&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;fe237dc9&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- frames (2419e09d)&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;4c726c4b&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e2b6ce39&lt;/font&gt;)&lt;/div&gt;\"]\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;f1526ee0&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (006efeef)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#FF0000\"&gt;d953dea4&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;e57e7555&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font color=\"#FF0000\"&gt;3ec3826d&lt;/font&gt; \u2192 &lt;font color=\"#00FF00\"&gt;9f7ea40c&lt;/font&gt;)&lt;/div&gt;\"]\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt\n\n\n    style example_crop stroke:#FFAA00,stroke-width:2px\n    style example_face_detection stroke:#808080\n    style example_stt stroke:#FFAA00,stroke-width:2px\n    style example_video stroke:#FFAA00,stroke-width:2px</code></pre> <p>Info</p> <ul> <li> <p><code>\"example/video\"</code>, <code>\"example/crop\"</code>, and <code>\"example/stt\"</code> have changed</p> </li> <li> <p><code>\"example/face_detection\"</code> remained unchanged (depends only on <code>frames</code> and not on <code>audio</code>)</p> </li> <li> <p>Audio field versions have changed throughout the graph</p> </li> <li> <p>Frame field versions have stayed the same</p> </li> </ul>"},{"location":"guide/concepts/versioning/#incremental-computations","title":"Incremental Computations","text":"<p>The single most important piece of code in Metaxy is the <code>resolve_update</code> method. For a given feature, it takes the inputs (1), computes the expected provenances for the given feature, and compares it with the current state in the metadata store. Learn more about this process here.</p> <ol> <li>metadata from the upstream features</li> </ol> <p>The Python pipeline needs to handle the result of <code>resolve_update</code> call:</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes provenance_by_field and identifies changes\n    increment = store.resolve_update(DownstreamFeature)\n\n    # Process only the changed samples\n</code></pre> <p>The <code>increment</code> object has attributes for new upstream samples, samples identified as stale, and samples that have been removed from the upstream metadata.</p>"},{"location":"guide/concepts/definitions/external-features/","title":"External Features","text":""},{"location":"guide/concepts/definitions/external-features/#external-features","title":"External Features","text":"<p>Advanced Concept</p> <p>External features are an advanced way of organizing Metaxy feature definitions. It's only needed when depending on features from separate Python environments.</p> <p>External features are stubs pointing at features actually defined in other Metaxy projects and not available at runtime. External feature definitions are stored in a <code>metaxy.lock</code> file and are loaded automatically when <code>metaxy.init</code> is called.</p> Manually Registering External Features <p>Users who need more control or cannot use the <code>metaxy.lock</code> file for some reasons (1) can create them manually:</p> <pre><code>import metaxy as mx\n\nexternal_feature = mx.FeatureDefinition.external(\n    spec=mx.FeatureSpec(key=\"a/b/c\", id_columns=[\"id\"]),\n    project=\"external-project\",\n)\n\nmx.FeatureGraph.get().add_feature_definition(external_feature)\n</code></pre> <ol> <li>Please let us know why via a GitHub Issue</li> </ol>"},{"location":"guide/concepts/definitions/external-features/#metaxylock-file","title":"<code>metaxy.lock</code> file","text":"<p>Metaxy can automatically generate a <code>metaxy.lock</code> file with external feature definitions from other Metaxy projects. In order to do this:</p> <ol> <li> <p>Run <code>metaxy push</code> in other Metaxy projects - this will serialize feature definitions to the metadata store.</p> <p>Tip</p> <p>This command is expected to be executed as part of the CI/CD pipeline.</p> </li> <li> <p>Run <code>metaxy lock</code> in the current Metaxy project - this pulls external feature definitions from the metadata store into a <code>metaxy.lock</code> file. Only explicit feature dependencies are pulled in. Subsequent <code>metaxy.init</code> calls will now automatically add these feature definitions to the feature graph.</p> </li> </ol> <p>Multi-environment setup</p> <pre><code>root/\n\u251c\u2500\u2500 project_a/\n\u2502   \u251c\u2500\u2500 .venv/\n\u2502   \u251c\u2500\u2500 metaxy.toml\n\u2502   \u251c\u2500\u2500 metaxy.lock\n\u2502   \u2514\u2500\u2500 features.py\n\u2514\u2500\u2500 project_b/\n    \u251c\u2500\u2500 .venv/\n    \u251c\u2500\u2500 metaxy.toml\n    \u251c\u2500\u2500 metaxy.lock\n    \u2514\u2500\u2500 features.py\n</code></pre> <p>Features from both projects can freely depend on each other:</p> <p> project_b/features.py<pre><code>import metaxy as mx\n\nclass FeatureB(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        id_columns=[\"id\"],\n        deps=[\"feature/from/project/a\"]\n    )\n):\n    id: str\n</code></pre></p> <p>Then, the following commands:</p> <pre><code>cd project_a &amp;&amp; metaxy push\ncd project_b &amp;&amp; metaxy lock\n</code></pre> <p>will populate <code>project_b/metaxy.lock</code> with feature definition from <code>project_a</code>.</p>"},{"location":"guide/concepts/definitions/external-features/#syncing-external-features","title":"Syncing External Features","text":"<p>External features only exist on the feature graph until actual feature definitions are loaded from the metadata store to replace them. This can be done with <code>metaxy.sync_external_features</code>.</p> <pre><code>import metaxy as mx\n\n# Sync external features from the metadata store\nmx.sync_external_features(store)\n</code></pre> <p>Pydantic Schema Limitation</p> <p>Features loaded from the metadata store have their JSON schema preserved from when they were originally saved. However, the Python class may not be available anymore. Operations that require the actual Python class, such as model instantiation or validation, will not work for these features.</p> <p>Users are expected to keep the lock file up to date, but Metaxy has a few safe guards to protect users from using stale external features. If the actual feature pulled from the metadata store has a different version than the one in the lock file, <code>sync_external_features</code> emits warnings. An exception can be raised instead by:</p> <ul> <li> <p>passing <code>on_conflict=\"raise\"</code> to <code>sync_external_features</code></p> </li> <li> <p>passing <code>--locked</code> to Metaxy CLI commands</p> </li> <li> <p>setting <code>locked</code> to <code>True</code> in the global Metaxy configuration. This can be done either in the config file or via the <code>METAXY_LOCKED</code> environment variable.</p> </li> </ul> <p>Tip</p> <p>We strongly recommend setting <code>METAXY_LOCKED=1</code> in production</p> <p>Additionally, the following actions always trigger <code>sync_external_features</code>:</p> <ul> <li> <p>pushing feature definitions to the metadata store (e.g. <code>metaxy push</code> CLI)</p> </li> <li> <p><code>MetadataStore.read</code></p> </li> <li> <p><code>MetadataStore.resolve_update</code></p> </li> </ul> <p>Disabling Automatic Syncing</p> <p>This behavior can be disabled by setting <code>sync=False</code> in the global Metaxy configuration. However, we advise to keep it enabled, because <code>sync_external_features</code> is very lightweight on the first call and a no-op on subsequent calls. It only does anything if the current feature graph does not contain any external features.</p>"},{"location":"guide/concepts/definitions/features/","title":"Features","text":"<p>Metaxy has a declarative  feature system inspired by Dagster's Software-Defined Assets and Nix.</p> <p>Metaxy is responsible for providing correct metadata to users. Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage.</p> Data vs Metadata Clarifications <p>Metaxy features represent tabular metadata, typically containing references to external multimodal data such as files, images, or videos.</p> Subject Description Data The actual multimodal data itself, such as images, audio files, video files, text documents, and other raw content that your pipelines process and transform. Metadata Information about the data, typically including references to where data is stored (e.g., object store keys), plus additional descriptive entries such as video length, file size, format, version, and other attributes. <p>Metaxy does not interact with data and is not responsible for its content. As an edge case, Metaxy may also manage pure metadata tables that do not reference any external data.</p> <p>Keeping Historical Data</p> <p>Include <code>metaxy_data_version</code> in your data path to avoid collisions between different versions of the same data sample. Doing this will ensure that newer samples are never written over older ones.</p>"},{"location":"guide/concepts/definitions/features/#feature-definitions","title":"Feature Definitions","text":"<p>These examples make use of Metaxy's syntactic sugar.</p> <p>To create a Metaxy feature, extend the <code>BaseFeature</code> class (1).</p> <ol> <li>It's a Pydantic model.</li> </ol> <pre><code>import metaxy as mx\n\n\nclass VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"raw/video\", id_columns=[\"video_id\"])):\n    path: str\n</code></pre> <p>Abstract</p> <p>Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p> <p>Users must provide one or more ID columns (1) to <code>FeatureSpec</code>, telling Metaxy how to uniquely identify feature samples.</p> <ol> <li>ID columns are almost a primary key. The difference is quite subtle: Metaxy may interact with storage systems which do not technically have the concept of a primary key and may allow multiple rows to have the same ID columns (which are deduplicated by Metaxy).</li> </ol> <p>Since <code>\"raw/video\"</code> is a root feature, it doesn't have any dependencies.</p> <p>That's it! Easy.</p> <p>Why classes?</p> <p>Some of the tooling Metaxy is aiming to integrate with, such as SQLModel or Lance are using class-based table definitions. It was practical to start from this interface, since it's somewhat more complicated to implement and support. More feature definition and registration methods are likely to be introduced in the future, since Metaxy doesn't use the class information in any way (1). Additionally, users may want to construct instances of these Pydantic classes, and Pydantic can be used for data validation and type safety. We will explore other interfaces in <code>anam-org/metaxy#800</code>.</p> <ol> <li>That's a little lie. The Dagster integration uses the original class to extract the table schema for visualization purposes, but we are exploring alternative solutions in <code>anam-org/metaxy</code></li> </ol> <p>Tip</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(key=\"processed/transcript\", id_columns=[\"video_id\"], deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> The God <code>FeatureGraph</code> object <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly).</p> <p>Hurray! You get the idea.</p>"},{"location":"guide/concepts/definitions/features/#field-level-lineage","title":"Field-Level Lineage","text":"<p>A core (1) feature of Metaxy is the concept of field-level lineage. These are used to define dependencies between logical fields of features.</p> <ol> <li>really a killer </li> </ol> <p>Abstract</p> <p>A Metaxy field is not to be confused with metadata column. Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy. (1)</p> <ol> <li>columns can be defined with Pydantic fields </li> </ol> <p>Fields refer to data and are purely logical - users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>\"raw/video\"</code> feature - an <code>.mp4</code> file - may have <code>frames</code> and <code>audio</code> fields.</p> <p>At this point, careful readers have probably noticed that the <code>\"processed/transcript\"</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express this with Metaxy:</p> <pre><code>class Transcript(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(key=\"processed/transcript\", id_columns=[\"video_id\"], fields=[\n        mx.FieldSpec(\n            key=\"text\",\n            deps=[mx.FieldDep(feature=VideoFeature, fields=[\"audio\"])],\n        )\n    ],),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> <p>The Data Versioning docs explain more about how Metaxy calculates versions for different components of a feature graph.</p>"},{"location":"guide/concepts/definitions/features/#attaching-custom-metadata","title":"Attaching custom metadata","text":"<p>Users can attach arbitrary JSON-like metadata dictionary to feature specs, typically used for declaring ownership, providing information to third-party tooling, or documentation purposes. This metadata does not influence graph topology or the versioning system.</p>"},{"location":"guide/concepts/definitions/optional-dependencies/","title":"Optional Dependencies","text":"<p>By default, Metaxy assumes that every upstream feature must have a matching sample in order to materialize a downstream one. This means that samples with at least one missing upstream sample are not included in the result of <code>MetadataStore.resolve_update</code>.</p> <p>To customize this behavior, it is possible to mark specific upstream features as optional:</p> <pre><code>import metaxy as mx\n\n\nclass RawVideo(mx.BaseFeature, spec=mx.FeatureSpec(key=\"raw/video\", id_columns=[\"video_id\"])):\n    path: str\n\n\nclass AudioTranscript(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/transcript\", id_columns=[\"video_id\"], deps=[RawVideo])\n):\n    text: str\n\n\nclass EnrichedVideo(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"enriched/video\",\n        id_columns=[\"video_id\"],\n        deps=[\n            mx.FeatureDep(feature=RawVideo),\n            mx.FeatureDep(feature=AudioTranscript, optional=True),\n        ],\n    ),\n):\n    pass\n</code></pre> <p>In this example, even if some audio transcripts have not been extracted, the downstream feature will still be allowed to process these samples, leaving it to the user to decide how to handle missing data.</p>"},{"location":"guide/concepts/definitions/relationship/","title":"Lineage Relationships","text":"<p>Metaxy supports a few common mappings from parent to child samples out of the box. These include:</p> <ul> <li> <p><code>1:1</code> mapping with <code>LineageRelationship.identity</code> (the default one)</p> </li> <li> <p><code>1:N</code> mapping with <code>LineageRelationship.expansion</code></p> </li> <li> <p><code>N:1</code> mapping with <code>LineageRelationship.aggregation</code></p> </li> </ul> <p>Tip</p> <p>Always use these classmethods to create instances of lineage relationships. They use Pydantic's discriminated unions under the hood to ensure correct type construction.</p>"},{"location":"guide/concepts/definitions/relationship/#examples","title":"Examples","text":"<ul> <li>1:N expansion</li> <li>N:1 aggregation</li> </ul>"},{"location":"guide/concepts/lifecycle/deployment/","title":"Deployment","text":""},{"location":"guide/concepts/lifecycle/deployment/#production","title":"Production","text":"<ol> <li> <p>Make sure to set <code>METAXY_STORE</code> to <code>\"prod\"</code> in your production environment. This will make it the default Metaxy metadata store without any code changes.</p> </li> <li> <p>Add the following step to your production deployment pipeline:</p> <pre><code>mx push --store prod\n</code></pre> <p>This will persist feature definitions in the metadata store, enabling feature history tracking and multi-project setups.</p> </li> </ol> <p>Tip</p> <p>We strongly recommend setting <code>METAXY_LOCKED</code> to <code>true</code> in production when using external features.</p>"},{"location":"guide/concepts/lifecycle/deployment/#branch-deployments","title":"Branch Deployments","text":"<p>Branch Deployments, also known as Preview (1) Deployments, are ephemeral environments typically created to test changes in a production-like setting. They are usually created by CI/CD for Pull Requests. Some tooling like Dagster has built-in support for Branch Deployments, and so does Metaxy.</p> <ol> <li>or Review Environments, or Feature Branches, or whatever</li> </ol> <p>In order to benefit from Branch Deployments, it is recommended to configure a separate store, templated by some kind of a deployment identifier. For example:</p> metaxy.toml<pre><code>[stores.branch]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"s3://branch-bucket/${PULL_REQUEST_ID}\"\nfallback_stores = [\"prod\"]\n\n[stores.prod] title=\"metaxy.toml\"\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"s3://my-prod-bucket/metaxy\"\n</code></pre> <p>It is beneficial to have it fallback to <code>prod</code> store to avoid having to materialize all upstream features to a given one of interest (that's being tested).</p> <p>Of course, the Branch Deployment CD needs to do:</p> <pre><code>mx push --store branch\n</code></pre> <p>And make sure to set <code>METAXY_STORE</code> to <code>\"branch\"</code> in the branch deployment environment.</p>"},{"location":"guide/concepts/lifecycle/development/","title":"Local Development","text":"<p>Metaxy supports local-first development workflows.</p> <p>It all starts from the metadata store. The default metadata store name in Metaxy configuration is <code>\"dev\"</code> (1). Configure it in the config file:</p> <ol> <li>of course, it can be tweaked to something like <code>\"local\"</code></li> </ol> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"${HOME}/.metaxy/dev\"\n</code></pre> <p>Metaxy APIs and CLI commands will automatically use the default store unless specified otherwise. It is a good practice to rely on the default store detection in order to easily swap it to another store in other environments:</p> <pre><code>import metaxy as mx\n\nstore = mx.init().get_store()\n</code></pre>"},{"location":"guide/concepts/lifecycle/development/#using-the-metaxy-cli","title":"Using the Metaxy CLI","text":"<p>Metaxy provides a CLI which is useful for local development. Here are some of the things you can do with it:</p> <ul> <li><code>mx list features</code> - view the features available via feature discovery</li> </ul>"},{"location":"guide/concepts/lifecycle/development/#fallback-store","title":"Fallback Store","text":"<p>You'll probably want to configure the <code>dev</code> store to pull missing data from production. Configure <code>fallback_stores</code> in order to achieve this:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"${HOME}/.metaxy/dev\"\nfallback_stores = [\"prod\"]\n\n[stores.prod]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\nroot_path = \"s3://my-prod-bucket/metaxy\"\n</code></pre>"},{"location":"guide/concepts/lifecycle/testing/","title":"Testing Metaxy Code","text":"<p>This guide covers patterns for testing your Metaxy code. As always, Metaxy must be explicitly initialized with <code>init</code>:</p> <pre><code>import metaxy as mx\n\nmx.init()\n</code></pre> <p>This is typically done in a <code>pytest</code> fixture.</p>"},{"location":"guide/concepts/lifecycle/testing/#ephemeral-configuration","title":"Ephemeral Configuration","text":"<p>The current Metaxy configuration is available via a <code>MetaxyConfig.get()</code> singleton. It is often desired to provide custom Metaxy configuration in tests.</p> <p>This can be achieved by constructing a <code>MetaxyConfig</code> instance and activating it via a context manager. It's best if this setup is performed via <code>pytest</code> fixtures:</p> <pre><code>import pytest\nimport metaxy as mx\n\n\n@pytest.fixture(autouse=True)\ndef metaxy_config():\n    with mx.MetaxyConfig(project=\"my-project\").use() as config:\n        yield config\n\n\ndef test_my_config():\n    assert mx.MetaxyConfig.get().project == \"my-project\"\n</code></pre> <p>The config object can be explicitly passed to <code>init</code>. This can be used to adjust how feature discovery is performed.</p>"},{"location":"guide/concepts/lifecycle/testing/#configuring-metaxy-plugins","title":"Configuring Metaxy Plugins","text":"<p>Plugins can be configured via a dictionary where keys are plugin names and values are plugin-specific configuration objects:</p> <pre><code>from metaxy.config import MetaxyConfig\nfrom metaxy.ext.sqlmodel import SQLModelPluginConfig\n\nwith MetaxyConfig(\n    ext={\n        \"sqlmodel\": SQLModelPluginConfig(\n            enable=True,\n            inject_primary_key=True,\n        )\n    }\n).use() as cfg:\n    sqlmodel_config = MetaxyConfig.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    assert sqlmodel_config.inject_primary_key is True\n</code></pre>"},{"location":"guide/concepts/lifecycle/testing/#multi-project-testing","title":"Multi-Project Testing","text":"<p>When working with multi-project setups, it's a good idea to provide an explicit path to the <code>metaxy.lock</code> file.</p>"},{"location":"guide/concepts/lifecycle/testing/#ephemeral-feature-graphs","title":"Ephemeral Feature Graphs","text":"<p>By default, Metaxy uses a single global feature graph where all features are registered. During testing, you might want to construct your own, clean and isolated feature graphs.</p>"},{"location":"guide/concepts/lifecycle/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>import pytest\nimport metaxy as mx\nfrom metaxy.models.feature import FeatureGraph\n\n\n@pytest.fixture(autouse=True)\ndef isolated_graph():\n    with FeatureGraph().use() as g:\n        yield g\n\n\ndef test_my_feature():\n    class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test/feature\", id_columns=[\"id\"])):\n        id: str\n\n    # Test operations here\n    assert isolated_graph.get_feature_definition(\"test/feature\") is not None\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one. Multiple graphs instances can be created at the same time, but only one will be used for feature registration.</p>"},{"location":"guide/concepts/lifecycle/testing/#testing-with-production-data","title":"Testing with production data","text":"<p>It's often a good idea to setup \"integration\" test for data by using real data samples from production. It's often unavoidable in data applications, as this production data may be nearly impossible to replicate or mock.</p> <p>To achieve this with Metaxy, configure fallback stores for your testing metadata store to pull upstream data from production.</p>"},{"location":"guide/concepts/lifecycle/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing <code>AUTO_CREATE_TABLES</code> Warnings","text":"<p>When using certain database-based (1) metadata stores with <code>auto_create_tables</code> set to <code>True</code>, Metaxy emits warnings to remind you not to use this in production. It may be desired to suppress these warnings in your test suite.</p> <ol> <li>pun intended</li> </ol> <p>To suppress these warnings in your test suite, use <code>pytest</code>'s <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but <code>pytest</code> filters it from test output.</p>"},{"location":"guide/quickstart/quickstart/","title":"Quickstart","text":"<p> View Source on GitHub</p>"},{"location":"guide/quickstart/quickstart/#first-metaxy-application","title":"First Metaxy Application","text":""},{"location":"guide/quickstart/quickstart/#1-install-metaxy","title":"1. Install Metaxy","text":"<p>Let's choose a backend for our first <code>MetadataStore</code>. A good option for local development is DeltaLake. Let's install it:</p> <pre><code>pip install 'metaxy[delta]'\n</code></pre> <p>Now the metadata store can be created as:</p> <pre><code>from metaxy.ext.metadata_stores.delta import DeltaMetadataStore\n\nstore = DeltaMetadataStore(\"/tmp/quickstart.delta\")\n</code></pre>"},{"location":"guide/quickstart/quickstart/#2-define-your-first-feature","title":"2. Define your first Feature","text":"<p>Any Metaxy project must have at least one root feature.</p> features.py<pre><code>import metaxy as mx\nfrom pydantic import Field\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video\",\n        id_columns=[\"id\"],\n    ),\n):\n    raw_video_path: str = Field(description=\"Path to the raw video file\")\n    id: str = Field(description=\"Unique identifier for the video\")\n    path: str = Field(description=\"Path to the processed video file\")\n</code></pre>"},{"location":"guide/quickstart/quickstart/#3-resolve-a-root-increment","title":"3. Resolve a root increment","text":"<p>Root features are a bit special. They are entry points into the Metaxy world. Because of that, will have to provide a <code>samples</code> argument to <code>MetadataStore.resolve_update</code>, which is typically not required for non-root features.</p> <p>Tip</p> <p>The only requirement for this dataframe is to have a <code>metaxy_provenance_by_field</code> column (1).</p> <ol> <li>and to have appropriate ID columns</li> </ol> pipeline.py<pre><code># Prepare a DataFrame with incoming metadata\nsamples = pl.DataFrame(\n    {\n        \"id\": [\"vid_001\", \"vid_002\", \"vid_003\"],\n        \"raw_video_path\": [\n            \"/data/raw/vid_001.mp4\",\n            \"/data/raw/vid_002.mp4\",\n            \"/data/raw/vid_003.mp4\",\n        ],\n        \"metaxy_provenance_by_field\": [\n            {\"default\": \"a1n892ja\"},  # can be a hash sum\n            {\"default\": \"2024-01-15T10:30:00Z\"},  # or a modified_on timestamp\n            {\"default\": \"v1.2.3\"},  # or just any string\n        ],\n    }\n)\n\nwith store:\n    increment = store.resolve_update(Video, samples=samples)\n</code></pre> <p>The <code>increment</code> object is an instance of <code>Increment</code> and contains three dataframes:</p> <ul> <li> <p><code>increment.new</code>: new samples which were not previously recorded</p> </li> <li> <p><code>increment.stale</code>: samples which were previously recorded but have now changed</p> </li> <li> <p><code>increment.orphaned</code>: samples which were previously recorded but are no longer present in the input <code>samples</code> DataFrame</p> </li> </ul> <p>It's up to you how to handle these dataframes. Usually there will be a processing step iterating over all the rows in <code>increment.new</code> and <code>increment.stale</code> (possibly in parallel, using something like Ray), while <code>increment.orphaned</code> may be used to cleanup the no longer needed data and metadata.</p> <p>These dataframes have pre-computed provenance columns which should not be modified and eventually should be written to the metadata store.</p> <p>Tip</p> <p>The dataframes will have a <code>metaxy_data_version</code> column which is recommended to be used for storage paths:</p> <p> pipeline.py<pre><code>to_process = pl.concat([increment.new.to_polars(), increment.stale.to_polars()])\n\nresult = []\nfor row in to_process.iter_rows(named=True):\n    path = f\"/data/processed/{row['id']}/{row['metaxy_data_version']}/video.mp4\"\n    result.append({**row, \"path\": path})\n</code></pre> </p>"},{"location":"guide/quickstart/quickstart/#4-record-metadata-for-processed-samples","title":"4. Record metadata for processed samples","text":"<p>Once done, write the metadata for the processed samples:</p> pipeline.py<pre><code>if result:\n    with store.open(\"w\"):\n        store.write(Video, pl.DataFrame(result))\n</code></pre> <p>Recorded samples will no longer be returned by <code>MetadataStore.resolve_update</code> during future pipeline runs, unless the incoming <code>metaxy_provenance_by_field</code> values are updated.</p> Flushing Metadata In The Background <p>Usually it's desired to write metadata to the metadata store as soon as it becomes available. This ensures the pipeline can resume processing after a failure and no data is lost. <code>BufferedMetadataWriter</code> can be used to achieve this: it writes metadata in real-time from a background thread.</p>"},{"location":"guide/quickstart/quickstart/#feature-dependencies","title":"Feature Dependencies","text":"<p>Now let's add a downstream feature. We can use <code>deps</code> field on <code>FeatureSpec</code> in order to do that. We will make a simple feature that extracts the audio track from a video.</p> features.py<pre><code>class Audio(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"audio\",\n        deps=[Video],\n        id_columns=[\"id\"],\n    ),\n):\n    id: str = Field(description=\"Unique identifier for the audio\")\n    path: str = Field(description=\"Path to the audio file\")\n</code></pre> <p>And call the familiar <code>resolve_update</code> API:</p> pipeline.py<pre><code>with store:\n    audio_increment = store.resolve_update(Audio)\n</code></pre> <p>That's all! The increment can be handled similarly to the <code>\"video\"</code> feature.</p>"},{"location":"guide/quickstart/quickstart/#advanced-feature-definitions","title":"Advanced Feature Definitions","text":"<p>Learn how to mark dependencies as optional, specify field-level versions and dependencies, lineage types, and other advanced Metaxy features in definitions docs.</p>"},{"location":"guide/quickstart/quickstart/#whats-next","title":"What's Next?","text":"<p>Here are a few more useful links:</p> <ul> <li>Learn more about Metaxy concepts</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Invoke <code>mx</code> CLI from your terminal</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul>"},{"location":"integrations/","title":"Metaxy Integrations","text":""},{"location":"integrations/#orchestration","title":"Orchestration","text":"<ul> <li> <p> Dagster</p> <p> Orchestration \u2022 Data Platform</p> <p>Seamlessly integrate Metaxy with Dagster with the power of <code>@metaxify</code> and the <code>MetaxyIOManager</code>.</p> <p>Recommended</p> <p>Metaxy has been built with Dagster in mind. This integration is the best way to organize, materialize and observe multiple Metaxy features at scale.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#metadata-stores","title":"Metadata Stores","text":"<p>Learn more about metadata stores here.</p> <ul> <li> <p>Google BigQuery BigQuery</p> <p> Database</p> <p>Use Google BigQuery - scalable serverless analytical database on GCP.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>ClickHouse ClickHouse</p> <p> Database</p> <p>Leverage the lightning-fast analytical ClickHouse database for large metadata volume and high-throughput setups.</p> <p>Recommended</p> <p>Ideal for production.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>Delta Lake Delta Lake</p> <p> Storage \u2022 Lakehouse</p> <p>Store metadata in Delta Lake format in local files or remote object stores (S3, GCS, and others). (1)</p> <p>Recommended</p> <p>Ideal for dev environments.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>DuckDB DuckDB</p> <p> Database \u2022  Storage</p> <p>Use DuckDB - a fast analytical database with support for local and remote compute.</p> <p>Warning</p> <p>Local DuckDB is not recommended for production due to parallel writes limitations.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>DuckDB DuckLake</p> <p> Storage \u2022 Lakehouse</p> <p>Use the very performant DuckLake lakehouse format for storing Metaxy metadata.</p> <p> Integration docs</p> </li> <li> <p>LanceDB LanceDB</p> <p> Database \u2022  Storage</p> <p>Use the multimodal LanceDB database or Lance storage format. (2)</p> <p> Integration docs</p> <p> API docs</p> </li> </ul> <ol> <li> <p>uses a local versioning engine implemented in Polars and <code>polars-hash</code></p> </li> <li> <p>uses a local versioning engine implemented in Polars and <code>polars-hash</code></p> </li> </ol>"},{"location":"integrations/#compute","title":"Compute","text":"<ul> <li> <p>Ray Ray</p> <p> Compute \u2022 Distributed</p> <p>Use Metaxy with Ray for distributed computing workloads.</p> <p> Integration docs</p> </li> </ul>"},{"location":"integrations/#plugins","title":"Plugins","text":"<ul> <li> <p>SQLAlchemy SQLAlchemy</p> <p> ORM \u2022 Database</p> <p>Retrieve SQLAlchemy URLs and <code>MetaData</code> for the current Metaxy project from Metaxy <code>MetadataStore</code> objects.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p> SQLModel SQLModel</p> <p> ORM \u2022 Database</p> <p>Adds <code>SQLModel</code> capabilities to <code>metaxy.BaseFeature</code> class.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#ai","title":"AI","text":"<ul> <li> <p> Claude Code</p> <p> AI \u2022 LLM</p> <p>Use Metaxy with Claude Code through the official plugin, providing the <code>/metaxy</code> skill and MCP tools.</p> <p> Integration docs</p> </li> <li> <p> MCP Server</p> <p> AI \u2022 LLM</p> <p>Expose Metaxy's feature graph and metadata store operations to AI assistants via the Model Context Protocol.</p> <p> Integration docs</p> </li> </ul>"},{"location":"integrations/ai/claude/","title":"Claude Code Plugin","text":"<p>A Claude Code plugin that provides additional context for working with Metaxy projects.</p>"},{"location":"integrations/ai/claude/#features","title":"Features","text":"<ul> <li><code>/metaxy</code> skill: Guidance on working with Metaxy, including feature definitions, versioning, and metadata stores</li> <li>MCP tools: Explore feature graphs and query metadata directly from Claude Code via the MCP server</li> </ul>"},{"location":"integrations/ai/claude/#installation","title":"Installation","text":"<pre><code>/plugin marketplace add anam-org/metaxy\n/plugin install metaxy\n</code></pre>"},{"location":"integrations/ai/claude/#requirements","title":"Requirements","text":"<p><code>uv</code> must be installed. The plugin starts the MCP server via <code>uv run</code> to use the project's Python environment.</p>"},{"location":"integrations/ai/mcp/","title":"MCP Server","text":"<p>The Model Context Protocol (MCP) server exposes Metaxy's feature graph and metadata store operations to AI assistants, enabling them to explore your feature definitions and query metadata.</p>"},{"location":"integrations/ai/mcp/#installation","title":"Installation","text":"<p>Install Metaxy with the <code>mcp</code> extra:</p> uvpip <pre><code>uv add metaxy[mcp]\n</code></pre> <pre><code>pip install metaxy[mcp]\n</code></pre>"},{"location":"integrations/ai/mcp/#running-the-server","title":"Running the Server","text":"<p>Run the MCP server from your Metaxy project directory:</p> <pre><code>metaxy mcp # (1)!\n</code></pre> <ol> <li>Use <code>uv run metaxy mcp</code> to run the server within the project's Python environment.</li> </ol> <p>The server uses the standard Metaxy configuration discovery, loading <code>metaxy.toml</code> from the current directory or parent directories.</p>"},{"location":"integrations/ai/mcp/#configuration","title":"Configuration","text":""},{"location":"integrations/ai/mcp/#claude-code","title":"Claude Code","text":"<p>Add the MCP server to your project's <code>.claude/settings.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"metaxy\": {\n      \"command\": \"metaxy\",\n      \"args\": [\"mcp\"]\n    }\n  }\n}\n</code></pre>"},{"location":"integrations/ai/mcp/#available-tools","title":"Available Tools","text":"<p>The MCP server provides the following tools:</p>"},{"location":"integrations/ai/mcp/#get_config","title":"<code>get_config</code>","text":"<p>Get the current Metaxy configuration as JSON.</p> <p>Returns:</p> <p>The full Metaxy configuration serialized as JSON, including all settings like project, store, entrypoints, stores, migrations_dir, etc.</p>"},{"location":"integrations/ai/mcp/#get_feature","title":"<code>get_feature</code>","text":"<p>Get the complete specification for a feature.</p> <p>Parameters:</p> Name Type Required Default Description <code>feature_key</code> <code>str</code> Yes \u2014 Feature key in slash notation (e.g., \"video/processing\") <p>Returns:</p> <p>Complete feature specification as a dictionary including fields, dependencies, id_columns, and other metadata</p>"},{"location":"integrations/ai/mcp/#get_metadata","title":"<code>get_metadata</code>","text":"<p>Query metadata for a feature from a store.</p> <p>Parameters:</p> Name Type Required Default Description <code>feature_key</code> <code>str</code> Yes \u2014 Feature key in slash notation (e.g., \"video/processing\") <code>store_name</code> <code>str</code> Yes \u2014 Name of the metadata store to read from <code>columns</code> <code>list[str] | None</code> No <code>None</code> List of columns to select (None for all) <code>filters</code> <code>list[str] | None</code> No <code>None</code> List of SQL-like filter expressions (e.g., \"column &gt; 5\", \"name == 'foo'\") <code>with_feature_history</code> <code>bool</code> No <code>False</code> Only return current (non-superseded) rows <code>with_sample_history</code> <code>bool</code> No <code>False</code> Only return latest version of each row <code>include_soft_deleted</code> <code>bool</code> No <code>False</code> Include soft-deleted rows <code>allow_fallback</code> <code>bool</code> No <code>True</code> If True, check fallback stores when feature is not found in the primary store <code>sort_by</code> <code>list[str] | None</code> No <code>None</code> List of column names to sort by <code>descending</code> <code>bool | list[bool]</code> No <code>False</code> Sort descending (bool for all columns, or list per column) <code>limit</code> <code>int</code> No <code>50</code> Maximum number of rows to return <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li>columns: List of column names</li> <li>rows: List of row dictionaries</li> <li>total_rows: Number of rows returned</li> </ul>"},{"location":"integrations/ai/mcp/#get_store","title":"<code>get_store</code>","text":"<p>Get display information for a metadata store.</p> <p>Parameters:</p> Name Type Required Default Description <code>store_name</code> <code>str</code> Yes \u2014 Name of the store to get info for <p>Returns:</p> <p>Human-readable display string for the store (e.g., \"DuckDBMetadataStore(database=/tmp/db.duckdb)\")</p>"},{"location":"integrations/ai/mcp/#list_features","title":"<code>list_features</code>","text":"<p>List all registered features with their metadata.</p> <p>Matches the output format of <code>mx list features --format json</code>.</p> <p>Parameters:</p> Name Type Required Default Description <code>project</code> <code>str | None</code> No <code>None</code> Filter by project name (optional) <code>verbose</code> <code>bool</code> No <code>False</code> Include detailed field information and dependencies <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li>feature_count: Total number of features</li> <li>features: List of feature dictionaries with key, version, is_root, project, import_path, field_count, fields, and optionally deps</li> </ul>"},{"location":"integrations/ai/mcp/#list_stores","title":"<code>list_stores</code>","text":"<p>List all configured metadata stores.</p> <p>Returns:</p> <p>List of dictionaries with 'name' and 'type' for each store</p>"},{"location":"integrations/compute/ray/","title":"Ray integration for Metaxy","text":"<p>Metaxy has basic integration with Ray to assist with setting up Ray Data jobs.</p> <p>Ray Environment Setup</p> <p>It's critically important for Metaxy to resolve correct configuration and feature graph on the Ray worker.</p> <ul> <li> <p>ensure <code>METAXY_CONFIG</code> points to the correct Metaxy config file</p> </li> <li> <p>configure <code>worker_process_setup_hook</code> parameter of RuntimeEnv to run <code>metaxy.init</code> before anything else on the Ray worker</p> </li> </ul> Per-task setup <p>Additionally, <code>RAY_USER_SETUP_FUNCTION</code> can be configured to execute a Python function on every Ray task startup</p> <p>members: true</p>"},{"location":"integrations/compute/ray/#metaxy.ext.ray","title":"metaxy.ext.ray","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray-classes","title":"Classes","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink","title":"metaxy.ext.ray.MetaxyDatasink","text":"<pre><code>MetaxyDatasink(\n    feature: CoercibleToFeatureKey,\n    store: MetadataStore,\n    config: MetaxyConfig | None = None,\n)\n</code></pre> <p>               Bases: <code>Datasink[_WriteTaskResult]</code></p> <p>A Ray Data Datasink for writing to a Metaxy metadata store.</p> <p>Example</p> <pre><code>import metaxy as mx\nimport ray\n\ncfg = mx.init()\ndataset = ...  # a ray.data.Dataset\n\ndatasink = MetaxyDatasink(\n    feature=\"my/feature\",\n    store=cfg.get_store(),\n    config=cfg,\n)\ndataset.write_datasink(datasink)\n\nprint(f\"Wrote {datasink.result.rows_written} rows, {datasink.result.rows_failed} failed\")\n</code></pre> <p>Note</p> <p>In the future this Datasink will support writing multiple features at once.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to write metadata for.</p> </li> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to write to.</p> </li> <li> <code>config</code>               (<code>MetaxyConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration. Will be auto-discovered by the worker if not provided.</p> <p>Warning</p> <p>Ensure the Ray environment is set up properly when not passing <code>config</code> explicitly. This can be achieved by setting <code>METAXY_CONFIG</code> and other <code>METAXY_</code> environment variables. The best practice is to pass <code>config</code> explicitly to avoid surprises.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def __init__(\n    self,\n    feature: mx.CoercibleToFeatureKey,\n    store: mx.MetadataStore,\n    config: mx.MetaxyConfig | None = None,\n):\n    self.config = mx.init(config)\n\n    self.store = store\n    self.config = config\n\n    self._feature_key = mx.coerce_to_feature_key(feature)\n\n    # Populated after write completes\n    self._result: MetaxyWriteResult | None = None\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink-attributes","title":"Attributes","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.result","title":"result  <code>property</code>","text":"<pre><code>result: MetaxyWriteResult\n</code></pre> <p>Result of the write operation.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If accessed before the write operation completes.</p> </li> </ul>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink-functions","title":"Functions","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.write","title":"write","text":"<pre><code>write(\n    blocks: Iterable[Block], ctx: TaskContext\n) -&gt; _WriteTaskResult\n</code></pre> <p>Write blocks of metadata to the store.</p> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def write(\n    self,\n    blocks: Iterable[Block],\n    ctx: TaskContext,\n) -&gt; _WriteTaskResult:\n    \"\"\"Write blocks of metadata to the store.\"\"\"\n    # Initialize metaxy on the worker - config and features are needed for write\n    config = mx.init(self.config)\n    if config.sync:\n        mx.sync_external_features(self.store)\n\n    rows_written = 0\n    rows_failed = 0\n\n    for i, block in enumerate(blocks):\n        block_accessor = BlockAccessor.for_block(block)\n        num_rows = block_accessor.num_rows()\n\n        try:\n            with self.store.open(\"w\"):\n                self.store.write(self._feature_key, block)\n            rows_written += num_rows\n        except Exception:\n            logger.exception(\n                f\"Failed to write {num_rows} metadata rows for feature {self._feature_key.to_string()} block {i} of task {ctx.task_idx} ({ctx.op_name})\"\n            )\n            rows_failed += num_rows\n\n    return _WriteTaskResult(rows_written=rows_written, rows_failed=rows_failed)\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasink.on_write_complete","title":"on_write_complete","text":"<pre><code>on_write_complete(\n    write_result: WriteResult[_WriteTaskResult],\n) -&gt; None\n</code></pre> <p>Aggregate write statistics from all tasks.</p> Source code in <code>src/metaxy/ext/ray/datasink.py</code> <pre><code>def on_write_complete(self, write_result: WriteResult[_WriteTaskResult]) -&gt; None:\n    \"\"\"Aggregate write statistics from all tasks.\"\"\"\n    rows_written = 0\n    rows_failed = 0\n\n    for task_result in write_result.write_returns:\n        rows_written += task_result.rows_written\n        rows_failed += task_result.rows_failed\n\n    self._result = MetaxyWriteResult(rows_written=rows_written, rows_failed=rows_failed)\n\n    logger.info(\n        f\"MetaxyDatasink write complete for {self._feature_key.to_string()}: \"\n        f\"{rows_written} rows written, {rows_failed} rows failed\"\n    )\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource","title":"metaxy.ext.ray.MetaxyDatasource","text":"<pre><code>MetaxyDatasource(\n    feature: CoercibleToFeatureKey,\n    store: MetadataStore,\n    config: MetaxyConfig | None = None,\n    *,\n    incremental: bool = False,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n)\n</code></pre> <p>               Bases: <code>Datasource</code></p> <p>A Ray Data Datasource for reading from a Metaxy metadata store.</p> <p>This datasource reads metadata entries from a Metaxy metadata store as Ray Data blocks, associated with a specific feature key.</p> <p>Example</p> <pre><code>import metaxy as mx\nimport ray\n\ncfg = mx.init()\n\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n    )\n)\n</code></pre> <p>with filters and column selection</p> <pre><code>import narwhals as nw\n\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n        filters=[nw.col(\"value\") &gt; 10],\n        columns=[\"sample_uid\", \"value\"],\n    )\n)\n</code></pre> <p>incremental mode</p> <pre><code># Read only samples that need processing\nds = ray.data.read_datasource(\n    MetaxyDatasource(\n        feature=\"my/feature\",\n        store=cfg.get_store(),\n        config=cfg,\n        incremental=True,\n    )\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for.</p> </li> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to read from.</p> </li> <li> <code>config</code>               (<code>MetaxyConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration. Will be auto-discovered by the worker if not provided.</p> <p>Warning</p> <p>Ensure the Ray environment is set up properly when not passing <code>config</code> explicitly. This can be achieved by setting <code>METAXY_CONFIG</code> and other <code>METAXY_</code> environment variables. The best practice is to pass <code>config</code> explicitly to avoid any surprises.</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return only samples that need processing (new and stale). Adds a <code>metaxy_status</code> column with values:</p> <ul> <li> <p><code>\"new\"</code>: samples that have not been processed yet</p> </li> <li> <p><code>\"stale\"</code>: samples that have been processed but have to be reprocessed</p> </li> </ul> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply.</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on main store miss.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature version.</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature version to filter by (mutually exclusive with <code>with_feature_history=False</code>).</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> <li> <code>include_soft_deleted</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, include soft-deleted rows in the result.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def __init__(\n    self,\n    feature: mx.CoercibleToFeatureKey,\n    store: mx.MetadataStore,\n    config: mx.MetaxyConfig | None = None,\n    *,\n    incremental: bool = False,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n):\n    self.config = mx.init(config)\n    self.store = store\n    self.incremental = incremental\n    self.feature_version = feature_version\n    self.filters = list(filters) if filters else None\n    self.columns = list(columns) if columns else None\n    self.allow_fallback = allow_fallback\n    self.with_feature_history = with_feature_history\n    self.with_sample_history = with_sample_history\n    self.include_soft_deleted = include_soft_deleted\n\n    self._feature_key = mx.coerce_to_feature_key(feature)\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource-functions","title":"Functions","text":""},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource.get_read_tasks","title":"get_read_tasks","text":"<pre><code>get_read_tasks(\n    parallelism: int, per_task_row_limit: int | None = None\n) -&gt; list[ReadTask]\n</code></pre> <p>Return read tasks for the feature metadata.</p> <p>Parameters:</p> <ul> <li> <code>parallelism</code>               (<code>int</code>)           \u2013            <p>Requested parallelism level (currently ignored, returns single task).</p> </li> <li> <code>per_task_row_limit</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum rows per returned block. If set, the data will be split into multiple blocks of at most this size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[ReadTask]</code>           \u2013            <p>List containing a single ReadTask that may return multiple blocks.</p> </li> </ul> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def get_read_tasks(self, parallelism: int, per_task_row_limit: int | None = None) -&gt; list[ReadTask]:\n    \"\"\"Return read tasks for the feature metadata.\n\n    Args:\n        parallelism: Requested parallelism level (currently ignored, returns single task).\n        per_task_row_limit: Maximum rows per returned block. If set, the data will be\n            split into multiple blocks of at most this size.\n\n    Returns:\n        List containing a single ReadTask that may return multiple blocks.\n    \"\"\"\n    num_rows = self._get_row_count()\n\n    # Capture self for the closure\n    datasource = self\n    row_limit = per_task_row_limit\n\n    def read_fn() -&gt; list[pa.Table]:\n        mx.init(datasource.config)\n\n        with datasource.store:\n            lf = datasource._read_lazy()\n            table = lf.collect(backend=\"pyarrow\").to_arrow()\n            batches = table.to_batches(max_chunksize=row_limit)\n            return [pa.Table.from_batches([b]) for b in batches]\n\n    metadata = BlockMetadata(\n        num_rows=num_rows,\n        size_bytes=None,\n        input_files=None,\n        exec_stats=None,\n    )\n\n    return [ReadTask(read_fn, metadata)]\n</code></pre>"},{"location":"integrations/compute/ray/#metaxy.ext.ray.MetaxyDatasource.estimate_inmemory_data_size","title":"estimate_inmemory_data_size","text":"<pre><code>estimate_inmemory_data_size() -&gt; int | None\n</code></pre> <p>Return an estimate of in-memory data size, or None if unknown.</p> Source code in <code>src/metaxy/ext/ray/datasource.py</code> <pre><code>def estimate_inmemory_data_size(self) -&gt; int | None:\n    \"\"\"Return an estimate of in-memory data size, or None if unknown.\"\"\"\n    return None\n</code></pre>"},{"location":"integrations/metadata-stores/","title":"Metadata Stores","text":"<p>Metadata stores may come in two flavors.</p>"},{"location":"integrations/metadata-stores/#database-backed","title":"Database-Backed","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible. (1)</p> <ol> <li> <p> Typically (1) the entire <code>MetadataStore.resolve_update</code> can be executed externally!</p> </li> <li> <p>Except the cases enumerated in [../../guide/concepts/metadata-stores.md]</p> </li> </ol> <p>These metadata stores can be found here.</p> <p>Warning</p> <p>Metaxy does not handle infrastructure setup. Make sure to have large tables partitioned as appropriate for your use case.</p> <p>Example</p> <p>ClickHouse is an excellent choice for a production metadata store.</p> <p>Tip</p> <p>Some of them such as LanceDB or DuckDB can also act as local compute engines.</p>"},{"location":"integrations/metadata-stores/#storage-only","title":"Storage Only","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>The available storage-only stores can be found here.</p> <p>Example</p> <p>DeltaLake is an excellent choice for a storage-only metadata store.</p>"},{"location":"integrations/metadata-stores/#choosing-the-right-metadata-store","title":"Choosing the Right Metadata Store","text":"<p>Compute-backed stores are typically more performant, but require additional infrastructure and maintenance.</p> <p>For production environments that need to handle big metadata volumes, consider database-backed stores.</p> <p>For development, testing, branch deployments, and other scenarios where you want to keep things simple, consider using a storage-only store.</p> <p>Warning</p> <p>Not all metadata stores support parallel writes. For example, using DuckDB with files requires application level work-arounds.</p>"},{"location":"integrations/metadata-stores/#reference","title":"Reference","text":"<ul> <li>Learn more about using metadata stores</li> </ul>"},{"location":"integrations/metadata-stores/databases/","title":"Database-Backed Metadata Stores","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible (typically the entire <code>MetadataStore.resolve_update</code> can be executed externally).</p>"},{"location":"integrations/metadata-stores/databases/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>Metaxy + BigQuery</li> <li>Metaxy + ClickHouse</li> <li>Metaxy + DuckDB</li> <li>Metaxy + Ibis</li> <li>Metaxy + LanceDB</li> </ul>"},{"location":"integrations/metadata-stores/databases/bigquery/","title":"Metaxy + BigQuery","text":"<p>Experimental</p> <p>This functionality is experimental.</p> <p>BigQuery is a serverless data warehouse managed by Google Cloud. To use Metaxy with BigQuery, configure <code>BigQueryMetadataStore</code>. Versioning computations run natively in BigQuery.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[bigquery]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#api-reference","title":"API Reference","text":""},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery","title":"metaxy.ext.metadata_stores.bigquery","text":"<p>BigQuery metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStore","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStore","text":"<pre><code>BigQueryMetadataStore(\n    project_id: str | None = None,\n    dataset_id: str | None = None,\n    *,\n    credentials_path: str | None = None,\n    credentials: Any | None = None,\n    location: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>BigQuery metadata store using Ibis backend.</p> Warning <p>It's on the user to set up infrastructure for Metaxy correctly. Make sure to have large tables partitioned as appropriate for your use case.</p> Note <p>BigQuery automatically optimizes queries on partitioned tables. When tables are partitioned (e.g., by date or ingestion time with _PARTITIONTIME), BigQuery will automatically prune partitions based on WHERE clauses in queries, without needing explicit configuration in the metadata store. Make sure to use appropriate <code>filters</code> when calling BigQueryMetadataStore.read.</p> Basic Connection <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n)\n</code></pre> With Service Account <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    credentials_path=\"/path/to/service-account.json\",\n)\n</code></pre> With Location Configuration <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    location=\"EU\",  # Specify data location\n)\n</code></pre> With Custom Hash Algorithm <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    hash_algorithm=HashAlgorithm.SHA256,  # Use SHA256 instead of default FARMHASH\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>project_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud project ID containing the dataset. Can also be set via GOOGLE_CLOUD_PROJECT environment variable.</p> </li> <li> <code>dataset_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>BigQuery dataset name for storing metadata tables. If not provided, uses the default dataset for the project.</p> </li> <li> <code>credentials_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to service account JSON file. Alternative to passing credentials object directly.</p> </li> <li> <code>credentials</code>               (<code>Any | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud credentials object. If not provided, uses default credentials from environment.</p> </li> <li> <code>location</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Default location for BigQuery resources (e.g., \"US\", \"EU\"). If not specified, BigQuery determines based on dataset location.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional Ibis BigQuery connection parameters. Overrides individual parameters if provided.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to <code>IbisMetadataStore</code></p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-bigquery not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither project_id nor connection_params provided</p> </li> </ul> Note <p>Authentication priority: 1. Explicit credentials or credentials_path 2. Application Default Credentials (ADC) 3. Google Cloud SDK credentials</p> <p>BigQuery automatically handles partition pruning when querying partitioned tables. If your tables are partitioned (e.g., by date or ingestion time), BigQuery will automatically optimize queries with appropriate WHERE clauses on the partition column.</p> Example <pre><code># Using environment authentication\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n)\n\n# Using service account\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    credentials_path=\"/path/to/key.json\",\n)\n\n# With location specification\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    location=\"EU\",\n)\n</code></pre> Source code in <code>src/metaxy/ext/metadata_stores/bigquery.py</code> <pre><code>def __init__(\n    self,\n    project_id: str | None = None,\n    dataset_id: str | None = None,\n    *,\n    credentials_path: str | None = None,\n    credentials: Any | None = None,\n    location: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [BigQuery](https://cloud.google.com/bigquery) metadata store.\n\n    Args:\n        project_id: Google Cloud project ID containing the dataset.\n            Can also be set via GOOGLE_CLOUD_PROJECT environment variable.\n        dataset_id: BigQuery dataset name for storing metadata tables.\n            If not provided, uses the default dataset for the project.\n        credentials_path: Path to service account JSON file.\n            Alternative to passing credentials object directly.\n        credentials: Google Cloud credentials object.\n            If not provided, uses default credentials from environment.\n        location: Default location for BigQuery resources (e.g., \"US\", \"EU\").\n            If not specified, BigQuery determines based on dataset location.\n        connection_params: Additional Ibis BigQuery connection parameters.\n            Overrides individual parameters if provided.\n        fallback_stores: Ordered list of read-only fallback stores.\n        **kwargs: Passed to [`IbisMetadataStore`][metaxy.metadata_store.ibis.IbisMetadataStore]\n\n    Raises:\n        ImportError: If ibis-bigquery not installed\n        ValueError: If neither project_id nor connection_params provided\n\n    Note:\n        Authentication priority:\n        1. Explicit credentials or credentials_path\n        2. Application Default Credentials (ADC)\n        3. Google Cloud SDK credentials\n\n        BigQuery automatically handles partition pruning when querying partitioned tables.\n        If your tables are partitioned (e.g., by date or ingestion time), BigQuery will\n        automatically optimize queries with appropriate WHERE clauses on the partition column.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Using environment authentication\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n        )\n\n        # Using service account\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            credentials_path=\"/path/to/key.json\",\n        )\n\n        # With location specification\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            location=\"EU\",\n        )\n        ```\n    \"\"\"\n    # Build connection parameters if not provided\n    if connection_params is None:\n        connection_params = self._build_connection_params(\n            project_id=project_id,\n            dataset_id=dataset_id,\n            credentials_path=credentials_path,\n            credentials=credentials,\n            location=location,\n        )\n\n    # Validate we have minimum required parameters\n    if \"project_id\" not in connection_params and project_id is None:\n        raise ValueError(\n            \"Must provide either project_id or connection_params with project_id. Example: project_id='my-project'\"\n        )\n\n    # Store parameters for display\n    self.project_id = project_id or connection_params.get(\"project_id\")\n    self.dataset_id = dataset_id or connection_params.get(\"dataset_id\", \"\")\n\n    # Initialize Ibis store with BigQuery backend\n    super().__init__(\n        backend=\"bigquery\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#configuration","title":"Configuration","text":"<p>Configuration for BigQueryMetadataStore.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStore\"\n\n[stores.dev.config]\nproject_id = \"my-project\"\ndataset_id = \"my_dataset\"\ncredentials_path = \"/path/to/service-account.json\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for BigQueryMetadataStore.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    project_id = \\\"my-project\\\"\\n    dataset_id = \\\"my_dataset\\\"\\n    credentials_path = \\\"/path/to/service-account.json\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    },\n    \"connection_string\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis connection string (e.g., 'clickhouse://host:9000/db').\",\n      \"title\": \"Connection String\"\n    },\n    \"backend\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis backend name (e.g., 'clickhouse', 'postgres', 'duckdb').\",\n      \"mkdocs_metaxy_hide\": true,\n      \"title\": \"Backend\"\n    },\n    \"connection_params\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Backend-specific connection parameters.\",\n      \"title\": \"Connection Params\"\n    },\n    \"table_prefix\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Optional prefix for all table names.\",\n      \"title\": \"Table Prefix\"\n    },\n    \"auto_create_tables\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"If True, create tables on open. For development/testing only.\",\n      \"title\": \"Auto Create Tables\"\n    },\n    \"project_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Google Cloud project ID containing the dataset.\",\n      \"title\": \"Project Id\"\n    },\n    \"dataset_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"BigQuery dataset name for storing metadata tables.\",\n      \"title\": \"Dataset Id\"\n    },\n    \"credentials_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Path to service account JSON file.\",\n      \"title\": \"Credentials Path\"\n    },\n    \"credentials\": {\n      \"anyOf\": [\n        {},\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Google Cloud credentials object.\",\n      \"title\": \"Credentials\"\n    },\n    \"location\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Default location for BigQuery resources (e.g., 'US', 'EU').\",\n      \"title\": \"Location\"\n    }\n  },\n  \"title\": \"BigQueryMetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nproject_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nproject_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__PROJECT_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ndataset_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ndataset_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATASET_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ncredentials_path = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ncredentials_path = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS_PATH=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ncredentials = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ncredentials = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nlocation = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nlocation = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LOCATION=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.fallback_stores","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.hash_algorithm","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.versioning_engine","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.connection_string","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.connection_string  <code>pydantic-field</code>","text":"<pre><code>connection_string: str | None = None\n</code></pre> <p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.connection_params","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.connection_params  <code>pydantic-field</code>","text":"<pre><code>connection_params: dict[str, Any] | None = None\n</code></pre> <p>Backend-specific connection parameters.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.table_prefix","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.table_prefix  <code>pydantic-field</code>","text":"<pre><code>table_prefix: str | None = None\n</code></pre> <p>Optional prefix for all table names.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.auto_create_tables","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.auto_create_tables  <code>pydantic-field</code>","text":"<pre><code>auto_create_tables: bool | None = None\n</code></pre> <p>If True, create tables on open. For development/testing only.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.project_id","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.project_id  <code>pydantic-field</code>","text":"<pre><code>project_id: str | None = None\n</code></pre> <p>Google Cloud project ID containing the dataset.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.dataset_id","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.dataset_id  <code>pydantic-field</code>","text":"<pre><code>dataset_id: str | None = None\n</code></pre> <p>BigQuery dataset name for storing metadata tables.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.credentials_path","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.credentials_path  <code>pydantic-field</code>","text":"<pre><code>credentials_path: str | None = None\n</code></pre> <p>Path to service account JSON file.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.credentials","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.credentials  <code>pydantic-field</code>","text":"<pre><code>credentials: Any | None = None\n</code></pre> <p>Google Cloud credentials object.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.location","title":"metaxy.ext.metadata_stores.bigquery.BigQueryMetadataStoreConfig.location  <code>pydantic-field</code>","text":"<pre><code>location: str | None = None\n</code></pre> <p>Default location for BigQuery resources (e.g., 'US', 'EU').</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/","title":"Metaxy + ClickHouse","text":"<p>ClickHouse is a (1) column-oriented OLAP database designed for real-time analytics. To use Metaxy with ClickHouse, configure <code>ClickHouseMetadataStore</code>. Versioning computations run natively in ClickHouse, making it well-suited for high-throughput production workloads.</p> <ol> <li>extremely fast</li> </ol>"},{"location":"integrations/metadata-stores/databases/clickhouse/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[clickhouse]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxys-versioning-struct-columns","title":"Metaxy's Versioning Struct Columns","text":"<p>Metaxy uses struct columns (<code>metaxy_provenance_by_field</code>, <code>metaxy_data_version_by_field</code>) to track field-level versioning. In Python world this corresponds to <code>dict[str, str]</code>. In ClickHouse, there are several options to represent these columns.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#how-clickhouse-handles-structs","title":"How ClickHouse Handles Structs","text":"<p>ClickHouse offers multiple approaches to represent Metaxy's structured versioning columns:</p> Type Description Use Case <code>Map(String, String)</code> Native key-value map Recommended for Metaxy because of dynamic keys <code>JSON</code> Native JSON with typed subcolumns Less performant than <code>Map(String, String)</code> but more flexible than <code>Nested</code> <code>Nested(field_1 String, ...)</code> Static struct with named fields More performant than <code>Map(String, String)</code> but keys are static <p>Recommended: <code>Map(String, String)</code></p> <p>For Metaxy's <code>metaxy_provenance_by_field</code> and <code>metaxy_data_version_by_field</code> columns, use <code>Map(String, String)</code>:</p> <ul> <li> <p>No migrations required when feature fields change</p> </li> <li> <p>Good performance for key-value lookups</p> </li> </ul> <p>Special Map columns handling</p> <p>Metaxy transforms its system columns (<code>metaxy_provenance_by_field</code>, <code>metaxy_data_version_by_field</code>):</p> <ul> <li> <p>Reading: System Map columns are converted into Ibis Structs (e.g., <code>Struct[{\"field_a\": str, \"field_b\": str}]</code>)</p> </li> <li> <p>Writing: If the input comes from Polars, then Polars Structs are converted into expected ClickHouse Map format</p> </li> </ul> <p>User-defined Map columns are not transformed. They remain as <code>List[Struct[{\"key\": str, \"value\": str}]]</code> (Arrow's Map representation). Make sure to use the right format when providing a Polars DataFrame for writing.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#sqlalchemy-and-alembic-migrations","title":"SQLAlchemy and Alembic Migrations","text":"<p>For SQLAlchemy and Alembic migrations support, use the <code>clickhouse-sqlalchemy</code> driver with the native protocol:</p> <pre><code>pip install clickhouse-sqlalchemy\n</code></pre> <p>Use Native Clickhouse Protocol</p> <p>The HTTP protocol has limited reflection support. Always use the native protocol (<code>clickhouse+native://</code>) for full SQLAlchemy/Alembic compatibility:</p> <pre><code>connection_string = \"clickhouse+native://user:pass@localhost:9000/default\"\n</code></pre> <p>The <code>ClickHouseMetadataStore.sqlalchemy_url</code> property is tweaked to return the native connection string variant.</p> Alternative: ClickHouse Connect <p>Alternatively, use the official <code>clickhouse-connect</code> driver.</p> <p>Alembic Integration</p> <p>See Alembic setup guide for additional instructions on how to use Alembic with Metaxy.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#performance-optimization","title":"Performance Optimization","text":"<p>Table Design</p> <p>For optimal query performance, create your ClickHouse tables with:</p> <ul> <li>Partitioning: Partition your tables!</li> <li>Ordering: It's probably a good idea to use <code>(metaxy_feature_version, &lt;id_columns&gt;, metaxy_updated_at)</code></li> </ul>"},{"location":"integrations/metadata-stores/databases/clickhouse/#api-reference","title":"API Reference","text":""},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse","title":"metaxy.ext.metadata_stores.clickhouse","text":"<p>This module implements <code>IbisMetadataStore</code> for ClickHouse.</p> <p>It takes care of some ClickHouse-specific logic such as <code>nw.Struct</code> type conversion against ClickHouse types such as <code>Map(K,V)</code>.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStore","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStore","text":"<pre><code>ClickHouseMetadataStore(\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_cast_struct_for_map: bool = True,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>ClickHouse metadata store using Ibis backend.</p> Connection Parameters <pre><code>store = ClickHouseMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\n        \"host\": \"localhost\",\n        \"port\": 8443,\n        \"database\": \"default\",\n        \"user\": \"default\",\n        \"password\": \"\",\n    },\n    hash_algorithm=HashAlgorithm.XXHASH64,\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ClickHouse connection string.</p> <p>Format: <code>clickhouse://[user[:password]@]host[:port]/database[?param=value]</code></p> <p>Example:     <pre><code>\"clickhouse://localhost:8443/default\"\n</code></pre></p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Alternative to connection_string, specify params as dict:</p> <ul> <li> <p>host: Server host</p> </li> <li> <p>port: Server port (default: <code>8443</code>)</p> </li> <li> <p>database: Database name</p> </li> <li> <p>user: Username</p> </li> <li> <p>password: Password</p> </li> <li> <p>secure: Use secure connection (default: <code>False</code>)</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>auto_cast_struct_for_map</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to auto-convert DataFrame user-defined Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to <code>IbisMetadataStore</code>`</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-clickhouse not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor connection_params provided</p> </li> </ul> Source code in <code>src/metaxy/ext/metadata_stores/clickhouse.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    auto_cast_struct_for_map: bool = True,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [ClickHouse](https://clickhouse.com/) metadata store.\n\n    Args:\n        connection_string: ClickHouse connection string.\n\n            Format: `clickhouse://[user[:password]@]host[:port]/database[?param=value]`\n\n            Example:\n                ```\n                \"clickhouse://localhost:8443/default\"\n                ```\n\n        connection_params: Alternative to connection_string, specify params as dict:\n\n            - host: Server host\n\n            - port: Server port (default: `8443`)\n\n            - database: Database name\n\n            - user: Username\n\n            - password: Password\n\n            - secure: Use secure connection (default: `False`)\n\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        auto_cast_struct_for_map: whether to auto-convert DataFrame user-defined Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.\n\n        **kwargs: Passed to [`IbisMetadataStore`][metaxy.metadata_store.ibis.IbisMetadataStore]`\n\n    Raises:\n        ImportError: If ibis-clickhouse not installed\n        ValueError: If neither connection_string nor connection_params provided\n    \"\"\"\n    if connection_string is None and connection_params is None:\n        raise ValueError(\n            \"Must provide either connection_string or connection_params. \"\n            \"Example: connection_string='clickhouse://localhost:8443/default'\"\n        )\n\n    # Cache for ClickHouse table schemas (cleared on close)\n    self._ch_schema_cache: dict[str, IbisSchema] = {}\n\n    # Store auto_cast_struct_for_map setting\n    self.auto_cast_struct_for_map = auto_cast_struct_for_map\n\n    # Initialize Ibis store with ClickHouse backend\n    super().__init__(\n        connection_string=connection_string,\n        backend=\"clickhouse\" if connection_string is None else None,\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#configuration","title":"Configuration","text":"<p>Configuration for ClickHouseMetadataStore.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStore\"\n\n[stores.dev.config]\nconnection_string = \"clickhouse://localhost:8443/default\"\nhash_algorithm = \"xxhash64\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for ClickHouseMetadataStore.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    connection_string = \\\"clickhouse://localhost:8443/default\\\"\\n    hash_algorithm = \\\"xxhash64\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    },\n    \"connection_string\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis connection string (e.g., 'clickhouse://host:9000/db').\",\n      \"title\": \"Connection String\"\n    },\n    \"backend\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis backend name (e.g., 'clickhouse', 'postgres', 'duckdb').\",\n      \"mkdocs_metaxy_hide\": true,\n      \"title\": \"Backend\"\n    },\n    \"connection_params\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Backend-specific connection parameters.\",\n      \"title\": \"Connection Params\"\n    },\n    \"table_prefix\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Optional prefix for all table names.\",\n      \"title\": \"Table Prefix\"\n    },\n    \"auto_create_tables\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"If True, create tables on open. For development/testing only.\",\n      \"title\": \"Auto Create Tables\"\n    },\n    \"auto_cast_struct_for_map\": {\n      \"default\": true,\n      \"description\": \"Auto-convert DataFrame Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.\",\n      \"title\": \"Auto Cast Struct For Map\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"ClickHouseMetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nauto_cast_struct_for_map = true\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nauto_cast_struct_for_map = true\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CAST_STRUCT_FOR_MAP=true\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.fallback_stores","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.hash_algorithm","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.versioning_engine","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.connection_string","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.connection_string  <code>pydantic-field</code>","text":"<pre><code>connection_string: str | None = None\n</code></pre> <p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.connection_params","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.connection_params  <code>pydantic-field</code>","text":"<pre><code>connection_params: dict[str, Any] | None = None\n</code></pre> <p>Backend-specific connection parameters.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.table_prefix","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.table_prefix  <code>pydantic-field</code>","text":"<pre><code>table_prefix: str | None = None\n</code></pre> <p>Optional prefix for all table names.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.auto_create_tables","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.auto_create_tables  <code>pydantic-field</code>","text":"<pre><code>auto_create_tables: bool | None = None\n</code></pre> <p>If True, create tables on open. For development/testing only.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.auto_cast_struct_for_map","title":"metaxy.ext.metadata_stores.clickhouse.ClickHouseMetadataStoreConfig.auto_cast_struct_for_map  <code>pydantic-field</code>","text":"<pre><code>auto_cast_struct_for_map: bool = True\n</code></pre> <p>Auto-convert DataFrame Struct columns to Map format on write when the ClickHouse column is Map type. Metaxy system columns are always converted.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/","title":"Metaxy + DuckDB","text":"<p>DuckDB is an embedded analytical database. To use Metaxy with DuckDB, configure <code>DuckDBMetadataStore</code>. This runs versioning computations natively in DuckDB.</p> <p>Warning</p> <p>File-based DuckDB does not (currently) support concurrent writes. If multiple writers are a requirement (e.g. with distributed data processing), consider using Motherduck, DuckLake with a <code>PostgreSQL</code> catalog, or refer to DuckDB's documentation to learn about implementing application-side work-arounds.</p> <p>Tip</p> <p>The Delta Lake metadata store might be a better alternative for concurrent writes (with it's Polars-based versioning engine being as fast as DuckDB).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[duckdb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#api-reference","title":"API Reference","text":""},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb","title":"metaxy.ext.metadata_stores.duckdb","text":"<p>DuckDB metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore","text":"<pre><code>DuckDBMetadataStore(\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[str | ExtensionSpec] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    ducklake: DuckLakeConfig | None = None,\n    **kwargs,\n)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>DuckDB metadata store using Ibis backend.</p> Local File <pre><code>store = DuckDBMetadataStore(\"metadata.db\")\n</code></pre> With extensions <pre><code># With extensions\nstore = DuckDBMetadataStore(\"md:my_database\", extensions=[\"spatial\"])\n</code></pre> <p>Parameters:</p> <ul> <li> <code>database</code>               (<code>str | Path</code>)           \u2013            <p>Database connection string or path. - File path: <code>\"metadata.db\"</code> or <code>Path(\"metadata.db\")</code></p> <ul> <li> <p>In-memory: <code>\":memory:\"</code></p> </li> <li> <p>MotherDuck: <code>\"md:my_database\"</code> or <code>\"md:my_database?motherduck_token=...\"</code></p> </li> <li> <p>S3: <code>\"s3://bucket/path/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>HTTPS: <code>\"https://example.com/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>Any valid DuckDB connection string</p> </li> </ul> </li> <li> <code>config</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})</p> </li> <li> <code>extensions</code>               (<code>Sequence[str | ExtensionSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of DuckDB extensions to install and load on open. Supports strings (community repo) or metaxy.ext.metadata_stores.duckdb.ExtensionSpec instances.</p> </li> <li> <code>ducklake</code>               (<code>DuckLakeConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckLake attachment configuration. Learn more here.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> </ul> Source code in <code>src/metaxy/ext/metadata_stores/duckdb.py</code> <pre><code>def __init__(\n    self,\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[str | ExtensionSpec] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    ducklake: DuckLakeConfig | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize [DuckDB](https://duckdb.org/) metadata store.\n\n    Args:\n        database: Database connection string or path.\n            - File path: `\"metadata.db\"` or `Path(\"metadata.db\")`\n\n            - In-memory: `\":memory:\"`\n\n            - MotherDuck: `\"md:my_database\"` or `\"md:my_database?motherduck_token=...\"`\n\n            - S3: `\"s3://bucket/path/database.duckdb\"` (read-only via ATTACH)\n\n            - HTTPS: `\"https://example.com/database.duckdb\"` (read-only via ATTACH)\n\n            - Any valid DuckDB connection string\n\n        config: Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})\n        extensions: List of DuckDB extensions to install and load on open.\n            Supports strings (community repo) or\n            [metaxy.ext.metadata_stores.duckdb.ExtensionSpec][] instances.\n        ducklake: Optional [DuckLake](https://ducklake.select/) attachment configuration.\n            Learn more [here](/integrations/metadata-stores/storage/ducklake.md).\n        fallback_stores: Ordered list of read-only fallback stores.\n    \"\"\"\n    database_str = str(database)\n\n    connection_params = {\"database\": database_str}\n    if config:\n        connection_params.update(config)\n\n    self.database = database_str\n    self.extensions: list[ExtensionSpec] = _normalise_extensions(extensions or [])\n\n    self._ducklake_config: DuckLakeConfig | None = None\n    self._ducklake_attachment: DuckLakeAttachmentManager | None = None\n    if ducklake is not None:\n        existing_names = {ext.name for ext in self.extensions}\n        if \"ducklake\" not in existing_names:\n            self.extensions.append(ExtensionSpec(name=\"ducklake\"))\n        if isinstance(ducklake.catalog, MotherDuckCatalogConfig) and \"motherduck\" not in existing_names:\n            self.extensions.append(ExtensionSpec(name=\"motherduck\"))\n        self._ducklake_config = ducklake\n        self._ducklake_attachment = DuckLakeAttachmentManager(ducklake, store_name=kwargs.get(\"name\"))\n\n    if \"hashfuncs\" not in {ext.name for ext in self.extensions}:\n        self.extensions.append(ExtensionSpec(name=\"hashfuncs\", repository=\"community\"))\n\n    super().__init__(\n        backend=\"duckdb\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.ExtensionSpec","title":"metaxy.ext.metadata_stores.duckdb.ExtensionSpec  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB extension specification accepted by DuckDBMetadataStore.</p> Show JSON schema: <pre><code>{\n  \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"repository\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Repository\"\n    },\n    \"init_sql\": {\n      \"default\": [],\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Init Sql\",\n      \"type\": \"array\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"ExtensionSpec\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#configuration","title":"Configuration","text":"<p>Configuration for DuckDBMetadataStore.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\n\n[stores.dev.config]\ndatabase = \"metadata.db\"\nextensions = [\"hashfuncs\"]\nhash_algorithm = \"xxhash64\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DuckDBCatalogConfig\": {\n      \"description\": \"DuckDB file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"duckdb\",\n          \"default\": \"duckdb\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"uri\": {\n          \"title\": \"Uri\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"uri\"\n      ],\n      \"title\": \"DuckDBCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"DuckLakeConfig\": {\n      \"description\": \"[DuckLake](https://ducklake.select/) attachment configuration for a DuckDB connection.\",\n      \"properties\": {\n        \"catalog\": {\n          \"description\": \"Metadata catalog backend (DuckDB, SQLite, PostgreSQL, or MotherDuck).\",\n          \"discriminator\": {\n            \"mapping\": {\n              \"duckdb\": \"#/$defs/DuckDBCatalogConfig\",\n              \"motherduck\": \"#/$defs/MotherDuckCatalogConfig\",\n              \"postgres\": \"#/$defs/PostgresCatalogConfig\",\n              \"sqlite\": \"#/$defs/SQLiteCatalogConfig\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/DuckDBCatalogConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/SQLiteCatalogConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/PostgresCatalogConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/MotherDuckCatalogConfig\"\n            }\n          ],\n          \"title\": \"Catalog\"\n        },\n        \"storage\": {\n          \"anyOf\": [\n            {\n              \"discriminator\": {\n                \"mapping\": {\n                  \"gcs\": \"#/$defs/GCSStorageConfig\",\n                  \"local\": \"#/$defs/LocalStorageConfig\",\n                  \"r2\": \"#/$defs/R2StorageConfig\",\n                  \"s3\": \"#/$defs/S3StorageConfig\"\n                },\n                \"propertyName\": \"type\"\n              },\n              \"oneOf\": [\n                {\n                  \"$ref\": \"#/$defs/LocalStorageConfig\"\n                },\n                {\n                  \"$ref\": \"#/$defs/S3StorageConfig\"\n                },\n                {\n                  \"$ref\": \"#/$defs/R2StorageConfig\"\n                },\n                {\n                  \"$ref\": \"#/$defs/GCSStorageConfig\"\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Data storage backend (local filesystem, S3, R2, or GCS). Not required for MotherDuck.\",\n          \"title\": \"Storage\"\n        },\n        \"alias\": {\n          \"default\": \"ducklake\",\n          \"description\": \"DuckDB catalog alias for the attached DuckLake database.\",\n          \"title\": \"Alias\",\n          \"type\": \"string\"\n        },\n        \"attach_options\": {\n          \"additionalProperties\": true,\n          \"description\": \"Extra [DuckLake](https://ducklake.select/) ATTACH options (e.g., api_version, override_data_path).\",\n          \"title\": \"Attach Options\",\n          \"type\": \"object\"\n        },\n        \"data_inlining_row_limit\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Store inserts smaller than this row count directly in the metadata catalog instead of creating Parquet files.\",\n          \"title\": \"Data Inlining Row Limit\"\n        }\n      },\n      \"required\": [\n        \"catalog\"\n      ],\n      \"title\": \"DuckLakeConfig\",\n      \"type\": \"object\"\n    },\n    \"ExtensionSpec\": {\n      \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\",\n      \"properties\": {\n        \"name\": {\n          \"title\": \"Name\",\n          \"type\": \"string\"\n        },\n        \"repository\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Repository\"\n        },\n        \"init_sql\": {\n          \"default\": [],\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Init Sql\",\n          \"type\": \"array\"\n        }\n      },\n      \"required\": [\n        \"name\"\n      ],\n      \"title\": \"ExtensionSpec\",\n      \"type\": \"object\"\n    },\n    \"GCSStorageConfig\": {\n      \"description\": \"Google Cloud Storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE GCS`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#gcs-secrets) secret.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"gcs\",\n          \"default\": \"gcs\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"data_path\": {\n          \"title\": \"Data Path\",\n          \"type\": \"string\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"data_path\",\n        \"secret_name\"\n      ],\n      \"title\": \"GCSStorageConfig\",\n      \"type\": \"object\"\n    },\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    },\n    \"LocalStorageConfig\": {\n      \"description\": \"Local filesystem storage backend for DuckLake.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"local\",\n          \"default\": \"local\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"path\"\n      ],\n      \"title\": \"LocalStorageConfig\",\n      \"type\": \"object\"\n    },\n    \"MotherDuckCatalogConfig\": {\n      \"description\": \"[MotherDuck](https://motherduck.com/)-managed metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"motherduck\",\n          \"default\": \"motherduck\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"database\": {\n          \"title\": \"Database\",\n          \"type\": \"string\"\n        },\n        \"region\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"AWS region of the MotherDuck-managed S3 storage (e.g. 'eu-central-1').\",\n          \"title\": \"Region\"\n        }\n      },\n      \"required\": [\n        \"database\"\n      ],\n      \"title\": \"MotherDuckCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"PostgresCatalogConfig\": {\n      \"description\": \"PostgreSQL metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"postgres\",\n          \"default\": \"postgres\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"user\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"User\"\n        },\n        \"password\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"default\": 5432,\n          \"title\": \"Port\",\n          \"type\": \"integer\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"secret_name\"\n      ],\n      \"title\": \"PostgresCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"R2StorageConfig\": {\n      \"description\": \"Cloudflare R2 storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE R2`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#r2-secrets) secret.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"r2\",\n          \"default\": \"r2\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"account_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Account Id\"\n        },\n        \"data_path\": {\n          \"title\": \"Data Path\",\n          \"type\": \"string\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"data_path\",\n        \"secret_name\"\n      ],\n      \"title\": \"R2StorageConfig\",\n      \"type\": \"object\"\n    },\n    \"S3StorageConfig\": {\n      \"description\": \"[S3 storage](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api) backend for DuckLake.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"s3\",\n          \"default\": \"s3\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"endpoint\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Endpoint\"\n        },\n        \"bucket\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Bucket\"\n        },\n        \"prefix\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Prefix\"\n        },\n        \"region\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Region\"\n        },\n        \"url_style\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url Style\"\n        },\n        \"use_ssl\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Use Ssl\"\n        },\n        \"scope\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Scope\"\n        },\n        \"data_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Path\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"secret_name\"\n      ],\n      \"title\": \"S3StorageConfig\",\n      \"type\": \"object\"\n    },\n    \"SQLiteCatalogConfig\": {\n      \"description\": \"SQLite file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"sqlite\",\n          \"default\": \"sqlite\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"uri\": {\n          \"title\": \"Uri\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"uri\"\n      ],\n      \"title\": \"SQLiteCatalogConfig\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for DuckDBMetadataStore.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    database = \\\"metadata.db\\\"\\n    extensions = [\\\"hashfuncs\\\"]\\n    hash_algorithm = \\\"xxhash64\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    },\n    \"connection_string\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis connection string (e.g., 'clickhouse://host:9000/db').\",\n      \"title\": \"Connection String\"\n    },\n    \"backend\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Ibis backend name (e.g., 'clickhouse', 'postgres', 'duckdb').\",\n      \"mkdocs_metaxy_hide\": true,\n      \"title\": \"Backend\"\n    },\n    \"connection_params\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Backend-specific connection parameters.\",\n      \"title\": \"Connection Params\"\n    },\n    \"table_prefix\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Optional prefix for all table names.\",\n      \"title\": \"Table Prefix\"\n    },\n    \"auto_create_tables\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"If True, create tables on open. For development/testing only.\",\n      \"title\": \"Auto Create Tables\"\n    },\n    \"database\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"format\": \"path\",\n          \"type\": \"string\"\n        }\n      ],\n      \"description\": \"Database path (:memory:, file path, or md:database).\",\n      \"title\": \"Database\"\n    },\n    \"config\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"DuckDB configuration settings (e.g., {'threads': '4'}).\",\n      \"title\": \"Config\"\n    },\n    \"extensions\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"anyOf\": [\n              {\n                \"type\": \"string\"\n              },\n              {\n                \"$ref\": \"#/$defs/ExtensionSpec\"\n              }\n            ]\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"DuckDB extensions to install and load on open.\",\n      \"title\": \"Extensions\"\n    },\n    \"ducklake\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/DuckLakeConfig\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"DuckLake attachment configuration. Learn more [here](/integrations/metadata-stores/storage/ducklake.md).\"\n    }\n  },\n  \"required\": [\n    \"database\"\n  ],\n  \"title\": \"DuckDBMetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_string = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ntable_prefix = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ndatabase = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ndatabase = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATABASE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconfig = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconfig = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONFIG=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.fallback_stores","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.hash_algorithm","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.versioning_engine","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.connection_string","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.connection_string  <code>pydantic-field</code>","text":"<pre><code>connection_string: str | None = None\n</code></pre> <p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.connection_params","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.connection_params  <code>pydantic-field</code>","text":"<pre><code>connection_params: dict[str, Any] | None = None\n</code></pre> <p>Backend-specific connection parameters.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.table_prefix","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.table_prefix  <code>pydantic-field</code>","text":"<pre><code>table_prefix: str | None = None\n</code></pre> <p>Optional prefix for all table names.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.auto_create_tables","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.auto_create_tables  <code>pydantic-field</code>","text":"<pre><code>auto_create_tables: bool | None = None\n</code></pre> <p>If True, create tables on open. For development/testing only.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.database","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.database  <code>pydantic-field</code>","text":"<pre><code>database: str | Path\n</code></pre> <p>Database path (:memory:, file path, or md:database).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.config","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.config  <code>pydantic-field</code>","text":"<pre><code>config: dict[str, str] | None = None\n</code></pre> <p>DuckDB configuration settings (e.g., {'threads': '4'}).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.extensions","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.extensions  <code>pydantic-field</code>","text":"<pre><code>extensions: Sequence[str | ExtensionSpec] | None = None\n</code></pre> <p>DuckDB extensions to install and load on open.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.ducklake","title":"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStoreConfig.ducklake  <code>pydantic-field</code>","text":"<pre><code>ducklake: DuckLakeConfig | None = None\n</code></pre> <p>DuckLake attachment configuration. Learn more here.</p>"},{"location":"integrations/metadata-stores/databases/ibis/","title":"Metaxy + Ibis","text":"<p>Metaxy uses Ibis as a portable dataframe abstraction for SQL-based metadata stores. The <code>IbisMetadataStore</code> is the base class for all SQL-backed stores.</p>"},{"location":"integrations/metadata-stores/databases/ibis/#metaxy.metadata_store.ibis","title":"metaxy.metadata_store.ibis","text":"<p>Ibis-based metadata store for SQL databases.</p> <p>Supports any SQL database that Ibis supports: - DuckDB, PostgreSQL, MySQL (local/embedded) - ClickHouse, Snowflake, BigQuery (cloud analytical) - And 20+ other backends</p>"},{"location":"integrations/metadata-stores/databases/ibis/#metaxy.metadata_store.ibis.IbisMetadataStore","title":"metaxy.metadata_store.ibis.IbisMetadataStore","text":"<pre><code>IbisMetadataStore(\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code>, <code>ABC</code></p> <p>Generic SQL metadata store using Ibis.</p> <p>Supports any Ibis backend that supports struct types, such as: DuckDB, PostgreSQL, ClickHouse, and others.</p> Warning <p>Backends without native struct support (e.g., SQLite) are NOT supported.</p> <p>Storage layout: - Each feature gets its own table: {feature}__{key} - System tables: metaxy__system__feature_versions, metaxy__system__migrations - Uses Ibis for cross-database compatibility</p> <p>Note: Uses MD5 hash by default for cross-database compatibility. DuckDBMetadataStore overrides this with dynamic algorithm detection. For other backends, override the calculator instance variable with backend-specific implementations.</p> Example <pre><code># ClickHouse\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# PostgreSQL\nstore = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\n\n# DuckDB (use DuckDBMetadataStore instead for better hash support)\nstore = IbisMetadataStore(\"duckdb:///metadata.db\")\n\nwith store:\n    store.write(MyFeature, df)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use. - \"auto\": Prefer the store's native engine, fall back to Polars if needed - \"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible - \"polars\": Always use the Polars engine</p> </li> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis connection string (e.g., \"clickhouse://host:9000/db\") If provided, backend and connection_params are ignored.</p> </li> <li> <code>backend</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\") Used with connection_params for more control.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Backend-specific connection parameters e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}</p> </li> <li> <code>table_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional prefix applied to all feature and system table names. Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL identifier when combined with the generated table name.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor backend is provided</p> </li> <li> <code>ImportError</code>             \u2013            <p>If Ibis or required backend driver not installed</p> </li> </ul> Example <pre><code># Using connection string\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# Using backend + params\nstore = IbisMetadataStore(backend=\"clickhouse\", connection_params={\"host\": \"localhost\", \"port\": 9000})\n</code></pre> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def __init__(\n    self,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize Ibis metadata store.\n\n    Args:\n        versioning_engine: Which versioning engine to use.\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n            - \"polars\": Always use the Polars engine\n        connection_string: Ibis connection string (e.g., \"clickhouse://host:9000/db\")\n            If provided, backend and connection_params are ignored.\n        backend: Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\")\n            Used with connection_params for more control.\n        connection_params: Backend-specific connection parameters\n            e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}\n        table_prefix: Optional prefix applied to all feature and system table names.\n            Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL\n            identifier when combined with the generated table name.\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n\n    Raises:\n        ValueError: If neither connection_string nor backend is provided\n        ImportError: If Ibis or required backend driver not installed\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Using connection string\n        store = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n        # Using backend + params\n        store = IbisMetadataStore(backend=\"clickhouse\", connection_params={\"host\": \"localhost\", \"port\": 9000})\n        ```\n    \"\"\"\n    from ibis.backends.sql import SQLBackend\n\n    self.connection_string = connection_string\n    self.backend = backend\n    self.connection_params = connection_params or {}\n    self._conn: SQLBackend | None = None\n    self._table_prefix = table_prefix or \"\"\n\n    super().__init__(\n        **kwargs,\n        versioning_engine=versioning_engine,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/","title":"Metaxy + LanceDB","text":"<p>Experimental</p> <p>This functionality is experimental.</p> <p>LanceDB is an vector database built on the Lance columnar format. To use Metaxy with LanceDB, configure <code>LanceDBMetadataStore</code>. It uses the in-memory Polars engine for versioning computations. LanceDB handles schema evolution, transactions, and compaction automatically.</p> <p>It runs embedded (local directory) or against external storage (object stores, HTTP endpoints, LanceDB Cloud), so you can use the same store type for local development and cloud workloads.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#installation","title":"Installation","text":"<p>The backend relies on <code>lancedb</code>, which is shipped with Metaxy's <code>lancedb</code> extras.</p> <pre><code>pip install 'metaxy[lancedb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-targets","title":"Storage Targets","text":"<p>Point <code>uri</code> at any supported URI (<code>s3://</code>, <code>gs://</code>, <code>az://</code>, <code>db://</code>, ...) and forward credentials with the platform's native mechanism (environment variables, IAM roles, workload identity, etc.). LanceDB supports local filesystem, S3, GCS, Azure, LanceDB Cloud, and remote HTTP/HTTPS endpoints.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-layout","title":"Storage Layout","text":"<p>All tables are stored within a single LanceDB database at the configured URI location. Each feature gets its own Lance table.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#api-reference","title":"API Reference","text":""},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb","title":"metaxy.ext.metadata_stores.lancedb","text":"<p>LanceDB metadata store implementation.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStore","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStore","text":"<pre><code>LanceDBMetadataStore(\n    uri: str | Path,\n    *,\n    fallback_stores: list[MetadataStore] | None = None,\n    connect_kwargs: dict[str, Any] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>LanceDB metadata store for vector and structured data.</p> <p>LanceDB is a columnar database optimized for vector search and multimodal data. Each feature is stored in its own Lance table within the database directory. Uses Polars components for data processing (no native SQL execution).</p> <p>Storage layout:</p> <ul> <li> <p>Each feature gets its own table: <code>{namespace}__{feature_name}</code></p> </li> <li> <p>Tables are stored as Lance format in the directory specified by the URI</p> </li> <li> <p>LanceDB handles schema evolution, transactions, and compaction automatically</p> </li> </ul> Local Directory <pre><code>from pathlib import Path\nfrom metaxy.ext.metadata_stores.lancedb import LanceDBMetadataStore\n\n# Local filesystem\nstore = LanceDBMetadataStore(Path(\"/path/to/featuregraph\"))\n</code></pre> Object Storage (S3, GCS, Azure) <pre><code># object store (requires credentials)\nstore = LanceDBMetadataStore(\"s3:///path/to/featuregraph\")\n</code></pre> LanceDB Cloud <pre><code>import os\n\n# Option 1: Environment variable\nos.environ[\"LANCEDB_API_KEY\"] = \"your-api-key\"\nstore = LanceDBMetadataStore(\"db://my-database\")\n\n# Option 2: Explicit credentials\nstore = LanceDBMetadataStore(\n    \"db://my-database\", connect_kwargs={\"api_key\": \"your-api-key\", \"region\": \"us-east-1\"}\n)\n</code></pre> <p>The database directory is created automatically if it doesn't exist (local paths only). Tables are created on-demand when features are first written.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str | Path</code>)           \u2013            <p>Directory path or URI for LanceDB tables. Supports:</p> <ul> <li> <p>Local path: <code>\"./metadata\"</code> or <code>Path(\"/data/metaxy/lancedb\")</code></p> </li> <li> <p>Object stores: <code>s3://</code>, <code>gs://</code>, <code>az://</code> (requires cloud credentials)</p> </li> <li> <p>LanceDB Cloud: <code>\"db://database-name\"</code> (requires API key)</p> </li> <li> <p>Remote HTTP/HTTPS: Any URI supported by LanceDB</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. When reading features not found in this store, Metaxy searches fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.</p> </li> <li> <code>connect_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed directly to lancedb.connect(). Useful for LanceDB Cloud credentials (api_key, region) when you cannot rely on environment variables.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.base.MetadataStore (e.g., hash_algorithm, hash_truncation_length, prefer_native)</p> </li> </ul> Note <p>Unlike SQL stores, LanceDB doesn't require explicit table creation. Tables are created automatically when writing metadata.</p> Source code in <code>src/metaxy/ext/metadata_stores/lancedb.py</code> <pre><code>def __init__(\n    self,\n    uri: str | Path,\n    *,\n    fallback_stores: list[MetadataStore] | None = None,\n    connect_kwargs: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [LanceDB](https://lancedb.com/docs/) metadata store.\n\n    The database directory is created automatically if it doesn't exist (local paths only).\n    Tables are created on-demand when features are first written.\n\n    Args:\n        uri: Directory path or URI for LanceDB tables. Supports:\n\n            - **Local path**: `\"./metadata\"` or `Path(\"/data/metaxy/lancedb\")`\n\n            - **Object stores**: `s3://`, `gs://`, `az://` (requires cloud credentials)\n\n            - **LanceDB Cloud**: `\"db://database-name\"` (requires API key)\n\n            - **Remote HTTP/HTTPS**: Any URI supported by LanceDB\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            When reading features not found in this store, Metaxy searches\n            fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.\n        connect_kwargs: Extra keyword arguments passed directly to\n            [lancedb.connect()](https://lancedb.github.io/lancedb/python/python/#lancedb.connect).\n            Useful for LanceDB Cloud credentials (api_key, region) when you cannot\n            rely on environment variables.\n        **kwargs: Passed to [metaxy.metadata_store.base.MetadataStore][]\n            (e.g., hash_algorithm, hash_truncation_length, prefer_native)\n\n    Note:\n        Unlike SQL stores, LanceDB doesn't require explicit table creation.\n        Tables are created automatically when writing metadata.\n    \"\"\"\n    self.uri: str = str(uri)\n    self._conn: Any | None = None\n    self._connect_kwargs = connect_kwargs or {}\n    super().__init__(\n        fallback_stores=fallback_stores,\n        auto_create_tables=True,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#configuration","title":"Configuration","text":"<p>Configuration for LanceDBMetadataStore.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStore\"\n\n[stores.dev.config]\nuri = \"/path/to/featuregraph\"\n\n[stores.dev.config.connect_kwargs]\napi_key = \"your-api-key\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for LanceDBMetadataStore.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    uri = \\\"/path/to/featuregraph\\\"\\n\\n    [stores.dev.config.connect_kwargs]\\n    api_key = \\\"your-api-key\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    },\n    \"uri\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"format\": \"path\",\n          \"type\": \"string\"\n        }\n      ],\n      \"description\": \"Directory path or URI for LanceDB tables.\",\n      \"title\": \"Uri\"\n    },\n    \"connect_kwargs\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Extra keyword arguments passed to lancedb.connect().\",\n      \"title\": \"Connect Kwargs\"\n    }\n  },\n  \"required\": [\n    \"uri\"\n  ],\n  \"title\": \"LanceDBMetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nuri = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nuri = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__URI=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nconnect_kwargs = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nconnect_kwargs = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECT_KWARGS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.fallback_stores","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.hash_algorithm","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.versioning_engine","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.uri","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.uri  <code>pydantic-field</code>","text":"<pre><code>uri: str | Path\n</code></pre> <p>Directory path or URI for LanceDB tables.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.connect_kwargs","title":"metaxy.ext.metadata_stores.lancedb.LanceDBMetadataStoreConfig.connect_kwargs  <code>pydantic-field</code>","text":"<pre><code>connect_kwargs: dict[str, Any] | None = None\n</code></pre> <p>Extra keyword arguments passed to lancedb.connect().</p>"},{"location":"integrations/metadata-stores/storage/","title":"Storage-Only Metadata Stores","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>Recommended</p> <p>DeltaLake is an excellent choice for a storage-only metadata store with no infrastructure requirements.</p>"},{"location":"integrations/metadata-stores/storage/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>Metaxy + Delta Lake</li> <li>Metaxy + DuckLake</li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/","title":"Metaxy + Delta Lake","text":"<p>Delta Lake is an open-source lakehouse storage format with ACID transactions and schema enforcement. To use Metaxy with Delta Lake, configure <code>DeltaMetadataStore</code>. It persists metadata as Delta tables and uses an in-memory Polars engine for versioning computations.</p> <p>It supports the local filesystem and remote object stores.</p> <p>Tip</p> <p>If Polars 1.37 or greater is installed, lazy Polars frames are sinked via <code>LazyFrame.sink_delta</code>, avoiding unnecessary materialization.</p>"},{"location":"integrations/metadata-stores/storage/delta/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[delta]'\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#api-reference","title":"API Reference","text":""},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta","title":"metaxy.ext.metadata_stores.delta","text":"<p>Delta Lake metadata store implemented with delta-rs.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStore","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStore","text":"<pre><code>DeltaMetadataStore(\n    root_path: str | Path,\n    *,\n    storage_options: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    layout: Literal[\"flat\", \"nested\"] = \"nested\",\n    delta_write_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>Delta Lake metadata store backed by delta-rs.</p> <p>It stores feature metadata in Delta Lake tables located under <code>root_path</code>. It uses the Polars versioning engine for provenance calculations.</p> <p>Tip</p> <p>If Polars 1.37 or greater is installed, lazy Polars frames are sinked via <code>LazyFrame.sink_delta</code>, avoiding unnecessary materialization.</p> <p>Example:</p> <pre><code>```py\nfrom metaxy.ext.metadata_stores.delta import DeltaMetadataStore\n\nstore = DeltaMetadataStore(\n    root_path=\"s3://my-bucket/metaxy\",\n    storage_options={\"AWS_REGION\": \"us-west-2\"},\n)\n```\n</code></pre> <p>Parameters:</p> <ul> <li> <code>root_path</code>               (<code>str | Path</code>)           \u2013            <p>Base directory or URI where feature tables are stored. Supports local paths (<code>/path/to/dir</code>), <code>s3://</code> URLs, and other object store URIs.</p> </li> <li> <code>storage_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Storage backend options passed to delta-rs. Example: <code>{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}</code> See https://delta-io.github.io/delta-rs/ for details on supported options.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>layout</code>               (<code>Literal['flat', 'nested']</code>, default:                   <code>'nested'</code> )           \u2013            <p>Directory layout for feature tables. Options:</p> <ul> <li> <p><code>\"nested\"</code>: Feature tables stored in nested directories <code>{part1}/{part2}.delta</code></p> </li> <li> <p><code>\"flat\"</code>: Feature tables stored as <code>{part1}__{part2}.delta</code></p> </li> </ul> </li> <li> <code>delta_write_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional options passed to <code>deltalake.write_deltalake</code>. Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Forwarded to metaxy.metadata_store.base.MetadataStore.</p> </li> </ul> Source code in <code>src/metaxy/ext/metadata_stores/delta.py</code> <pre><code>def __init__(\n    self,\n    root_path: str | Path,\n    *,\n    storage_options: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    layout: Literal[\"flat\", \"nested\"] = \"nested\",\n    delta_write_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize Delta Lake metadata store.\n\n    Args:\n        root_path: Base directory or URI where feature tables are stored.\n            Supports local paths (`/path/to/dir`), `s3://` URLs, and other object store URIs.\n        storage_options: Storage backend options passed to delta-rs.\n            Example: `{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}`\n            See https://delta-io.github.io/delta-rs/ for details on supported options.\n        fallback_stores: Ordered list of read-only fallback stores.\n        layout: Directory layout for feature tables. Options:\n\n            - `\"nested\"`: Feature tables stored in nested directories `{part1}/{part2}.delta`\n\n            - `\"flat\"`: Feature tables stored as `{part1}__{part2}.delta`\n\n        delta_write_options: Additional options passed to [`deltalake.write_deltalake`][deltalake.write_deltalake].\n            Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}\n        **kwargs: Forwarded to [metaxy.metadata_store.base.MetadataStore][metaxy.metadata_store.base.MetadataStore].\n    \"\"\"\n    self.storage_options = storage_options or {}\n    if layout not in (\"flat\", \"nested\"):\n        raise ValueError(f\"Invalid layout: {layout}. Must be 'flat' or 'nested'.\")\n    self.layout = layout\n    self.delta_write_options = delta_write_options or {}\n\n    root_str = str(root_path)\n    self._is_remote = not is_local_path(root_str)\n\n    if self._is_remote:\n        # Remote path (S3, Azure, GCS, etc.)\n        self._root_uri = root_str.rstrip(\"/\")\n    else:\n        # Local path (including file:// and local:// URLs)\n        if root_str.startswith(\"file://\"):\n            # Strip file:// prefix\n            root_str = root_str[7:]\n        elif root_str.startswith(\"local://\"):\n            # Strip local:// prefix\n            root_str = root_str[8:]\n        local_path = Path(root_str).expanduser().resolve()\n        self._root_uri = str(local_path)\n\n    super().__init__(\n        fallback_stores=fallback_stores,\n        versioning_engine=\"polars\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#configuration","title":"Configuration","text":"<p>Configuration for DeltaMetadataStore.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\n\n[stores.dev.config]\nroot_path = \"s3://my-bucket/metaxy\"\nlayout = \"nested\"\n\n[stores.dev.config.storage_options]\nAWS_REGION = \"us-west-2\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for DeltaMetadataStore.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    root_path = \\\"s3://my-bucket/metaxy\\\"\\n    layout = \\\"nested\\\"\\n\\n    [stores.dev.config.storage_options]\\n    AWS_REGION = \\\"us-west-2\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    },\n    \"root_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"format\": \"path\",\n          \"type\": \"string\"\n        }\n      ],\n      \"description\": \"Base directory or URI where feature tables are stored.\",\n      \"title\": \"Root Path\"\n    },\n    \"storage_options\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Storage backend options passed to delta-rs.\",\n      \"title\": \"Storage Options\"\n    },\n    \"layout\": {\n      \"default\": \"nested\",\n      \"description\": \"Directory layout for feature tables ('nested' or 'flat').\",\n      \"enum\": [\n        \"flat\",\n        \"nested\"\n      ],\n      \"title\": \"Layout\",\n      \"type\": \"string\"\n    },\n    \"delta_write_options\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Options passed to [`deltalake.write_deltalake`][deltalake.write_deltalake].\",\n      \"title\": \"Delta Write Options\"\n    }\n  },\n  \"required\": [\n    \"root_path\"\n  ],\n  \"title\": \"DeltaMetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> <li><code>extra</code>: <code>forbid</code></li> </ul> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nroot_path = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nroot_path = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__ROOT_PATH=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nstorage_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nstorage_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__STORAGE_OPTIONS=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LAYOUT=nested\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\ndelta_write_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\ndelta_write_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DELTA_WRITE_OPTIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.fallback_stores","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.hash_algorithm","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.versioning_engine","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.root_path","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.root_path  <code>pydantic-field</code>","text":"<pre><code>root_path: str | Path\n</code></pre> <p>Base directory or URI where feature tables are stored.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.storage_options","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.storage_options  <code>pydantic-field</code>","text":"<pre><code>storage_options: dict[str, Any] | None = None\n</code></pre> <p>Storage backend options passed to delta-rs.</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.layout","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.layout  <code>pydantic-field</code>","text":"<pre><code>layout: Literal['flat', 'nested'] = 'nested'\n</code></pre> <p>Directory layout for feature tables ('nested' or 'flat').</p>"},{"location":"integrations/metadata-stores/storage/delta/#metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.delta_write_options","title":"metaxy.ext.metadata_stores.delta.DeltaMetadataStoreConfig.delta_write_options  <code>pydantic-field</code>","text":"<pre><code>delta_write_options: dict[str, Any] | None = None\n</code></pre> <p>Options passed to <code>deltalake.write_deltalake</code>.</p>"},{"location":"integrations/metadata-stores/storage/ducklake/","title":"Metaxy + DuckLake","text":"<p>Experimental</p> <p>This functionality is experimental.</p> <p>DuckLake is a modern LakeHouse which uses a relational database as metadata catalog.</p> <p>Currently, there is only one production-ready implementation of DuckLake - via DuckDB, and the built-in <code>DuckDBMetadataStore</code> can be configured to use DuckLake as its storage backend. Learn more about the DuckDB integration here.</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#configuration","title":"Configuration","text":"<p>There are two main parts that configure DuckLake: a catalog (where the transaction log and other metadata is stored) and a storage (where the data files (1) live).</p> <ol> <li>Parquet files</li> </ol> <p>Each piece of configuration that manages secrets (e.g. PostgreSQL, S3, R2, GCS) requires a <code>secret_name</code> parameter. Metaxy uses this name to either create a new DuckDB secret (when inline credentials are provided) or reference a pre-existing one (when only the name is given).</p> <p>Tip</p> <p>To use the credential chain (IAM roles, environment variables, etc.) instead of static S3 credentials, set <code>secret_parameters = { provider = \"credential_chain\" }</code>. Learn more in DuckDB docs.</p> <p>Example Configuration</p> <pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\n\n[stores.dev.config.ducklake.catalog]\ntype = \"postgres\"\nsecret_name = \"my_pg_secret\"\nhost = \"localhost\"\nport = 5432\ndatabase = \"ducklake_meta\"\nuser = \"ducklake\"\npassword = \"changeme\"\n\n[stores.dev.config.ducklake.storage]\ntype = \"s3\"\nsecret_name = \"my_s3_secret\"\nbucket = \"my-ducklake-bucket\"\nkey_id = \"AKIA...\"\nsecret = \"...\"\nregion = \"eu-central-1\"\n</code></pre> <p>See the DuckLake example to learn more.</p> <p>DuckLake attachment configuration for a DuckDB connection.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"DuckDBCatalogConfig\": {\n      \"description\": \"DuckDB file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"duckdb\",\n          \"default\": \"duckdb\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"uri\": {\n          \"title\": \"Uri\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"uri\"\n      ],\n      \"title\": \"DuckDBCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"GCSStorageConfig\": {\n      \"description\": \"Google Cloud Storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE GCS`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#gcs-secrets) secret.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"gcs\",\n          \"default\": \"gcs\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"data_path\": {\n          \"title\": \"Data Path\",\n          \"type\": \"string\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"data_path\",\n        \"secret_name\"\n      ],\n      \"title\": \"GCSStorageConfig\",\n      \"type\": \"object\"\n    },\n    \"LocalStorageConfig\": {\n      \"description\": \"Local filesystem storage backend for DuckLake.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"local\",\n          \"default\": \"local\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"path\": {\n          \"title\": \"Path\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"path\"\n      ],\n      \"title\": \"LocalStorageConfig\",\n      \"type\": \"object\"\n    },\n    \"MotherDuckCatalogConfig\": {\n      \"description\": \"[MotherDuck](https://motherduck.com/)-managed metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"motherduck\",\n          \"default\": \"motherduck\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"database\": {\n          \"title\": \"Database\",\n          \"type\": \"string\"\n        },\n        \"region\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"AWS region of the MotherDuck-managed S3 storage (e.g. 'eu-central-1').\",\n          \"title\": \"Region\"\n        }\n      },\n      \"required\": [\n        \"database\"\n      ],\n      \"title\": \"MotherDuckCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"PostgresCatalogConfig\": {\n      \"description\": \"PostgreSQL metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"postgres\",\n          \"default\": \"postgres\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"database\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Database\"\n        },\n        \"user\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"User\"\n        },\n        \"password\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Password\"\n        },\n        \"host\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Host\"\n        },\n        \"port\": {\n          \"default\": 5432,\n          \"title\": \"Port\",\n          \"type\": \"integer\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"secret_name\"\n      ],\n      \"title\": \"PostgresCatalogConfig\",\n      \"type\": \"object\"\n    },\n    \"R2StorageConfig\": {\n      \"description\": \"Cloudflare R2 storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE R2`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#r2-secrets) secret.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"r2\",\n          \"default\": \"r2\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"account_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Account Id\"\n        },\n        \"data_path\": {\n          \"title\": \"Data Path\",\n          \"type\": \"string\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"data_path\",\n        \"secret_name\"\n      ],\n      \"title\": \"R2StorageConfig\",\n      \"type\": \"object\"\n    },\n    \"S3StorageConfig\": {\n      \"description\": \"[S3 storage](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api) backend for DuckLake.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"s3\",\n          \"default\": \"s3\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"key_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Key Id\"\n        },\n        \"secret\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret\"\n        },\n        \"endpoint\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Endpoint\"\n        },\n        \"bucket\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Bucket\"\n        },\n        \"prefix\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Prefix\"\n        },\n        \"region\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Region\"\n        },\n        \"url_style\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Url Style\"\n        },\n        \"use_ssl\": {\n          \"anyOf\": [\n            {\n              \"type\": \"boolean\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Use Ssl\"\n        },\n        \"scope\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Scope\"\n        },\n        \"data_path\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Data Path\"\n        },\n        \"secret_name\": {\n          \"title\": \"Secret Name\",\n          \"type\": \"string\"\n        },\n        \"secret_parameters\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": true,\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Secret Parameters\"\n        }\n      },\n      \"required\": [\n        \"secret_name\"\n      ],\n      \"title\": \"S3StorageConfig\",\n      \"type\": \"object\"\n    },\n    \"SQLiteCatalogConfig\": {\n      \"description\": \"SQLite file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"sqlite\",\n          \"default\": \"sqlite\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"uri\": {\n          \"title\": \"Uri\",\n          \"type\": \"string\"\n        }\n      },\n      \"required\": [\n        \"uri\"\n      ],\n      \"title\": \"SQLiteCatalogConfig\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"[DuckLake](https://ducklake.select/) attachment configuration for a DuckDB connection.\",\n  \"properties\": {\n    \"catalog\": {\n      \"description\": \"Metadata catalog backend (DuckDB, SQLite, PostgreSQL, or MotherDuck).\",\n      \"discriminator\": {\n        \"mapping\": {\n          \"duckdb\": \"#/$defs/DuckDBCatalogConfig\",\n          \"motherduck\": \"#/$defs/MotherDuckCatalogConfig\",\n          \"postgres\": \"#/$defs/PostgresCatalogConfig\",\n          \"sqlite\": \"#/$defs/SQLiteCatalogConfig\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/DuckDBCatalogConfig\"\n        },\n        {\n          \"$ref\": \"#/$defs/SQLiteCatalogConfig\"\n        },\n        {\n          \"$ref\": \"#/$defs/PostgresCatalogConfig\"\n        },\n        {\n          \"$ref\": \"#/$defs/MotherDuckCatalogConfig\"\n        }\n      ],\n      \"title\": \"Catalog\"\n    },\n    \"storage\": {\n      \"anyOf\": [\n        {\n          \"discriminator\": {\n            \"mapping\": {\n              \"gcs\": \"#/$defs/GCSStorageConfig\",\n              \"local\": \"#/$defs/LocalStorageConfig\",\n              \"r2\": \"#/$defs/R2StorageConfig\",\n              \"s3\": \"#/$defs/S3StorageConfig\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/LocalStorageConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/S3StorageConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/R2StorageConfig\"\n            },\n            {\n              \"$ref\": \"#/$defs/GCSStorageConfig\"\n            }\n          ]\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Data storage backend (local filesystem, S3, R2, or GCS). Not required for MotherDuck.\",\n      \"title\": \"Storage\"\n    },\n    \"alias\": {\n      \"default\": \"ducklake\",\n      \"description\": \"DuckDB catalog alias for the attached DuckLake database.\",\n      \"title\": \"Alias\",\n      \"type\": \"string\"\n    },\n    \"attach_options\": {\n      \"additionalProperties\": true,\n      \"description\": \"Extra [DuckLake](https://ducklake.select/) ATTACH options (e.g., api_version, override_data_path).\",\n      \"title\": \"Attach Options\",\n      \"type\": \"object\"\n    },\n    \"data_inlining_row_limit\": {\n      \"anyOf\": [\n        {\n          \"type\": \"integer\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Store inserts smaller than this row count directly in the metadata catalog instead of creating Parquet files.\",\n      \"title\": \"Data Inlining Row Limit\"\n    }\n  },\n  \"required\": [\n    \"catalog\"\n  ],\n  \"title\": \"DuckLakeConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ALIAS=ducklake\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\nattach_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\nattach_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ATTACH_OPTIONS={}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\ndata_inlining_row_limit = 0\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\ndata_inlining_row_limit = 0\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__DATA_INLINING_ROW_LIMIT=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.catalog","title":"metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.catalog  <code>pydantic-field</code>","text":"<pre><code>catalog: (\n    DuckDBCatalogConfig\n    | SQLiteCatalogConfig\n    | PostgresCatalogConfig\n    | MotherDuckCatalogConfig\n)\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.storage","title":"metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.storage  <code>pydantic-field</code>","text":"<pre><code>storage: (\n    LocalStorageConfig\n    | S3StorageConfig\n    | R2StorageConfig\n    | GCSStorageConfig\n    | None\n) = None\n</code></pre> <p>Data storage backend (local filesystem, S3, R2, or GCS). Not required for MotherDuck.</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.alias","title":"metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.alias  <code>pydantic-field</code>","text":"<pre><code>alias: str = 'ducklake'\n</code></pre> <p>DuckDB catalog alias for the attached DuckLake database.</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.attach_options","title":"metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.attach_options  <code>pydantic-field</code>","text":"<pre><code>attach_options: dict[str, Any]\n</code></pre> <p>Extra DuckLake ATTACH options (e.g., api_version, override_data_path).</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.data_inlining_row_limit","title":"metaxy.ext.metadata_stores.ducklake.DuckLakeConfig.data_inlining_row_limit  <code>pydantic-field</code>","text":"<pre><code>data_inlining_row_limit: int | None = None\n</code></pre> <p>Store inserts smaller than this row count directly in the metadata catalog instead of creating Parquet files.</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#catalog-backends","title":"Catalog Backends","text":"<p>DuckDB file-based metadata backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"DuckDB file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"duckdb\",\n      \"default\": \"duckdb\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"uri\": {\n      \"title\": \"Uri\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"uri\"\n  ],\n  \"title\": \"DuckDBCatalogConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nuri = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nuri = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__URI=...\n</code></pre> <p>SQLite file-based metadata backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"SQLite file-based metadata backend for [DuckLake](https://ducklake.select/).\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"sqlite\",\n      \"default\": \"sqlite\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"uri\": {\n      \"title\": \"Uri\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"uri\"\n  ],\n  \"title\": \"SQLiteCatalogConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nuri = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nuri = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__URI=...\n</code></pre> <p>PostgreSQL metadata backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"PostgreSQL metadata backend for [DuckLake](https://ducklake.select/).\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"postgres\",\n      \"default\": \"postgres\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"database\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Database\"\n    },\n    \"user\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"User\"\n    },\n    \"password\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Password\"\n    },\n    \"host\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Host\"\n    },\n    \"port\": {\n      \"default\": 5432,\n      \"title\": \"Port\",\n      \"type\": \"integer\"\n    },\n    \"secret_name\": {\n      \"title\": \"Secret Name\",\n      \"type\": \"string\"\n    },\n    \"secret_parameters\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret Parameters\"\n    }\n  },\n  \"required\": [\n    \"secret_name\"\n  ],\n  \"title\": \"PostgresCatalogConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\ndatabase = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\ndatabase = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__DATABASE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nuser = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nuser = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__USER=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\npassword = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\npassword = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__PASSWORD=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nhost = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nhost = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__HOST=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nport = 5432\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nport = 5432\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__PORT=5432\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nsecret_name = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nsecret_name = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__SECRET_NAME=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nsecret_parameters = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nsecret_parameters = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__SECRET_PARAMETERS=...\n</code></pre> <p>MotherDuck-managed metadata backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"[MotherDuck](https://motherduck.com/)-managed metadata backend for [DuckLake](https://ducklake.select/).\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"motherduck\",\n      \"default\": \"motherduck\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"database\": {\n      \"title\": \"Database\",\n      \"type\": \"string\"\n    },\n    \"region\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"AWS region of the MotherDuck-managed S3 storage (e.g. 'eu-central-1').\",\n      \"title\": \"Region\"\n    }\n  },\n  \"required\": [\n    \"database\"\n  ],\n  \"title\": \"MotherDuckCatalogConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\ndatabase = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\ndatabase = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__DATABASE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.catalog]\nregion = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.catalog]\nregion = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__CATALOG__REGION=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.DuckDBCatalogConfig.uri","title":"metaxy.ext.metadata_stores.ducklake.DuckDBCatalogConfig.uri  <code>pydantic-field</code>","text":"<pre><code>uri: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.SQLiteCatalogConfig.uri","title":"metaxy.ext.metadata_stores.ducklake.SQLiteCatalogConfig.uri  <code>pydantic-field</code>","text":"<pre><code>uri: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.database","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.database  <code>pydantic-field</code>","text":"<pre><code>database: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.user","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.user  <code>pydantic-field</code>","text":"<pre><code>user: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.password","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.password  <code>pydantic-field</code>","text":"<pre><code>password: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.host","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.host  <code>pydantic-field</code>","text":"<pre><code>host: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.port","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.port  <code>pydantic-field</code>","text":"<pre><code>port: int = 5432\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.secret_name","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.secret_name  <code>pydantic-field</code>","text":"<pre><code>secret_name: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.secret_parameters","title":"metaxy.ext.metadata_stores.ducklake.PostgresCatalogConfig.secret_parameters  <code>pydantic-field</code>","text":"<pre><code>secret_parameters: dict[str, Any] | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.MotherDuckCatalogConfig.database","title":"metaxy.ext.metadata_stores.ducklake.MotherDuckCatalogConfig.database  <code>pydantic-field</code>","text":"<pre><code>database: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.MotherDuckCatalogConfig.region","title":"metaxy.ext.metadata_stores.ducklake.MotherDuckCatalogConfig.region  <code>pydantic-field</code>","text":"<pre><code>region: str | None = None\n</code></pre> <p>AWS region of the MotherDuck-managed S3 storage (e.g. 'eu-central-1').</p>"},{"location":"integrations/metadata-stores/storage/ducklake/#storage-backends","title":"Storage Backends","text":"<p>Local filesystem storage backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Local filesystem storage backend for DuckLake.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"local\",\n      \"default\": \"local\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"path\": {\n      \"title\": \"Path\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"path\"\n  ],\n  \"title\": \"LocalStorageConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\npath = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\npath = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__PATH=...\n</code></pre> <p>S3 storage backend for DuckLake.</p> Show JSON schema: <pre><code>{\n  \"description\": \"[S3 storage](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api) backend for DuckLake.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"s3\",\n      \"default\": \"s3\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Key Id\"\n    },\n    \"secret\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret\"\n    },\n    \"endpoint\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Endpoint\"\n    },\n    \"bucket\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Bucket\"\n    },\n    \"prefix\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Prefix\"\n    },\n    \"region\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Region\"\n    },\n    \"url_style\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Url Style\"\n    },\n    \"use_ssl\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Use Ssl\"\n    },\n    \"scope\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Scope\"\n    },\n    \"data_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Data Path\"\n    },\n    \"secret_name\": {\n      \"title\": \"Secret Name\",\n      \"type\": \"string\"\n    },\n    \"secret_parameters\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret Parameters\"\n    }\n  },\n  \"required\": [\n    \"secret_name\"\n  ],\n  \"title\": \"S3StorageConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__KEY_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nendpoint = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nendpoint = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__ENDPOINT=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nbucket = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nbucket = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__BUCKET=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nprefix = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nprefix = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__PREFIX=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nregion = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nregion = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__REGION=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nurl_style = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nurl_style = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__URL_STYLE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nuse_ssl = false\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nuse_ssl = false\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__USE_SSL=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nscope = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nscope = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SCOPE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__DATA_PATH=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_NAME=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_PARAMETERS=...\n</code></pre> <p>Cloudflare R2 storage backend for DuckLake.</p> <p>Uses the DuckDB <code>TYPE R2</code> secret.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Cloudflare R2 storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE R2`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#r2-secrets) secret.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"r2\",\n      \"default\": \"r2\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Key Id\"\n    },\n    \"secret\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret\"\n    },\n    \"account_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Account Id\"\n    },\n    \"data_path\": {\n      \"title\": \"Data Path\",\n      \"type\": \"string\"\n    },\n    \"secret_name\": {\n      \"title\": \"Secret Name\",\n      \"type\": \"string\"\n    },\n    \"secret_parameters\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret Parameters\"\n    }\n  },\n  \"required\": [\n    \"data_path\",\n    \"secret_name\"\n  ],\n  \"title\": \"R2StorageConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__KEY_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\naccount_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\naccount_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__ACCOUNT_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__DATA_PATH=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_NAME=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_PARAMETERS=...\n</code></pre> <p>Google Cloud Storage backend for DuckLake.</p> <p>Uses the DuckDB <code>TYPE GCS</code> secret.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Google Cloud Storage backend for [DuckLake](https://ducklake.select/).\\n\\nUses the DuckDB [`TYPE GCS`](https://duckdb.org/docs/stable/core_extensions/httpfs/s3api#gcs-secrets) secret.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"gcs\",\n      \"default\": \"gcs\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"key_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Key Id\"\n    },\n    \"secret\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret\"\n    },\n    \"data_path\": {\n      \"title\": \"Data Path\",\n      \"type\": \"string\"\n    },\n    \"secret_name\": {\n      \"title\": \"Secret Name\",\n      \"type\": \"string\"\n    },\n    \"secret_parameters\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": true,\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Secret Parameters\"\n    }\n  },\n  \"required\": [\n    \"data_path\",\n    \"secret_name\"\n  ],\n  \"title\": \"GCSStorageConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nkey_id = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__KEY_ID=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\ndata_path = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__DATA_PATH=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_name = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_NAME=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake.storage]\nsecret_parameters = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE__SECRET_PARAMETERS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.LocalStorageConfig.path","title":"metaxy.ext.metadata_stores.ducklake.LocalStorageConfig.path  <code>pydantic-field</code>","text":"<pre><code>path: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.key_id","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.key_id  <code>pydantic-field</code>","text":"<pre><code>key_id: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret  <code>pydantic-field</code>","text":"<pre><code>secret: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.endpoint","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.endpoint  <code>pydantic-field</code>","text":"<pre><code>endpoint: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.bucket","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.bucket  <code>pydantic-field</code>","text":"<pre><code>bucket: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.prefix","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.prefix  <code>pydantic-field</code>","text":"<pre><code>prefix: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.region","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.region  <code>pydantic-field</code>","text":"<pre><code>region: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.url_style","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.url_style  <code>pydantic-field</code>","text":"<pre><code>url_style: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.use_ssl","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.use_ssl  <code>pydantic-field</code>","text":"<pre><code>use_ssl: bool | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.scope","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.scope  <code>pydantic-field</code>","text":"<pre><code>scope: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.data_path","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.data_path  <code>pydantic-field</code>","text":"<pre><code>data_path: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret_name","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret_name  <code>pydantic-field</code>","text":"<pre><code>secret_name: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret_parameters","title":"metaxy.ext.metadata_stores.ducklake.S3StorageConfig.secret_parameters  <code>pydantic-field</code>","text":"<pre><code>secret_parameters: dict[str, Any] | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.key_id","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.key_id  <code>pydantic-field</code>","text":"<pre><code>key_id: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret  <code>pydantic-field</code>","text":"<pre><code>secret: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.account_id","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.account_id  <code>pydantic-field</code>","text":"<pre><code>account_id: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.data_path","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.data_path  <code>pydantic-field</code>","text":"<pre><code>data_path: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret_name","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret_name  <code>pydantic-field</code>","text":"<pre><code>secret_name: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret_parameters","title":"metaxy.ext.metadata_stores.ducklake.R2StorageConfig.secret_parameters  <code>pydantic-field</code>","text":"<pre><code>secret_parameters: dict[str, Any] | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.key_id","title":"metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.key_id  <code>pydantic-field</code>","text":"<pre><code>key_id: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret","title":"metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret  <code>pydantic-field</code>","text":"<pre><code>secret: str | None = None\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.data_path","title":"metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.data_path  <code>pydantic-field</code>","text":"<pre><code>data_path: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret_name","title":"metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret_name  <code>pydantic-field</code>","text":"<pre><code>secret_name: str\n</code></pre>"},{"location":"integrations/metadata-stores/storage/ducklake/#metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret_parameters","title":"metaxy.ext.metadata_stores.ducklake.GCSStorageConfig.secret_parameters  <code>pydantic-field</code>","text":"<pre><code>secret_parameters: dict[str, Any] | None = None\n</code></pre>"},{"location":"integrations/orchestration/","title":"Metaxy + Orchestrators","text":"<p>Metaxy can manage feature metadata, but it's not opinionated about how and where to materialize the data itself.</p> <p>This task is typically handled by an orchestrator.</p>"},{"location":"integrations/orchestration/#dagster","title":"Dagster","text":"<p>Metaxy has been built with the Dagster integration in mind from the very beginning. Dagster is an excellent choice for managing Metaxy jobs.</p>"},{"location":"integrations/orchestration/dagster/","title":"Metaxy + Dagster","text":"<p>Metaxy's dependency system has been originally inspired by Dagster.</p> <p>Because of this, Metaxy code can be naturally composed with Dagster code, Metaxy concepts map directly into Dagster concepts, and the provided <code>@metaxify</code> decorator makes this process effortless.</p> <p>The only step that has to be taken in order to inject Metaxy into Dagster assets is to associate the Dagster asset with the Metaxy feature.</p> <p>Unleash the full power of <code>@metaxify</code> on Dagster!</p> <p>Example</p> <p>Set the well-known <code>\"metaxy/feature\"</code> key (1):</p> <ol> <li> point it to... the Metaxy feature key!</li> </ol> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\nclass MyMetaxyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/metaxy/feature\", id_columns=[\"id\"])):\n    id: str\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/metaxy/feature\"})\ndef my_asset():\n    pass\n</code></pre> <p><code>@metaxify</code> will take care of injecting information (such as asset dependencies or metadata) from the Metaxy feature to the Dagster asset. Learn more about <code>@metaxify</code> (with example screenshots) here.</p>"},{"location":"integrations/orchestration/dagster/#whats-in-the-box","title":"What's in the box","text":"<p>This integration provides:</p> <ul> <li> <p><code>metaxify</code> - a decorator that enriches Dagster asset definitions with Metaxy information such as upstream dependencies, description, metadata, code version, table schema, column lineage, and so on. More info and some screenshots here.</p> </li> <li> <p><code>MetaxyIOManager</code> - an IO manager that reads and writes Dagster assets that are Metaxy features and logs useful runtime metadata.</p> </li> <li> <p><code>MetaxyStoreFromConfigResource</code> - a resource that provides access to <code>MetadataStore</code></p> </li> <li> <p><code>generate_materialize_results</code> / <code>generate_observe_results</code> - generators for yielding <code>dagster.MaterializeResult</code> or <code>dagster.ObserveResult</code> events from Dagster assets (and multi-assets), with automatic topological ordering, partition filtering, logging row counts, and setting Dagster data versions.</p> </li> <li> <p><code>observable_metaxy_asset</code> - a decorator that creates observable source assets for monitoring external Metaxy features.</p> </li> </ul>"},{"location":"integrations/orchestration/dagster/#quick-start","title":"Quick Start","text":""},{"location":"integrations/orchestration/dagster/#1-define-metaxy-features","title":"1. Define Metaxy Features","text":"defs.py<pre><code># Upstream feature\nupstream_spec = mx.FeatureSpec(\n    key=\"audio/embeddings\",\n    id_columns=[\"audio_id\"],\n    fields=[\"embedding\"],\n)\n\n\nclass AudioEmbeddings(mx.BaseFeature, spec=upstream_spec):\n    audio_id: str\n\n\n# Downstream feature that depends on upstream\ndownstream_spec = mx.FeatureSpec(\n    key=\"audio/clusters\",\n    id_columns=[\"audio_id\"],\n    deps=[AudioEmbeddings],\n)\n\n\nclass AudioClusters(mx.BaseFeature, spec=downstream_spec):\n    audio_id: str\n    mean: float\n    std: float\n</code></pre>"},{"location":"integrations/orchestration/dagster/#2-define-dagster-assets","title":"2. Define Dagster Assets","text":"<p>Root Asset</p> <p>Let's define an asset that doesn't have any upstream Metaxy features.</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/embeddings\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_embeddings(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    # somehow, acquire root source data\n    samples = pl.DataFrame(\n        {\n            \"audio_id\": [\"a1\", \"a2\", \"a3\"],\n            \"metaxy_provenance_by_field\": [\n                {\"embedding\": \"hash1\"},\n                {\"embedding\": \"hash2\"},\n                {\"embedding\": \"hash3\"},\n            ],\n        }\n    )\n\n    # resolve the increment with Metaxy\n\n    with store:\n        increment = store.resolve_update(\"audio/embeddings\", samples=samples)  # noqa: F841\n\n    # Compute embeddings...\n\n    df = ...  # at this point this dataframe should have `mean` and `std` columns set\n\n    # either write embeddings metadata via Metaxy\n    # or return a dataframe to write it via MetaxyIOManager\n\n    return df\n</code></pre> <p>Downstream Asset</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/clusters\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_clusters(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    with store:\n        # Get IDs that need recomputation\n        update = store.resolve_update(AudioClusters)  # noqa: F841\n    ...\n</code></pre> <p>Non-Metaxy Downstream Asset</p> <pre><code>@dg.asset(\n    ins={\n        \"clusters\": dg.AssetIn(\n            key=[\"audio\", \"clusters\"],\n        )\n    },\n)\ndef cluster_report(clusters: nw.LazyFrame):\n    # clusters is a narwhals LazyFrame loaded via MetaxyIOManager\n    df = clusters.collect().to_polars()\n    # Generate a report...\n    return {\"total_clusters\": df.select(\"cluster_id\").n_unique()}\n</code></pre>"},{"location":"integrations/orchestration/dagster/#3-create-dagster-definitions","title":"3. Create Dagster Definitions","text":"defs.py<pre><code>store = mxd.MetaxyStoreFromConfigResource(name=\"dev\")\nmetaxy_io_manager = mxd.MetaxyIOManager(store=store)\n\n\n@dg.definitions\ndef definitions():\n    mx.init()  # (1)!\n\n    return dg.Definitions(\n        assets=[  # ty: ignore[invalid-argument-type]\n            audio_embeddings,\n            audio_clusters,\n            cluster_report,\n        ],\n        resources={\n            \"store\": store,\n            \"metaxy_io_manager\": metaxy_io_manager,\n        },\n    )\n</code></pre> <ol> <li>This loads Metaxy configuration and feature definitions</li> </ol>"},{"location":"integrations/orchestration/dagster/#4-start-dagster","title":"4. Start Dagster","text":"<pre><code>dg dev -f defs.py\n</code></pre> <p>Materialize your assets and let Metaxy take care of state and versioning!</p>"},{"location":"integrations/orchestration/dagster/#observable-source-assets","title":"Observable Source Assets","text":"<p>Use <code>observable_metaxy_asset</code> to create observable source assets that monitor external Metaxy features. This is useful when Metaxy features are populated outside of Dagster (e.g., by external pipelines) and you want Dagster to track their data versions.</p> <p>Basic Observable Asset</p> <pre><code>import dagster as dg\nimport narwhals as nw\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\nclass ExternalFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"external/feature\", id_columns=[\"id\"])):\n    id: str\n\n@mxd.observable_metaxy_asset(key=\"dagster/asset/key\", feature=\"external/feature\")\ndef external_data(context, store: dg.ResourceParam[mx.MetadataStore], lazy_df: nw.LazyFrame):\n    # build a custom metadata dict\n    metadata = {\"row_count\": lazy_df.collect().shape[0]}\n    return metadata\n</code></pre> <p>The observation automatically tracks:</p> <ul> <li>Data version: Uses <code>mean(metaxy_created_at)</code> to detect both additions and deletions</li> <li>Row count: Logged as <code>dagster/row_count</code> metadata</li> </ul>"},{"location":"integrations/orchestration/dagster/#deletion-workflows-with-dagster-ops","title":"Deletion workflows with Dagster ops","text":"<p>The Dagster integration provides a <code>delete</code> op:</p> <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n# Define a job with the delete op\n@dg.job(resource_defs={\"metaxy_store\": mxd.MetaxyStoreFromConfigResource(name=\"default\")})\ndef cleanup_job():\n    mxd.delete()\n</code></pre> <p><code>filters</code> is a list of SQL WHERE clause strings (e.g., <code>[\"status = 'inactive'\", \"age &gt; 18\"]</code>) that are parsed into Narwhals expressions. Multiple filters are combined with AND logic. See the filter expressions guide for supported syntax. Set <code>soft=False</code> to physically remove rows.</p>"},{"location":"integrations/orchestration/dagster/#reference","title":"Reference","text":"<ul> <li>API</li> <li>Dagster docs</li> </ul>"},{"location":"integrations/orchestration/dagster/api/","title":"Dagster Integration API Reference","text":"<p>Integration docs</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster","title":"metaxy.ext.dagster","text":""},{"location":"integrations/orchestration/dagster/api/#decorators","title":"Decorators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.metaxify.metaxify","title":"metaxy.ext.dagster.metaxify.metaxify","text":"<pre><code>metaxify(\n    _asset: _T | None = None,\n    *,\n    key: CoercibleToAssetKey | None = None,\n    key_prefix: CoercibleToAssetKeyPrefix | None = None,\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n)\n</code></pre> <p>Inject Metaxy metadata into a Dagster <code>AssetsDefinition</code> or <code>AssetSpec</code>.</p> <p>Affects assets with <code>metaxy/feature</code> metadata set.</p> <p>Learn more about <code>@metaxify</code> and see example screenshots here.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToAssetKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit asset key that overrides all other key resolution logic. Cannot be used with <code>key_prefix</code> or with multi-asset definitions that produce multiple outputs.</p> </li> <li> <code>key_prefix</code>               (<code>CoercibleToAssetKeyPrefix | None</code>, default:                   <code>None</code> )           \u2013            <p>Prefix to prepend to the resolved asset key. Also applied to upstream dependency keys. Cannot be used with <code>key</code>.</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version into the asset's code version. The version is appended in the format <code>metaxy:&lt;version&gt;</code>.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set the asset description from the feature class docstring if the asset doesn't already have a description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject Pydantic field definitions as Dagster column schema. Field types are converted to strings, and field descriptions are used as column descriptions.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column-level lineage into the asset metadata under <code>dagster/column_lineage</code>. Uses Pydantic model fields to track column provenance via <code>FeatureDep.rename</code>, <code>FeatureDep.lineage</code>, and direct pass-through.</p> </li> </ul> <p>Tip</p> <p>Multiple Dagster assets can contribute to the same Metaxy feature. This is a perfectly valid setup since Metaxy writes are append-only. In order to do this, set the following metadata keys:</p> <pre><code>- `\"metaxy/feature\"` pointing to the same Metaxy feature key\n- `\"metaxy/partition\"` should be set to a dictionary mapping column names to values produced by the specific Dagster asset\n</code></pre> <p>Example</p> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(\n    metadata={\"metaxy/feature\": \"my/feature/key\"},\n)\ndef my_asset(store: mx.MetadataStore):\n    with store:\n        increment = store.resolve_update(\"my/feature/key\")\n    ...\n</code></pre> With <code>@multi_asset</code> <p>Multiple Metaxy features can be produced by the same <code>@multi_asset</code>. (1)</p> <ol> <li>Typically, they are produced independently of each other</li> </ol> <pre><code>@mxd.metaxify()\n@dg.multi_asset(\n    specs=[\n        dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"feature/a\"}),\n        dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"feature/b\"}),\n    ]\n)\ndef my_multi_asset(): ...\n</code></pre> With <code>dagster.AssetSpec</code> <pre><code>asset_spec = dg.AssetSpec(\n    key=\"my_asset\",\n    metadata={\"metaxy/feature\": \"my/feature/key\"},\n)\nasset_spec = mxd.metaxify()(asset_spec)\n</code></pre> Multiple Dagster assets contributing to the same Metaxy feature <pre><code>@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"metaxy/partition\": {\"dataset\": \"a\"},\n    },\n)\ndef my_feature_dataset_a(): ...\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"metaxy/partition\": {\"dataset\": \"b\"},\n    },\n)\ndef my_feature_dataset_b(): ...\n</code></pre> Source code in <code>src/metaxy/ext/dagster/metaxify.py</code> <pre><code>def __init__(\n    self,\n    _asset: \"_T | None\" = None,\n    *,\n    key: CoercibleToAssetKey | None = None,\n    key_prefix: CoercibleToAssetKeyPrefix | None = None,\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n) -&gt; None:\n    # Actual initialization happens in __new__, but we set defaults here for type checkers\n    self.key = dg.AssetKey.from_coercible(key) if key is not None else None\n    self.key_prefix = dg.AssetKey.from_coercible(key_prefix) if key_prefix is not None else None\n    self.inject_metaxy_kind = inject_metaxy_kind\n    self.inject_code_version = inject_code_version\n    self.set_description = set_description\n    self.inject_column_schema = inject_column_schema\n    self.inject_column_lineage = inject_column_lineage\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observable.observable_metaxy_asset","title":"metaxy.ext.dagster.observable.observable_metaxy_asset","text":"<pre><code>observable_metaxy_asset(\n    feature: CoercibleToFeatureKey,\n    *,\n    store_resource_key: str = \"store\",\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    **observable_kwargs: Any,\n)\n</code></pre> <p>Decorator to create an observable source asset for a Metaxy feature.</p> <p>The observation reads the feature's metadata from the store, counts rows, and uses <code>mean(metaxy_created_at)</code> as the data version to track changes. Using mean ensures that both additions and deletions are detected.</p> <p>The decorated function receives <code>(context, store, lazy_df)</code> and can return a dict of additional metadata to include in the observation.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to observe.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set description from feature class docstring.</p> </li> <li> <code>**observable_kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to <code>@observable_source_asset</code> (key, group_name, tags, metadata, description, partitions_def, etc.)</p> </li> </ul> Example <pre><code>import metaxy.ext.dagster as mxd\nfrom myproject.features import ExternalFeature\n\n\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data(context, store, lazy_df):\n    pass\n\n\n# With custom metadata - return a dict\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data_with_metrics(context, store, lazy_df):\n    # Run aggregations in the database\n    total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n    return {\"custom/total\": total}\n</code></pre> Note <p><code>observable_source_asset</code> does not support <code>deps</code>. Upstream Metaxy feature dependencies from the feature spec are not propagated to the SourceAsset.</p> Source code in <code>src/metaxy/ext/dagster/observable.py</code> <pre><code>@public\ndef observable_metaxy_asset(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    store_resource_key: str = \"store\",\n    # metaxify kwargs\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    # observable_source_asset kwargs\n    **observable_kwargs: Any,\n):\n    \"\"\"Decorator to create an observable source asset for a Metaxy feature.\n\n    The observation reads the feature's metadata from the store, counts rows,\n    and uses `mean(metaxy_created_at)` as the data version to track changes.\n    Using mean ensures that both additions and deletions are detected.\n\n    The decorated function receives `(context, store, lazy_df)` and can return\n    a dict of additional metadata to include in the observation.\n\n    Args:\n        feature: The Metaxy feature to observe.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        inject_metaxy_kind: Whether to inject `\"metaxy\"` kind into asset kinds.\n        inject_code_version: Whether to inject the Metaxy feature code version.\n        set_description: Whether to set description from feature class docstring.\n        **observable_kwargs: Passed to `@observable_source_asset`\n            (key, group_name, tags, metadata, description, partitions_def, etc.)\n\n    Example:\n        ```python\n        import metaxy.ext.dagster as mxd\n        from myproject.features import ExternalFeature\n\n\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data(context, store, lazy_df):\n            pass\n\n\n        # With custom metadata - return a dict\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data_with_metrics(context, store, lazy_df):\n            # Run aggregations in the database\n            total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n            return {\"custom/total\": total}\n        ```\n\n    Note:\n        `observable_source_asset` does not support `deps`. Upstream Metaxy feature\n        dependencies from the feature spec are not propagated to the SourceAsset.\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n\n    def decorator(fn: Callable[..., Any]) -&gt; dg.SourceAsset:\n        # Build an AssetSpec from kwargs and enrich with metaxify\n        # Merge user metadata with metaxy/feature\n        user_metadata = observable_kwargs.pop(\"metadata\", None) or {}\n        spec = dg.AssetSpec(\n            key=observable_kwargs.pop(\"key\", None) or fn.__name__,  # ty: ignore[unresolved-attribute]\n            group_name=observable_kwargs.pop(\"group_name\", None),\n            tags=observable_kwargs.pop(\"tags\", None),\n            metadata={\n                **user_metadata,\n                DAGSTER_METAXY_FEATURE_METADATA_KEY: feature_key.to_string(),\n            },\n            description=observable_kwargs.pop(\"description\", None),\n        )\n        enriched = metaxify(\n            inject_metaxy_kind=inject_metaxy_kind,\n            inject_code_version=inject_code_version,\n            set_description=set_description,\n        )(spec)\n\n        def _observe(context: dg.AssetExecutionContext) -&gt; dg.ObserveResult:\n            store: mx.MetadataStore = getattr(context.resources, store_resource_key)\n\n            # Check for metaxy/partition metadata to apply filtering\n            metaxy_partition = enriched.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n            filters = build_metaxy_partition_filter(metaxy_partition)\n\n            with store:\n                lazy_df = store.read(feature_key, filters=filters)\n                stats = compute_stats_from_lazy_frame(lazy_df)\n\n                # Call the user's function - it can return additional metadata\n                extra_metadata = fn(context, store, lazy_df) or {}\n\n            metadata: dict[str, Any] = {\"dagster/row_count\": stats.row_count}\n            metadata.update(extra_metadata)\n\n            return dg.ObserveResult(\n                data_version=stats.data_version,\n                metadata=metadata,\n                tags=build_feature_event_tags(feature_key),\n            )\n\n        # Apply observable_source_asset decorator\n        return dg.observable_source_asset(\n            key=enriched.key,\n            description=enriched.description,\n            group_name=enriched.group_name,\n            tags=dict(enriched.tags) if enriched.tags else None,\n            metadata=dict(enriched.metadata) if enriched.metadata else None,\n            required_resource_keys={store_resource_key},\n            **observable_kwargs,\n        )(_observe)\n\n    return decorator\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#io-manager","title":"IO Manager","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager","title":"metaxy.ext.dagster.MetaxyIOManager","text":"<p>               Bases: <code>ConfigurableIOManager</code></p> <p>MetaxyIOManager is a Dagster IOManager that reads and writes data to/from Metaxy's <code>MetadataStore</code>.</p> <p>It automatically attaches Metaxy feature and store metadata to Dagster materialization events and handles partitioned assets.</p> <p>Always set <code>\"metaxy/feature\"</code> Dagster metadata</p> <p>This IOManager is using <code>\"metaxy/feature\"</code> Dagster metadata key to map Dagster assets into Metaxy features. It expects it to be set on the assets being loaded or materialized.</p> Example <pre><code>import dagster as dg\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n    }\n)\ndef my_asset(): ...\n</code></pre> <p>Defining Partitioned Assets</p> <p>To tell Metaxy which column to use when filtering partitioned assets, set <code>\"partition_by\"</code> Dagster metadata key.</p> Example <pre><code>import dagster as dg\n\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"partition_by\": \"date\",\n    }\n)\ndef my_partitioned_asset(): ...\n</code></pre> <p>This key is commonly used to configure partitioning behavior by various Dagster IO managers.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.load_input","title":"metaxy.ext.dagster.MetaxyIOManager.load_input","text":"<pre><code>load_input(context: InputContext) -&gt; LazyFrame[Any]\n</code></pre> <p>Load feature metadata from <code>MetadataStore</code>.</p> <p>Reads metadata for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. For partitioned assets, filters to the current partition using the column specified in <code>\"partition_by\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InputContext</code>)           \u2013            <p>Dagster input context containing asset metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>A narwhals LazyFrame with the feature metadata.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def load_input(self, context: \"dg.InputContext\") -&gt; nw.LazyFrame[Any]:\n    \"\"\"Load feature metadata from [`MetadataStore`][metaxy.MetadataStore].\n\n    Reads metadata for the feature specified in the asset's `\"metaxy/feature\"` metadata.\n    For partitioned assets, filters to the current partition using the column specified\n    in `\"partition_by\"` metadata.\n\n    Args:\n        context: Dagster input context containing asset metadata.\n\n    Returns:\n        A narwhals LazyFrame with the feature metadata.\n    \"\"\"\n    with self.metadata_store:\n        feature_key = self._feature_key_from_context(context)\n\n        # Build partition filters from context (handles partition_by and metaxy/partition)\n        filters = build_partition_filter_from_input_context(context)\n\n        # Read metadata with store info in a single call (avoids extra network round-trip)\n        lazy_frame, resolved_store = self.metadata_store.read(\n            feature=feature_key,\n            filters=filters,\n            with_store_info=True,\n        )\n\n        # Build input metadata from resolved store\n        # metaxy/store shows where data was actually found (may be a fallback store)\n        resolved_from = resolved_store.get_store_info(feature_key)\n        input_metadata: dict[str, Any] = {\n            \"name\": self.metadata_store.name,\n            \"metaxy/store\": resolved_store.display(),\n            \"resolved_from\": resolved_from,\n        }\n\n        # Map resolved store metadata to dagster standard keys\n        if \"table_name\" in resolved_from:\n            input_metadata[\"dagster/table_name\"] = resolved_from[\"table_name\"]\n        if \"uri\" in resolved_from:\n            input_metadata[\"dagster/uri\"] = dg.MetadataValue.path(resolved_from[\"uri\"])\n\n        # Only add input metadata if we have exactly one partition key\n        # (add_input_metadata internally uses asset_partition_key which fails with multiple)\n        has_single_partition = context.has_asset_partitions and len(list(context.asset_partition_keys)) == 1\n        if input_metadata and (not context.has_asset_partitions or has_single_partition):\n            context.add_input_metadata(input_metadata, description=\"Metadata Store Info\")\n\n        return lazy_frame\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.handle_output","title":"metaxy.ext.dagster.MetaxyIOManager.handle_output","text":"<pre><code>handle_output(\n    context: OutputContext, obj: MetaxyOutput\n) -&gt; None\n</code></pre> <p>Write feature metadata to <code>MetadataStore</code>.</p> <p>Writes the output dataframe to the metadata store for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. Also logs metadata about the feature and store to Dagster's materialization events.</p> <p>If <code>obj</code> is <code>None</code>, only metadata logging is performed (no data is written).</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>OutputContext</code>)           \u2013            <p>Dagster output context containing asset metadata.</p> </li> <li> <code>obj</code>               (<code>MetaxyOutput</code>)           \u2013            <p>A narwhals-compatible dataframe to write, or None to skip writing.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def handle_output(self, context: \"dg.OutputContext\", obj: MetaxyOutput) -&gt; None:\n    \"\"\"Write feature metadata to [`MetadataStore`][metaxy.MetadataStore].\n\n    Writes the output dataframe to the metadata store for the feature specified\n    in the asset's `\"metaxy/feature\"` metadata. Also logs metadata about the\n    feature and store to Dagster's materialization events.\n\n    If `obj` is `None`, only metadata logging is performed (no data is written).\n\n    Args:\n        context: Dagster output context containing asset metadata.\n        obj: A narwhals-compatible dataframe to write, or None to skip writing.\n    \"\"\"\n    assert DAGSTER_METAXY_FEATURE_METADATA_KEY in context.definition_metadata, (\n        f'Missing `\"{DAGSTER_METAXY_FEATURE_METADATA_KEY}\"` key in asset metadata'\n    )\n    key = self._feature_key_from_context(context)\n    feature = mx.get_feature_by_key(key)\n\n    if obj is not None:\n        context.log.debug(f'Writing metadata for Metaxy feature \"{key.to_string()}\" into {self.metadata_store}')\n        with self.metadata_store.open(\"w\"):\n            self.metadata_store.write(feature=feature, df=obj)\n        context.log.debug(f'Metadata written for Metaxy feature \"{key.to_string()}\" into {self.metadata_store}')\n    else:\n        context.log.debug(\n            f'The output corresponds to Metaxy feature \"{key.to_string()}\" stored in {self.metadata_store}'\n        )\n\n    self._log_output_metadata(context)\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-types","title":"Dagster Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.dagster_type.feature_to_dagster_type","title":"metaxy.ext.dagster.dagster_type.feature_to_dagster_type","text":"<pre><code>feature_to_dagster_type(\n    feature: CoercibleToFeatureKey,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n    metadata: Mapping[str, Any] | None = None,\n) -&gt; DagsterType\n</code></pre> <p>Build a Dagster type from a Metaxy feature.</p> <p>Creates a <code>dagster.DagsterType</code> that validates outputs are <code>MetaxyOutput</code> (i.e., narwhals-compatible dataframes or <code>None</code>) and includes metadata derived from the feature's Pydantic model fields.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to create a type for. Can be a feature class, feature key, or string that can be coerced to a feature key.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom name for the DagsterType. Defaults to the feature's table name (e.g., \"project__feature_name\").</p> </li> <li> <code>description</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom description. Defaults to the feature class docstring or a generated description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the column schema as metadata. The schema is derived from Pydantic model fields.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column lineage as metadata. The lineage is derived from feature dependencies.</p> </li> <li> <code>metadata</code>               (<code>Mapping[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom metadata to inject into the DagsterType.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DagsterType</code>           \u2013            <p>A DagsterType configured for the Metaxy feature with appropriate</p> </li> <li> <code>DagsterType</code>           \u2013            <p>type checking and metadata.</p> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>import dagster as dg\nimport polars as pl\nimport metaxy.ext.dagster as mxd\nfrom myproject.features import MyFeature  # Your Metaxy feature class\n\n\n@mxd.metaxify(feature=MyFeature)\n@dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\ndef my_asset():\n    return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n</code></pre> <p>See also</p> <ul> <li><code>metaxify</code>: Decorator for injecting   Metaxy metadata into Dagster assets.</li> <li><code>MetaxyOutput</code>: The type alias for valid   Metaxy outputs.</li> </ul> Source code in <code>src/metaxy/ext/dagster/dagster_type.py</code> <pre><code>@public\ndef feature_to_dagster_type(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n    metadata: Mapping[str, Any] | None = None,\n) -&gt; dg.DagsterType:\n    \"\"\"Build a Dagster type from a Metaxy feature.\n\n    Creates a `dagster.DagsterType` that validates outputs are\n    [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput] (i.e., narwhals-compatible\n    dataframes or `None`) and includes metadata derived from the feature's Pydantic\n    model fields.\n\n    Args:\n        feature: The Metaxy feature to create a type for. Can be a feature class,\n            feature key, or string that can be coerced to a feature key.\n        name: Optional custom name for the DagsterType. Defaults to the feature's\n            table name (e.g., \"project__feature_name\").\n        description: Optional custom description. Defaults to the feature class\n            docstring or a generated description.\n        inject_column_schema: Whether to inject the column schema as metadata.\n            The schema is derived from Pydantic model fields.\n        inject_column_lineage: Whether to inject column lineage as metadata.\n            The lineage is derived from feature dependencies.\n        metadata: Optional custom metadata to inject into the DagsterType.\n\n    Returns:\n        A DagsterType configured for the Metaxy feature with appropriate\n        type checking and metadata.\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        import dagster as dg\n        import polars as pl\n        import metaxy.ext.dagster as mxd\n        from myproject.features import MyFeature  # Your Metaxy feature class\n\n\n        @mxd.metaxify(feature=MyFeature)\n        @dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\n        def my_asset():\n            return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n        ```\n\n    !!! info \"See also\"\n        - [`metaxify`][metaxy.ext.dagster.metaxify.metaxify]: Decorator for injecting\n          Metaxy metadata into Dagster assets.\n        - [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput]: The type alias for valid\n          Metaxy outputs.\n    \"\"\"\n    from metaxy.ext.dagster.io_manager import MetaxyOutput\n\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_def = mx.get_feature_by_key(feature_key)\n\n    # For build_column_schema, prefer the original class if provided\n    # (handles cases where class is defined inside a function and can't be imported)\n    feature_for_schema: mx.FeatureDefinition | type[mx.BaseFeature]\n    if isinstance(feature, type) and issubclass(feature, mx.BaseFeature):\n        feature_for_schema = feature\n    else:\n        feature_for_schema = feature_def\n\n    # Determine name\n    type_name = name or feature_key.table_name\n\n    # Determine description - use schema description if available, else default\n    if description is None:\n        schema_desc = feature_def.feature_schema.get(\"description\")\n        if schema_desc:\n            description = schema_desc\n        else:\n            description = f\"Metaxy feature '{feature_key.to_string()}'.\"\n\n    # Build metadata - start with custom metadata if provided\n    final_metadata: dict[str, Any] = dict(metadata) if metadata else {}\n    final_metadata[DAGSTER_METAXY_INFO_METADATA_KEY] = build_feature_info_metadata(feature_key)\n    # Skip column schema for external features (no Python class to extract schema from)\n    if inject_column_schema and not feature_def.is_external:\n        column_schema = build_column_schema(feature_for_schema)\n        if column_schema is not None:\n            final_metadata[DAGSTER_COLUMN_SCHEMA_METADATA_KEY] = column_schema\n\n    # Skip column lineage for external features (no Python class to extract columns from)\n    if inject_column_lineage and not feature_def.is_external:\n        column_lineage = build_column_lineage(feature_for_schema)\n        if column_lineage is not None:\n            final_metadata[DAGSTER_COLUMN_LINEAGE_METADATA_KEY] = column_lineage\n\n    dagster_type = dg.DagsterType(\n        type_check_fn=_create_type_check_fn(feature_key),\n        name=type_name,\n        description=description,\n        typing_type=MetaxyOutput,\n        metadata=final_metadata,\n    )\n\n    return dagster_type\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-event-generators","title":"Dagster Event Generators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_materialize_results","title":"metaxy.ext.dagster.utils.generate_materialize_results","text":"<pre><code>generate_materialize_results(\n    context: AssetExecutionContext | OpExecutionContext,\n    store: MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[AssetSpec] | None = None,\n) -&gt; Iterator[MaterializeResult[None]]\n</code></pre> <p>Generate <code>dagster.MaterializeResult</code> events for assets in topological order.</p> <p>Yields a <code>MaterializeResult</code> for each asset spec, sorted by their associated Metaxy features in topological order (dependencies before dependents). Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext | OpExecutionContext</code>)           \u2013            <p>The Dagster execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete Dagster asset specs. Required when using <code>OpExecutionContext</code>. Optional for <code>AssetExecutionContext</code> (defaults to <code>context.assets_def.specs</code>).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>MaterializeResult[None]</code>           \u2013            <p>Materialization result for each asset in topological order.</p> </li> </ul> Example <p>Using with <code>@multi_asset</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n\n@metaxify\n@dg.multi_asset(specs=specs)\ndef my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    # ... compute and write data ...\n    yield from generate_materialize_results(context, store)\n</code></pre></p> <p>Using with <code>@op</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n]\n\n\n@dg.op\ndef my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n    # ... compute and write data ...\n    yield from generate_materialize_results(context, store, specs=specs)\n</code></pre></p> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef generate_materialize_results(\n    context: dg.AssetExecutionContext | dg.OpExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.MaterializeResult[None]]:\n    \"\"\"Generate `dagster.MaterializeResult` events for assets in topological order.\n\n    Yields a `MaterializeResult` for each asset spec, sorted by their associated\n    Metaxy features in topological order (dependencies before dependents).\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Concrete Dagster asset specs. Required when using `OpExecutionContext`.\n            Optional for `AssetExecutionContext` (defaults to `context.assets_def.specs`).\n\n    Yields:\n        Materialization result for each asset in topological order.\n\n    Example:\n        Using with `@multi_asset`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n\n        @metaxify\n        @dg.multi_asset(specs=specs)\n        def my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            # ... compute and write data ...\n            yield from generate_materialize_results(context, store)\n        ```\n\n        Using with `@op`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n        ]\n\n\n        @dg.op\n        def my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n            # ... compute and write data ...\n            yield from generate_materialize_results(context, store, specs=specs)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    if specs is None:\n        if not isinstance(context, dg.AssetExecutionContext):\n            raise ValueError(\"specs must be provided when using OpExecutionContext\")\n        specs = context.assets_def.specs\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_col = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_KEY)\n        metaxy_partition = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n\n        with store:  # ty: ignore[invalid-context-manager]\n            try:\n                # Build runtime metadata (handles reading, filtering, and stats internally)\n                metadata, stats = build_runtime_feature_metadata(\n                    key,\n                    store,\n                    context,\n                    partition_col=partition_col,\n                    metaxy_partition=metaxy_partition,\n                )\n            except FeatureNotFoundError:\n                context.log.exception(f\"Feature {key.to_string()} not found in store, skipping materialization result\")\n                continue\n\n            # Get materialized-in-run count if materialization_id is set\n            if store.materialization_id is not None:  # ty: ignore[possibly-missing-attribute]\n                mat_df = store.read(  # ty: ignore[possibly-missing-attribute]\n                    key,\n                    filters=[\n                        nw.col(METAXY_MATERIALIZATION_ID) == store.materialization_id  # ty: ignore[possibly-missing-attribute]\n                    ],\n                )\n                metadata[\"metaxy/materialized_in_run\"] = mat_df.select(nw.len()).collect().item(0, 0)\n\n        yield dg.MaterializeResult(\n            value=None,\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n            tags=build_feature_event_tags(key),\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_observe_results","title":"metaxy.ext.dagster.utils.generate_observe_results","text":"<pre><code>generate_observe_results(\n    context: AssetExecutionContext | OpExecutionContext,\n    store: MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[AssetSpec] | None = None,\n) -&gt; Iterator[ObserveResult]\n</code></pre> <p>Generate <code>dagster.ObserveResult</code> events for assets in topological order.</p> <p>Yields an <code>ObserveResult</code> for each asset spec that has <code>\"metaxy/feature\"</code> metadata key set, sorted by their associated Metaxy features in topological order. Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext | OpExecutionContext</code>)           \u2013            <p>The Dagster execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete Dagster asset specs. Required when using <code>OpExecutionContext</code>. Optional for <code>AssetExecutionContext</code> (defaults to <code>context.assets_def.specs</code>).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ObserveResult</code>           \u2013            <p>Observation result for each asset in topological order.</p> </li> </ul> Example <p>Using with <code>@multi_observable_source_asset</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n\n@metaxify\n@dg.multi_observable_source_asset(specs=specs)\ndef my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    yield from generate_observe_results(context, store)\n</code></pre></p> <p>Using with <code>@op</code>: <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n]\n\n\n@dg.op\ndef my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n    yield from generate_observe_results(context, store, specs=specs)\n</code></pre></p> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef generate_observe_results(\n    context: dg.AssetExecutionContext | dg.OpExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.ObserveResult]:\n    \"\"\"Generate `dagster.ObserveResult` events for assets in topological order.\n\n    Yields an `ObserveResult` for each asset spec that has `\"metaxy/feature\"` metadata key set, sorted by their associated\n    Metaxy features in topological order.\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Concrete Dagster asset specs. Required when using `OpExecutionContext`.\n            Optional for `AssetExecutionContext` (defaults to `context.assets_def.specs`).\n\n    Yields:\n        Observation result for each asset in topological order.\n\n    Example:\n        Using with `@multi_observable_source_asset`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n\n        @metaxify\n        @dg.multi_observable_source_asset(specs=specs)\n        def my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            yield from generate_observe_results(context, store)\n        ```\n\n        Using with `@op`:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n        ]\n\n\n        @dg.op\n        def my_op(context: dg.OpExecutionContext, store: mx.MetadataStore):\n            yield from generate_observe_results(context, store, specs=specs)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    if specs is None:\n        if not isinstance(context, dg.AssetExecutionContext):\n            raise ValueError(\"specs must be provided when using OpExecutionContext\")\n        specs = context.assets_def.specs\n\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_col = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_KEY)\n        metaxy_partition = asset_spec.metadata.get(DAGSTER_METAXY_PARTITION_METADATA_KEY)\n\n        with store:  # ty: ignore[invalid-context-manager]\n            try:\n                # Build runtime metadata (handles reading, filtering, and stats internally)\n                # For observers with no metaxy_partition, this reads all data\n                metadata, stats = build_runtime_feature_metadata(\n                    key,\n                    store,\n                    context,\n                    partition_col=partition_col,\n                    metaxy_partition=metaxy_partition,\n                )\n            except FeatureNotFoundError:\n                context.log.exception(f\"Feature {key.to_string()} not found in store, skipping observation result\")\n                continue\n\n        yield dg.ObserveResult(\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n            tags=build_feature_event_tags(key),\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.build_feature_info_metadata","title":"metaxy.ext.dagster.utils.build_feature_info_metadata","text":"<pre><code>build_feature_info_metadata(\n    feature: CoercibleToFeatureKey,\n) -&gt; dict[str, Any]\n</code></pre> <p>Build feature info metadata dict for Dagster assets.</p> <p>Creates a dictionary with information about the Metaxy feature that can be used as Dagster asset metadata under the <code>\"metaxy/feature_info\"</code> key.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature (class, key, or string).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A nested dictionary containing:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>feature</code>: Feature information</li> <li><code>project</code>: The project name</li> <li><code>spec</code>: The full feature spec as a dict (via <code>model_dump()</code>)</li> <li><code>version</code>: The feature version string</li> <li><code>type</code>: The feature class module path</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>metaxy</code>: Metaxy library information</li> <li><code>version</code>: The metaxy library version</li> </ul> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>from metaxy.ext.dagster.utils import build_feature_info_metadata\n\ninfo = build_feature_info_metadata(MyFeature)\n# {\n#     \"feature\": {\n#         \"project\": \"my_project\",\n#         \"spec\": {...},  # Full FeatureSpec model_dump()\n#         \"version\": \"my__feature@abc123\",\n#         \"type\": \"myproject.features\",\n#     },\n#     \"metaxy\": {\n#         \"version\": \"0.1.0\",\n#     },\n# }\n</code></pre> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>@public\ndef build_feature_info_metadata(\n    feature: mx.CoercibleToFeatureKey,\n) -&gt; dict[str, Any]:\n    \"\"\"Build feature info metadata dict for Dagster assets.\n\n    Creates a dictionary with information about the Metaxy feature that can be\n    used as Dagster asset metadata under the `\"metaxy/feature_info\"` key.\n\n    Args:\n        feature: The Metaxy feature (class, key, or string).\n\n    Returns:\n        A nested dictionary containing:\n\n        - `feature`: Feature information\n            - `project`: The project name\n            - `spec`: The full feature spec as a dict (via `model_dump()`)\n            - `version`: The feature version string\n            - `type`: The feature class module path\n        - `metaxy`: Metaxy library information\n            - `version`: The metaxy library version\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        from metaxy.ext.dagster.utils import build_feature_info_metadata\n\n        info = build_feature_info_metadata(MyFeature)\n        # {\n        #     \"feature\": {\n        #         \"project\": \"my_project\",\n        #         \"spec\": {...},  # Full FeatureSpec model_dump()\n        #         \"version\": \"my__feature@abc123\",\n        #         \"type\": \"myproject.features\",\n        #     },\n        #     \"metaxy\": {\n        #         \"version\": \"0.1.0\",\n        #     },\n        # }\n        ```\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_def = mx.get_feature_by_key(feature_key)\n    feature_version = mx.current_graph().get_feature_version(feature_key)\n\n    return {\n        \"feature\": {\n            \"project\": feature_def.project,\n            \"spec\": feature_def.spec.model_dump(mode=\"json\"),\n            \"version\": feature_version,\n            \"type\": feature_def.feature_class_path,\n        },\n        \"metaxy\": {\n            \"version\": mx.__version__,\n            \"plugins\": mx.MetaxyConfig.get().plugins,\n        },\n    }\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#observation-jobs","title":"Observation Jobs","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observation_job.build_metaxy_multi_observation_job","title":"metaxy.ext.dagster.observation_job.build_metaxy_multi_observation_job","text":"<pre><code>build_metaxy_multi_observation_job(\n    name: str,\n    *,\n    asset_selection: AssetSelection | None = None,\n    defs: Definitions | None = None,\n    assets: Sequence[\n        AssetSpec | AssetsDefinition | SourceAsset\n    ]\n    | None = None,\n    store_resource_key: str = \"store\",\n    tags: Mapping[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; JobDefinition\n</code></pre> <p>Build a dynamic Dagster job that observes multiple Metaxy feature assets.</p> <p>Creates a job that dynamically spawns one op per asset, yielding <code>AssetObservation</code> events. Uses Dagster's dynamic orchestration to process multiple assets in parallel.</p> <p>Tip</p> <p>This is a very powerful way to observe all your Metaxy features at once. Use it in combination with a Dagster schedule to run it periodically.</p> <p>Provide either: - <code>asset_selection</code> and <code>defs</code>: Select assets from a   <code>Definitions</code> object</p> <ul> <li><code>assets</code>: Direct list of assets to observe</li> </ul> <p>Note</p> <p>All selected assets must share the same partitioning (if any).</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name for the job.</p> </li> <li> <code>asset_selection</code>               (<code>AssetSelection | None</code>, default:                   <code>None</code> )           \u2013            <p>An <code>AssetSelection</code> specifying which assets to observe. Must be used together with <code>defs</code>.</p> </li> <li> <code>defs</code>               (<code>Definitions | None</code>, default:                   <code>None</code> )           \u2013            <p>The <code>Definitions</code> object to resolve the selection against. Must be used together with <code>asset_selection</code>.</p> </li> <li> <code>assets</code>               (<code>Sequence[AssetSpec | AssetsDefinition | SourceAsset] | None</code>, default:                   <code>None</code> )           \u2013            <p>Direct sequence of assets to observe. Each item can be an <code>AssetSpec</code>, <code>AssetsDefinition</code>, or <code>SourceAsset</code>. Cannot be used together with <code>asset_selection</code>/<code>defs</code>.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>tags</code>               (<code>Mapping[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional tags to apply to the job.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments passed to the <code>@job</code> decorator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>JobDefinition</code>           \u2013            <p>A Dagster job definition that observes all matching Metaxy assets.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no specs have <code>metaxy/feature</code> metadata, if assets have inconsistent <code>partitions_def</code>, or if invalid argument combinations are provided.</p> </li> </ul> Example <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature_a\"})\ndef feature_a(): ...\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature_b\"})\ndef feature_b(): ...\n\n\n# Option 1: Using asset_selection + defs\nmy_defs = dg.Definitions(assets=[feature_a, feature_b])\nobservation_job = mxd.build_metaxy_multi_observation_job(\n    name=\"observe_my_features\",\n    asset_selection=dg.AssetSelection.kind(\"metaxy\"),\n    defs=my_defs,\n)\n\n# Option 2: Using direct assets list\nobservation_job = mxd.build_metaxy_multi_observation_job(\n    name=\"observe_my_features\",\n    assets=[feature_a, feature_b],\n)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/observation_job.py</code> <pre><code>@public\ndef build_metaxy_multi_observation_job(\n    name: str,\n    *,\n    asset_selection: dg.AssetSelection | None = None,\n    defs: dg.Definitions | None = None,\n    assets: Sequence[dg.AssetSpec | dg.AssetsDefinition | dg.SourceAsset] | None = None,\n    store_resource_key: str = \"store\",\n    tags: Mapping[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; dg.JobDefinition:\n    \"\"\"Build a dynamic Dagster job that observes multiple Metaxy feature assets.\n\n    Creates a job that dynamically spawns one op per asset, yielding\n    [`AssetObservation`](https://docs.dagster.io/api/python-api/ops#dagster.AssetObservation) events.\n    Uses Dagster's dynamic orchestration to process multiple assets in parallel.\n\n    !!! tip\n        This is a very powerful way to observe all your Metaxy features at once.\n        Use it in combination with a [Dagster schedule](https://docs.dagster.io/concepts/schedules)\n        to run it periodically.\n\n    Provide either:\n    - `asset_selection` and `defs`: Select assets from a\n      [`Definitions`](https://docs.dagster.io/api/python-api/definitions#dagster.Definitions) object\n\n    - `assets`: Direct list of assets to observe\n\n    !!! note\n        All selected assets must share the same partitioning (if any).\n\n    Args:\n        name: Name for the job.\n        asset_selection: An `AssetSelection` specifying which assets to observe.\n            Must be used together with `defs`.\n        defs: The `Definitions` object to resolve the selection against.\n            Must be used together with `asset_selection`.\n        assets: Direct sequence of assets to observe. Each item can be an\n            `AssetSpec`, `AssetsDefinition`, or `SourceAsset`.\n            Cannot be used together with `asset_selection`/`defs`.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        tags: Optional tags to apply to the job.\n        **kwargs: Additional keyword arguments passed to the\n            [`@job`](https://docs.dagster.io/api/python-api/jobs#dagster.job) decorator.\n\n    Returns:\n        A Dagster job definition that observes all matching Metaxy assets.\n\n    Raises:\n        ValueError: If no specs have `metaxy/feature` metadata, if assets have\n            inconsistent `partitions_def`, or if invalid argument combinations\n            are provided.\n\n    Example:\n        ```python\n        import dagster as dg\n        import metaxy.ext.dagster as mxd\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature_a\"})\n        def feature_a(): ...\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature_b\"})\n        def feature_b(): ...\n\n\n        # Option 1: Using asset_selection + defs\n        my_defs = dg.Definitions(assets=[feature_a, feature_b])\n        observation_job = mxd.build_metaxy_multi_observation_job(\n            name=\"observe_my_features\",\n            asset_selection=dg.AssetSelection.kind(\"metaxy\"),\n            defs=my_defs,\n        )\n\n        # Option 2: Using direct assets list\n        observation_job = mxd.build_metaxy_multi_observation_job(\n            name=\"observe_my_features\",\n            assets=[feature_a, feature_b],\n        )\n        ```\n    \"\"\"\n    tags = tags or {}\n\n    # Validate argument combinations\n    has_selection = asset_selection is not None or defs is not None\n    has_assets = assets is not None\n\n    if has_selection and has_assets:\n        raise ValueError(\n            \"Cannot provide both 'assets' and 'asset_selection'/'defs'. \"\n            \"Use either asset_selection + defs, or assets alone.\"\n        )\n\n    if not has_selection and not has_assets:\n        raise ValueError(\"Must provide either 'asset_selection' + 'defs', or 'assets'.\")\n\n    if has_selection:\n        if asset_selection is None:\n            raise ValueError(\"'defs' requires 'asset_selection' to be provided.\")\n        if defs is None:\n            raise ValueError(\"'asset_selection' requires 'defs' to be provided.\")\n\n        # Resolve selection using defs\n        all_assets_defs = list(defs.resolve_asset_graph().assets_defs)\n        selected_keys = asset_selection.resolve(all_assets_defs)\n\n        # Get specs for selected keys, with partitions_def\n        metaxy_specs: list[dg.AssetSpec] = []\n        partitions_defs: list[dg.PartitionsDefinition | None] = []\n\n        for asset_def in all_assets_defs:\n            for spec in asset_def.specs:\n                if spec.key in selected_keys:\n                    if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                        metaxy_specs.append(spec)\n                        partitions_defs.append(asset_def.partitions_def)\n    else:\n        # Direct assets list\n        assert assets is not None\n        metaxy_specs = []\n        partitions_defs = []\n\n        for asset in assets:\n            if isinstance(asset, dg.AssetSpec):\n                if asset.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                    metaxy_specs.append(asset)\n                    partitions_defs.append(asset.partitions_def)\n            elif isinstance(asset, dg.AssetsDefinition):\n                for spec in asset.specs:\n                    if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None:\n                        metaxy_specs.append(spec)\n                        partitions_defs.append(asset.partitions_def)\n            elif isinstance(asset, dg.SourceAsset):\n                # SourceAsset doesn't have metaxy/feature metadata typically\n                pass\n            else:\n                raise TypeError(f\"Expected AssetSpec, AssetsDefinition, or SourceAsset, got {type(asset).__name__}\")\n\n    if not metaxy_specs:\n        raise ValueError(\n            \"No assets have specs with 'metaxy/feature' metadata. \"\n            \"Ensure your assets have metadata={'metaxy/feature': 'feature/key'}.\"\n        )\n\n    # Validate all specs have the same partitions_def\n    first_partitions_def = partitions_defs[0]\n    for i, pdef in enumerate(partitions_defs[1:], start=1):\n        if pdef != first_partitions_def:\n            raise ValueError(\n                f\"All assets must have the same partitions_def. \"\n                f\"Asset 0 has {first_partitions_def}, but asset {i} has {pdef}.\"\n            )\n    partitions_def = first_partitions_def\n\n    # Build feature keys for description (may have duplicates when multiple assets share a feature)\n    feature_keys = [\n        mx.coerce_to_feature_key(spec.metadata[DAGSTER_METAXY_FEATURE_METADATA_KEY]) for spec in metaxy_specs\n    ]\n\n    # Build a mapping of asset key -&gt; spec for the dynamic op\n    # This ensures each asset gets its own op, even if multiple assets share the same feature\n    spec_by_asset_key = {spec.key.to_user_string(): spec for spec in metaxy_specs}\n    all_asset_keys = list(spec_by_asset_key.keys())\n\n    # Config class for runtime filtering of assets to observe\n    class _ObserveAssetsConfig(dg.Config):\n        asset_keys: list[str] = all_asset_keys\n\n    # Op that emits dynamic outputs for each asset, optionally filtered by config\n    @dg.op(\n        name=f\"{name}_fanout\",\n        out=dg.DynamicOut(str),\n        config_schema=_ObserveAssetsConfig.to_config_schema(),\n    )\n    def fanout_assets(context: dg.OpExecutionContext) -&gt; Any:\n        config = _ObserveAssetsConfig.model_validate(context.op_config)\n\n        # Validate that requested asset keys exist\n        requested_keys = set(config.asset_keys)\n        available_keys = set(spec_by_asset_key.keys())\n        invalid_keys = requested_keys - available_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Requested asset keys not found in job: {sorted(invalid_keys)}. \"\n                f\"Available keys: {sorted(available_keys)}\"\n            )\n        asset_keys_to_observe = [k for k in spec_by_asset_key if k in requested_keys]\n\n        for asset_key_str in asset_keys_to_observe:\n            # Use asset key (with / replaced by __) as mapping key for Dagster identifiers\n            safe_mapping_key = asset_key_str.replace(\"/\", \"__\")\n            yield dg.DynamicOutput(asset_key_str, mapping_key=safe_mapping_key)\n\n    # Build the shared observation op\n    observe_op = _build_observation_op_for_specs(\n        name=f\"{name}_observe\",\n        spec_by_asset_key=spec_by_asset_key,\n        store_resource_key=store_resource_key,\n    )\n\n    # Build job metadata with asset references\n    job_metadata: dict[str, Any] = {\n        \"metaxy/features\": [fk.to_string() for fk in feature_keys],\n    }\n    for spec in metaxy_specs:\n        job_metadata[f\"metaxy/asset/{spec.key.to_user_string()}\"] = dg.MetadataValue.asset(spec.key)\n\n    # Build description as markdown list showing both assets and features\n    asset_list = \"\\n\".join(\n        f\"- `{spec.key.to_user_string()}` \u2192 `{spec.metadata[DAGSTER_METAXY_FEATURE_METADATA_KEY]}`\"\n        for spec in metaxy_specs\n    )\n    description = f\"Observe {len(metaxy_specs)} Metaxy assets:\\n\\n{asset_list}\"\n\n    @dg.job(\n        name=name,\n        partitions_def=partitions_def,\n        tags=tags,\n        description=description,\n        metadata=job_metadata,\n        **kwargs,\n    )\n    def observation_job() -&gt; None:\n        asset_keys_dynamic = fanout_assets()\n        asset_keys_dynamic.map(observe_op)\n\n    return observation_job\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observation_job.build_metaxy_observation_job","title":"metaxy.ext.dagster.observation_job.build_metaxy_observation_job","text":"<pre><code>build_metaxy_observation_job(\n    asset: AssetSpec | AssetsDefinition,\n    *,\n    store_resource_key: str = \"store\",\n    tags: dict[str, str] | None = None,\n) -&gt; list[JobDefinition]\n</code></pre> <p>Build Dagster job(s) that observe Metaxy feature asset(s).</p> <p>Creates job(s) that yield <code>AssetObservation</code> events for the given asset. The job can be run independently from asset materialization, e.g., on a schedule.</p> <p>Returns one job per <code>metaxy/feature</code> spec found in the asset.</p> <p>Jobs are constructed with matching partitions definitions. Job names are always derived as <code>observe_&lt;FeatureKey.table_name()&gt;</code>.</p> <p>Parameters:</p> <ul> <li> <code>asset</code>               (<code>AssetSpec | AssetsDefinition</code>)           \u2013            <p>Asset spec or asset definition to observe. Must have <code>metaxy/feature</code> metadata on at least one spec.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>tags</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional tags to apply to the job(s).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[JobDefinition]</code>           \u2013            <p>List of Dagster job definitions, one per <code>metaxy/feature</code> spec.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no specs have <code>metaxy/feature</code> metadata.</p> </li> </ul> Example <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/feature\"})\ndef my_asset(): ...\n\n\n# Build the observation job - partitions_def is extracted automatically\nobservation_job = mxd.build_metaxy_observation_job(my_asset)\n\n# Include in your Definitions\ndefs = dg.Definitions(\n    jobs=[observation_job],\n    resources={\"store\": my_store_resource},\n)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/observation_job.py</code> <pre><code>@public\ndef build_metaxy_observation_job(\n    asset: dg.AssetSpec | dg.AssetsDefinition,\n    *,\n    store_resource_key: str = \"store\",\n    tags: dict[str, str] | None = None,\n) -&gt; list[dg.JobDefinition]:\n    \"\"\"Build Dagster job(s) that observe Metaxy feature asset(s).\n\n    Creates job(s) that yield `AssetObservation` events for the given asset.\n    The job can be run independently from asset materialization, e.g., on a schedule.\n\n    Returns one job per `metaxy/feature` spec found in the asset.\n\n    Jobs are constructed with matching partitions definitions.\n    Job names are always derived as `observe_&lt;FeatureKey.table_name()&gt;`.\n\n    Args:\n        asset: Asset spec or asset definition to observe. Must have `metaxy/feature`\n            metadata on at least one spec.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        tags: Optional tags to apply to the job(s).\n\n    Returns:\n        List of Dagster job definitions, one per `metaxy/feature` spec.\n\n    Raises:\n        ValueError: If no specs have `metaxy/feature` metadata.\n\n    Example:\n        ```python\n        import dagster as dg\n        import metaxy.ext.dagster as mxd\n\n\n        @mxd.metaxify()\n        @dg.asset(metadata={\"metaxy/feature\": \"my/feature\"})\n        def my_asset(): ...\n\n\n        # Build the observation job - partitions_def is extracted automatically\n        observation_job = mxd.build_metaxy_observation_job(my_asset)\n\n        # Include in your Definitions\n        defs = dg.Definitions(\n            jobs=[observation_job],\n            resources={\"store\": my_store_resource},\n        )\n        ```\n    \"\"\"\n    # Extract specs and partitions_def from asset\n    if isinstance(asset, dg.AssetSpec):\n        specs = [asset]\n        partitions_def = None\n    elif isinstance(asset, dg.AssetsDefinition):\n        specs = list(asset.specs)\n        partitions_def = asset.partitions_def\n    else:\n        raise TypeError(f\"Expected AssetSpec or AssetsDefinition, got {type(asset).__name__}\")\n\n    # Filter to specs with metaxy/feature metadata\n    metaxy_specs = [spec for spec in specs if spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY) is not None]\n\n    if not metaxy_specs:\n        raise ValueError(\n            \"Asset has no specs with 'metaxy/feature' metadata. \"\n            \"Ensure your asset has metadata={'metaxy/feature': 'feature/key'}.\"\n        )\n\n    # Build jobs for each metaxy spec\n    jobs = [\n        _build_observation_job_for_spec(\n            spec,\n            partitions_def=partitions_def,\n            store_resource_key=store_resource_key,\n            tags=tags,\n        )\n        for spec in metaxy_specs\n    ]\n\n    return jobs\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#resources","title":"Resources","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource  <code>pydantic-model</code>","text":"<p>               Bases: <code>ConfigurableResource[MetadataStore]</code></p> <p>This resource creates a <code>metaxy.MetadataStore</code> based on the current Metaxy configuration (<code>metaxy.toml</code>).</p> <p>If <code>name</code> is not provided, the default store will be used. The default store name can be set with <code>store = \"my_name\"</code> in <code>metaxy.toml</code> or with<code>$METAXY_STORE</code> environment variable.</p> Show JSON schema: <pre><code>{\n  \"description\": \"This resource creates a [`metaxy.MetadataStore`](https://docs.metaxy.io/latest/guide/concepts/metadata-stores/) based on the current Metaxy configuration (`metaxy.toml`).\\n\\nIf `name` is not provided, the default store will be used.\\nThe default store name can be set with `store = \\\"my_name\\\"` in `metaxy.toml` or with` $METAXY_STORE` environment variable.\",\n  \"properties\": {\n    \"name\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Name\"\n    }\n  },\n  \"title\": \"MetaxyStoreFromConfigResource\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","text":"<pre><code>create_resource(\n    context: InitResourceContext,\n) -&gt; MetadataStore\n</code></pre> <p>Create a MetadataStore from the Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InitResourceContext</code>)           \u2013            <p>Dagster resource initialization context.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore</code>           \u2013            <p>A MetadataStore configured with the Dagster run ID as the materialization ID.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/resources.py</code> <pre><code>def create_resource(self, context: dg.InitResourceContext) -&gt; mx.MetadataStore:\n    \"\"\"Create a MetadataStore from the Metaxy configuration.\n\n    Args:\n        context: Dagster resource initialization context.\n\n    Returns:\n        A MetadataStore configured with the Dagster run ID as the materialization ID.\n    \"\"\"\n    assert context.run is not None\n    return mx.MetaxyConfig.get().get_store(self.name, materialization_id=context.run.run_id)\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#helpers","title":"Helpers","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.FeatureStats","title":"metaxy.ext.dagster.utils.FeatureStats","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Statistics about a feature's metadata for Dagster events.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.selection.select_metaxy_assets","title":"metaxy.ext.dagster.selection.select_metaxy_assets","text":"<pre><code>select_metaxy_assets(\n    *,\n    project: str | None = None,\n    feature: CoercibleToFeatureKey | None = None,\n) -&gt; AssetSelection\n</code></pre> <p>Select Metaxy assets by project and/or feature.</p> <p>This helper creates an <code>AssetSelection</code> that filters assets tagged by <code>@metaxify</code>.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by project name. If None, uses <code>MetaxyConfig.get().project</code>.</p> </li> <li> <code>feature</code>               (<code>CoercibleToFeatureKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature key. If provided, further narrows the selection.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AssetSelection</code>           \u2013            <p>An <code>AssetSelection</code> that can be used with <code>dg.define_asset_job</code>,</p> </li> <li> <code>AssetSelection</code>           \u2013            <p><code>dg.materialize</code>, or <code>AssetSelection</code> operations like <code>|</code> and <code>&amp;</code>.</p> </li> </ul> Select all Metaxy assets in current project <pre><code>import metaxy.ext.dagster as mxd\n\nall_metaxy = mxd.select_metaxy_assets()\n</code></pre> Select assets for a specific project <pre><code>prod_assets = mxd.select_metaxy_assets(project=\"production\")\n</code></pre> Select a specific feature's assets <pre><code>feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n</code></pre> Use with asset jobs <pre><code>metaxy_job = dg.define_asset_job(\n    name=\"materialize_metaxy\",\n    selection=mxd.select_metaxy_assets(),\n)\n</code></pre> Combine with other selections <pre><code># All metaxy assets plus some other assets\ncombined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n# Metaxy assets that are also in a specific group\nfiltered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n</code></pre> Source code in <code>src/metaxy/ext/dagster/selection.py</code> <pre><code>@public\ndef select_metaxy_assets(\n    *,\n    project: str | None = None,\n    feature: mx.CoercibleToFeatureKey | None = None,\n) -&gt; dg.AssetSelection:\n    \"\"\"Select Metaxy assets by project and/or feature.\n\n    This helper creates an `AssetSelection` that filters assets tagged by `@metaxify`.\n\n    Args:\n        project: Filter by project name. If None, uses `MetaxyConfig.get().project`.\n        feature: Filter by specific feature key. If provided, further narrows the selection.\n\n    Returns:\n        An `AssetSelection` that can be used with `dg.define_asset_job`,\n        `dg.materialize`, or `AssetSelection` operations like `|` and `&amp;`.\n\n    Example: Select all Metaxy assets in current project\n        ```python\n        import metaxy.ext.dagster as mxd\n\n        all_metaxy = mxd.select_metaxy_assets()\n        ```\n\n    Example: Select assets for a specific project\n        ```python\n        prod_assets = mxd.select_metaxy_assets(project=\"production\")\n        ```\n\n    Example: Select a specific feature's assets\n        ```python\n        feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n        ```\n\n    Example: Use with asset jobs\n        ```python\n        metaxy_job = dg.define_asset_job(\n            name=\"materialize_metaxy\",\n            selection=mxd.select_metaxy_assets(),\n        )\n        ```\n\n    Example: Combine with other selections\n        ```python\n        # All metaxy assets plus some other assets\n        combined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n        # Metaxy assets that are also in a specific group\n        filtered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n        ```\n    \"\"\"\n    resolved_project = project if project is not None else mx.MetaxyConfig.get().project\n    if resolved_project is None:\n        raise ValueError(\"project must be specified or configured in MetaxyConfig\")\n\n    selection = dg.AssetSelection.tag(DAGSTER_METAXY_PROJECT_TAG_KEY, resolved_project)\n\n    if feature is not None:\n        feature_key = mx.coerce_to_feature_key(feature)\n        selection = selection &amp; dg.AssetSelection.tag(DAGSTER_METAXY_FEATURE_METADATA_KEY, str(feature_key))\n\n    return selection\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#types","title":"Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyOutput","title":"metaxy.ext.dagster.MetaxyOutput  <code>module-attribute</code>","text":"<pre><code>MetaxyOutput = IntoFrame | None\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#constants","title":"Constants","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_FEATURE_METADATA_KEY = 'metaxy/feature'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_KIND","title":"metaxy.ext.dagster.DAGSTER_METAXY_KIND  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_KIND = 'metaxy'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_INFO_METADATA_KEY = 'metaxy/info'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_PARTITION_KEY = 'partition_by'\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/","title":"Metaxify","text":"<p>The <code>@metaxify</code> decorator can be used to automatically enrich Dagster assets definitions with information taken from Metaxy features.</p> <p>It's highly recommended to <code>@metaxify</code> all your Dagster assets that produce Metaxy features. It's also recommended to use it in combination with <code>MetaxyIOManager</code>.</p> <p><code>@metaxify</code> modifies most of the attributes available on the asset spec.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#deps","title":"Deps","text":"<p>Upstream Metaxy features are injected into <code>deps</code>.</p> Dagster UICode <p></p> <p> <code>models/landmarker_v1</code> is an upstream non-metaxy Dagster asset.</p> <pre><code>import metaxy as mx\n\nclass Chunk(mx.BaseFeature, spec=mx.FeatureSpec(key=\"chunk\", id_columns=[\"id\"])):\n    id: str\n\nclass BodyPose(mx.BaseFeature, spec=mx.FeatureSpec(key=\"body/pose\", id_columns=[\"id\"])):\n    id: str\n\nspec = mx.FeatureSpec(key=\"downstream\", id_columns=[\"id\"], deps=[Chunk, BodyPose])\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#code-version","title":"Code Version","text":"<p>Metaxy's feature spec code version is appended to the asset's code version in the format <code>metaxy:&lt;version&gt;</code>.</p> <p></p>"},{"location":"integrations/orchestration/dagster/metaxify/#description","title":"Description","text":"<p>The Metaxy feature class docstring is used if the asset spec doesn't have a description set.</p> Dagster UICode <p></p> <pre><code>import metaxy as mx\n\nclass AudioFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/feature\", id_columns=[\"id\"])):\n    \"\"\"Scene chunk audio with optional waveform visualization.\"\"\"\n    id: str\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#metadata","title":"Metadata","text":"<p><code>@metaxify</code> injects static metadata into the asset spec.</p> <p>All standard metadata types are supported. Additionally, <code>metaxy/info</code> is added. It contains the Metaxy feature spec, Metaxy project, Metaxy version and enabled Metaxy plugins.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#column-schema","title":"Column Schema","text":"<p>Pydantic fields schema is injected into the asset metadata under <code>dagster/column_schema</code>. Field types are converted to strings, and field descriptions are used as column descriptions. If the asset already has a column schema defined, Metaxy columns are appended (user-defined columns take precedence for columns with the same name).</p> <p>Warning</p> <p>Pydantic feature schema may not match the corresponding table schema in the metadata store. This will be improved in the future.</p> Dagster UICode <p></p> <pre><code>import metaxy as mx\nfrom pydantic import Field\n\nclass AudioFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"audio/feature2\", id_columns=[\"id\"])):\n    id: str\n    duration: float = Field(description=\"duration in seconds\")\n    sample_rate: int = Field(description=\"sample rate in Hz\")\n    channels: int = Field(description=\"number of audio channels\")\n    codec: str = Field(description=\"audio codec\")\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#column-lineage","title":"Column Lineage","text":"<p>Column lineage is injected into the asset metadata under <code>dagster/column_lineage</code>.</p> <p>Tracks which upstream columns each downstream column depends on by analyzing:</p> <ul> <li> <p>Direct pass-through: Columns with the same name in both upstream and downstream features.</p> </li> <li> <p><code>FeatureDep.rename</code>: Renamed columns trace back to their original upstream column names.</p> </li> <li> <p><code>FeatureDep.lineage</code>: ID column relationships based on lineage type (identity, aggregation, expansion).</p> </li> </ul> <p>Column lineage is derived from Pydantic model fields on the feature class. If the asset already has column lineage defined, Metaxy lineage is merged with user-defined lineage (user-defined dependencies are appended to Metaxy-detected dependencies for each column).</p>"},{"location":"integrations/orchestration/dagster/metaxify/#kinds","title":"Kinds","text":"<p><code>\"metaxy\"</code> kind is injected into asset kinds if <code>inject_metaxy_kind</code> is <code>True</code> and there are less than 3 kinds currently.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#tags","title":"Tags","text":"<p><code>metaxy/feature</code> and <code>metaxy/project</code> are injected into the asset tags.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#arbitrary-asset-attributes","title":"Arbitrary Asset Attributes","text":"<p>All keys from <code>\"dagster/attributes\"</code> in the feature spec metadata (such as <code>group_name</code>, <code>owners</code>, <code>tags</code>) are applied to the Dagster asset spec (with replacement).</p>"},{"location":"integrations/plugins/","title":"Metaxy Plugins","text":"<p>These integrations typically are Python libraries or have to be enabled in <code>metaxy.toml</code>.</p>"},{"location":"integrations/plugins/#available-plugins","title":"Available Plugins","text":"<ul> <li>SQLAlchemy</li> <li>SQLModel</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/","title":"SQLAlchemy","text":"<p>Experimental</p> <p>This functionality is experimental.</p> <p>Metaxy provides helpers for integrating with SQLAlchemy. These helpers allow to construct <code>sqlalchemy.MetaData</code> objects for user-defined feature tables and for Metaxy system tables.</p> <p>This integration is convenient for setting up Alembic migrations.</p> <p>SQLModel Features</p> <p>Check out our SQLModel integration for metaclass features that combine Metaxy features with SQLModel ORM models. This is the recommended way to use Metaxy with SQLAlchemy.</p>"},{"location":"integrations/plugins/sqlalchemy/#alembic-integration","title":"Alembic Integration","text":"<p>Alembic is a database migration toolkit for SQLAlchemy.</p> <p>The two helper functions: <code>filter_feature_sqla_metadata</code> and <code>get_system_slqa_metadata</code> can be used to retrieve an SQLAlchemy URL and <code>MetaData</code> object for a given <code>IbisMetadataStore</code>.</p> <p><code>filter_feature_sqla_metadata</code> returns table metadata for the user-defined tables, while <code>get_system_slqa_metadata</code> returns metadata for Metaxy's system tables.</p> <p>Call <code>init</code> first</p> <p>You must call <code>init</code> before using <code>filter_feature_sqla_metadata</code> to ensure all features are loaded into the feature graph.</p> <p>Here is an example Alembic <code>env.py</code> that uses the Metaxy SQLAlchemy integration:</p> env.py<pre><code>from alembic import context\nfrom metaxy import init\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n\n# Alembic Config object\nconfig = context.config\n\nmetaxy_cfg = init()\nstore = metaxy_cfg.get_store(\"my_store\")\n\n# import your SQLAlchemy metadata from somewhere\nmy_metadata = ...\n\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n\n# Configure Alembic\nconfig.set_main_option(\"sqlalchemy.url\", url)\n\n\n# continue with the standard Alembic workflow\n</code></pre> <p>Naming Conventions</p> <p>For consistent, deterministic and readable constraint names in your migrations, configure naming conventions.</p>"},{"location":"integrations/plugins/sqlalchemy/#multi-store-setup","title":"Multi-Store Setup","text":"<p>You can configure separate metadata stores for different environments:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"dev_metadata.duckdb\" }\n\n[stores.prod]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"prod_metadata.duckdb\" }\n</code></pre> <p>Then create multiple Alembic directories and register them with Alembic:</p> alembic.ini<pre><code>[dev]\nscript_location = alembic/dev\n\n[prod]\nscript_location = alembic/prod\n</code></pre> <p>Separate Alembic Version Tables</p> <p>When using multiple Alembic environments (e.g., system tables vs feature tables), configure separate version tables to avoid conflicts. Set up separate script locations in <code>alembic.ini</code>:</p> alembic.ini<pre><code>[dev:metaxy_system]\nscript_location = alembic/dev/system\n\n[dev:metaxy_features]\nscript_location = alembic/dev/features\n</code></pre> <p>Then pass <code>version_table</code> to <code>context.configure()</code> in each env.py:</p> alembic/dev/system/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_system\",\n)\n</code></pre> alembic/dev/features/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_features\",\n)\n</code></pre> <p>Each environment now tracks migrations independently:</p> <ul> <li><code>alembic_version_metaxy_system</code> for system tables</li> <li><code>alembic_version_metaxy_features</code> for feature tables</li> </ul> <p>Create and run migrations separately:</p> <pre><code>alembic -n dev:metaxy_system revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_features revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_system upgrade head\nalembic -n dev:metaxy_features upgrade head\n</code></pre> <p>The two environments now can be managed independently:</p> devprod alembic/dev/env.py<pre><code>from metaxy import init\nconfig = init()\nstore = config.get_store(\"dev\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n dev upgrade head\n</code></pre> alembic/prod/env.py<pre><code>from metaxy import init\nconfig = init()\nstore = config.get_store(\"prod\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n prod upgrade head\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#alembic-sqlmodel","title":"Alembic + SQLModel","text":"<p>To throw <code>SQLModel</code> into the mix, make sure to use the SQLModel integration and pass <code>sqlmodel.SQLModel.metadata</code> into <code>filter_feature_sqla_metadata</code>.</p>"},{"location":"integrations/plugins/sqlalchemy/#api-reference","title":"API Reference","text":""},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy","title":"metaxy.ext.sqlalchemy","text":"<p>SQLAlchemy integration for metaxy.</p> <p>This module provides SQLAlchemy table definitions and helpers for metaxy. These can be used with migration tools like Alembic.</p> <p>The main functions return tuples of (sqlalchemy_url, metadata) for easy integration with migration tools:</p> <ul> <li><code>get_system_slqa_metadata</code>: Get URL and system table metadata for a store</li> <li><code>filter_feature_sqla_metadata</code>: Get URL and feature table metadata for a store</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","title":"metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","text":"<pre><code>filter_feature_sqla_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Experimental</p> <p>This functionality is experimental.</p> <p>Get SQLAlchemy URL and feature table metadata for a metadata store.</p> <p>This function filters the source metadata to include only feature tables belonging to the specified project, and returns the connection URL for the store.</p> <p>This function must be called after init() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.               If False, include all features.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> <li> <code>ImportError</code>             \u2013            <p>If source_metadata is None and SQLModel is not installed</p> </li> </ul> Note <p>Metadata stores do their best at providing the correct <code>sqlalchemy_url</code>, so you typically don't need to modify the output of this function.</p> <p>Example: Basic Usage</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\nfrom sqlalchemy import MetaData\n\n# Load features first\nmx.init()\n\n# Get store instance\nconfig = mx.MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\nmy_metadata = MetaData()\n# ... define tables in my_metadata ...\n\n# apply the filter function\nurl, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n```\n</code></pre> <p>Example: With SQLModel</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom sqlmodel import SQLModel\n\nurl, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>@experimental\n@public\ndef filter_feature_sqla_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and feature table metadata for a metadata store.\n\n    This function filters the source metadata to include only feature tables\n    belonging to the specified project, and returns the connection URL for the store.\n\n    This function must be called after init() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance\n        source_metadata: Source SQLAlchemy MetaData to filter.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n                          If False, include all features.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n        protocol: Optional protocol to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n        ImportError: If source_metadata is None and SQLModel is not installed\n\n    Note:\n        Metadata stores do their best at providing the correct `sqlalchemy_url`, so you typically don't need to modify the output of this function.\n\n    Example: Basic Usage\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n        from sqlalchemy import MetaData\n\n        # Load features first\n        mx.init()\n\n        # Get store instance\n        config = mx.MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        my_metadata = MetaData()\n        # ... define tables in my_metadata ...\n\n        # apply the filter function\n        url, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n        ```\n\n    Example: With SQLModel\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from sqlmodel import SQLModel\n\n        url, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n        ```\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n    metadata = _get_features_metadata(\n        source_metadata=source_metadata,\n        store=store,\n        project=project,\n        filter_by_project=filter_by_project,\n        inject_primary_key=inject_primary_key,\n        inject_index=inject_index,\n    )\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.get_system_slqa_metadata","title":"metaxy.ext.sqlalchemy.get_system_slqa_metadata","text":"<pre><code>get_system_slqa_metadata(\n    store: IbisMetadataStore,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Experimental</p> <p>This functionality is experimental.</p> <p>Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.</p> <p>This function retrieves both the connection URL and system table metadata for a store, with the store's <code>table_prefix</code> automatically applied to table names.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol (drivername) to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, system_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> Note <p>Metadata stores do their best at providing the correct <code>sqlalchemy_url</code>, so you typically don't need to modify the output of this function.</p> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>@public\n@experimental\ndef get_system_slqa_metadata(\n    store: IbisMetadataStore,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.\n\n    This function retrieves both the connection URL and system table metadata\n    for a store, with the store's `table_prefix` automatically applied to table names.\n\n    Args:\n        store: IbisMetadataStore instance\n        protocol: Optional protocol (drivername) to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, system_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n\n    Note:\n        Metadata stores do their best at providing the correct `sqlalchemy_url`, so you typically don't need to modify the output of this function.\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n    metadata = _get_system_metadata(table_prefix=store._table_prefix)\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#configuration","title":"Configuration","text":"<p>Configuration for SQLAlchemy integration.</p> <p>This plugin provides helpers for working with SQLAlchemy metadata and table definitions.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for SQLAlchemy integration.\\n\\nThis plugin provides helpers for working with SQLAlchemy metadata\\nand table definitions.\",\n  \"properties\": {\n    \"enable\": {\n      \"default\": false,\n      \"description\": \"Whether to enable the plugin.\",\n      \"title\": \"Enable\",\n      \"type\": \"boolean\"\n    },\n    \"inject_primary_key\": {\n      \"default\": false,\n      \"description\": \"Automatically inject composite primary key constraints on user-defined feature tables. The key is composed of ID columns, `metaxy_created_at`, and `metaxy_data_version`.\",\n      \"title\": \"Inject Primary Key\",\n      \"type\": \"boolean\"\n    },\n    \"inject_index\": {\n      \"default\": false,\n      \"description\": \"Automatically inject composite index on user-defined feature tables. The index covers ID columns, `metaxy_created_at`, and `metaxy_data_version`.\",\n      \"title\": \"Inject Index\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"SQLAlchemyConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>env_prefix</code>: <code>METAXY_EXT__SQLALCHEMY_</code></li> <li><code>extra</code>: <code>forbid</code></li> </ul> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__ENABLE=false\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_PRIMARY_KEY=false\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_INDEX=false\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.SQLAlchemyConfig.enable","title":"metaxy.ext.sqlalchemy.SQLAlchemyConfig.enable  <code>pydantic-field</code>","text":"<pre><code>enable: bool = False\n</code></pre> <p>Whether to enable the plugin.</p>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.SQLAlchemyConfig.inject_primary_key","title":"metaxy.ext.sqlalchemy.SQLAlchemyConfig.inject_primary_key  <code>pydantic-field</code>","text":"<pre><code>inject_primary_key: bool = False\n</code></pre> <p>Automatically inject composite primary key constraints on user-defined feature tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p>"},{"location":"integrations/plugins/sqlalchemy/#metaxy.ext.sqlalchemy.SQLAlchemyConfig.inject_index","title":"metaxy.ext.sqlalchemy.SQLAlchemyConfig.inject_index  <code>pydantic-field</code>","text":"<pre><code>inject_index: bool = False\n</code></pre> <p>Automatically inject composite index on user-defined feature tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p>"},{"location":"integrations/plugins/sqlmodel/","title":"SQLModel","text":"<p>Experimental</p> <p>This functionality is experimental.</p> <p>SQLModel combines SQLAlchemy and Pydantic into a single ORM. If you want your Metaxy feature definitions to double as ORM models, enable the SQLModel integration. This exposes user-defined feature tables directly to SQLAlchemy.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores.</p> <p>Database Migrations</p> <p>For database migration management with Alembic, see the SQLAlchemy integration guide.</p>"},{"location":"integrations/plugins/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install 'metaxy[sqlmodel]'\n</code></pre> <p>and has to be enabled explicitly:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_ENABLE=true\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#usage","title":"Usage","text":"<p>The SQLModel integration provides <code>BaseSQLModelFeature</code> which combines the functionality of a Metaxy feature and an SQLModel table.</p> <pre><code>import metaxy as mx\nimport metaxy.ext.sqlmodel as mxsql\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    mxsql.BaseSQLModelFeature,\n    table=True,\n    spec=mx.FeatureSpec(\n        key=FeatureKey([\"video\"]),\n        id_columns=[\"video_id\"],\n        fields=[\n            \"frames\",\n            \"duration\",\n        ],\n    ),\n):\n    # User-defined metadata columns\n    video_id: str\n    path: str\n    duration: float\n</code></pre> <p>Do Not Use Server-Generated IDs</p> <p>ID columns should not be server-generated because they are typically used to determine data locations such as object storage keys, so they have to be defined before metadata is inserted into the database</p> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in the plugin configuration.</p>"},{"location":"integrations/plugins/sqlmodel/#database-migrations","title":"Database Migrations","text":"<p>When using SQLModel features with Alembic or other migration tools, use <code>filter_feature_sqlmodel_metadata()</code> to transform table names and filter metadata.</p> <p>Table Name Transformation</p> <p>Pass <code>SQLModel.metadata</code> to <code>filter_feature_sqlmodel_metadata()</code> and it will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <pre><code>from sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom metaxy.config import MetaxyConfig\nfrom metaxy import init\n\nconfig = init()\nstore = config.get_store()\n\n# Transform SQLModel metadata with table_prefix\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nfrom alembic import context\n\ncontext.configure(url=url, target_metadata=target_metadata)\n</code></pre> <p>The <code>filter_feature_sqlmodel_metadata()</code> function:</p> <ul> <li>Transforms table names by adding the store's <code>table_prefix</code></li> <li>Filters tables by project (configurable)</li> <li>Returns the SQLAlchemy URL for the store</li> <li>Optionally injects primary key and index constraints</li> </ul> <p>See the SQLAlchemy integration guide for complete Alembic setup examples.</p> <p>Separate Alembic Version Tables</p> <p>When managing both system tables and feature tables with Alembic, use separate version tables to avoid conflicts. See the Multi-Store Setup section for configuration details.</p>"},{"location":"integrations/plugins/sqlmodel/#api-reference","title":"API Reference","text":""},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel","title":"metaxy.ext.sqlmodel","text":""},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.BaseSQLModelFeature","title":"metaxy.ext.sqlmodel.BaseSQLModelFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>SQLModel</code>, <code>BaseFeature</code></p> <p>Base class for <code>Metaxy</code> features that are also <code>SQLModel</code> tables.</p> <p>Example</p> <p> <pre><code>from metaxy.integrations.sqlmodel import BaseSQLModelFeature\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    BaseSQLModelFeature,\n    table=True,\n    spec=mx.FeatureSpec(\n        key=mx.FeatureKey([\"video\"]),\n        id_columns=[\"uid\"],\n        fields=[\n            mx.FieldSpec(\n                key=mx.FieldKey([\"video_file\"]),\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n    uid: str = Field(primary_key=True)\n    path: str\n    duration: float\n\n    # Now you can use both Metaxy and SQLModel features:\n    # - VideoFeature.feature_version() -&gt; Metaxy versioning\n    # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\n</code></pre></p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Base class for `Metaxy` features that are also `SQLModel` tables.\\n\\n!!! example\\n\\n    &lt;!-- skip next --&gt;\\n    ```py\\n    from metaxy.integrations.sqlmodel import BaseSQLModelFeature\\n    from sqlmodel import Field\\n\\n\\n    class VideoFeature(\\n        BaseSQLModelFeature,\\n        table=True,\\n        spec=mx.FeatureSpec(\\n            key=mx.FeatureKey([\\\"video\\\"]),\\n            id_columns=[\\\"uid\\\"],\\n            fields=[\\n                mx.FieldSpec(\\n                    key=mx.FieldKey([\\\"video_file\\\"]),\\n                    code_version=\\\"1\\\",\\n                ),\\n            ],\\n        ),\\n    ):\\n        uid: str = Field(primary_key=True)\\n        path: str\\n        duration: float\\n\\n        # Now you can use both Metaxy and SQLModel features:\\n        # - VideoFeature.feature_version() -&gt; Metaxy versioning\\n        # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\\n    ```\",\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"default\": null,\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_project_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph snapshot\",\n      \"title\": \"Metaxy Project Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    },\n    \"metaxy_updated_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was last updated (UTC)\",\n      \"title\": \"Metaxy Updated At\"\n    },\n    \"metaxy_deleted_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Soft delete timestamp (UTC); null means active row\",\n      \"title\": \"Metaxy Deleted At\"\n    }\n  },\n  \"title\": \"BaseSQLModelFeature\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'frozen': False}</code></li> </ul>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","title":"metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","text":"<pre><code>filter_feature_sqlmodel_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.</p> <p>This function transforms SQLModel table names to include the store's table_prefix, ensuring that table names in the metadata match what's expected in the database.</p> <p>You can pass <code>SQLModel.metadata</code> directly - this function will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <p>This function must be called after init() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).             Tables are looked up in this metadata by their unprefixed names.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> <li> <code>protocol</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional protocol to replace the existing one in the URL. Useful when Ibis uses a different protocol than SQLAlchemy requires.</p> </li> <li> <code>port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional port to replace the existing one in the URL. Useful when the SQLAlchemy driver uses a different port than Ibis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> Note <p>For ClickHouse, the <code>sqlalchemy_url</code> property already returns the native protocol with port 9000, so you typically don't need to override these.</p> <p>Example: Basic Usage</p> <pre><code>&lt;!-- skip next --&gt;\n```py\nfrom sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom alembic import context\n\n# Load features first\nmx.init()\n\n# Get store instance\nconfig = mx.MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\n# Filter SQLModel metadata with prefix transformation\nurl, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\ncontext.configure(url=url, target_metadata=target_metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlmodel/plugin.py</code> <pre><code>@public\ndef filter_feature_sqlmodel_metadata(\n    store: \"IbisMetadataStore\",\n    source_metadata: \"MetaData\",\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    protocol: str | None = None,\n    port: int | None = None,\n) -&gt; tuple[str, \"MetaData\"]:\n    \"\"\"Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.\n\n    This function transforms SQLModel table names to include the store's table_prefix,\n    ensuring that table names in the metadata match what's expected in the database.\n\n    You can pass `SQLModel.metadata` directly - this function will transform table names\n    by adding the store's `table_prefix`. The returned metadata will have prefixed table\n    names that match the actual database tables.\n\n    This function must be called after init() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)\n        source_metadata: Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).\n                        Tables are looked up in this metadata by their unprefixed names.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n        protocol: Optional protocol to replace the existing one in the URL.\n            Useful when Ibis uses a different protocol than SQLAlchemy requires.\n        port: Optional port to replace the existing one in the URL.\n            Useful when the SQLAlchemy driver uses a different port than Ibis.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n\n    Note:\n        For ClickHouse, the `sqlalchemy_url` property already returns the native\n        protocol with port 9000, so you typically don't need to override these.\n\n    Example: Basic Usage\n\n        &lt;!-- skip next --&gt;\n        ```py\n        from sqlmodel import SQLModel\n        from metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\n        from alembic import context\n\n        # Load features first\n        mx.init()\n\n        # Get store instance\n        config = mx.MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        # Filter SQLModel metadata with prefix transformation\n        url, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n        # Use with Alembic env.py\n        url, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n        context.configure(url=url, target_metadata=target_metadata)\n        ```\n    \"\"\"\n    from sqlalchemy import MetaData\n\n    from metaxy.ext.sqlalchemy.plugin import _get_store_sqlalchemy_url\n\n    config = MetaxyConfig.get(load=True)\n\n    if project is None:\n        project = config.project\n\n    # Check plugin config for defaults\n    sqlmodel_config = config.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    if inject_primary_key is None:\n        inject_primary_key = sqlmodel_config.inject_primary_key\n    if inject_index is None:\n        inject_index = sqlmodel_config.inject_index\n\n    url = _get_store_sqlalchemy_url(store, protocol=protocol, port=port)\n\n    # Create new metadata with transformed table names\n    filtered_metadata = MetaData()\n\n    # Get the FeatureGraph to look up feature classes by key\n    from metaxy.models.feature import FeatureGraph\n\n    feature_graph = FeatureGraph.get_active()\n\n    # Iterate over tables in source metadata\n    for table_name, original_table in source_metadata.tables.items():\n        # Check if this table has Metaxy feature metadata\n        if metaxy_system_info := original_table.info.get(\"metaxy-system\"):\n            metaxy_info = MetaxyTableInfo.model_validate(metaxy_system_info)\n            feature_key = metaxy_info.feature_key\n        else:\n            continue\n        # Look up the feature definition from the FeatureGraph\n        definition = feature_graph.feature_definitions_by_key.get(feature_key)\n        if definition is None:\n            # Skip tables for features that aren't registered\n            continue\n\n        # Filter by project if requested\n        if filter_by_project:\n            if definition.project != project:\n                continue\n\n        # Compute prefixed name using store's table_prefix\n        prefixed_name = store.get_table_name(feature_key)\n\n        # Copy table to new metadata with prefixed name\n        new_table = original_table.to_metadata(filtered_metadata, name=prefixed_name)\n\n        # Inject constraints if requested\n        if inject_primary_key or inject_index:\n            from metaxy.ext.sqlalchemy.plugin import _inject_constraints\n\n            _inject_constraints(\n                table=new_table,\n                spec=definition.spec,\n                inject_primary_key=inject_primary_key,\n                inject_index=inject_index,\n            )\n\n    return url, filtered_metadata\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#configuration","title":"Configuration","text":"<p>Configuration for SQLModel integration.</p> <p>This plugin enhances SQLModel-based features with automatic table name inference and optional primary key injection.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Configuration for SQLModel integration.\\n\\nThis plugin enhances SQLModel-based features with automatic table name\\ninference and optional primary key injection.\",\n  \"properties\": {\n    \"enable\": {\n      \"default\": false,\n      \"description\": \"Whether to enable the plugin.\",\n      \"title\": \"Enable\",\n      \"type\": \"boolean\"\n    },\n    \"inject_primary_key\": {\n      \"default\": false,\n      \"description\": \"Automatically inject composite primary key constraints on SQLModel tables. The key is composed of ID columns, `metaxy_created_at`, and `metaxy_data_version`.\",\n      \"title\": \"Inject Primary Key\",\n      \"type\": \"boolean\"\n    },\n    \"inject_index\": {\n      \"default\": false,\n      \"description\": \"Automatically inject composite index on SQLModel tables. The index covers ID columns, `metaxy_created_at`, and `metaxy_data_version`.\",\n      \"title\": \"Inject Index\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"title\": \"SQLModelPluginConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>env_prefix</code>: <code>METAXY_EXT__SQLMODEL_</code></li> <li><code>extra</code>: <code>forbid</code></li> </ul> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__ENABLE=false\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_PRIMARY_KEY=false\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_INDEX=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.SQLModelPluginConfig.enable","title":"metaxy.ext.sqlmodel.SQLModelPluginConfig.enable  <code>pydantic-field</code>","text":"<pre><code>enable: bool = False\n</code></pre> <p>Whether to enable the plugin.</p>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.SQLModelPluginConfig.inject_primary_key","title":"metaxy.ext.sqlmodel.SQLModelPluginConfig.inject_primary_key  <code>pydantic-field</code>","text":"<pre><code>inject_primary_key: bool = False\n</code></pre> <p>Automatically inject composite primary key constraints on SQLModel tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p>"},{"location":"integrations/plugins/sqlmodel/#metaxy.ext.sqlmodel.SQLModelPluginConfig.inject_index","title":"metaxy.ext.sqlmodel.SQLModelPluginConfig.inject_index  <code>pydantic-field</code>","text":"<pre><code>inject_index: bool = False\n</code></pre> <p>Automatically inject composite index on SQLModel tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p>"},{"location":"metaxy/design/","title":"Design Choices","text":"<p>As discussed on the front page and the Pitch, Metaxy aims to be pluggable, reliable, scalable, and developer-friendly. Here are some of the design choices we made to achieve these goals.</p>"},{"location":"metaxy/design/#storage","title":"Storage","text":"Data vs Metadata Clarifications <p>Metaxy features represent tabular metadata, typically containing references to external multimodal data such as files, images, or videos.</p> Subject Description Data The actual multimodal data itself, such as images, audio files, video files, text documents, and other raw content that your pipelines process and transform. Metadata Information about the data, typically including references to where data is stored (e.g., object store keys), plus additional descriptive entries such as video length, file size, format, version, and other attributes. <p>Metaxy does not interact with data and is not responsible for its content. As an edge case, Metaxy may also manage pure metadata tables that do not reference any external data.</p> <p>Metaxy is designed to be compatible with storage systems which satisfy the following requirements:</p> <ul> <li> <p>has an append operation</p> </li> <li> <p>can store map-like elements (e.g. dictionaries)</p> Lifting This Requirement <p>Unfortunately, the most popular database - PostgreSQL - does not satisfy it. While PostgreSQL is not an ideal choice for a Metaxy Metadata Store for other reasons (mainly being analytical queries performance), we recognize the need to support it and are exploring a solution in <code>anam-org/metaxy#223</code>. This requirement may not be necessary in the future.</p> </li> </ul> <p>This allows Metaxy to target modern data warehouses (e.g. ClickHouse, BigQuery, Snowflake, or the more minimalistic DuckDB) and storage formats such as DeltaLake, Iceberg, DuckLake, and anything compatible with Apache Arrow.</p> <p>The Metaxy abstraction that implements these design choices and is used to interact with storage systems is known as Metadata Store.</p>"},{"location":"metaxy/design/#table-schema","title":"Table Schema","text":"<p>Metaxy uses the same storage layout for all storage systems. Each feature gets a separate table.</p> <p>Here is how a typical Metaxy feature table looks like:</p> id metaxy_feature_version metaxy_data_version metaxy_data_version_by_field metaxy_provenance metaxy_provenance_by_field metaxy_created_at metaxy_updated_at metaxy_deleted_at video_001 a1b2c3d4 e7f8a9b0 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> e7f8a9b0 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> 2024-01-15T10:30:00Z 2024-01-15T10:30:00Z null video_002 a1b2c3d4 c1e4b9d8 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> c1e4b9d8 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> 2024-01-15T10:31:00Z 2024-01-15T10:31:00Z null video_003 a1b2c3d4 k1j2ah7v <code>{\"audio\": \"custom01\", \"frames\": \"custom02\"}</code> a8e2f4c9 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> 2024-01-15T10:32:00Z 2024-01-16T14:20:00Z null video_001 f5d6e7c8 b2c3d4e5 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> b2c3d4e5 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> 2024-01-18T09:00:00Z 2024-01-18T09:00:00Z null <p>It can also contain custom user-defined columns (1).</p> <ol> <li>and in fact, <code>id</code> is such a column, because ID columns are customizable</li> </ol> <p>Info</p> <p><code>metaxy_data_version</code>/<code>metaxy_data_version_by_field</code> and <code>metaxy_provenance</code>/<code>metaxy_provenance_by_field</code> serve a slightly different purpose. Provenance columns hold static versioning information entirely defined by the Metaxy framework. Data version defaults to the same value as provenance, but can be customized by the user at runtime, for example by deriving it from the contents of the computed sample. Learn more here.</p> <p>All historical records for a given feature are stored in the same table. They can be separated by the following system columns:</p> <ul> <li> <p><code>metaxy_feature_version</code> is shared among multiple rows and is changed on any of the feature or upstream feature <code>code_version</code> changes</p> </li> <li> <p><code>metaxy_data_version</code>, <code>metaxy_data_version_by_field</code>, <code>metaxy_provenance</code>, <code>metaxy_provenance_by_field</code> carry versioning and provenance information about the specific row</p> </li> <li> <p><code>metaxy_created_at</code>, <code>metaxy_updated_at</code>, <code>metaxy_deleted_at</code> allow to identify the latest active row for a given feature version</p> </li> </ul>"},{"location":"metaxy/design/#metadata-operations","title":"Metadata Operations","text":"<p>Metaxy tables are immutable. Once written, a row is never modified or deleted (1).</p> <ol> <li>but users can delete rows manually if needed</li> </ol> <p>As discussed earlier, writing metadata in Metaxy is done by appending to a feature table. Subsequent writes with the same feature version effectively act as overwrites. This is achieved by filtering out older rows using the <code>metaxy_updated_at</code> columns (1). Soft-deletes are implemented as appends as well.</p> <ol> <li>also known as merge-on-read</li> </ol> <p>The append-only design choice has a few significant benefits:</p> <ul> <li> <p>unlocks easier and lock-free setups for multiple writers</p> </li> <li> <p>ensures existing and historical metadata can never be lost or corrupted</p> </li> </ul> <p>Tip</p> <p>Users can implement storage cleanup based on their specific needs and constraints.</p> <ul> <li> <p>avoids additional write-time checks or operations, which has performance benefits</p> </li> <li> <p>allows Metaxy to be used with storage systems which lack ACID guarantees and do not support transactions</p> </li> </ul> <p>Info</p> <p>These design choices come with a cost of increased storage usage. But storage is cheap while mistakes aren't.</p>"},{"location":"metaxy/design/#dataframe-api","title":"DataFrame API","text":"<p>In order to be versatile and support different compute engines, Metaxy uses Narwhals for DataFrame manipulations.</p> <p>This allows metadata store implementations to reuse the same code. Currently only a thin subset requires storage-specific implementations (1).</p> <p>(1) such as providing database-specific hashing syntax and some other operations</p>"},{"location":"metaxy/design/#compute","title":"Compute","text":"<p>Increment resolution in Metaxy involves running computations: every time the user requests an increment for a given feature, Metaxy has to join upstream features, hash their versions, and filter out samples that have already been processed. This can be performed either locally (typically favored in development environments) or remotely (achieves better performance in production). Metaxy supports both options: databases for remote compute and storage-only metadata stores for embedded compute (1).</p> <ol> <li>e.g. Polars or DuckDB</li> </ol> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store.</p> <p>When can local computations happen instead</p> <p>Metaxy's versioning engine runs on the local Polars versioning engine if:</p> <ol> <li> <p>The metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</p> </li> <li> <p>The user explicitly requested to keep the computations local by setting <code>versioning_engine=\"polars\"</code> when instantiating the metadata store.</p> </li> <li> <p>A fallback store had to be used to retrieve one of the parent features missing in the current store.</p> </li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p> <p>All metadata store implementations are guaranteed to return equivalent results. They are continuously tested against the reference Polars implementation.</p>"},{"location":"metaxy/design/#whats-next","title":"\ud83d\ude80 What's Next?","text":"<ul> <li>Itching to write some Metaxy code? Jump to Quickstart.</li> <li>Learn more about Metaxy concepts</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Invoke <code>mx</code> CLI from your terminal</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul>"},{"location":"metaxy/pitch/","title":"The Metaxy Pitch","text":"<p>Here is why we think Metaxy is so cool. Metaxy is...</p>"},{"location":"metaxy/pitch/#composable","title":"\ud83e\udde9 Composable","text":"<p>Bring your own... really everything. Metaxy is a universal glue for metadata. Use it with:</p> <ul> <li>Your database or storage format of choice to keep metadata where you want. DuckDB, ClickHouse and 20+ databases via Ibis (1), lakehouse storage formats such as DeltaLake or DuckLake, and other solutions such as LanceDB. All of this is available through a unified interface.</li> <li>Your favorite dataframe library: Polars, Pandas, or even run all Metaxy computations in the DB thanks to Narwhals</li> <li>Orchestrators: see the excellent Dagster integration </li> <li>Compute frameworks like Ray. We totally don't care how data (2) is produced or where it is stored.</li> <li>Version tracking methods. By default, Metaxy uses Merkle Trees to track changes in metadata. However, users can provide their own data versions and use content-based hashing techniques (3) if needed.</li> </ul> <ol> <li>while we don't (yet) ship native support for all these databases, the base <code>IbisMetadataStore</code> can be easily extended to handle additional databases</li> <li>so not the tables but the actual stuff: images, videos, texts, etc.</li> <li>from naive <code>sha256</code> to more sophisticated semantic hashing</li> </ol>"},{"location":"metaxy/pitch/#reliable","title":"\ud83e\udea8 Reliable","text":"<p>Metaxy is obsessively tested across all supported tabular compute engines. We guarantee to produce versioning hashes that are consistent across DBs and local compute engines. We really have tested this very well! (1)</p> <ol> <li>We have a rich test suite that's run against Linux, Windows and MacOS on all supported Python versions</li> </ol> <p>Metadata is organized in append-only tables. Metaxy never attempts to modify historical metadata, (1) ensuring that data integrity is maintained and historical metadata can be easily retrieved.</p> <ol> <li>But provides the ability to perform hard and soft deletions</li> </ol>"},{"location":"metaxy/pitch/#scalable","title":"\ud83d\udcc8 Scalable","text":"<p>Metaxy is built with performance in mind: all operations default to run in the DB, the storage layout is designed with the goal of supporting parallel writers and bulk insertions.</p> <p>Feature definitions can be split across independent Python modules and packages and automatically loaded via packaging entry points. This enables collaboration across teams and projects.</p> <p>We also have a Ray integration which simplifies working with Metaxy from distributed workflows.</p>"},{"location":"metaxy/pitch/#developer-friendly","title":"\ud83e\uddd1\u200d\ud83d\udcbb Developer Friendly","text":"<p>Metaxy provides a clean, intuitive Python API with syntactic sugar that simplifies common feature definitions. The feature discovery system enables effortless feature dependency management.</p> <p>The library includes comprehensive type hints (1), and utilizes Pydantic for feature definitions. There's first-class support for local development (2), testing, preview environments, and CI/CD workflows.</p> <ol> <li>with all the typing shenanigans you would expect from a project as serious as ours</li> <li>the reference local versioning engine is implemented in Polars and <code>polars-hash</code></li> </ol> <p>The included CLI tool allows easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats. You can even drop your database in one command! (1)</p> <ol> <li>that's almost a joke</li> </ol> <p>Hopefully this was impressive enough and has sparked some interest in Metaxy!</p>"},{"location":"metaxy/pitch/#whats-next","title":"What's Next?","text":"<ul> <li>Learn about Metaxy design choices</li> <li>Itching to write some Metaxy code? Jump to Quickstart.</li> <li>Learn more about Metaxy concepts</li> <li>View complete, end-to-end examples</li> <li>Explore Metaxy integrations</li> <li>Invoke <code>mx</code> CLI from your terminal</li> <li>Learn how to configure Metaxy</li> <li>Get lost in our API Reference</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>Technical documentation for Metaxy.</p> <ul> <li> <p>CLI - Command-line interface</p> </li> <li> <p>API - Python interface</p> </li> <li> <p>Configuration - Configuration options</p> </li> <li> <p>Storage Layout - Feature table schema documentation</p> </li> </ul>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Warning</p> <p>The CLI is not stable yet.</p> <pre><code>metaxy COMMAND\n</code></pre> <p>Metaxy CLI.</p> <p>Auto-discovers configuration (<code>metaxy.toml</code> or <code>pyproject.toml</code>) in current or parent directories. Feature definitions are collected via feature discovery. Supports loading environment variables from a <code>.env</code> file in the current directory.</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>config</code><ul> <li><code>print</code></li> </ul> </li> <li><code>describe</code><ul> <li><code>graph</code></li> <li><code>features</code></li> </ul> </li> <li><code>graph</code><ul> <li><code>history</code></li> <li><code>render</code></li> </ul> </li> <li><code>list</code><ul> <li><code>features</code></li> <li><code>stores</code></li> </ul> </li> <li><code>metadata</code><ul> <li><code>status</code></li> <li><code>delete</code></li> <li><code>copy</code></li> </ul> </li> <li><code>mcp</code></li> <li><code>push</code></li> <li><code>lock</code></li> </ul> <p>Commands:</p> <ul> <li><code>config</code>: Manage Metaxy configuration</li> <li><code>describe</code>: Describe Metaxy entities in detail</li> <li><code>graph</code>: Manage feature graphs</li> <li><code>list</code>: List Metaxy entities</li> <li><code>lock</code>: Generate <code>metaxy.lock</code> file with external feature definitions fetched from the metadata store.</li> <li><code>mcp</code>: MCP server commands.</li> <li><code>metadata</code>: Manage Metaxy metadata in metadata stores.</li> <li><code>push</code>: Push feature definitions to the metadata store</li> <li><code>shell</code>: Start interactive shell.</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-shell","title":"metaxy shell","text":"<pre><code>metaxy shell\n</code></pre> <p>Start interactive shell.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-config","title":"metaxy config","text":"<p>Manage Metaxy configuration</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-config-print","title":"metaxy config print","text":"<pre><code>metaxy config print [OPTIONS]\n</code></pre> <p>Print the current Metaxy configuration.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'toml' (with syntax highlighting) or 'json'.  [choices: toml, json] [default: toml]</li> </ul>"},{"location":"reference/cli/#metaxy-describe","title":"metaxy describe","text":"<p>Describe Metaxy entities in detail</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-describe-graph","title":"metaxy describe graph","text":"<pre><code>metaxy describe graph [ARGS]\n</code></pre> <p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no dependents) - Project breakdown (if there some features are defined in different projects)</p> <p>Parameters:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Project version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-describe-features","title":"metaxy describe features","text":"<pre><code>metaxy describe features [OPTIONS] [ARGS]\n</code></pre> <p>Describe one or more features in detail.</p> <p>Shows comprehensive information about features including project, key, version, description, fields with their versions and dependencies.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-graph","title":"metaxy graph","text":"<p>Manage feature graphs</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"metaxy graph history","text":"<pre><code>metaxy graph history [ARGS]\n</code></pre> <p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing project versions, when they were recorded, and feature counts.</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"metaxy graph render","text":"<pre><code>metaxy graph render [OPTIONS] [ARGS]\n</code></pre> <p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:   - graph (default): Hierarchical tree view   - cards: Panel/card-based view with dependency edges - mermaid: Mermaid flowchart markup - graphviz: Graphviz DOT format</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>--type, -t</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>--output, -o</code>: Output file path (default: stdout)</li> <li><code>--snapshot</code>: Project version to render (default: current graph from code)</li> <li><code>--store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: False]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: False]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: True]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: True]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: True]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: False]</li> <li><code>--show-project-version, --no-show-project-version</code>: Show graph project version in output  [default: True]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: True]</li> <li><code>--title</code>: Custom title for the graph. Defaults to 'Feature Graph' or 'Feature Graph Changes' for diffs.</li> </ul>"},{"location":"reference/cli/#metaxy-list","title":"metaxy list","text":"<p>List Metaxy entities</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-list-features","title":"metaxy list features","text":"<pre><code>metaxy list features [OPTIONS]\n</code></pre> <p>List Metaxy features in the current project.</p> <p>Parameters:</p> <ul> <li><code>--verbose, --no-verbose, -v</code>: Show detailed information including field dependencies and versions.  [default: False]</li> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/cli/#metaxy-list-stores","title":"metaxy list stores","text":"<pre><code>metaxy list stores [OPTIONS]\n</code></pre> <p>List configured metadata stores.</p> <p>Parameters:</p> <ul> <li><code>--format, -f</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata","title":"metaxy metadata","text":"<p>Manage Metaxy metadata in metadata stores.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-status","title":"metaxy metadata status","text":"<pre><code>metaxy metadata status [OPTIONS] [ARGS]\n</code></pre> <p>Check metadata completeness and freshness for specified features.</p> <p>Parameters:</p> <ul> <li><code>--store</code>: Metadata store name (defaults to configured default store).</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--global-filter, --empty-global-filter</code>: SQL WHERE clause filter applied to all features being selected (including upstream). Can be repeated.</li> <li><code>--snapshot-id</code>: Check metadata against a specific project version.</li> <li><code>--assert-in-sync, --no-assert-in-sync</code>: Exit with error if any feature needs updates or metadata is missing.  [default: False]</li> <li><code>--verbose, --no-verbose</code>: Whether to display sample slices of dataframes.  [default: False]</li> <li><code>--progress, --no-progress</code>: Display progress percentage showing how many input units have been processed at least once. Stale samples are counted as processed.  [default: False]</li> <li><code>--allow-fallback-stores, --no-allow-fallback-stores</code>: Whether to read metadata from fallback stores.  [default: True]</li> <li><code>--format</code>:   [choices: plain, json] [default: plain]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-delete","title":"metaxy metadata delete","text":"<pre><code>metaxy metadata delete [OPTIONS] [ARGS]\n</code></pre> <p>Delete metadata rows matching filters.</p> <p>Parameters:</p> <ul> <li><code>--store</code>: Metadata store name (defaults to configured default store).</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--soft, --hard</code>: Whether to mark records with deletion timestamps vs physically remove them.  [default: True]</li> <li><code>--with-feature-history, --no-with-feature-history</code>: Include rows from all historical feature versions (by default, only current version is affected).  [default: False]</li> <li><code>--yes, --no-yes</code>: Confirm deletion without prompting (required for hard deletes without filters).  [default: False]</li> <li><code>--dry-run, --no-dry-run</code>: Preview deletion: show features, filters, and row counts without executing.  [default: False]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-copy","title":"metaxy metadata copy","text":"<pre><code>metaxy metadata copy --from STR --to STR [OPTIONS] [ARGS]\n</code></pre> <p>Copy metadata from one store to another.</p> <p>Copies metadata for specified features from source to destination store. By default, copies all versions (--with-feature-history) and deduplicates by keeping only the latest row per sample (--no-with-sample-history).</p> <p>Parameters:</p> <ul> <li><code>--from</code>: Source store name to copy metadata from.  [required]</li> <li><code>--to</code>: Destination store name to copy metadata to.  [required]</li> <li><code>--filter, --empty-filter</code>: SQL WHERE clause filter applied to the result of the status increment. Can be repeated.</li> <li><code>--with-feature-history, --no-with-feature-history</code>: Include rows from all historical feature versions (by default, only current version is copied).  [default: True]</li> <li><code>--with-sample-history, --no-with-sample-history</code>: Include all historical materializations per sample (by default, deduplicates by id_columns).  [default: False]</li> </ul> <p>Feature Selection:</p> <ul> <li><code>FEATURES, --features, --empty-features</code>: Feature keys (e.g., 'my_feature' or 'namespace/feature').  [default: ()]</li> <li><code>ALL-FEATURES, --all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-mcp","title":"metaxy mcp","text":"<pre><code>metaxy mcp\n</code></pre> <p>MCP server commands.</p> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-push","title":"metaxy push","text":"<pre><code>metaxy push [OPTIONS] [ARGS]\n</code></pre> <p>Push feature definitions to the metadata store</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)  [env: METAXY_STORE]</li> <li><code>--tags, -t</code>: Arbitrary key-value pairs to attach to the pushed snapshot. Example: <code>--tags.git_commit abc123def</code>.  [env: METAXY_TAGS]</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/cli/#metaxy-lock","title":"metaxy lock","text":"<pre><code>metaxy lock [OPTIONS] [ARGS]\n</code></pre> <p>Generate <code>metaxy.lock</code> file with external feature definitions fetched from the metadata store.</p> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)  [env: METAXY_STORE]</li> <li><code>--output, -o</code>: Output file path (defaults to metaxy.lock in config directory)  [env: METAXY_OUTPUT] [default: \"\"]</li> </ul> <p>Global:</p> <ul> <li><code>--config-file</code>: Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: False]</li> <li><code>--sync, --no-sync</code>: Load external feature definitions from the metadata store before executing the command.  [env: METAXY_SYNC] [default: False]</li> <li><code>--locked, --no-locked</code>: When used with --sync, raise an error if external feature versions don't match the actual versions from the metadata store.  [env: METAXY_LOCKED] [default: False]</li> </ul>"},{"location":"reference/configuration/","title":"Configuration","text":""},{"location":"reference/configuration/#configuring-metaxy","title":"Configuring Metaxy","text":"<p>Metaxy can be configured using TOML configuration files, environment variables, or programmatically.</p> <p>Either TOML-based or environment-based configuration is required to use the Metaxy CLI.</p> <p>Example</p> metaxy.toml<pre><code>project = \"my_package\"\n\n[stores.dev]\ntype = \"metaxy.ext.metadata_stores.delta.DeltaMetadataStore\"\n[stores.dev.config]\nroot_path = \"${HOME}/.metaxy/metadata\"\n</code></pre>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol>"},{"location":"reference/configuration/#config-discovery","title":"Config Discovery","text":"<p>Configuration files are discovered automatically by searching in the current or parent directories. <code>metaxy.toml</code> takes precedence over <code>pyproject.toml</code>.</p>"},{"location":"reference/configuration/#templating-environment-variables","title":"Templating Environment Variables","text":"<p>Metaxy supports templating environment variables in configuration files using the <code>${VARIABLE_NAME}</code> syntax.</p> <p>Example</p> metaxy.toml<pre><code>[stores.branch.config]\nroot_path = \"s3://my-bucket/${BRANCH_NAME}\"\n</code></pre>"},{"location":"reference/configuration/#configuration-options","title":"Configuration Options","text":"<p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> <p>Environment variables can be templated with <code>${MY_VAR:-default}</code> syntax.</p> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"PluginConfig\": {\n      \"additionalProperties\": true,\n      \"description\": \"Configuration for Metaxy plugins\",\n      \"properties\": {\n        \"enable\": {\n          \"default\": false,\n          \"description\": \"Whether to enable the plugin.\",\n          \"title\": \"Enable\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"PluginConfig\",\n      \"type\": \"object\"\n    },\n    \"StoreConfig\": {\n      \"additionalProperties\": false,\n      \"description\": \"Configuration options for metadata stores.\",\n      \"properties\": {\n        \"type\": {\n          \"description\": \"Full import path to metadata store class (e.g., `\\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"`)\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"config\": {\n          \"additionalProperties\": true,\n          \"description\": \"Store-specific configuration parameters (constructor kwargs). Includes `fallback_stores`, database connection parameters, etc.\",\n          \"title\": \"Config\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"type\"\n      ],\n      \"title\": \"StoreConfig\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Main Metaxy configuration.\\n\\nLoads from (in order of precedence):\\n\\n1. Init arguments\\n\\n2. Environment variables (METAXY_*)\\n\\n3. Config file (`metaxy.toml` or `[tool.metaxy]` in `pyproject.toml` )\\n\\nEnvironment variables can be templated with `${MY_VAR:-default}` syntax.\\n\\nExample: Accessing current configuration\\n    &lt;!-- skip next --&gt;\\n    ```py\\n    config = MetaxyConfig.load()\\n    ```\\n\\nExample: Getting a configured metadata store\\n    ```py\\n    store = config.get_store(\\\"prod\\\")\\n    ```\",\n  \"properties\": {\n    \"store\": {\n      \"default\": \"dev\",\n      \"description\": \"Default metadata store to use\",\n      \"title\": \"Store\",\n      \"type\": \"string\"\n    },\n    \"stores\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/StoreConfig\"\n      },\n      \"description\": \"Named store configurations\",\n      \"title\": \"Stores\",\n      \"type\": \"object\"\n    },\n    \"migrations_dir\": {\n      \"default\": \".metaxy/migrations\",\n      \"description\": \"Directory where migration files are stored\",\n      \"title\": \"Migrations Dir\",\n      \"type\": \"string\"\n    },\n    \"entrypoints\": {\n      \"description\": \"List of Python module paths to load for feature discovery\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Entrypoints\",\n      \"type\": \"array\"\n    },\n    \"theme\": {\n      \"default\": \"default\",\n      \"description\": \"Graph rendering theme for CLI visualization\",\n      \"title\": \"Theme\",\n      \"type\": \"string\"\n    },\n    \"ext\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/PluginConfig\"\n      },\n      \"description\": \"Configuration for Metaxy integrations with third-party tools\",\n      \"title\": \"Ext\",\n      \"type\": \"object\"\n    },\n    \"hash_truncation_length\": {\n      \"default\": 8,\n      \"description\": \"Truncate hash values to this length.\",\n      \"minimum\": 8,\n      \"title\": \"Hash Truncation Length\",\n      \"type\": \"integer\"\n    },\n    \"auto_create_tables\": {\n      \"default\": false,\n      \"description\": \"Auto-create tables when opening stores. It is not advised to enable this setting in production.\",\n      \"title\": \"Auto Create Tables\",\n      \"type\": \"boolean\"\n    },\n    \"project\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"[Project](/guide/concepts/projects.md) name. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (`/`) or double underscores (`__`)\",\n      \"title\": \"Project\"\n    },\n    \"locked\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Whether to raise an error if an external feature doesn't have a matching feature version when [syncing external features][metaxy.sync_external_features] from the metadata store.\",\n      \"title\": \"Locked\"\n    },\n    \"sync\": {\n      \"default\": true,\n      \"description\": \"Whether to automatically [sync external feature definitions][metaxy.sync_external_features] from the metadata during some operations. It's recommended to keep this enabled as it ensures versioning correctness for external feature definitions with a negligible performance impact.\",\n      \"title\": \"Sync\",\n      \"type\": \"boolean\"\n    },\n    \"metaxy_lock_path\": {\n      \"default\": \"metaxy.lock\",\n      \"description\": \"Relative or absolute path to the lock file, resolved from the config file's location.\",\n      \"title\": \"Metaxy Lock Path\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"MetaxyConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>env_prefix</code>: <code>METAXY_</code></li> <li><code>env_nested_delimiter</code>: <code>__</code></li> <li><code>frozen</code>: <code>True</code></li> </ul> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <pre><code>export METAXY_STORE=dev\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\nentrypoints = []\n</code></pre> <pre><code>export METAXY_ENTRYPOINTS=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>hash_truncation_length = 8\n</code></pre> <pre><code>[tool.metaxy]\nhash_truncation_length = 8\n</code></pre> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=8\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>project = \"...\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"...\"\n</code></pre> <pre><code>export METAXY_PROJECT=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>locked = false\n</code></pre> <pre><code>[tool.metaxy]\nlocked = false\n</code></pre> <pre><code>export METAXY_LOCKED=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>sync = true\n</code></pre> <pre><code>[tool.metaxy]\nsync = true\n</code></pre> <pre><code>export METAXY_SYNC=true\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>metaxy_lock_path = \"metaxy.lock\"\n</code></pre> <pre><code>[tool.metaxy]\nmetaxy_lock_path = \"metaxy.lock\"\n</code></pre> <pre><code>export METAXY_METAXY_LOCK_PATH=metaxy.lock\n</code></pre> <p>The <code>stores</code> field configures metadata store backends. This is a mapping of store names to their configurations. The default store is named <code>\"dev\"</code>.</p> <p>Configuration options for metadata stores.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Configuration options for metadata stores.\",\n  \"properties\": {\n    \"type\": {\n      \"description\": \"Full import path to metadata store class (e.g., `\\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"`)\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"config\": {\n      \"additionalProperties\": true,\n      \"description\": \"Store-specific configuration parameters (constructor kwargs). Includes `fallback_stores`, database connection parameters, etc.\",\n      \"title\": \"Config\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"type\"\n  ],\n  \"title\": \"StoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>forbid</code></li> <li><code>frozen</code>: <code>True</code></li> </ul> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev]\ntype = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev]\ntype = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__TYPE=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev]\nconfig = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev]\nconfig = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG={}\n</code></pre>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.store","title":"metaxy.config.MetaxyConfig.store  <code>pydantic-field</code>","text":"<pre><code>store: str = 'dev'\n</code></pre> <p>Default metadata store to use</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.stores","title":"metaxy.config.MetaxyConfig.stores  <code>pydantic-field</code>","text":"<pre><code>stores: dict[str, StoreConfig]\n</code></pre> <p>Named store configurations</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.entrypoints","title":"metaxy.config.MetaxyConfig.entrypoints  <code>pydantic-field</code>","text":"<pre><code>entrypoints: list[str]\n</code></pre> <p>List of Python module paths to load for feature discovery</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.hash_truncation_length","title":"metaxy.config.MetaxyConfig.hash_truncation_length  <code>pydantic-field</code>","text":"<pre><code>hash_truncation_length: int = 8\n</code></pre> <p>Truncate hash values to this length.</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.auto_create_tables","title":"metaxy.config.MetaxyConfig.auto_create_tables  <code>pydantic-field</code>","text":"<pre><code>auto_create_tables: bool = False\n</code></pre> <p>Auto-create tables when opening stores. It is not advised to enable this setting in production.</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.project","title":"metaxy.config.MetaxyConfig.project  <code>pydantic-field</code>","text":"<pre><code>project: str | None = None\n</code></pre> <p>Project name. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (<code>/</code>) or double underscores (<code>__</code>)</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.locked","title":"metaxy.config.MetaxyConfig.locked  <code>pydantic-field</code>","text":"<pre><code>locked: bool | None = None\n</code></pre> <p>Whether to raise an error if an external feature doesn't have a matching feature version when syncing external features from the metadata store.</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.sync","title":"metaxy.config.MetaxyConfig.sync  <code>pydantic-field</code>","text":"<pre><code>sync: bool = True\n</code></pre> <p>Whether to automatically sync external feature definitions from the metadata during some operations. It's recommended to keep this enabled as it ensures versioning correctness for external feature definitions with a negligible performance impact.</p>"},{"location":"reference/configuration/#metaxy.config.MetaxyConfig.metaxy_lock_path","title":"metaxy.config.MetaxyConfig.metaxy_lock_path  <code>pydantic-field</code>","text":"<pre><code>metaxy_lock_path: str = 'metaxy.lock'\n</code></pre> <p>Relative or absolute path to the lock file, resolved from the config file's location.</p>"},{"location":"reference/configuration/#metaxy.StoreConfig.type","title":"metaxy.StoreConfig.type  <code>pydantic-field</code>","text":"<pre><code>type: str\n</code></pre> <p>Full import path to metadata store class (e.g., <code>\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"</code>)</p>"},{"location":"reference/configuration/#metaxy.StoreConfig.config","title":"metaxy.StoreConfig.config  <code>pydantic-field</code>","text":"<pre><code>config: dict[str, Any]\n</code></pre> <p>Store-specific configuration parameters (constructor kwargs). Includes <code>fallback_stores</code>, database connection parameters, etc.</p>"},{"location":"reference/system-columns/","title":"System Columns","text":"<p>Metaxy reserves a set of system-managed columns that it attaches to user-defined feature metadata tables. These columns are part of the storage interface and are used by the metadata store. Learn more about the storage layout design here.</p>"},{"location":"reference/system-columns/#canonical-column-names","title":"Canonical column names","text":"Canonical name Explanation Level Type <code>metaxy_provenance_by_field</code> Derived from upstream data versions and code version per field sample struct <code>metaxy_provenance</code> Hash of <code>metaxy_provenance_by_field</code> sample string <code>metaxy_data_version_by_field</code> Defaults to <code>metaxy_provenance_by_field</code>, can be user-defined sample struct <code>metaxy_data_version</code> Hash of <code>metaxy_data_version_by_field</code> sample string <code>metaxy_feature_version</code> Derived from versions of relevant upstream fields feature string <code>metaxy_project_version</code> Derived from all Metaxy features which belong to the same Project project string <code>metaxy_definition_version</code> Hash of the feature spec and Pydantic model schema feature string <code>metaxy_created_at</code> Timestamp when the metadata row was created sample string <code>metaxy_updated_at</code> Timestamp when the metadata row was last written to the store sample string <code>metaxy_deleted_at</code> Timestamp when the metadata row was soft-deleted (null if active) sample string <code>metaxy_materialization_id</code> External orchestration run ID (e.g., Dagster, Airflow) for tracking run string <p>All system column names start with the <code>metaxy_</code> prefix.</p>"},{"location":"reference/system-columns/#example-table","title":"Example Table","text":"id metaxy_feature_version metaxy_data_version metaxy_data_version_by_field metaxy_provenance metaxy_provenance_by_field metaxy_created_at metaxy_updated_at metaxy_deleted_at video_001 a1b2c3d4 e7f8a9b0 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> e7f8a9b0 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> 2024-01-15T10:30:00Z 2024-01-15T10:30:00Z null video_002 a1b2c3d4 c1e4b9d8 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> c1e4b9d8 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> 2024-01-15T10:31:00Z 2024-01-15T10:31:00Z null video_003 a1b2c3d4 k1j2ah7v <code>{\"audio\": \"custom01\", \"frames\": \"custom02\"}</code> a8e2f4c9 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> 2024-01-15T10:32:00Z 2024-01-16T14:20:00Z null video_001 f5d6e7c8 b2c3d4e5 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> b2c3d4e5 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> 2024-01-18T09:00:00Z 2024-01-18T09:00:00Z null <p>It can also contain custom user-defined columns (1).</p> <ol> <li>and in fact, <code>id</code> is such a column, because ID columns are customizable</li> </ol>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#metaxy","title":"<code>metaxy</code>","text":"<p>The top-level <code>metaxy</code> module provides the main public API for Metaxy. It is typically aliased as <code>mx</code>:</p> <pre><code>import metaxy as mx\n</code></pre>"},{"location":"reference/api/#api-reference-sections","title":"API Reference Sections","text":"<ul> <li>Definitions</li> <li>Configuration</li> <li>Constants</li> <li>Initialization</li> <li>Types</li> <li>Utils</li> </ul>"},{"location":"reference/api/config/","title":"Configuration","text":"<p>This is the Python SDK for Metaxy's configuration. See config file reference to learn how to configure Metaxy via <code>TOML</code> files.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig","title":"metaxy.MetaxyConfig  <code>pydantic-model</code>","text":"<pre><code>MetaxyConfig(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool\n    | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool\n    | list[str]\n    | tuple[str, ...]\n    | None = None,\n    _cli_settings_source: CliSettingsSource[Any]\n    | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool\n    | Literal[\"all\", \"no_enums\"]\n    | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]]\n    | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n)\n</code></pre> <p>               Bases: <code>BaseSettings</code></p> <p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> <p>Environment variables can be templated with <code>${MY_VAR:-default}</code> syntax.</p> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"PluginConfig\": {\n      \"additionalProperties\": true,\n      \"description\": \"Configuration for Metaxy plugins\",\n      \"properties\": {\n        \"enable\": {\n          \"default\": false,\n          \"description\": \"Whether to enable the plugin.\",\n          \"title\": \"Enable\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"title\": \"PluginConfig\",\n      \"type\": \"object\"\n    },\n    \"StoreConfig\": {\n      \"additionalProperties\": false,\n      \"description\": \"Configuration options for metadata stores.\",\n      \"properties\": {\n        \"type\": {\n          \"description\": \"Full import path to metadata store class (e.g., `\\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"`)\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"config\": {\n          \"additionalProperties\": true,\n          \"description\": \"Store-specific configuration parameters (constructor kwargs). Includes `fallback_stores`, database connection parameters, etc.\",\n          \"title\": \"Config\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"type\"\n      ],\n      \"title\": \"StoreConfig\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Main Metaxy configuration.\\n\\nLoads from (in order of precedence):\\n\\n1. Init arguments\\n\\n2. Environment variables (METAXY_*)\\n\\n3. Config file (`metaxy.toml` or `[tool.metaxy]` in `pyproject.toml` )\\n\\nEnvironment variables can be templated with `${MY_VAR:-default}` syntax.\\n\\nExample: Accessing current configuration\\n    &lt;!-- skip next --&gt;\\n    ```py\\n    config = MetaxyConfig.load()\\n    ```\\n\\nExample: Getting a configured metadata store\\n    ```py\\n    store = config.get_store(\\\"prod\\\")\\n    ```\",\n  \"properties\": {\n    \"store\": {\n      \"default\": \"dev\",\n      \"description\": \"Default metadata store to use\",\n      \"title\": \"Store\",\n      \"type\": \"string\"\n    },\n    \"stores\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/StoreConfig\"\n      },\n      \"description\": \"Named store configurations\",\n      \"title\": \"Stores\",\n      \"type\": \"object\"\n    },\n    \"migrations_dir\": {\n      \"default\": \".metaxy/migrations\",\n      \"description\": \"Directory where migration files are stored\",\n      \"title\": \"Migrations Dir\",\n      \"type\": \"string\"\n    },\n    \"entrypoints\": {\n      \"description\": \"List of Python module paths to load for feature discovery\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Entrypoints\",\n      \"type\": \"array\"\n    },\n    \"theme\": {\n      \"default\": \"default\",\n      \"description\": \"Graph rendering theme for CLI visualization\",\n      \"title\": \"Theme\",\n      \"type\": \"string\"\n    },\n    \"ext\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/PluginConfig\"\n      },\n      \"description\": \"Configuration for Metaxy integrations with third-party tools\",\n      \"title\": \"Ext\",\n      \"type\": \"object\"\n    },\n    \"hash_truncation_length\": {\n      \"default\": 8,\n      \"description\": \"Truncate hash values to this length.\",\n      \"minimum\": 8,\n      \"title\": \"Hash Truncation Length\",\n      \"type\": \"integer\"\n    },\n    \"auto_create_tables\": {\n      \"default\": false,\n      \"description\": \"Auto-create tables when opening stores. It is not advised to enable this setting in production.\",\n      \"title\": \"Auto Create Tables\",\n      \"type\": \"boolean\"\n    },\n    \"project\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"[Project](/guide/concepts/projects.md) name. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (`/`) or double underscores (`__`)\",\n      \"title\": \"Project\"\n    },\n    \"locked\": {\n      \"anyOf\": [\n        {\n          \"type\": \"boolean\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Whether to raise an error if an external feature doesn't have a matching feature version when [syncing external features][metaxy.sync_external_features] from the metadata store.\",\n      \"title\": \"Locked\"\n    },\n    \"sync\": {\n      \"default\": true,\n      \"description\": \"Whether to automatically [sync external feature definitions][metaxy.sync_external_features] from the metadata during some operations. It's recommended to keep this enabled as it ensures versioning correctness for external feature definitions with a negligible performance impact.\",\n      \"title\": \"Sync\",\n      \"type\": \"boolean\"\n    },\n    \"metaxy_lock_path\": {\n      \"default\": \"metaxy.lock\",\n      \"description\": \"Relative or absolute path to the lock file, resolved from the config file's location.\",\n      \"title\": \"Metaxy Lock Path\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"MetaxyConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>env_prefix</code>: <code>METAXY_</code></li> <li><code>env_nested_delimiter</code>: <code>__</code></li> <li><code>frozen</code>: <code>True</code></li> </ul> Source code in <code>.venv/lib/python3.10/site-packages/pydantic_settings/main.py</code> <pre><code>def __init__(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool | list[str] | tuple[str, ...] | None = None,\n    _cli_settings_source: CliSettingsSource[Any] | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool | Literal['all', 'no_enums'] | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]] | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n) -&gt; None:\n    super().__init__(\n        **__pydantic_self__._settings_build_values(\n            values,\n            _case_sensitive=_case_sensitive,\n            _nested_model_default_partial_update=_nested_model_default_partial_update,\n            _env_prefix=_env_prefix,\n            _env_file=_env_file,\n            _env_file_encoding=_env_file_encoding,\n            _env_ignore_empty=_env_ignore_empty,\n            _env_nested_delimiter=_env_nested_delimiter,\n            _env_nested_max_split=_env_nested_max_split,\n            _env_parse_none_str=_env_parse_none_str,\n            _env_parse_enums=_env_parse_enums,\n            _cli_prog_name=_cli_prog_name,\n            _cli_parse_args=_cli_parse_args,\n            _cli_settings_source=_cli_settings_source,\n            _cli_parse_none_str=_cli_parse_none_str,\n            _cli_hide_none_type=_cli_hide_none_type,\n            _cli_avoid_json=_cli_avoid_json,\n            _cli_enforce_required=_cli_enforce_required,\n            _cli_use_class_docs_for_groups=_cli_use_class_docs_for_groups,\n            _cli_exit_on_error=_cli_exit_on_error,\n            _cli_prefix=_cli_prefix,\n            _cli_flag_prefix_char=_cli_flag_prefix_char,\n            _cli_implicit_flags=_cli_implicit_flags,\n            _cli_ignore_unknown_args=_cli_ignore_unknown_args,\n            _cli_kebab_case=_cli_kebab_case,\n            _cli_shortcuts=_cli_shortcuts,\n            _secrets_dir=_secrets_dir,\n        )\n    )\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.store","title":"metaxy.MetaxyConfig.store  <code>pydantic-field</code>","text":"<pre><code>store: str = 'dev'\n</code></pre> <p>Default metadata store to use</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.stores","title":"metaxy.MetaxyConfig.stores  <code>pydantic-field</code>","text":"<pre><code>stores: dict[str, StoreConfig]\n</code></pre> <p>Named store configurations</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.migrations_dir","title":"metaxy.MetaxyConfig.migrations_dir  <code>pydantic-field</code>","text":"<pre><code>migrations_dir: str = '.metaxy/migrations'\n</code></pre> <p>Directory where migration files are stored</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.entrypoints","title":"metaxy.MetaxyConfig.entrypoints  <code>pydantic-field</code>","text":"<pre><code>entrypoints: list[str]\n</code></pre> <p>List of Python module paths to load for feature discovery</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.theme","title":"metaxy.MetaxyConfig.theme  <code>pydantic-field</code>","text":"<pre><code>theme: str = 'default'\n</code></pre> <p>Graph rendering theme for CLI visualization</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.ext","title":"metaxy.MetaxyConfig.ext  <code>pydantic-field</code>","text":"<pre><code>ext: dict[str, PluginConfig]\n</code></pre> <p>Configuration for Metaxy integrations with third-party tools</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.hash_truncation_length","title":"metaxy.MetaxyConfig.hash_truncation_length  <code>pydantic-field</code>","text":"<pre><code>hash_truncation_length: int = 8\n</code></pre> <p>Truncate hash values to this length.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.auto_create_tables","title":"metaxy.MetaxyConfig.auto_create_tables  <code>pydantic-field</code>","text":"<pre><code>auto_create_tables: bool = False\n</code></pre> <p>Auto-create tables when opening stores. It is not advised to enable this setting in production.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.project","title":"metaxy.MetaxyConfig.project  <code>pydantic-field</code>","text":"<pre><code>project: str | None = None\n</code></pre> <p>Project name. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (<code>/</code>) or double underscores (<code>__</code>)</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.locked","title":"metaxy.MetaxyConfig.locked  <code>pydantic-field</code>","text":"<pre><code>locked: bool | None = None\n</code></pre> <p>Whether to raise an error if an external feature doesn't have a matching feature version when syncing external features from the metadata store.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.sync","title":"metaxy.MetaxyConfig.sync  <code>pydantic-field</code>","text":"<pre><code>sync: bool = True\n</code></pre> <p>Whether to automatically sync external feature definitions from the metadata during some operations. It's recommended to keep this enabled as it ensures versioning correctness for external feature definitions with a negligible performance impact.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.metaxy_lock_path","title":"metaxy.MetaxyConfig.metaxy_lock_path  <code>pydantic-field</code>","text":"<pre><code>metaxy_lock_path: str = 'metaxy.lock'\n</code></pre> <p>Relative or absolute path to the lock file, resolved from the config file's location.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.config_file","title":"metaxy.MetaxyConfig.config_file  <code>property</code>","text":"<pre><code>config_file: Path | None\n</code></pre> <p>The config file path used to load this configuration.</p> <p>Returns <code>None</code> if the config was created directly (not via <code>MetaxyConfig.load</code>).</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.lock_file","title":"metaxy.MetaxyConfig.lock_file  <code>property</code>","text":"<pre><code>lock_file: Path | None\n</code></pre> <p>The resolved lock file path.</p> <p>Returns the absolute path if <code>metaxy_lock_path</code> is absolute, otherwise resolves it relative to the config file's directory.</p> <p>Returns <code>None</code> if the path is relative and no config file is set.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.plugins","title":"metaxy.MetaxyConfig.plugins  <code>property</code>","text":"<pre><code>plugins: list[str]\n</code></pre> <p>Returns all enabled plugin names from ext configuration.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_project","title":"metaxy.MetaxyConfig.validate_project  <code>pydantic-validator</code>","text":"<pre><code>validate_project(v: str | None) -&gt; str | None\n</code></pre> <p>Validate project name follows naming rules.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"project\")\n@classmethod\ndef validate_project(cls, v: str | None) -&gt; str | None:\n    \"\"\"Validate project name follows naming rules.\"\"\"\n    if v is None:\n        return None\n    if not v:\n        raise ValueError(\"project name cannot be empty\")\n    if \"/\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain forward slashes (/). \"\n            f\"Forward slashes are reserved for FeatureKey separation\"\n        )\n    if \"__\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain double underscores (__). \"\n            f\"Double underscores are reserved for table name generation\"\n        )\n    import re\n\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", v):\n        raise ValueError(f\"project name '{v}' must contain only alphanumeric characters, underscores, and hyphens\")\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_plugin","title":"metaxy.MetaxyConfig.get_plugin  <code>classmethod</code>","text":"<pre><code>get_plugin(\n    name: str, plugin_cls: type[PluginConfigT]\n) -&gt; PluginConfigT\n</code></pre> <p>Get the plugin config from the global Metaxy config.</p> <p>Unlike <code>get()</code>, this method does not warn when the global config is not initialized. This is intentional because plugins may call this at import time to read their configuration, and returning default plugin config is always safe.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get_plugin(cls, name: str, plugin_cls: type[PluginConfigT]) -&gt; PluginConfigT:\n    \"\"\"Get the plugin config from the global Metaxy config.\n\n    Unlike `get()`, this method does not warn when the global config is not\n    initialized. This is intentional because plugins may call this at import\n    time to read their configuration, and returning default plugin config\n    is always safe.\n    \"\"\"\n    ext = cls.get(_allow_default_config=True).ext\n    if name in ext:\n        existing = ext[name]\n        if isinstance(existing, plugin_cls):\n            # Already the correct type\n            plugin = existing\n        else:\n            # Convert from generic PluginConfig or dict to specific plugin class\n            plugin = plugin_cls.model_validate(existing.model_dump())\n    else:\n        # Return default config if plugin not configured\n        plugin = plugin_cls()\n    return plugin\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.settings_customise_sources","title":"metaxy.MetaxyConfig.settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources: init \u2192 env \u2192 TOML.</p> <p>Priority (first wins): 1. Init arguments 2. Environment variables 3. TOML file</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources: init \u2192 env \u2192 TOML.\n\n    Priority (first wins):\n    1. Init arguments\n    2. Environment variables\n    3. TOML file\n    \"\"\"\n    toml_settings = TomlConfigSettingsSource(settings_cls)\n    return (init_settings, env_settings, toml_settings)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get","title":"metaxy.MetaxyConfig.get  <code>classmethod</code>","text":"<pre><code>get(\n    *,\n    load: bool = False,\n    _allow_default_config: bool = False,\n) -&gt; MetaxyConfig\n</code></pre> <p>Get the current Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>load</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and config is not set, calls <code>MetaxyConfig.load()</code> to load configuration from file. Useful for plugins that need config but don't want to require manual initialization.</p> </li> <li> <code>_allow_default_config</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Internal parameter. When True, returns default config without warning if global config is not set. Used by methods like <code>get_plugin</code> that may be called at import time.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get(cls, *, load: bool = False, _allow_default_config: bool = False) -&gt; \"MetaxyConfig\":\n    \"\"\"Get the current Metaxy configuration.\n\n    Args:\n        load: If True and config is not set, calls `MetaxyConfig.load()` to\n            load configuration from file. Useful for plugins that need config\n            but don't want to require manual initialization.\n        _allow_default_config: Internal parameter. When True, returns default\n            config without warning if global config is not set. Used by methods\n            like `get_plugin` that may be called at import time.\n    \"\"\"\n    cfg = _config_override.get() or _global_config\n    if cfg is None:\n        if load:\n            return cls.load()\n        if not _allow_default_config:\n            warnings.warn(\n                UserWarning(\n                    \"Global Metaxy configuration not initialized. It can be set with MetaxyConfig.set(config) typically after loading it from a toml file. Returning default configuration (with environment variables and other pydantic settings sources resolved).\"\n                ),\n                stacklevel=2,\n            )\n        return cls()\n    else:\n        return cfg\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.set","title":"metaxy.MetaxyConfig.set  <code>classmethod</code>","text":"<pre><code>set(config: Self | None) -&gt; None\n</code></pre> <p>Set the current Metaxy configuration (visible to all threads).</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef set(cls, config: Self | None) -&gt; None:\n    \"\"\"Set the current Metaxy configuration (visible to all threads).\"\"\"\n    global _global_config\n    _global_config = config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.is_set","title":"metaxy.MetaxyConfig.is_set  <code>classmethod</code>","text":"<pre><code>is_set() -&gt; bool\n</code></pre> <p>Check if the current Metaxy configuration is set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef is_set(cls) -&gt; bool:\n    \"\"\"Check if the current Metaxy configuration is set.\"\"\"\n    return _config_override.get() is not None or _global_config is not None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.reset","title":"metaxy.MetaxyConfig.reset  <code>classmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the current Metaxy configuration to None.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset the current Metaxy configuration to None.\"\"\"\n    global _global_config\n    _global_config = None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.use","title":"metaxy.MetaxyConfig.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Use this configuration temporarily, restoring previous config on exit.</p> Example <pre><code>test_config = MetaxyConfig(project=\"test\")\nwith test_config.use():\n    # Code here uses test config\n    assert MetaxyConfig.get().project == \"test\"\n# Previous config restored\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Use this configuration temporarily, restoring previous config on exit.\n\n    Example:\n        ```py\n        test_config = MetaxyConfig(project=\"test\")\n        with test_config.use():\n            # Code here uses test config\n            assert MetaxyConfig.get().project == \"test\"\n        # Previous config restored\n        ```\n    \"\"\"\n    token = _config_override.set(self)\n    try:\n        yield self\n    finally:\n        _config_override.reset(token)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.load","title":"metaxy.MetaxyConfig.load  <code>classmethod</code>","text":"<pre><code>load(\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; MetaxyConfig\n</code></pre> <p>Load config with auto-discovery and parent directory search.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional config file path.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set this parameter</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Search parent directories for config file</p> </li> <li> <code>auto_discovery_start</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to start search from. Defaults to current working directory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>Loaded config (TOML + env vars merged)</p> </li> </ul> Example <pre><code># Auto-discover with parent search\nconfig = MetaxyConfig.load()\n\n# Explicit file\nconfig = MetaxyConfig.load(\"custom.toml\")\n\n# Auto-discover without parent search\nconfig = MetaxyConfig.load(search_parents=False)\n\n# Auto-discover from a specific directory\nconfig = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; \"MetaxyConfig\":\n    \"\"\"Load config with auto-discovery and parent directory search.\n\n    Args:\n        config_file: Optional config file path.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set this parameter\n\n        search_parents: Search parent directories for config file\n        auto_discovery_start: Directory to start search from.\n            Defaults to current working directory.\n\n    Returns:\n        Loaded config (TOML + env vars merged)\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Auto-discover with parent search\n        config = MetaxyConfig.load()\n\n        # Explicit file\n        config = MetaxyConfig.load(\"custom.toml\")\n\n        # Auto-discover without parent search\n        config = MetaxyConfig.load(search_parents=False)\n\n        # Auto-discover from a specific directory\n        config = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n        ```\n    \"\"\"\n    # Search for config file if not explicitly provided\n\n    if config_from_env := os.getenv(\"METAXY_CONFIG\"):\n        config_file = Path(config_from_env)\n\n    if config_file is None and search_parents:\n        config_file = cls._discover_config_with_parents(auto_discovery_start)\n\n    # For explicit file, temporarily patch the TomlConfigSettingsSource\n    # to use that file, then use normal instantiation\n    # This ensures env vars still work\n\n    if config_file:\n        # Create a custom settings source class for this file\n        toml_path = Path(config_file)\n\n        class CustomTomlSource(TomlConfigSettingsSource):\n            def __init__(self, settings_cls: type[BaseSettings]):\n                # Skip auto-discovery, use explicit file\n                super(TomlConfigSettingsSource, self).__init__(settings_cls)\n                self.toml_file = toml_path\n                self.toml_data = self._load_toml()\n\n        # Customize sources to use custom TOML file\n        original_method = cls.settings_customise_sources\n\n        @classmethod\n        def custom_sources(\n            cls_inner,\n            settings_cls,\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n        ):\n            toml_settings = CustomTomlSource(settings_cls)\n            return (init_settings, env_settings, toml_settings)\n\n        # Temporarily replace method\n        cls.settings_customise_sources = custom_sources  # ty: ignore[invalid-assignment]\n        try:\n            config = cls()\n        finally:\n            cls.settings_customise_sources = original_method  # ty: ignore[invalid-assignment]\n        # Store the resolved config file path\n        config._config_file = toml_path.resolve()\n    else:\n        # Use default sources (auto-discovery + env vars)\n        config = cls()\n        # No config file used\n        config._config_file = None\n\n    cls.set(config)\n\n    # Load plugins after config is set (plugins may access MetaxyConfig.get())\n    config._load_plugins()\n\n    return config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_store","title":"metaxy.MetaxyConfig.get_store","text":"<pre><code>get_store(\n    name: str | None = None,\n    *,\n    expected_type: Literal[None] = None,\n    **kwargs: Any,\n) -&gt; MetadataStore\n</code></pre><pre><code>get_store(\n    name: str | None = None,\n    *,\n    expected_type: type[StoreTypeT],\n    **kwargs: Any,\n) -&gt; StoreTypeT\n</code></pre> <p>Instantiate metadata store by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Store name (uses config.store if None)</p> </li> <li> <code>expected_type</code>               (<code>type[StoreTypeT] | None</code>, default:                   <code>None</code> )           \u2013            <p>Expected type of the store. If the actual store type does not match the expected type, a <code>TypeError</code> is raised.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the store constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore | StoreTypeT</code>           \u2013            <p>Instantiated metadata store</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store name not found in config, or if fallback stores have different hash algorithms than the parent store</p> </li> <li> <code>ImportError</code>             \u2013            <p>If store class cannot be imported</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the actual store type does not match the expected type</p> </li> </ul> Example <pre><code>store = config.get_store(\"prod\")\n\n# Use default store\nstore = config.get_store()\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>def get_store(\n    self,\n    name: str | None = None,\n    *,\n    expected_type: type[StoreTypeT] | None = None,\n    **kwargs: Any,\n) -&gt; \"MetadataStore | StoreTypeT\":\n    \"\"\"Instantiate metadata store by name.\n\n    Args:\n        name: Store name (uses config.store if None)\n        expected_type: Expected type of the store.\n            If the actual store type does not match the expected type, a `TypeError` is raised.\n        **kwargs: Additional keyword arguments to pass to the store constructor.\n\n    Returns:\n        Instantiated metadata store\n\n    Raises:\n        ValueError: If store name not found in config, or if fallback stores\n            have different hash algorithms than the parent store\n        ImportError: If store class cannot be imported\n        TypeError: If the actual store type does not match the expected type\n\n    Example:\n        ```py\n        store = config.get_store(\"prod\")\n\n        # Use default store\n        store = config.get_store()\n        ```\n    \"\"\"\n    from metaxy.versioning.types import HashAlgorithm\n\n    if len(self.stores) == 0:\n        raise InvalidConfigError.from_config(\n            self,\n            \"No Metaxy stores available. They should be configured in metaxy.toml|pyproject.toml or via environment variables.\",\n        )\n\n    name = name or self.store\n\n    if name not in self.stores:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' not found in config. Available stores: {list(self.stores.keys())}\",\n        )\n\n    store_config = self.stores[name]\n\n    # Get store class (lazily imported on first access)\n    try:\n        store_class = store_config.type_cls\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to import store class '{store_config.type}' for store '{name}': {e}\",\n        ) from e\n\n    if expected_type is not None and not issubclass(store_class, expected_type):\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' is not of type '{expected_type.__name__}'\",\n        )\n\n    # Extract configuration and prepare for typed config model\n    config_copy = store_config.config.copy()\n\n    # Get hash_algorithm from config (if specified) and convert to enum\n    configured_hash_algorithm = config_copy.get(\"hash_algorithm\")\n    if configured_hash_algorithm is not None:\n        # Convert string to enum if needed\n        if isinstance(configured_hash_algorithm, str):\n            configured_hash_algorithm = HashAlgorithm(configured_hash_algorithm)\n            config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n    else:\n        # Don't set a default here - let the store choose its own default\n        configured_hash_algorithm = None\n\n    # Get the store's config model class and create typed config\n    config_model_cls = store_class.config_model()\n\n    # Get auto_create_tables from global config only if the config model supports it\n    if (\n        \"auto_create_tables\" not in config_copy\n        and self.auto_create_tables is not None\n        and \"auto_create_tables\" in config_model_cls.model_fields\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"auto_create_tables\"] = self.auto_create_tables\n\n    # Separate kwargs into config fields and extra constructor args\n    config_fields = set(config_model_cls.model_fields.keys())\n    extra_kwargs = {}\n    for key, value in kwargs.items():\n        if key in config_fields:\n            config_copy[key] = value\n        else:\n            extra_kwargs[key] = value\n\n    try:\n        typed_config = config_model_cls.model_validate(config_copy)\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to validate config for store '{name}': {e}\",\n        ) from e\n\n    # Instantiate using from_config() - fallback stores are resolved via MetaxyConfig.get()\n    # Use self.use() to ensure this config is available for fallback resolution\n    try:\n        with self.use():\n            store = store_class.from_config(typed_config, name=name, **extra_kwargs)\n    except InvalidConfigError:\n        # Don't re-wrap InvalidConfigError (e.g., from nested fallback store resolution)\n        raise\n    except Exception as e:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Failed to instantiate store '{name}' ({store_class.__name__}): {e}\",\n        ) from e\n\n    # Verify the store actually uses the hash algorithm we configured\n    # (in case a store subclass overrides the default or ignores the parameter)\n    # Only check if we explicitly configured a hash algorithm\n    if configured_hash_algorithm is not None and store.hash_algorithm != configured_hash_algorithm:\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' ({store_class.__name__}) was configured with \"\n            f\"hash_algorithm='{configured_hash_algorithm.value}' but is using \"\n            f\"'{store.hash_algorithm.value}'. The store class may have overridden \"\n            f\"the hash algorithm. All stores must use the same hash algorithm.\",\n        )\n\n    if expected_type is not None and not isinstance(store, expected_type):\n        raise InvalidConfigError.from_config(\n            self,\n            f\"Store '{name}' is not of type '{expected_type.__name__}'\",\n        )\n\n    return store\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.to_toml","title":"metaxy.MetaxyConfig.to_toml","text":"<pre><code>to_toml() -&gt; str\n</code></pre> <p>Serialize to TOML string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>TOML representation of this configuration.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>def to_toml(self) -&gt; str:\n    \"\"\"Serialize to TOML string.\n\n    Returns:\n        TOML representation of this configuration.\n    \"\"\"\n    data = self.model_dump(mode=\"json\", by_alias=True)\n    # Remove None values (TOML doesn't support them)\n    data = _remove_none_values(data)\n    return tomli_w.dumps(data)\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig","title":"metaxy.StoreConfig  <code>pydantic-model</code>","text":"<pre><code>StoreConfig(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool\n    | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool\n    | list[str]\n    | tuple[str, ...]\n    | None = None,\n    _cli_settings_source: CliSettingsSource[Any]\n    | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool\n    | Literal[\"all\", \"no_enums\"]\n    | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]]\n    | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n)\n</code></pre> <p>               Bases: <code>BaseSettings</code></p> <p>Configuration options for metadata stores.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"description\": \"Configuration options for metadata stores.\",\n  \"properties\": {\n    \"type\": {\n      \"description\": \"Full import path to metadata store class (e.g., `\\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"`)\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"config\": {\n      \"additionalProperties\": true,\n      \"description\": \"Store-specific configuration parameters (constructor kwargs). Includes `fallback_stores`, database connection parameters, etc.\",\n      \"title\": \"Config\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"type\"\n  ],\n  \"title\": \"StoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>forbid</code></li> <li><code>frozen</code>: <code>True</code></li> </ul> Source code in <code>.venv/lib/python3.10/site-packages/pydantic_settings/main.py</code> <pre><code>def __init__(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool | list[str] | tuple[str, ...] | None = None,\n    _cli_settings_source: CliSettingsSource[Any] | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool | Literal['all', 'no_enums'] | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]] | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n) -&gt; None:\n    super().__init__(\n        **__pydantic_self__._settings_build_values(\n            values,\n            _case_sensitive=_case_sensitive,\n            _nested_model_default_partial_update=_nested_model_default_partial_update,\n            _env_prefix=_env_prefix,\n            _env_file=_env_file,\n            _env_file_encoding=_env_file_encoding,\n            _env_ignore_empty=_env_ignore_empty,\n            _env_nested_delimiter=_env_nested_delimiter,\n            _env_nested_max_split=_env_nested_max_split,\n            _env_parse_none_str=_env_parse_none_str,\n            _env_parse_enums=_env_parse_enums,\n            _cli_prog_name=_cli_prog_name,\n            _cli_parse_args=_cli_parse_args,\n            _cli_settings_source=_cli_settings_source,\n            _cli_parse_none_str=_cli_parse_none_str,\n            _cli_hide_none_type=_cli_hide_none_type,\n            _cli_avoid_json=_cli_avoid_json,\n            _cli_enforce_required=_cli_enforce_required,\n            _cli_use_class_docs_for_groups=_cli_use_class_docs_for_groups,\n            _cli_exit_on_error=_cli_exit_on_error,\n            _cli_prefix=_cli_prefix,\n            _cli_flag_prefix_char=_cli_flag_prefix_char,\n            _cli_implicit_flags=_cli_implicit_flags,\n            _cli_ignore_unknown_args=_cli_ignore_unknown_args,\n            _cli_kebab_case=_cli_kebab_case,\n            _cli_shortcuts=_cli_shortcuts,\n            _secrets_dir=_secrets_dir,\n        )\n    )\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.StoreConfig.type","title":"metaxy.StoreConfig.type  <code>pydantic-field</code>","text":"<pre><code>type: str\n</code></pre> <p>Full import path to metadata store class (e.g., <code>\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"</code>)</p>"},{"location":"reference/api/config/#metaxy.StoreConfig.config","title":"metaxy.StoreConfig.config  <code>pydantic-field</code>","text":"<pre><code>config: dict[str, Any]\n</code></pre> <p>Store-specific configuration parameters (constructor kwargs). Includes <code>fallback_stores</code>, database connection parameters, etc.</p>"},{"location":"reference/api/config/#metaxy.StoreConfig.type_cls","title":"metaxy.StoreConfig.type_cls  <code>cached</code> <code>property</code>","text":"<pre><code>type_cls: MetadataStoreT\n</code></pre> <p>Get the store class, importing lazily on first access.</p> <p>Returns:</p> <ul> <li> <code>MetadataStoreT</code>           \u2013            <p>The metadata store class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If the store class cannot be imported</p> </li> </ul>"},{"location":"reference/api/config/#metaxy.StoreConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.StoreConfig.to_toml","title":"metaxy.StoreConfig.to_toml","text":"<pre><code>to_toml() -&gt; str\n</code></pre> <p>Serialize to TOML string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>TOML representation of this store configuration.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>def to_toml(self) -&gt; str:\n    \"\"\"Serialize to TOML string.\n\n    Returns:\n        TOML representation of this store configuration.\n    \"\"\"\n    data = self.model_dump(mode=\"json\", by_alias=True)\n    return tomli_w.dumps(data)\n</code></pre>"},{"location":"reference/api/config/#metaxy.config.InvalidConfigError","title":"metaxy.config.InvalidConfigError","text":"<pre><code>InvalidConfigError(\n    message: str, *, config_file: Path | None = None\n)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Raised when Metaxy configuration is invalid.</p> <p>This error includes helpful context about where the configuration was loaded from and how environment variables can affect configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    *,\n    config_file: Path | None = None,\n):\n    self.config_file = config_file\n    self.base_message = message\n\n    # Build the full error message with context\n    parts = [message]\n\n    if config_file:\n        parts.append(f\"Config file: {config_file}\")\n\n    parts.append(\"Note: METAXY_* environment variables can override config file settings \")\n\n    super().__init__(\"\\n\".join(parts))\n</code></pre>"},{"location":"reference/api/config/#metaxy.config.InvalidConfigError-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.config.InvalidConfigError.from_config","title":"metaxy.config.InvalidConfigError.from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: MetaxyConfig, message: str\n) -&gt; InvalidConfigError\n</code></pre> <p>Create an InvalidConfigError from a MetaxyConfig instance.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetaxyConfig</code>)           \u2013            <p>The MetaxyConfig instance that has the invalid configuration.</p> </li> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>The error message describing what's wrong.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>InvalidConfigError</code>           \u2013            <p>An InvalidConfigError with context from the config.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef from_config(cls, config: \"MetaxyConfig\", message: str) -&gt; \"InvalidConfigError\":\n    \"\"\"Create an InvalidConfigError from a MetaxyConfig instance.\n\n    Args:\n        config: The MetaxyConfig instance that has the invalid configuration.\n        message: The error message describing what's wrong.\n\n    Returns:\n        An InvalidConfigError with context from the config.\n    \"\"\"\n    return cls(message, config_file=config._config_file)\n</code></pre>"},{"location":"reference/api/constants/","title":"Constants","text":""},{"location":"reference/api/constants/#metaxy.models.constants","title":"Constants","text":"<p>Shared constants for system-managed column names.</p> <p>All system columns use the metaxy_ prefix to avoid conflicts with user columns.</p>"},{"location":"reference/api/constants/#metaxy.models.constants-attributes","title":"Attributes","text":""},{"location":"reference/api/constants/#metaxy.models.constants.DEFAULT_CODE_VERSION","title":"metaxy.models.constants.DEFAULT_CODE_VERSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CODE_VERSION = '__metaxy_initial__'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMN_PREFIX","title":"metaxy.models.constants.SYSTEM_COLUMN_PREFIX  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMN_PREFIX = 'metaxy_'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD","title":"metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE_BY_FIELD = (\n    f\"{SYSTEM_COLUMN_PREFIX}provenance_by_field\"\n)\n</code></pre> <p>Field-level provenance hashes (struct column mapping field names to hashes).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE","title":"metaxy.models.constants.METAXY_PROVENANCE  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE = f'{SYSTEM_COLUMN_PREFIX}provenance'\n</code></pre> <p>Hash of<code>metaxy_provenance_by_field</code> -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_VERSION","title":"metaxy.models.constants.METAXY_FEATURE_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}feature_version\"\n)\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROJECT_VERSION","title":"metaxy.models.constants.METAXY_PROJECT_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_PROJECT_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}project_version\"\n)\n</code></pre> <p>Hash of the entire feature graph project version (recorded during deployment).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DEFINITION_VERSION","title":"metaxy.models.constants.METAXY_DEFINITION_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DEFINITION_VERSION = (\n    f\"{SYSTEM_COLUMN_PREFIX}definition_version\"\n)\n</code></pre> <p>Hash of the complete feature definition including Pydantic schema and feature spec.</p> <p>This comprehensive hash captures the feature definition (excluding project): - Pydantic model schema (field types, descriptions, validators, serializers, etc.) - Feature specification (dependencies, fields, code_versions, metadata)</p> <p>Project is stored separately. Used in system tables to detect when ANY part of a feature changes.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD","title":"metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION_BY_FIELD = (\n    f\"{SYSTEM_COLUMN_PREFIX}data_version_by_field\"\n)\n</code></pre> <p>Field-level data version hashes (struct column mapping field names to version hashes).</p> <p>Similar to provenance_by_field, but can be user-overridden to implement custom versioning (e.g., content hashes, timestamps, semantic versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION","title":"metaxy.models.constants.METAXY_DATA_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION = f'{SYSTEM_COLUMN_PREFIX}data_version'\n</code></pre> <p>Hash of metaxy_data_version_by_field -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_CREATED_AT","title":"metaxy.models.constants.METAXY_CREATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_CREATED_AT = f'{SYSTEM_COLUMN_PREFIX}created_at'\n</code></pre> <p>Timestamp when the metadata row was created.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_UPDATED_AT","title":"metaxy.models.constants.METAXY_UPDATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_UPDATED_AT = f'{SYSTEM_COLUMN_PREFIX}updated_at'\n</code></pre> <p>Timestamp when the metadata row was last updated (written to the store).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DELETED_AT","title":"metaxy.models.constants.METAXY_DELETED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_DELETED_AT = f'{SYSTEM_COLUMN_PREFIX}deleted_at'\n</code></pre> <p>Timestamp when the metadata row was soft-deleted.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_MATERIALIZATION_ID","title":"metaxy.models.constants.METAXY_MATERIALIZATION_ID  <code>module-attribute</code>","text":"<pre><code>METAXY_MATERIALIZATION_ID = (\n    f\"{SYSTEM_COLUMN_PREFIX}materialization_id\"\n)\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID, Airflow Run ID) for tracking pipeline executions.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.ALL_SYSTEM_COLUMNS","title":"metaxy.models.constants.ALL_SYSTEM_COLUMNS  <code>module-attribute</code>","text":"<pre><code>ALL_SYSTEM_COLUMNS = frozenset(\n    {\n        METAXY_PROVENANCE_BY_FIELD,\n        METAXY_PROVENANCE,\n        METAXY_FEATURE_VERSION,\n        METAXY_PROJECT_VERSION,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_DELETED_AT,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre> <p>All Metaxy-managed column names that are injected into feature tables.</p>"},{"location":"reference/api/constants/#metaxy.models.constants._DROPPABLE_COLUMNS","title":"metaxy.models.constants._DROPPABLE_COLUMNS  <code>module-attribute</code>","text":"<pre><code>_DROPPABLE_COLUMNS = frozenset(\n    {\n        METAXY_FEATURE_VERSION,\n        METAXY_PROJECT_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_DELETED_AT,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants._COLUMNS_TO_DROP_BEFORE_JOIN","title":"metaxy.models.constants._COLUMNS_TO_DROP_BEFORE_JOIN  <code>module-attribute</code>","text":"<pre><code>_COLUMNS_TO_DROP_BEFORE_JOIN = frozenset(\n    {\n        METAXY_FEATURE_VERSION,\n        METAXY_PROJECT_VERSION,\n        METAXY_CREATED_AT,\n        METAXY_UPDATED_AT,\n        METAXY_MATERIALIZATION_ID,\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE","title":"metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMNS_WITH_LINEAGE: frozenset[str] = frozenset(\n    {\n        METAXY_PROVENANCE_BY_FIELD,\n        METAXY_PROVENANCE,\n        METAXY_DATA_VERSION_BY_FIELD,\n        METAXY_DATA_VERSION,\n    }\n)\n</code></pre>"},{"location":"reference/api/initialization/","title":"Initialization API","text":""},{"location":"reference/api/initialization/#initialization","title":"Initialization","text":""},{"location":"reference/api/initialization/#metaxy.init","title":"metaxy.init","text":"<pre><code>init(\n    config: MetaxyConfig | Path | str | None = None,\n    search_parents: bool = True,\n) -&gt; MetaxyConfig\n</code></pre> <p>Main user-facing initialization function for Metaxy. It loads feature definitions and the Metaxy configuration.</p> <p>The feature graphs is populated with feature definitions discovered in the Metaxy project. External features are loaded from a <code>metaxy.lock</code> if it is found.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetaxyConfig | Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy configuration to use for initialization. Will be auto-discovered if not provided.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set the config file path.</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to search parent directories for configuration files during config discovery.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>The activated Metaxy configuration.</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>@public\ndef init(\n    config: MetaxyConfig | Path | str | None = None,\n    search_parents: bool = True,\n) -&gt; MetaxyConfig:\n    \"\"\"Main user-facing initialization function for Metaxy. It loads feature definitions and the Metaxy [configuration][metaxy.MetaxyConfig].\n\n    The feature graphs is populated with feature definitions [discovered](/guide/concepts/projects.md#feature-discovery) in the Metaxy project.\n    [External features](/guide/concepts/definitions/external-features.md) are loaded from a `metaxy.lock` if it is found.\n\n    Args:\n        config: Metaxy configuration to use for initialization. Will be auto-discovered if not provided.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set the config file path.\n\n        search_parents: Whether to search parent directories for configuration files during config discovery.\n\n    Returns:\n        The activated Metaxy configuration.\n    \"\"\"\n    from metaxy.utils.lock_file import load_lock_file\n\n    if isinstance(config, MetaxyConfig):\n        MetaxyConfig.set(config)\n    else:\n        config = MetaxyConfig.load(\n            config_file=config,\n            search_parents=search_parents,\n        )\n    load_lock_file(config)\n    load_features(config.entrypoints)\n    return config\n</code></pre>"},{"location":"reference/api/initialization/#metaxy.sync_external_features","title":"metaxy.sync_external_features","text":"<pre><code>sync_external_features(\n    store: MetadataStore,\n    *,\n    on_version_mismatch: Literal[\"warn\", \"error\"]\n    | None = None,\n) -&gt; list[FeatureDefinition]\n</code></pre> <p>Sync external feature definitions from a metadata store if the graph has any.</p> <p>This function loads feature definitions from the metadata store to replace external feature placeholders in the active graph. It also validates that the versions match and warns or errors on mismatches.</p> <p>Additionally, this function loads any feature keys specified in the <code>features</code> config field, warning if any of them are not found in the metadata store.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>Metadata store to load from. Will be opened automatically if not already open.</p> </li> <li> <code>on_version_mismatch</code>               (<code>Literal['warn', 'error'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional override for the <code>on_version_mismatch</code> setting on external feature definitions.</p> <p>Info</p> <p>Setting <code>MetaxyConfig.locked</code> to <code>True</code> takes precedence over this argument.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureDefinition]</code>           \u2013            <p>List of loaded FeatureDefinition objects.</p> </li> </ul> Example <pre><code>import metaxy as mx\n\n# Sync external features before running a pipeline\nmx.sync_external_features(store)\n\n# Or with explicit error handling\nmx.sync_external_features(store, on_version_mismatch=\"error\")\n</code></pre> Source code in <code>src/metaxy/utils/external_features.py</code> <pre><code>@public\ndef sync_external_features(\n    store: MetadataStore,\n    *,\n    on_version_mismatch: Literal[\"warn\", \"error\"] | None = None,\n) -&gt; list[FeatureDefinition]:\n    \"\"\"Sync external feature definitions from a metadata store if the graph has any.\n\n    This function loads feature definitions from the metadata store to replace\n    external feature placeholders in the active graph. It also validates that\n    the versions match and warns or errors on mismatches.\n\n    Additionally, this function loads any feature keys specified in the\n    `features` config field, warning if any of them are not found in the metadata store.\n\n    Args:\n        store: Metadata store to load from. Will be opened automatically if not already open.\n        on_version_mismatch: Optional override for the `on_version_mismatch` setting on external feature definitions.\n\n            !!! info\n                Setting [`MetaxyConfig.locked`][metaxy.MetaxyConfig] to `True` takes precedence over this argument.\n\n    Returns:\n        List of loaded FeatureDefinition objects.\n\n    Example:\n        ```python\n        import metaxy as mx\n\n        # Sync external features before running a pipeline\n        mx.sync_external_features(store)\n\n        # Or with explicit error handling\n        mx.sync_external_features(store, on_version_mismatch=\"error\")\n        ```\n    \"\"\"\n    from metaxy.config import MetaxyConfig\n    from metaxy.metadata_store.system import SystemTableStorage\n\n    graph = FeatureGraph.get_active()\n    config = MetaxyConfig.get(_allow_default_config=True)\n\n    if not graph.has_external_features:\n        return []\n\n    # Check if locked mode is enabled\n    if config.locked:\n        on_version_mismatch = \"error\"\n\n    # Record versions of external features BEFORE loading\n    external_versions_before: dict[FeatureKey, tuple[str, dict[str, str], FeatureDefinition]] = {}\n    external_keys: list[str] = []\n    for key, defn in graph.feature_definitions_by_key.items():\n        if defn.is_external:\n            external_versions_before[key] = (\n                graph.get_feature_version(key),\n                graph.get_feature_version_by_field(key),\n                defn,\n            )\n            external_keys.append(key.to_string())\n\n    # Use nullcontext if store is already open, otherwise open it\n    cm = nullcontext(store) if store._is_open else store\n    result: list[FeatureDefinition] = []\n    with cm:\n        storage = SystemTableStorage(store)\n        for key_str in external_keys:\n            try:\n                loaded = storage._load_feature_definitions_raw(\n                    filters=[nw.col(\"feature_key\") == key_str],\n                )\n                result.extend(loaded)\n            except Exception as e:\n                warnings.warn(\n                    f\"Skipping feature '{key_str}': failed to load from store: {e}\",\n                    InvalidStoredFeatureWarning,\n                )\n\n    # Check for version mismatches\n    _check_version_mismatches(graph, external_versions_before, on_version_mismatch)\n\n    # Warn if there are still unresolved external features after sync\n    remaining_external = list(sorted(d.spec.key for d in graph.feature_definitions_by_key.values() if d.is_external))\n    if remaining_external:\n        keys_str = \", \".join(str(k) for k in remaining_external)\n        warnings.warn(\n            f\"After syncing, {len(remaining_external)} external feature(s) could not be resolved \"\n            f\"from the metadata store: {keys_str}. \"\n            f\"These features may not exist in the store.\",\n            UnresolvedExternalFeatureWarning,\n            stacklevel=2,\n        )\n\n    return result\n</code></pre>"},{"location":"reference/api/types/","title":"Types","text":""},{"location":"reference/api/types/#versioning-engine","title":"Versioning Engine","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement","title":"metaxy.versioning.types.LazyIncrement  <code>dataclass</code>","text":"<pre><code>LazyIncrement(\n    *,\n    new: LazyFrame[Any],\n    stale: LazyFrame[Any],\n    orphaned: LazyFrame[Any],\n    input: LazyFrame[Any] | None = None,\n)\n</code></pre> <p>Result of an incremental update containing lazy dataframes.</p> <p>Attributes:</p> <ul> <li> <code>new</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>New samples from upstream not present in current metadata</p> </li> <li> <code>stale</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples with provenance different to what was processed before</p> </li> <li> <code>orphaned</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Samples that have been processed before but are no longer present in upstream</p> </li> <li> <code>input</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Joined upstream metadata with <code>FeatureDep</code> rules applied.</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.collect","title":"metaxy.versioning.types.LazyIncrement.collect","text":"<pre><code>collect(**kwargs: Any) -&gt; Increment\n</code></pre> <p>Collect all lazy frames to eager DataFrames.</p> <p>Tip</p> <p>If all lazy frames are Polars frames, leverages <code>polars.collect_all</code> to optimize the collection process and take advantage of common subplan elimination.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>backend-specific keyword arguments to pass to the collect method of the lazy frames.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment</code> (              <code>Increment</code> )          \u2013            <p>The collected increment.</p> </li> </ul> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self, **kwargs: Any) -&gt; Increment:\n    \"\"\"Collect all lazy frames to eager DataFrames.\n\n    !!! tip\n        If all lazy frames are Polars frames, leverages\n        [`polars.collect_all`](https://docs.pola.rs/api/python/stable/reference/api/polars.collect_all.html)\n        to optimize the collection process and take advantage of common subplan elimination.\n\n    Args:\n        **kwargs: backend-specific keyword arguments to pass to the collect method of the lazy frames.\n\n    Returns:\n        Increment: The collected increment.\n    \"\"\"\n    if (\n        self.new.implementation\n        == self.stale.implementation\n        == self.orphaned.implementation\n        == nw.Implementation.POLARS\n    ):\n        polars_eager_increment = PolarsLazyIncrement(\n            new=self.new.to_native(),\n            stale=self.stale.to_native(),\n            orphaned=self.orphaned.to_native(),\n        ).collect(**kwargs)\n        return Increment(\n            new=nw.from_native(polars_eager_increment.new),\n            stale=nw.from_native(polars_eager_increment.stale),\n            orphaned=nw.from_native(polars_eager_increment.orphaned),\n        )\n    else:\n        return Increment(\n            new=self.new.collect(**kwargs),\n            stale=self.stale.collect(**kwargs),\n            orphaned=self.orphaned.collect(**kwargs),\n        )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.to_polars","title":"metaxy.versioning.types.LazyIncrement.to_polars","text":"<pre><code>to_polars() -&gt; PolarsLazyIncrement\n</code></pre> <p>Convert to Polars.</p> <p>Tip</p> <p>If the Narwhals lazy frames are already backed by Polars, this is a no-op.</p> <p>Warning</p> <p>If the Narwhals lazy frames are not backed by Polars, this will trigger a full materialization for them.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def to_polars(self) -&gt; PolarsLazyIncrement:\n    \"\"\"Convert to Polars.\n\n    !!! tip\n        If the Narwhals lazy frames are already backed by Polars, this is a no-op.\n\n    !!! warning\n        If the Narwhals lazy frames are **not** backed by Polars, this will\n        trigger a full materialization for them.\n    \"\"\"\n    return PolarsLazyIncrement(\n        new=lazy_frame_to_polars(self.new),\n        stale=lazy_frame_to_polars(self.stale),\n        orphaned=lazy_frame_to_polars(self.orphaned),\n        input=lazy_frame_to_polars(self.input) if self.input is not None else None,\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment","title":"metaxy.versioning.types.Increment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing eager dataframes.</p> <p>Attributes:</p> <ul> <li> <code>new</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>New samples from upstream not present in current metadata</p> </li> <li> <code>stale</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>Samples with provenance different to what was processed before</p> </li> <li> <code>orphaned</code>               (<code>DataFrame[Any]</code>)           \u2013            <p>Samples that have been processed before but are no longer present in upstream</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.Increment.collect","title":"metaxy.versioning.types.Increment.collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>Convenience method that's a no-op.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; \"Increment\":\n    \"\"\"Convenience method that's a no-op.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment.to_polars","title":"metaxy.versioning.types.Increment.to_polars","text":"<pre><code>to_polars() -&gt; PolarsIncrement\n</code></pre> <p>Convert to Polars.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def to_polars(self) -&gt; PolarsIncrement:\n    \"\"\"Convert to Polars.\"\"\"\n    return PolarsIncrement(\n        new=self.new.to_polars(),\n        stale=self.stale.to_polars(),\n        orphaned=self.orphaned.to_polars(),\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement","title":"metaxy.versioning.types.PolarsIncrement","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Like <code>Increment</code>, but converted to Polars frames.</p> <p>Attributes:</p> <ul> <li> <code>new</code>               (<code>DataFrame</code>)           \u2013            <p>New samples from upstream not present in current metadata</p> </li> <li> <code>stale</code>               (<code>DataFrame</code>)           \u2013            <p>Samples with provenance different to what was processed before</p> </li> <li> <code>orphaned</code>               (<code>DataFrame</code>)           \u2013            <p>Samples that have been processed before but are no longer present in upstream</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.new","title":"metaxy.versioning.types.PolarsIncrement.new  <code>instance-attribute</code>","text":"<pre><code>new: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.stale","title":"metaxy.versioning.types.PolarsIncrement.stale  <code>instance-attribute</code>","text":"<pre><code>stale: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsIncrement.orphaned","title":"metaxy.versioning.types.PolarsIncrement.orphaned  <code>instance-attribute</code>","text":"<pre><code>orphaned: DataFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement","title":"metaxy.versioning.types.PolarsLazyIncrement  <code>dataclass</code>","text":"<pre><code>PolarsLazyIncrement(\n    *,\n    new: LazyFrame,\n    stale: LazyFrame,\n    orphaned: LazyFrame,\n    input: LazyFrame | None = None,\n)\n</code></pre> <p>Like <code>LazyIncrement</code>, but converted to Polars lazy frames.</p> <p>Attributes:</p> <ul> <li> <code>new</code>               (<code>LazyFrame</code>)           \u2013            <p>New samples from upstream not present in current metadata</p> </li> <li> <code>stale</code>               (<code>LazyFrame</code>)           \u2013            <p>Samples with provenance different to what was processed before</p> </li> <li> <code>orphaned</code>               (<code>LazyFrame</code>)           \u2013            <p>Samples that have been processed before but are no longer present in upstream</p> </li> <li> <code>input</code>               (<code>LazyFrame | None</code>)           \u2013            <p>Joined upstream metadata with <code>FeatureDep</code> rules applied.</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.new","title":"metaxy.versioning.types.PolarsLazyIncrement.new  <code>instance-attribute</code>","text":"<pre><code>new: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.stale","title":"metaxy.versioning.types.PolarsLazyIncrement.stale  <code>instance-attribute</code>","text":"<pre><code>stale: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.orphaned","title":"metaxy.versioning.types.PolarsLazyIncrement.orphaned  <code>instance-attribute</code>","text":"<pre><code>orphaned: LazyFrame\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.input","title":"metaxy.versioning.types.PolarsLazyIncrement.input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input: LazyFrame | None = None\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.PolarsLazyIncrement.collect","title":"metaxy.versioning.types.PolarsLazyIncrement.collect","text":"<pre><code>collect(**kwargs: Any) -&gt; PolarsIncrement\n</code></pre> <p>Collect into a <code>PolarsIncrement</code>.</p> <p>Tip</p> <p>Leverages <code>polars.collect_all</code> to optimize the collection process and take advantage of common subplan elimination.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>backend-specific keyword arguments to pass to the collect method of the lazy frames.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PolarsIncrement</code> (              <code>PolarsIncrement</code> )          \u2013            <p>The collected increment.</p> </li> </ul> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self, **kwargs: Any) -&gt; PolarsIncrement:\n    \"\"\"Collect into a [`PolarsIncrement`][metaxy.versioning.types.PolarsIncrement].\n\n    !!! tip\n        Leverages [`polars.collect_all`](https://docs.pola.rs/api/python/stable/reference/api/polars.collect_all.html)\n        to optimize the collection process and take advantage of common subplan elimination.\n\n    Args:\n        **kwargs: backend-specific keyword arguments to pass to the collect method of the lazy frames.\n\n    Returns:\n        PolarsIncrement: The collected increment.\n    \"\"\"\n    added, changed, removed = pl.collect_all([self.new, self.stale, self.orphaned], **kwargs)\n    return PolarsIncrement(added, changed, removed)  # ty: ignore[invalid-argument-type]\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm","title":"metaxy.HashAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Supported hash algorithms for field provenance calculation.</p> <p>These algorithms are chosen for: - Speed (non-cryptographic hashes preferred) - Cross-database availability - Good collision resistance for field provenance calculation</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH64","title":"metaxy.HashAlgorithm.XXHASH64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH64 = 'xxhash64'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH32","title":"metaxy.HashAlgorithm.XXHASH32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH32 = 'xxhash32'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.WYHASH","title":"metaxy.HashAlgorithm.WYHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WYHASH = 'wyhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.SHA256","title":"metaxy.HashAlgorithm.SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'sha256'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.MD5","title":"metaxy.HashAlgorithm.MD5  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MD5 = 'md5'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.FARMHASH","title":"metaxy.HashAlgorithm.FARMHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FARMHASH = 'farmhash'\n</code></pre>"},{"location":"reference/api/types/#keys","title":"Keys","text":"<p>Types for working with feature and field keys.</p>"},{"location":"reference/api/types/#canonical-keys","title":"Canonical Keys","text":""},{"location":"reference/api/types/#metaxy.FeatureKey","title":"metaxy.FeatureKey","text":"<pre><code>FeatureKey(parts: str)\n</code></pre><pre><code>FeatureKey(parts: Sequence[str])\n</code></pre><pre><code>FeatureKey(parts: FeatureKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Feature key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFeatureKey(\"a/b/c\")  # String format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey([\"a\", \"b\", \"c\"])  # List format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(FeatureKey([\"a\", \"b\", \"c\"]))  # FeatureKey copy\n# FeatureKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(\n    self,\n    parts: str | Sequence[str] | FeatureKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.FeatureKey.parts","title":"metaxy.FeatureKey.parts  <code>property</code>","text":"<pre><code>parts: tuple[str, ...]\n</code></pre> <p>Backward compatibility property for accessing root as parts.</p>"},{"location":"reference/api/types/#metaxy.FeatureKey.table_name","title":"metaxy.FeatureKey.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/types/#metaxy.FeatureKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FeatureKey.model_dump","title":"metaxy.FeatureKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to string format for JSON dict key compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to string format for JSON dict key compatibility.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__hash__","title":"metaxy.FeatureKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__eq__","title":"metaxy.FeatureKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.to_column_suffix","title":"metaxy.FeatureKey.to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey","title":"metaxy.FieldKey","text":"<pre><code>FieldKey(parts: str)\n</code></pre><pre><code>FieldKey(parts: Sequence[str])\n</code></pre><pre><code>FieldKey(parts: FieldKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(\n    self,\n    parts: str | Sequence[str] | FieldKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.FieldKey.parts","title":"metaxy.FieldKey.parts  <code>property</code>","text":"<pre><code>parts: tuple[str, ...]\n</code></pre> <p>Backward compatibility property for accessing root as parts.</p>"},{"location":"reference/api/types/#metaxy.FieldKey.table_name","title":"metaxy.FieldKey.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/types/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FieldKey.model_dump","title":"metaxy.FieldKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to string format for JSON dict key compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to string format for JSON dict key compatibility.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__hash__","title":"metaxy.FieldKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__eq__","title":"metaxy.FieldKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#type-annotations","title":"Type Annotations","text":"<p>These are typically used to annotate function parameters. Most APIs in Metaxy accepts them and perform type coercion into canonical types.</p>"},{"location":"reference/api/types/#metaxy.CoercibleToFeatureKey","title":"metaxy.CoercibleToFeatureKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFeatureKey: TypeAlias = \"str | Sequence[str] | FeatureKey | type[BaseFeature] | FeatureDefinition | FeatureSpec\"\n</code></pre> <p>Type alias for values that can be coerced to a <code>FeatureKey</code>.</p> <p>Accepted formats:</p> <ul> <li><code>str</code>: Slash-separated string like <code>\"raw/video\"</code> or <code>\"ml/embeddings/v2\"</code></li> <li><code>Sequence[str]</code>: sequences of parts like <code>[\"user\", \"profile\"]</code></li> <li><code>FeatureKey</code>: Pass through unchanged</li> <li><code>type[BaseFeature]</code>: Any <code>BaseFeature</code> subclass - extracts its key via <code>.spec().key</code></li> <li><code>FeatureDefinition</code>: Extracts its key via <code>.key</code></li> <li><code>FeatureSpec</code>: Extracts its key via <code>.key</code></li> </ul> Example <pre><code>key1 = \"raw/video\"\nkey2 = [\"raw\", \"video\"]\nkey3 = mx.FeatureKey(\"raw/video\")\nkey4 = MyFeatureClass  # where MyFeatureClass is a BaseFeature subclass\nkey5 = mx.FeatureDefinition(\"raw/video\", ...)\nkey6 = mx.FeatureSpec(key=\"raw/video\", id_columns=(\"id\",))\n</code></pre>"},{"location":"reference/api/types/#metaxy.CoercibleToFieldKey","title":"metaxy.CoercibleToFieldKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFieldKey: TypeAlias = (\n    str | Sequence[str] | FieldKey\n)\n</code></pre> <p>Type alias for values that can be coerced to a <code>FieldKey</code>.</p> <p>Accepted formats:</p> <ul> <li><code>str</code>: Slash-separated string like <code>\"audio/english\"</code></li> <li><code>Sequence[str]</code>: sequence of parts like <code>[\"audio\", \"english\"]</code></li> <li><code>FieldKey</code>: Pass through unchanged</li> </ul> Example <pre><code>key1 = \"audio/english\"\nkey2 = [\"audio\", \"english\"]\nkey3 = mx.FieldKey(\"audio/english\")\n</code></pre>"},{"location":"reference/api/types/#pydantic-type-annotations","title":"Pydantic Type Annotations","text":"<p>These types are used for type coercion into canonical types with Pydantic.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKey","title":"metaxy.ValidatedFeatureKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKey: TypeAlias = FeatureKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKey","title":"metaxy.ValidatedFieldKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKey: TypeAlias = FieldKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequence","title":"metaxy.ValidatedFeatureKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequence: TypeAlias = Sequence[\n    ValidatedFeatureKey\n]\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequence","title":"metaxy.ValidatedFieldKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequence: TypeAlias = Sequence[\n    ValidatedFieldKey\n]\n</code></pre>"},{"location":"reference/api/types/#adapters","title":"Adapters","text":"<p>These can perform type coercsion into canonical types in non-pydantic code.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeyAdapter","title":"metaxy.ValidatedFeatureKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeyAdapter: TypeAdapter[\n    ValidatedFeatureKey\n] = TypeAdapter(ValidatedFeatureKey)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequenceAdapter","title":"metaxy.ValidatedFeatureKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequenceAdapter: TypeAdapter[\n    ValidatedFeatureKeySequence\n] = TypeAdapter(ValidatedFeatureKeySequence)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeyAdapter","title":"metaxy.ValidatedFieldKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeyAdapter: TypeAdapter[ValidatedFieldKey] = (\n    TypeAdapter(ValidatedFieldKey)\n)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequenceAdapter","title":"metaxy.ValidatedFieldKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequenceAdapter: TypeAdapter[\n    ValidatedFieldKeySequence\n] = TypeAdapter(ValidatedFieldKeySequence)\n</code></pre>"},{"location":"reference/api/types/#other-types","title":"Other Types","text":""},{"location":"reference/api/types/#metaxy.models.types.PushResult","title":"metaxy.models.types.PushResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of recording a feature graph snapshot.</p> <p>Attributes:</p> <ul> <li> <code>project_version</code>               (<code>str</code>)           \u2013            <p>The deterministic hash of the graph's project version</p> </li> <li> <code>already_pushed</code>               (<code>bool</code>)           \u2013            <p>True if this project_version was already pushed previously</p> </li> <li> <code>updated_features</code>               (<code>list[str]</code>)           \u2013            <p>List of feature keys with updated information (changed definition_version)</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.IDColumns","title":"metaxy.IDColumns  <code>module-attribute</code>","text":"<pre><code>IDColumns: TypeAlias = Sequence[str]\n</code></pre>"},{"location":"reference/api/utils/","title":"Metaxy Utils API","text":""},{"location":"reference/api/utils/#metaxy.utils","title":"metaxy.utils","text":"<p>Utility modules for Metaxy.</p>"},{"location":"reference/api/utils/#metaxy.utils-classes","title":"Classes","text":""},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter","title":"metaxy.utils.BufferedMetadataWriter","text":"<pre><code>BufferedMetadataWriter(\n    store: MetadataStore,\n    flush_batch_size: int | None = None,\n    flush_interval: float = 2.0,\n    max_queue_size: int = 0,\n)\n</code></pre> <p>Buffered metadata writer with background flush thread.</p> <p>Queues data and writes to a MetadataStore in batches either when:</p> <ul> <li>The batch reaches <code>flush_batch_size</code> rows (if set)</li> <li><code>flush_interval</code> seconds have passed since last flush</li> </ul> <p>The writer runs a background thread that handles flushing, allowing the main thread to continue processing data without blocking on writes.</p> Example <pre><code>import polars as pl\n\nwith mx.BufferedMetadataWriter(store) as writer:\n    batch = {\n        \"my/feature\": pl.DataFrame(\n            {\n                \"id\": [\"x\"],\n                \"metaxy_provenance_by_field\": [{\"part_1\": \"h1\", \"part_2\": \"h2\"}],\n            }\n        )\n    }\n    writer.put(batch)\n\nwith store:\n    assert len(store.read(MyFeature).collect()) == 1\n</code></pre> Manual lifecycle management <p> <pre><code>writer = mx.BufferedMetadataWriter(store)\nwriter.start()\ntry:\n    for batch_dict in data_stream:\n        writer.put(batch_dict)\nfinally:\n    rows_written = writer.stop()\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>MetadataStore</code>)           \u2013            <p>The MetadataStore to write to. Must be opened before use.</p> </li> <li> <code>flush_batch_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of rows to accumulate before flushing. If not set, flushes are only triggered by <code>flush_interval</code> or when stopping the writer.</p> <p>Note</p> <p>Setting this triggers row counting which materializes lazy frames.</p> </li> <li> <code>flush_interval</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Maximum seconds between flushes. The timer resets after the end of each flush.</p> </li> <li> <code>max_queue_size</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Maximum number of pending items in the queue. When the queue is full, <code>put()</code> blocks until the background thread consumes an item. Defaults to an unlimited queue (the right choice in most cases).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the background thread encounters an error during flush.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __init__(\n    self,\n    store: MetadataStore,\n    flush_batch_size: int | None = None,\n    flush_interval: float = 2.0,\n    max_queue_size: int = 0,\n) -&gt; None:\n    self._store = store\n    self._flush_batch_size = flush_batch_size\n    self._flush_interval = flush_interval\n\n    self._queue: queue.Queue[QueueItem | _FlushSignal] = queue.Queue(maxsize=max_queue_size)\n    self._should_stop = threading.Event()\n    self._stopped = threading.Event()\n    self._num_written: dict[FeatureKey, int] = {}\n    self._lock = threading.Lock()\n    self._error: BaseException | None = None\n\n    self._thread: threading.Thread | None = None\n    self._started = False\n\n    # Capture context at construction time to propagate to background thread\n    # This is necessary because ContextVars don't propagate to child threads\n    self._graph = FeatureGraph.get_active()\n    self._config = MetaxyConfig.get()\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter-attributes","title":"Attributes","text":""},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.num_written","title":"num_written  <code>property</code>","text":"<pre><code>num_written: dict[FeatureKey, int]\n</code></pre> <p>Number of rows written so far per feature.</p> <p>This property is thread-safe and can be called while the writer is still running to check progress.</p> <p>Returns:</p> <ul> <li> <code>dict[FeatureKey, int]</code>           \u2013            <p>Dict mapping feature keys to number of rows successfully flushed to the store.</p> </li> </ul>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.has_error","title":"has_error  <code>property</code>","text":"<pre><code>has_error: bool\n</code></pre> <p>Check if the writer has encountered an error.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the background thread encountered an error, False otherwise.</p> </li> </ul>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter-functions","title":"Functions","text":""},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the background flush thread.</p> <p>This method must be called before putting data. When using the writer as a context manager, this is called automatically on entry.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the writer has already been started.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the background flush thread.\n\n    This method must be called before putting data. When using the writer\n    as a context manager, this is called automatically on entry.\n\n    Raises:\n        RuntimeError: If the writer has already been started.\n    \"\"\"\n    if self._started:\n        raise RuntimeError(\"Writer has already been started\")\n\n    self._thread = threading.Thread(target=self._run, daemon=True)\n    self._thread.start()\n    self._started = True\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.put","title":"put","text":"<pre><code>put(\n    batches: Mapping[CoercibleToFeatureKey, IntoFrame],\n) -&gt; None\n</code></pre> <p>Queue batches for writing.</p> <p>The batches are accumulated per-feature and written together using <code>[MetadataStore.write_multi][]</code>.</p> <p>Parameters:</p> <ul> <li> <code>batches</code>               (<code>Mapping[CoercibleToFeatureKey, IntoFrame]</code>)           \u2013            <p>Mapping from feature keys to dataframes. Dataframes can be of any type supported by Narwhals.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the writer has not been started, has been stopped, or encountered an error.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def put(self, batches: Mapping[CoercibleToFeatureKey, IntoFrame]) -&gt; None:\n    \"\"\"Queue batches for writing.\n\n    The batches are accumulated per-feature and written together using\n    `[MetadataStore.write_multi][]`.\n\n    Args:\n        batches: Mapping from feature keys to dataframes.\n            Dataframes can be of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n\n    Raises:\n        RuntimeError: If the writer has not been started, has been stopped,\n            or encountered an error.\n    \"\"\"\n    self._check_can_put()\n\n    # Convert all keys and values\n    converted: dict[FeatureKey, Frame] = {}\n    for key, batch in batches.items():\n        feature_key = ValidatedFeatureKeyAdapter.validate_python(key)\n        batch_nw = self._to_narwhals(batch)\n        converted[feature_key] = batch_nw\n\n    self._queue.put(converted)\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.flush","title":"flush","text":"<pre><code>flush(timeout: float = 30.0) -&gt; None\n</code></pre> <p>Flush all pending data to the store.</p> <p>Blocks until the background thread has flushed all currently accumulated data, or until the timeout is reached.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Maximum seconds to wait for the flush to complete.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the writer is not started, has stopped, has errored, the flush does not complete within the timeout, or the flush itself encounters an error.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def flush(self, timeout: float = 30.0) -&gt; None:\n    \"\"\"Flush all pending data to the store.\n\n    Blocks until the background thread has flushed all currently\n    accumulated data, or until the timeout is reached.\n\n    Args:\n        timeout: Maximum seconds to wait for the flush to complete.\n\n    Raises:\n        RuntimeError: If the writer is not started, has stopped, has errored,\n            the flush does not complete within the timeout, or the flush itself\n            encounters an error.\n    \"\"\"\n    self._check_can_put()\n    signal = _FlushSignal()\n    self._queue.put(signal)\n    if not signal.done.wait(timeout):\n        raise RuntimeError(f\"Flush did not complete within {timeout:.1f}s\")\n    if signal.error is not None:\n        raise RuntimeError(\"Flush failed\") from signal.error\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.stop","title":"stop","text":"<pre><code>stop(timeout: float = 30.0) -&gt; dict[FeatureKey, int]\n</code></pre> <p>Signal stop and wait for flush to complete.</p> <p>This method signals the background thread to stop, waits for it to finish flushing any remaining data, and returns the number of rows written per feature.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Maximum seconds to wait for the background thread. Defaults to 30.0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[FeatureKey, int]</code>           \u2013            <p>Dict mapping feature keys to number of rows written.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the background thread encountered an error during flush, or did not stop within the timeout.</p> </li> </ul> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def stop(self, timeout: float = 30.0) -&gt; dict[FeatureKey, int]:\n    \"\"\"Signal stop and wait for flush to complete.\n\n    This method signals the background thread to stop, waits for it to\n    finish flushing any remaining data, and returns the number of rows\n    written per feature.\n\n    Args:\n        timeout: Maximum seconds to wait for the background thread.\n            Defaults to 30.0.\n\n    Returns:\n        Dict mapping feature keys to number of rows written.\n\n    Raises:\n        RuntimeError: If the background thread encountered an error during flush,\n            or did not stop within the timeout.\n    \"\"\"\n    if not self._started or self._thread is None:\n        return {}\n\n    self._should_stop.set()\n    self._thread.join(timeout=timeout)\n\n    if self._thread.is_alive():\n        raise RuntimeError(f\"BufferedMetadataWriter background thread did not stop within {timeout:.1f}s\")\n\n    if self._error is not None:\n        raise RuntimeError(f\"Writer encountered an error: {self._error}\") from self._error\n\n    with self._lock:\n        return dict(self._num_written)\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; BufferedMetadataWriter\n</code></pre> <p>Enter context manager, starting the background thread.</p> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __enter__(self) -&gt; BufferedMetadataWriter:\n    \"\"\"Enter context manager, starting the background thread.\"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"reference/api/utils/#metaxy.utils.BufferedMetadataWriter.__exit__","title":"__exit__","text":"<pre><code>__exit__(\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None\n</code></pre> <p>Exit context manager, stopping the writer.</p> Source code in <code>src/metaxy/utils/batched_writer.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_val: BaseException | None,\n    exc_tb: TracebackType | None,\n) -&gt; None:\n    \"\"\"Exit context manager, stopping the writer.\"\"\"\n    try:\n        self.stop()\n    except RuntimeError:\n        if exc_type is None:\n            raise\n        logger.exception(\n            \"Error during BufferedMetadataWriter shutdown (suppressed to avoid masking original exception)\"\n        )\n</code></pre>"},{"location":"reference/api/definitions/","title":"Definitions","text":"<p>Metaxy's dependency specification system allows users to express dependencies between their features and their fields.</p>"},{"location":"reference/api/definitions/#featurespec","title":"<code>FeatureSpec</code>","text":"<p>FeatureSpec is the core of Metaxy's dependency specification system: it stores all the information about the parents, field mappings, and other metadata associated with a feature.</p>"},{"location":"reference/api/definitions/#feature","title":"Feature","text":"<p>A feature in Metaxy is used to model user-defined metadata. It must have a <code>FeatureSpec</code> instance associated with it. A <code>Feature</code> class is typically associated with a single table in the MetadataStore.</p>"},{"location":"reference/api/definitions/#fieldspec","title":"FieldSpec","text":"<p>A [field] in Metaxy is a logical slices of the data represented by feature metadata. Users are free to define their own fields as is suitable for them.</p> <p>Dependencies between fields are modeled with FieldDep and can be automatic (via field mappings) or explicitly set by users.</p>"},{"location":"reference/api/definitions/#graph","title":"Graph","text":"<p>All features live on a FeatureGraph object. The users don't typically interact with it outside of advanced use cases.</p>"},{"location":"reference/api/definitions/feature-spec/","title":"Feature Spec","text":"<p>Feature specs act as source of truth for all metadata related to features: their dependencies, fields, code versions, and so on.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec","title":"metaxy.FeatureSpec  <code>pydantic-model</code>","text":"<pre><code>FeatureSpec(\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n)\n</code></pre><pre><code>FeatureSpec(\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n)\n</code></pre> <p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"additionalProperties\": false,\n      \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    select: Optional sequence of column names to select from the upstream feature.\\n        By default, all columns are selected. System columns are always selected.\\n        Uses post-rename names when `rename` is also specified.\\n    rename: Optional mapping of old column names to new names.\\n        Applied before column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", select=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # Combined rename + select: select uses post-rename names\\n    mx.FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"},\\n        select=(\\\"new_name\\\", \\\"other_col\\\"),\\n    )\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\",\n          \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n        },\n        \"select\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Select\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        },\n        \"filters\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"SQL-like filter strings applied to this dependency.\",\n          \"title\": \"Filters\"\n        },\n        \"lineage\": {\n          \"$ref\": \"#/$defs/LineageRelationship\",\n          \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n        },\n        \"optional\": {\n          \"default\": false,\n          \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n          \"title\": \"Optional\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"id_columns\": {\n      \"description\": \"Columns that uniquely identify a sample in this feature.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    },\n    \"deps\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FeatureDep\"\n      },\n      \"title\": \"Deps\",\n      \"type\": \"array\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"additionalProperties\": false,\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": true,\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    },\n    \"description\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Human-readable description of this feature.\",\n      \"title\": \"Description\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"id_columns\"\n  ],\n  \"title\": \"FeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(\n    self,\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    metadata: dict[str, Any] | None = None,\n    description: str | None = None,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.id_columns","title":"metaxy.FeatureSpec.id_columns  <code>pydantic-field</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Columns that uniquely identify a sample in this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.metadata","title":"metaxy.FeatureSpec.metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, Any]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.description","title":"metaxy.FeatureSpec.description  <code>pydantic-field</code>","text":"<pre><code>description: str | None = None\n</code></pre> <p>Human-readable description of this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.deps_by_key","title":"metaxy.FeatureSpec.deps_by_key  <code>cached</code> <code>property</code>","text":"<pre><code>deps_by_key: Mapping[FeatureKey, FeatureDep]\n</code></pre> <p>Get dependencies indexed by their feature key.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.code_version","title":"metaxy.FeatureSpec.code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.feature_spec_version","title":"metaxy.FeatureSpec.feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = mx.FeatureSpec(\n    key=mx.FeatureKey([\"my\", \"feature\"]),\n    id_columns=[\"id\"],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.table_name","title":"metaxy.FeatureSpec.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_unique_field_keys","title":"metaxy.FeatureSpec.validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; Self\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; Self:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(f\"Duplicate field key found: {field.key}. All fields must have unique keys.\")\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_id_columns","title":"metaxy.FeatureSpec.validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; Self\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; Self:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\"id_columns must be non-empty if specified. Use None for default.\")\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#feature-dependencies","title":"Feature Dependencies","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep","title":"metaxy.FeatureDep  <code>pydantic-model</code>","text":"<pre><code>FeatureDep(\n    *,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | type[BaseFeature],\n    select: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    filters: Sequence[str] | None = None,\n    lineage: LineageRelationship | None = None,\n    optional: bool = False,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature dependency specification with optional column selection, renaming, and lineage.</p> <p>Attributes:</p> <ul> <li> <code>feature</code>               (<code>ValidatedFeatureKey</code>)           \u2013            <p>The feature key to depend on. Accepts string (\"a/b/c\"), list ([\"a\", \"b\", \"c\"]), FeatureKey instance, or BaseFeature class.</p> </li> <li> <code>select</code>               (<code>tuple[str, ...] | None</code>)           \u2013            <p>Optional sequence of column names to select from the upstream feature. By default, all columns are selected. System columns are always selected. Uses post-rename names when <code>rename</code> is also specified.</p> </li> <li> <code>rename</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Optional mapping of old column names to new names. Applied before column selection.</p> </li> <li> <code>fields_mapping</code>               (<code>FieldsMapping</code>)           \u2013            <p>Optional field mapping configuration for automatic field dependency resolution. When provided, fields without explicit deps will automatically map to matching upstream fields. Defaults to using <code>[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]</code>.</p> </li> <li> <code>filters</code>               (<code>tuple[Expr, ...]</code>)           \u2013            <p>Optional SQL-like filter strings applied to this dependency. Automatically parsed into Narwhals expressions (accessible via the <code>filters</code> property). Filters are automatically applied by FeatureDepTransformer after renames during all FeatureDep operations (including resolve_update and version computation).</p> </li> <li> <code>lineage</code>               (<code>LineageRelationship</code>)           \u2013            <p>The lineage relationship between this upstream dependency and the downstream feature. - <code>LineageRelationship.identity()</code> (default): 1:1 relationship, same cardinality - <code>LineageRelationship.aggregation(on=...)</code>: N:1, multiple upstream rows aggregate to one downstream - <code>LineageRelationship.expansion(on=...)</code>: 1:N, one upstream row expands to multiple downstream rows</p> </li> <li> <code>optional</code>               (<code>bool</code>)           \u2013            <p>Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata. Defaults to False (required dependency).</p> </li> </ul> Basic Usage <pre><code># Keep all columns with default field mapping (1:1 lineage)\nmx.FeatureDep(feature=\"upstream\")\n\n# Keep only specific columns\nmx.FeatureDep(feature=\"upstream/feature\", select=(\"col1\", \"col2\"))\n\n# Rename columns to avoid conflicts\nmx.FeatureDep(feature=\"upstream/feature\", rename={\"old_name\": \"new_name\"})\n\n# Combined rename + select: select uses post-rename names\nmx.FeatureDep(\n    feature=\"upstream/feature\",\n    rename={\"old_name\": \"new_name\"},\n    select=(\"new_name\", \"other_col\"),\n)\n\n# SQL filters\nmx.FeatureDep(feature=\"upstream\", filters=[\"age &gt;= 25\", \"status = 'active'\"])\n\n# Optional dependency (left join - samples preserved even if no match)\nmx.FeatureDep(feature=\"enrichment/data\", optional=True)\n</code></pre> Lineage Relationships <pre><code>from metaxy.models.lineage import LineageRelationship\n\n# Aggregation: many sensor readings aggregate to one hourly stat\nmx.FeatureDep(feature=\"sensor_readings\", lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"]))\n\n# Expansion: one video expands to many frames\nmx.FeatureDep(feature=\"video\", lineage=LineageRelationship.expansion(on=[\"video_id\"]))\n\n# Mixed lineage: aggregate from one parent, identity from another\n# In FeatureSpec:\ndeps = [\n    mx.FeatureDep(feature=\"readings\", lineage=LineageRelationship.aggregation(on=[\"sensor_id\"])),\n    mx.FeatureDep(feature=\"sensor_info\", lineage=LineageRelationship.identity()),\n]\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    select: Optional sequence of column names to select from the upstream feature.\\n        By default, all columns are selected. System columns are always selected.\\n        Uses post-rename names when `rename` is also specified.\\n    rename: Optional mapping of old column names to new names.\\n        Applied before column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", select=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # Combined rename + select: select uses post-rename names\\n    mx.FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"},\\n        select=(\\\"new_name\\\", \\\"other_col\\\"),\\n    )\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\",\n      \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n    },\n    \"select\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Select\"\n    },\n    \"rename\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Rename\"\n    },\n    \"fields_mapping\": {\n      \"$ref\": \"#/$defs/FieldsMapping\"\n    },\n    \"filters\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"SQL-like filter strings applied to this dependency.\",\n      \"title\": \"Filters\"\n    },\n    \"lineage\": {\n      \"$ref\": \"#/$defs/LineageRelationship\",\n      \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n    },\n    \"optional\": {\n      \"default\": false,\n      \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n      \"title\": \"Optional\",\n      \"type\": \"boolean\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FeatureDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>forbid</code></li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(\n    self,\n    *,\n    feature: str | Sequence[str] | FeatureKey | type[BaseFeature],\n    select: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    filters: Sequence[str] | None = None,\n    lineage: LineageRelationship | None = None,\n    optional: bool = False,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.sql_filters","title":"metaxy.FeatureDep.sql_filters  <code>pydantic-field</code>","text":"<pre><code>sql_filters: tuple[str, ...] | None = None\n</code></pre> <p>SQL-like filter strings applied to this dependency.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.lineage","title":"metaxy.FeatureDep.lineage  <code>pydantic-field</code>","text":"<pre><code>lineage: LineageRelationship\n</code></pre> <p>Lineage relationship between this upstream dependency and the downstream feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.optional","title":"metaxy.FeatureDep.optional  <code>pydantic-field</code>","text":"<pre><code>optional: bool = False\n</code></pre> <p>Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.filters","title":"metaxy.FeatureDep.filters  <code>cached</code> <code>property</code>","text":"<pre><code>filters: tuple[Expr, ...]\n</code></pre> <p>Parse sql_filters into Narwhals expressions.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.table_name","title":"metaxy.FeatureDep.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.feature.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature/","title":"Feature","text":"<p><code>BaseFeature</code> is the most important class in Metaxy. Features are defined by extending it.</p> <p>Code Version Access</p> <p>Retrieve a feature's code version from its spec: <code>MyFeature.spec().code_version</code>.</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature","title":"metaxy.BaseFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_project_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph project version\",\n      \"title\": \"Metaxy Project Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    }\n  },\n  \"title\": \"BaseFeature\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance_by_field","title":"metaxy.BaseFeature.metaxy_provenance_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance_by_field: dict[str, str]\n</code></pre> <p>Field-level provenance hashes (maps field names to hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance","title":"metaxy.BaseFeature.metaxy_provenance  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance: str | None = None\n</code></pre> <p>Hash of metaxy_provenance_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_feature_version","title":"metaxy.BaseFeature.metaxy_feature_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_feature_version: str | None = None\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_project_version","title":"metaxy.BaseFeature.metaxy_project_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_project_version: str | None = None\n</code></pre> <p>Hash of the entire feature graph project version</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version_by_field","title":"metaxy.BaseFeature.metaxy_data_version_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version_by_field: dict[str, str] | None = None\n</code></pre> <p>Field-level data version hashes (maps field names to version hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version","title":"metaxy.BaseFeature.metaxy_data_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version: str | None = None\n</code></pre> <p>Hash of metaxy_data_version_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_created_at","title":"metaxy.BaseFeature.metaxy_created_at  <code>pydantic-field</code>","text":"<pre><code>metaxy_created_at: AwareDatetime | None = None\n</code></pre> <p>Timestamp when the metadata row was created (UTC)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_materialization_id","title":"metaxy.BaseFeature.metaxy_materialization_id  <code>pydantic-field</code>","text":"<pre><code>metaxy_materialization_id: str | None = None\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_project","title":"metaxy.BaseFeature.metaxy_project  <code>classmethod</code>","text":"<pre><code>metaxy_project() -&gt; str\n</code></pre> <p>Return the project this feature belongs to.</p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef metaxy_project(cls) -&gt; str:\n    \"\"\"Return the project this feature belongs to.\"\"\"\n    return cls.__metaxy_project__\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.table_name","title":"metaxy.BaseFeature.table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/processing\", id_columns=[\"id\"])):\n    id: str\n\n\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/processing\", id_columns=[\"id\"])):\n            id: str\n\n\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_version","title":"metaxy.BaseFeature.feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature\", id_columns=[\"id\"])):\n    id: str\n\n\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature\", id_columns=[\"id\"])):\n            id: str\n\n\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_spec_version","title":"metaxy.BaseFeature.feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature2\", id_columns=[\"id\"])):\n    id: str\n\n\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for reproducibility and audit purposes.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"my/feature2\", id_columns=[\"id\"])):\n            id: str\n\n\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.provenance_by_field","title":"metaxy.BaseFeature.provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.load_input","title":"metaxy.BaseFeature.load_input  <code>classmethod</code>","text":"<pre><code>load_input(\n    joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]\n) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.select\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.get_feature_by_key","title":"metaxy.get_feature_by_key","text":"<pre><code>get_feature_by_key(\n    key: CoercibleToFeatureKey,\n) -&gt; FeatureDefinition\n</code></pre> <p>Get a FeatureDefinition by its key from the current graph.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up (can be FeatureKey, list of strings, slash-separated string, etc.)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>FeatureDefinition for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>@public\ndef get_feature_by_key(key: CoercibleToFeatureKey) -&gt; FeatureDefinition:\n    \"\"\"Get a FeatureDefinition by its key from the current graph.\n\n    Args:\n        key: Feature key to look up (can be FeatureKey, list of strings, slash-separated string, etc.)\n\n    Returns:\n        FeatureDefinition for the feature\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    return current_graph().get_feature_definition(key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition","title":"metaxy.FeatureDefinition  <code>pydantic-model</code>","text":"<p>               Bases: <code>FrozenBaseModel</code></p> <p>Complete feature definition wrapping all feature information.</p> <p>Attributes:</p> <ul> <li> <code>spec</code>               (<code>FeatureSpec</code>)           \u2013            <p>The complete feature specification</p> </li> <li> <code>feature_schema</code>               (<code>Json[dict[str, Any]]</code>)           \u2013            <p>Pydantic JSON schema dict for the feature model</p> </li> <li> <code>feature_class_path</code>               (<code>str | None</code>)           \u2013            <p>Python import path (e.g., 'myapp.features.VideoFeature')</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project this feature belongs to</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"additionalProperties\": false,\n      \"description\": \"Feature dependency specification with optional column selection, renaming, and lineage.\\n\\nAttributes:\\n    feature: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    select: Optional sequence of column names to select from the upstream feature.\\n        By default, all columns are selected. System columns are always selected.\\n        Uses post-rename names when `rename` is also specified.\\n    rename: Optional mapping of old column names to new names.\\n        Applied before column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n    lineage: The lineage relationship between this upstream dependency and the downstream feature.\\n        - `LineageRelationship.identity()` (default): 1:1 relationship, same cardinality\\n        - `LineageRelationship.aggregation(on=...)`: N:1, multiple upstream rows aggregate to one downstream\\n        - `LineageRelationship.expansion(on=...)`: 1:N, one upstream row expands to multiple downstream rows\\n    optional: Whether individual samples of the downstream feature can be computed without\\n        the corresponding samples of the upstream feature. If upstream samples are missing,\\n        they are going to be represented as NULL values in the joined upstream metadata.\\n        Defaults to False (required dependency).\\n\\nExample: Basic Usage\\n    ```py\\n    # Keep all columns with default field mapping (1:1 lineage)\\n    mx.FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep only specific columns\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", select=(\\\"col1\\\", \\\"col2\\\"))\\n\\n    # Rename columns to avoid conflicts\\n    mx.FeatureDep(feature=\\\"upstream/feature\\\", rename={\\\"old_name\\\": \\\"new_name\\\"})\\n\\n    # Combined rename + select: select uses post-rename names\\n    mx.FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"},\\n        select=(\\\"new_name\\\", \\\"other_col\\\"),\\n    )\\n\\n    # SQL filters\\n    mx.FeatureDep(feature=\\\"upstream\\\", filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"])\\n\\n    # Optional dependency (left join - samples preserved even if no match)\\n    mx.FeatureDep(feature=\\\"enrichment/data\\\", optional=True)\\n    ```\\n\\nExample: Lineage Relationships\\n    ```py\\n    from metaxy.models.lineage import LineageRelationship\\n\\n    # Aggregation: many sensor readings aggregate to one hourly stat\\n    mx.FeatureDep(feature=\\\"sensor_readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"]))\\n\\n    # Expansion: one video expands to many frames\\n    mx.FeatureDep(feature=\\\"video\\\", lineage=LineageRelationship.expansion(on=[\\\"video_id\\\"]))\\n\\n    # Mixed lineage: aggregate from one parent, identity from another\\n    # In FeatureSpec:\\n    deps = [\\n        mx.FeatureDep(feature=\\\"readings\\\", lineage=LineageRelationship.aggregation(on=[\\\"sensor_id\\\"])),\\n        mx.FeatureDep(feature=\\\"sensor_info\\\", lineage=LineageRelationship.identity()),\\n    ]\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\",\n          \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n        },\n        \"select\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Select\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        },\n        \"filters\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"SQL-like filter strings applied to this dependency.\",\n          \"title\": \"Filters\"\n        },\n        \"lineage\": {\n          \"$ref\": \"#/$defs/LineageRelationship\",\n          \"description\": \"Lineage relationship between this upstream dependency and the downstream feature.\"\n        },\n        \"optional\": {\n          \"default\": false,\n          \"description\": \"Whether individual samples of the downstream feature can be computed without the corresponding samples of the upstream feature. If upstream samples are missing, they are going to be represented as NULL values in the joined upstream metadata.\",\n          \"title\": \"Optional\",\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FeatureSpec\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"key\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"id_columns\": {\n          \"description\": \"Columns that uniquely identify a sample in this feature.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"Id Columns\",\n          \"type\": \"array\"\n        },\n        \"deps\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FeatureDep\"\n          },\n          \"title\": \"Deps\",\n          \"type\": \"array\"\n        },\n        \"fields\": {\n          \"items\": {\n            \"additionalProperties\": false,\n            \"properties\": {\n              \"key\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"code_version\": {\n                \"default\": \"__metaxy_initial__\",\n                \"title\": \"Code Version\",\n                \"type\": \"string\"\n              },\n              \"deps\": {\n                \"anyOf\": [\n                  {\n                    \"$ref\": \"#/$defs/SpecialFieldDep\"\n                  },\n                  {\n                    \"items\": {\n                      \"$ref\": \"#/$defs/FieldDep\"\n                    },\n                    \"type\": \"array\"\n                  }\n                ],\n                \"title\": \"Deps\"\n              }\n            },\n            \"title\": \"FieldSpec\",\n            \"type\": \"object\"\n          },\n          \"title\": \"Fields\",\n          \"type\": \"array\"\n        },\n        \"metadata\": {\n          \"additionalProperties\": true,\n          \"description\": \"Metadata attached to this feature.\",\n          \"title\": \"Metadata\",\n          \"type\": \"object\"\n        },\n        \"description\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Human-readable description of this feature.\",\n          \"title\": \"Description\"\n        }\n      },\n      \"required\": [\n        \"key\",\n        \"id_columns\"\n      ],\n      \"title\": \"FeatureSpec\",\n      \"type\": \"object\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Complete feature definition wrapping all feature information.\\n\\nAttributes:\\n    spec: The complete feature specification\\n    feature_schema: Pydantic JSON schema dict for the feature model\\n    feature_class_path: Python import path (e.g., 'myapp.features.VideoFeature')\\n    project: The metaxy project this feature belongs to\",\n  \"properties\": {\n    \"spec\": {\n      \"$ref\": \"#/$defs/FeatureSpec\",\n      \"description\": \"Complete feature specification\"\n    },\n    \"feature_schema\": {\n      \"contentMediaType\": \"application/json\",\n      \"contentSchema\": {\n        \"additionalProperties\": true,\n        \"type\": \"object\"\n      },\n      \"description\": \"Pydantic JSON schema dict\",\n      \"title\": \"Feature Schema\",\n      \"type\": \"string\"\n    },\n    \"feature_class_path\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Python import path\",\n      \"title\": \"Feature Class Path\"\n    },\n    \"project\": {\n      \"description\": \"The metaxy project this feature belongs to\",\n      \"minLength\": 1,\n      \"title\": \"Project\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\n    \"spec\",\n    \"feature_schema\",\n    \"project\"\n  ],\n  \"title\": \"FeatureDefinition\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.spec","title":"metaxy.FeatureDefinition.spec  <code>pydantic-field</code>","text":"<pre><code>spec: FeatureSpec\n</code></pre> <p>Complete feature specification</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_schema","title":"metaxy.FeatureDefinition.feature_schema  <code>pydantic-field</code>","text":"<pre><code>feature_schema: Json[dict[str, Any]]\n</code></pre> <p>Pydantic JSON schema dict</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_class_path","title":"metaxy.FeatureDefinition.feature_class_path  <code>pydantic-field</code>","text":"<pre><code>feature_class_path: str | None = None\n</code></pre> <p>Python import path</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.project","title":"metaxy.FeatureDefinition.project  <code>pydantic-field</code>","text":"<pre><code>project: str\n</code></pre> <p>The metaxy project this feature belongs to</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.feature_definition_version","title":"metaxy.FeatureDefinition.feature_definition_version  <code>cached</code> <code>property</code>","text":"<pre><code>feature_definition_version: str\n</code></pre> <p>Hash of spec + schema (excludes project).</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.key","title":"metaxy.FeatureDefinition.key  <code>property</code>","text":"<pre><code>key: FeatureKey\n</code></pre> <p>Get the feature key from the spec.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.table_name","title":"metaxy.FeatureDefinition.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.id_columns","title":"metaxy.FeatureDefinition.id_columns  <code>property</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Get ID columns from the spec.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.columns","title":"metaxy.FeatureDefinition.columns  <code>cached</code> <code>property</code>","text":"<pre><code>columns: Sequence[str]\n</code></pre> <p>Get column names from the feature schema.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.is_external","title":"metaxy.FeatureDefinition.is_external  <code>property</code>","text":"<pre><code>is_external: bool\n</code></pre> <p>Check if this is an external feature definition.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.provenance_by_field_override","title":"metaxy.FeatureDefinition.provenance_by_field_override  <code>property</code>","text":"<pre><code>provenance_by_field_override: dict[str, str]\n</code></pre> <p>The manually-specified field provenance map.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If no provenance override was set.</p> </li> </ul>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.has_provenance_override","title":"metaxy.FeatureDefinition.has_provenance_override  <code>property</code>","text":"<pre><code>has_provenance_override: bool\n</code></pre> <p>True if this external feature has a provenance override.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.on_version_mismatch","title":"metaxy.FeatureDefinition.on_version_mismatch  <code>property</code>","text":"<pre><code>on_version_mismatch: Literal['warn', 'error']\n</code></pre> <p>What to do when actual feature version differs from expected.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.source","title":"metaxy.FeatureDefinition.source  <code>property</code>","text":"<pre><code>source: str\n</code></pre> <p>Human-readable string describing where this definition came from.</p>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.from_feature_class","title":"metaxy.FeatureDefinition.from_feature_class  <code>classmethod</code>","text":"<pre><code>from_feature_class(\n    feature_cls: type[BaseFeature],\n) -&gt; FeatureDefinition\n</code></pre> <p>Create a FeatureDefinition from a Feature class.</p> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef from_feature_class(cls, feature_cls: type[BaseFeature]) -&gt; FeatureDefinition:\n    \"\"\"Create a FeatureDefinition from a Feature class.\"\"\"\n    spec = feature_cls.spec()\n\n    # Inject class docstring as description if not already set\n    if spec.description is None and feature_cls.__doc__:\n        spec = spec.model_copy(update={\"description\": inspect.cleandoc(feature_cls.__doc__)})\n\n    schema = feature_cls.model_json_schema()\n    class_path = f\"{feature_cls.__module__}.{feature_cls.__name__}\"\n    project = feature_cls.metaxy_project()\n\n    definition = cls(\n        spec=spec,\n        feature_schema=schema,\n        feature_class_path=class_path,\n        project=project,\n    )\n    definition._feature_class = feature_cls\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.from_stored_data","title":"metaxy.FeatureDefinition.from_stored_data  <code>classmethod</code>","text":"<pre><code>from_stored_data(\n    feature_spec: dict[str, Any] | str,\n    feature_schema: dict[str, Any] | str,\n    feature_class_path: str,\n    project: str,\n    source: str | None = None,\n) -&gt; FeatureDefinition\n</code></pre> <p>Create a FeatureDefinition from stored data.</p> <p>Handles JSON string or dict inputs for spec and schema fields.</p> <p>Parameters:</p> <ul> <li> <code>feature_spec</code>               (<code>dict[str, Any] | str</code>)           \u2013            <p>Feature specification as dict or JSON string.</p> </li> <li> <code>feature_schema</code>               (<code>dict[str, Any] | str</code>)           \u2013            <p>Pydantic JSON schema as dict or JSON string.</p> </li> <li> <code>feature_class_path</code>               (<code>str</code>)           \u2013            <p>Python import path of the feature class.</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project name.</p> </li> <li> <code>source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Human-readable string describing where this definition came from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>A new FeatureDefinition instance.</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef from_stored_data(\n    cls,\n    feature_spec: dict[str, Any] | str,\n    feature_schema: dict[str, Any] | str,\n    feature_class_path: str,\n    project: str,\n    source: str | None = None,\n) -&gt; FeatureDefinition:\n    \"\"\"Create a FeatureDefinition from stored data.\n\n    Handles JSON string or dict inputs for spec and schema fields.\n\n    Args:\n        feature_spec: Feature specification as dict or JSON string.\n        feature_schema: Pydantic JSON schema as dict or JSON string.\n        feature_class_path: Python import path of the feature class.\n        project: The metaxy project name.\n        source: Human-readable string describing where this definition came from.\n\n    Returns:\n        A new FeatureDefinition instance.\n    \"\"\"\n    import json\n\n    if isinstance(feature_spec, str):\n        feature_spec = json.loads(feature_spec)\n    if isinstance(feature_schema, str):\n        feature_schema = json.loads(feature_schema)\n\n    spec = FeatureSpec.model_validate(feature_spec)\n    definition = cls(\n        spec=spec,\n        feature_schema=feature_schema,\n        feature_class_path=feature_class_path,\n        project=project,\n    )\n    definition._source = source\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.external","title":"metaxy.FeatureDefinition.external  <code>classmethod</code>","text":"<pre><code>external(\n    *,\n    spec: FeatureSpec,\n    project: str,\n    feature_schema: dict[str, Any] | None = None,\n    provenance_by_field: dict[CoercibleToFieldKey, str]\n    | None = None,\n    on_version_mismatch: Literal[\"warn\", \"error\"] = \"warn\",\n    source: str | None = None,\n) -&gt; FeatureDefinition\n</code></pre> <p>Create an external FeatureDefinition without a Feature class.</p> <p>External features are definitions loaded from another project or system that don't have corresponding Python Feature classes in the current codebase.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>FeatureSpec</code>)           \u2013            <p>The feature specification.</p> </li> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The metaxy project this feature belongs to.</p> </li> <li> <code>feature_schema</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pydantic JSON schema dict describing the feature's fields. Typically doesn't have to be provided, unless some user code attempts to use it before the real feature definition is loaded from the metadata store. This argument is experimental and may be changed in the future.</p> </li> <li> <code>provenance_by_field</code>               (<code>dict[CoercibleToFieldKey, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional manually-specified field provenance map. Use this argument to avoid providing too many upstream external features. Make sure to provide the actual values from the real external feature.</p> </li> <li> <code>on_version_mismatch</code>               (<code>Literal['warn', 'error']</code>, default:                   <code>'warn'</code> )           \u2013            <p>How to handle a version mismatch if the actual feature loaded from the metadata store has a different version than the version specified in the corresponding external feature.</p> </li> <li> <code>source</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Human-readable string describing where this definition came from. If not provided, captures the call site location automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>A new FeatureDefinition marked as external.</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>@classmethod\ndef external(\n    cls,\n    *,\n    spec: FeatureSpec,\n    project: str,\n    feature_schema: dict[str, Any] | None = None,\n    provenance_by_field: dict[CoercibleToFieldKey, str] | None = None,\n    on_version_mismatch: Literal[\"warn\", \"error\"] = \"warn\",\n    source: str | None = None,\n) -&gt; FeatureDefinition:\n    \"\"\"Create an external FeatureDefinition without a Feature class.\n\n    External features are definitions loaded from another project or system\n    that don't have corresponding Python Feature classes in the current codebase.\n\n    Args:\n        spec: The feature specification.\n        project: The metaxy project this feature belongs to.\n        feature_schema: Pydantic JSON schema dict describing the feature's fields.\n            Typically doesn't have to be provided, unless some user code attempts\n            to use it before the real feature definition is loaded from the metadata store.\n            This argument is experimental and may be changed in the future.\n        provenance_by_field: Optional manually-specified field provenance map.\n            Use this argument to avoid providing too many upstream external features.\n            Make sure to provide the actual values from the real external feature.\n        on_version_mismatch: How to handle a version mismatch if the actual feature loaded from the\n            metadata store has a different version than the version specified in the corresponding external feature.\n        source: Human-readable string describing where this definition came from.\n            If not provided, captures the call site location automatically.\n\n    Returns:\n        A new FeatureDefinition marked as external.\n    \"\"\"\n    normalized_provenance: dict[str, str] | None = None\n    if provenance_by_field is not None:\n        normalized_provenance = {\n            ValidatedFieldKeyAdapter.validate_python(k).to_string(): v for k, v in provenance_by_field.items()\n        }\n\n    # Capture call site location if source is not provided\n    if source is None:\n        source = cls._capture_call_site()\n\n    definition = cls(\n        spec=spec,\n        feature_schema=feature_schema or {},\n        feature_class_path=None,\n        project=project,\n    )\n    definition._is_external = True\n    definition._provenance_by_field = normalized_provenance\n    definition._on_version_mismatch = on_version_mismatch\n    definition._source = source\n    return definition\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.FeatureDefinition.check_version_mismatch","title":"metaxy.FeatureDefinition.check_version_mismatch","text":"<pre><code>check_version_mismatch(\n    *,\n    expected_version: str,\n    actual_version: str,\n    expected_version_by_field: dict[str, str],\n    actual_version_by_field: dict[str, str],\n) -&gt; None\n</code></pre> <p>Check if the actual feature version matches expected version.</p> <p>Called by load_feature_definitions after loading external features from the metadata store, comparing provenance-carrying feature versions.</p> <p>Parameters:</p> <ul> <li> <code>expected_version</code>               (<code>str</code>)           \u2013            <p>The feature version before loading (from graph).</p> </li> <li> <code>actual_version</code>               (<code>str</code>)           \u2013            <p>The feature version after loading (from graph).</p> </li> <li> <code>expected_version_by_field</code>               (<code>dict[str, str]</code>)           \u2013            <p>Field-level versions before loading.</p> </li> <li> <code>actual_version_by_field</code>               (<code>dict[str, str]</code>)           \u2013            <p>Field-level versions after loading.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If versions mismatch and on_version_mismatch is \"error\".</p> </li> </ul> Source code in <code>src/metaxy/models/feature_definition.py</code> <pre><code>def check_version_mismatch(\n    self,\n    *,\n    expected_version: str,\n    actual_version: str,\n    expected_version_by_field: dict[str, str],\n    actual_version_by_field: dict[str, str],\n) -&gt; None:\n    \"\"\"Check if the actual feature version matches expected version.\n\n    Called by load_feature_definitions after loading external features from\n    the metadata store, comparing provenance-carrying feature versions.\n\n    Args:\n        expected_version: The feature version before loading (from graph).\n        actual_version: The feature version after loading (from graph).\n        expected_version_by_field: Field-level versions before loading.\n        actual_version_by_field: Field-level versions after loading.\n\n    Raises:\n        ValueError: If versions mismatch and on_version_mismatch is \"error\".\n    \"\"\"\n    if not self.is_external:\n        return\n\n    if expected_version == actual_version:\n        return\n\n    # Find which fields differ\n    mismatched_fields = []\n    all_fields = set(expected_version_by_field.keys()) | set(actual_version_by_field.keys())\n    for field in sorted(all_fields):\n        expected_field_ver = expected_version_by_field.get(field, \"&lt;missing&gt;\")\n        actual_field_ver = actual_version_by_field.get(field, \"&lt;missing&gt;\")\n        if expected_field_ver != actual_field_ver:\n            mismatched_fields.append(f\"  - {field}: expected '{expected_field_ver}', got '{actual_field_ver}'\")\n\n    field_details = \"\\n\".join(mismatched_fields) if mismatched_fields else \"  (no field-level details available)\"\n\n    message = (\n        f\"Version mismatch for external feature '{self.key}': \"\n        f\"expected feature version '{expected_version}', got '{actual_version}'.\\n\"\n        f\"Field-level mismatches:\\n{field_details}\\n\"\n        f\"The external feature definition may be out of sync with the metadata store.\"\n    )\n\n    if self._on_version_mismatch == \"error\":\n        raise ValueError(message)\n    warnings.warn(message, stacklevel=3)\n</code></pre>"},{"location":"reference/api/definitions/field/","title":"Field","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec","title":"metaxy.FieldSpec  <code>pydantic-model</code>","text":"<pre><code>FieldSpec(\n    *,\n    key: CoercibleToFieldKey | None = None,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FieldKey\"\n    },\n    \"code_version\": {\n      \"default\": \"__metaxy_initial__\",\n      \"title\": \"Code Version\",\n      \"type\": \"string\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/SpecialFieldDep\"\n        },\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldDep\"\n          },\n          \"type\": \"array\"\n        }\n      ],\n      \"title\": \"Deps\"\n    }\n  },\n  \"title\": \"FieldSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'extra': 'forbid'}</code></li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    *,\n    key: CoercibleToFieldKey | None = None,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec.__get_pydantic_core_schema__","title":"metaxy.FieldSpec.__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(source_type, handler)\n</code></pre> <p>Add custom validator to coerce strings to FieldSpec.</p> Source code in <code>src/metaxy/models/field.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler):\n    \"\"\"Add custom validator to coerce strings to FieldSpec.\"\"\"\n    from pydantic_core import core_schema\n\n    # Get the default schema\n    python_schema = handler(source_type)\n\n    # Wrap it with a before validator that converts strings\n    return core_schema.no_info_before_validator_function(\n        _validate_field_spec_from_string,\n        python_schema,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldDep","title":"metaxy.FieldDep  <code>pydantic-model</code>","text":"<pre><code>FieldDep(\n    *,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | FeatureSpec\n    | type[BaseFeature],\n    fields: list[CoercibleToFieldKey] | Literal[ALL] = ALL,\n)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"const\": \"__METAXY_ALL_DEP__\",\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"__METAXY_ALL_DEP__\",\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FieldDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'extra': 'forbid'}</code></li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    *,\n    feature: str | Sequence[str] | FeatureKey | \"FeatureSpec\" | type[\"BaseFeature\"],\n    fields: list[CoercibleToFieldKey] | Literal[SpecialFieldDep.ALL] = SpecialFieldDep.ALL,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/","title":"Fields Mapping","text":"<p>Metaxy provides a few helpers for defining field-level lineage:</p> <ul> <li>the default mapping that matches on field names or suffixes with <code>FieldsMapping.default</code></li> <li><code>specific</code> mapping with <code>FieldsMapping.specific</code></li> <li><code>all</code> mapping with <code>FieldsMapping.all</code></li> <li><code>none</code> mapping with <code>FieldsMapping.none</code></li> </ul> <p>Always use these classmethods to create instances of lineage relationships. Under the hood, they use Pydantic's discriminated union to ensure that the correct type is constructed based on the provided data.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping","title":"metaxy.models.fields_mapping.FieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for field mapping configurations.</p> <p>Field mappings define how a field automatically resolves its dependencies based on upstream feature fields. This is separate from explicit field dependencies which are defined directly.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n  \"properties\": {\n    \"mapping\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"all\": \"#/$defs/AllFieldsMapping\",\n          \"default\": \"#/$defs/DefaultFieldsMapping\",\n          \"none\": \"#/$defs/NoneFieldsMapping\",\n          \"specific\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/AllFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/NoneFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n        }\n      ],\n      \"title\": \"Mapping\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"FieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve field dependencies based on upstream feature fields.</p> <p>Invokes the provided mapping to resolve dependencies.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    \"\"\"Resolve field dependencies based on upstream feature fields.\n\n    Invokes the provided mapping to resolve dependencies.\n\n    Args:\n        context: The resolution context containing field key and upstream feature.\n\n    Returns:\n        Set of [FieldKey][metaxy.models.types.FieldKey] instances for matching fields\n    \"\"\"\n    return self.mapping.resolve_field_deps(context)\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.default","title":"metaxy.models.fields_mapping.FieldsMapping.default  <code>classmethod</code>","text":"<pre><code>default(\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self\n</code></pre> <p>Create a default field mapping configuration.</p> <p>Parameters:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef default(\n    cls,\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self:\n    \"\"\"Create a default field mapping configuration.\n\n    Args:\n        match_suffix: If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")\n        exclude_fields: List of field keys to exclude from auto-mapping\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(\n        mapping=DefaultFieldsMapping(\n            match_suffix=match_suffix,\n            exclude_fields=exclude_fields or [],\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.specific","title":"metaxy.models.fields_mapping.FieldsMapping.specific  <code>classmethod</code>","text":"<pre><code>specific(\n    mapping: dict[\n        CoercibleToFieldKey, set[CoercibleToFieldKey]\n    ],\n) -&gt; Self\n</code></pre> <p>Create a field mapping that maps downstream field keys into specific upstream field keys.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]</code>)           \u2013            <p>Mapping of downstream field keys to sets of upstream field keys. Keys and values can be strings, sequences, or FieldKey instances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef specific(cls, mapping: dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]) -&gt; Self:\n    \"\"\"Create a field mapping that maps downstream field keys into specific upstream field keys.\n\n    Args:\n        mapping: Mapping of downstream field keys to sets of upstream field keys.\n            Keys and values can be strings, sequences, or FieldKey instances.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    # Validate and coerce the mapping keys and values\n    validated_mapping: dict[FieldKey, set[FieldKey]] = {}\n    for key, value_set in mapping.items():\n        validated_key = ValidatedFieldKeyAdapter.validate_python(key)\n        validated_values = {ValidatedFieldKeyAdapter.validate_python(v) for v in value_set}\n        validated_mapping[validated_key] = validated_values\n\n    return cls(mapping=SpecificFieldsMapping(mapping=validated_mapping))\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.all","title":"metaxy.models.fields_mapping.FieldsMapping.all  <code>classmethod</code>","text":"<pre><code>all() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on all upstream fields.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef all(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on all upstream fields.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=AllFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.none","title":"metaxy.models.fields_mapping.FieldsMapping.none  <code>classmethod</code>","text":"<pre><code>none() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on no upstream fields.</p> <p>This is typically useful when explicitly defining FieldSpec.deps instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on no upstream fields.\n\n    This is typically useful when explicitly defining [FieldSpec.deps][metaxy.models.field.FieldSpec] instead.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=NoneFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMappingType","title":"metaxy.models.fields_mapping.FieldsMappingType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of fields mapping between a field key and the upstream field keys.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping","title":"metaxy.models.fields_mapping.DefaultFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Default automatic field mapping configuration.</p> <p>When used, automatically maps fields to matching upstream fields based on field keys.</p> <p>Attributes:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>)           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey]</code>)           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"default\",\n      \"default\": \"default\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"match_suffix\": {\n      \"default\": false,\n      \"title\": \"Match Suffix\",\n      \"type\": \"boolean\"\n    },\n    \"exclude_fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Exclude Fields\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"DefaultFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    res = set()\n\n    for upstream_field_key in context.upstream_feature_fields:\n        # Skip excluded fields\n        if upstream_field_key in self.exclude_fields:\n            continue\n\n        # Check for exact match\n        if upstream_field_key == context.field_key:\n            res.add(upstream_field_key)\n        # Check for suffix match if enabled\n        elif self.match_suffix and self._is_suffix_match(context.field_key, upstream_field_key):\n            res.add(upstream_field_key)\n\n    # If no fields matched, return ALL fields from this upstream feature\n    # (excluding any explicitly excluded fields)\n    if not res:\n        for upstream_field_key in context.upstream_feature_fields:\n            if upstream_field_key not in self.exclude_fields:\n                res.add(upstream_field_key)\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping","title":"metaxy.models.fields_mapping.SpecificFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on specific upstream fields.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"specific\",\n      \"default\": \"specific\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"mapping\": {\n      \"additionalProperties\": {\n        \"items\": {\n          \"$ref\": \"#/$defs/FieldKey\"\n        },\n        \"type\": \"array\",\n        \"uniqueItems\": true\n      },\n      \"propertyNames\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Mapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"SpecificFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    desired_upstream_fields = self.mapping.get(context.field_key, set())\n    return desired_upstream_fields &amp; context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping","title":"metaxy.models.fields_mapping.AllFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on all upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"all\",\n      \"default\": \"all\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"AllFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping","title":"metaxy.models.fields_mapping.NoneFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that never matches any upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that never matches any upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"none\",\n      \"default\": \"none\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"NoneFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return set()\n</code></pre>"},{"location":"reference/api/definitions/filters/","title":"String Filters","text":""},{"location":"reference/api/definitions/filters/#metaxy.models.filter_expression.parse_filter_string","title":"metaxy.models.filter_expression.parse_filter_string","text":"<pre><code>parse_filter_string(filter_string: str) -&gt; Expr\n</code></pre> <p>Parse a SQL WHERE-like string into a Narwhals expression.</p> <p>The parser understands SQL <code>WHERE</code> clauses composed of comparison operators, logical operators, parentheses, dotted identifiers, and literal values (strings, numbers, booleans, <code>NULL</code>).</p> <p>This functionality is implemented with SQLGlot.</p> Example <pre><code>parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n# Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n</code></pre> Source code in <code>src/metaxy/models/filter_expression.py</code> <pre><code>@public\ndef parse_filter_string(filter_string: str) -&gt; nw.Expr:\n    \"\"\"Parse a SQL WHERE-like string into a Narwhals expression.\n\n    The parser understands SQL `WHERE` clauses composed of comparison operators, logical operators, parentheses,\n    dotted identifiers, and literal values (strings, numbers, booleans, ``NULL``).\n\n    This functionality is implemented with [SQLGlot](https://sqlglot.com/).\n\n    Example:\n        ```python\n        parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n        # Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n        ```\n    \"\"\"\n    return NarwhalsFilter.model_validate(filter_string).to_expr()\n</code></pre>"},{"location":"reference/api/definitions/graph/","title":"Feature Graph","text":"<p><code>FeatureGraph</code> is a global \"God\" object that holds all the features loaded by Metaxy via the feature discovery mechanism.</p> <p>Users may interact with <code>FeatureGraph</code> when writing custom migrations, otherwise they are not exposed to it.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph","title":"metaxy.FeatureGraph","text":"<pre><code>FeatureGraph()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def __init__(self):\n    # Primary storage: FeatureDefinition objects\n    self.feature_definitions_by_key: dict[FeatureKey, FeatureDefinition] = {}\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.project_version","title":"metaxy.FeatureGraph.project_version  <code>property</code>","text":"<pre><code>project_version: str\n</code></pre> <p>Generate a project version for the current project's features.</p> <p>Uses feature_definition_version (spec + schema only), excluding external features. The project is determined from MetaxyConfig.project if set, otherwise from the graph's single project (via the <code>project</code> property).</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If MetaxyConfig.project is not set and the graph is empty or spans multiple projects.</p> </li> </ul>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.has_external_features","title":"metaxy.FeatureGraph.has_external_features  <code>property</code>","text":"<pre><code>has_external_features: bool\n</code></pre> <p>Check if any feature in the graph is an external feature.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.project","title":"metaxy.FeatureGraph.project  <code>property</code>","text":"<pre><code>project: str\n</code></pre> <p>The single project for all non-external features in this graph.</p> <p>Returns the project name if all non-external features belong to a single project.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the graph is empty or features span multiple projects.</p> </li> </ul>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-functions","title":"Functions","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature","title":"metaxy.FeatureGraph.add_feature","text":"<pre><code>add_feature(feature: type[BaseFeature]) -&gt; None\n</code></pre> <p>Add a feature class to the graph.</p> <p>Creates a FeatureDefinition from the class and delegates to add_feature_definition.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to register</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a feature with a different import path but the same key is already registered        or if duplicate column names would result from renaming operations</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature(self, feature: type[\"BaseFeature\"]) -&gt; None:\n    \"\"\"Add a feature class to the graph.\n\n    Creates a FeatureDefinition from the class and delegates to add_feature_definition.\n\n    Args:\n        feature: Feature class to register\n\n    Raises:\n        ValueError: If a feature with a different import path but the same key is already registered\n                   or if duplicate column names would result from renaming operations\n    \"\"\"\n    definition = FeatureDefinition.from_feature_class(feature)\n    self.add_feature_definition(definition)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature_definition","title":"metaxy.FeatureGraph.add_feature_definition","text":"<pre><code>add_feature_definition(\n    definition: FeatureDefinition,\n    on_conflict: Literal[\"raise\", \"ignore\"] = \"raise\",\n) -&gt; None\n</code></pre> <p>Add a feature to the graph.</p> <p>Interactions with External Features</p> <p>Normal features take priority over external features with the same key.</p> <p>Parameters:</p> <ul> <li> <code>definition</code>               (<code>FeatureDefinition</code>)           \u2013            <p>FeatureDefinition to register</p> </li> <li> <code>on_conflict</code>               (<code>Literal['raise', 'ignore']</code>, default:                   <code>'raise'</code> )           \u2013            <p>What to do if a feature with the same key is already registered</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a non-external feature with a different import path but the same key is already registered and <code>on_conflict</code> is <code>\"raise\"</code></p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature_definition(\n    self, definition: FeatureDefinition, on_conflict: Literal[\"raise\", \"ignore\"] = \"raise\"\n) -&gt; None:\n    \"\"\"Add a feature to the graph.\n\n    !!! note \"Interactions with External Features\"\n\n        Normal features take priority over external features with the same key.\n\n    Args:\n        definition: FeatureDefinition to register\n        on_conflict: What to do if a feature with the same key is already registered\n\n    Raises:\n        ValueError: If a non-external feature with a different import path but\n            the same key is already registered and `on_conflict` is `\"raise\"`\n    \"\"\"\n    key = definition.key\n\n    if key not in self.feature_definitions_by_key:\n        self.feature_definitions_by_key[key] = definition\n    elif definition.is_external and not self.feature_definitions_by_key[key].is_external:\n        # External features never overwrite non-external features\n        return\n    elif not definition.is_external and self.feature_definitions_by_key[key].is_external:\n        # Non-external features always replace external features\n        # Note: version mismatch checking is done in load_feature_definitions,\n        # not here, because we need the full graph context to compute\n        # provenance-carrying versions.\n        self.feature_definitions_by_key[key] = definition\n    elif definition.feature_class_path == self.feature_definitions_by_key[key].feature_class_path:\n        # Same class path - allow quiet replacement\n        self.feature_definitions_by_key[key] = definition\n    elif on_conflict == \"ignore\":\n        # Conflict exists but we're ignoring - keep existing definition\n        return\n    elif definition.is_external:\n        # Both external with different class paths - raise to be safe\n        raise ValueError(f\"External feature with key {key.to_string()} is already registered.\")\n    else:\n        # Both non-external with different class paths\n        raise ValueError(\n            f\"Feature with key {key.to_string()} already registered. \"\n            f\"Existing: {self.feature_definitions_by_key[key].feature_class_path}, \"\n            f\"New: {definition.feature_class_path}. \"\n            f\"Each feature key must be unique within a graph.\"\n        )\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_definition","title":"metaxy.FeatureGraph.get_feature_definition","text":"<pre><code>get_feature_definition(\n    key: CoercibleToFeatureKey,\n) -&gt; FeatureDefinition\n</code></pre> <p>Get a FeatureDefinition by its key.</p> <p>This is the primary method for accessing feature information.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDefinition</code>           \u2013            <p>FeatureDefinition for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_definition(self, key: CoercibleToFeatureKey) -&gt; FeatureDefinition:\n    \"\"\"Get a FeatureDefinition by its key.\n\n    This is the primary method for accessing feature information.\n\n    Args:\n        key: Feature key to look up\n\n    Returns:\n        FeatureDefinition for the feature\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    if validated_key not in self.feature_definitions_by_key:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.feature_definitions_by_key.keys()]}\"\n        )\n    return self.feature_definitions_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.remove_feature","title":"metaxy.FeatureGraph.remove_feature","text":"<pre><code>remove_feature(key: CoercibleToFeatureKey) -&gt; None\n</code></pre> <p>Remove a feature from the graph.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to remove. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def remove_feature(self, key: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Remove a feature from the graph.\n\n    Args:\n        key: Feature key to remove. Accepts types that can be converted into a feature key..\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    if validated_key not in self.feature_definitions_by_key:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.feature_definitions_by_key]}\"\n        )\n\n    del self.feature_definitions_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.list_features","title":"metaxy.FeatureGraph.list_features","text":"<pre><code>list_features(\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]\n</code></pre> <p>List all feature keys in the graph, optionally filtered by project(s).</p> <p>By default, filters features by the current project (first part of feature key). This prevents operations from affecting features in other projects.</p> <p>Parameters:</p> <ul> <li> <code>projects</code>               (<code>list[str] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name(s) to filter by. Can be: - None: Use current project from MetaxyConfig (if only_current_project=True) - str: Single project name - list[str]: Multiple project names</p> </li> <li> <code>only_current_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filter by current/specified project(s). If False, return all features regardless of project.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys</p> </li> </ul> Example <pre><code># Get features for specific project\nfeatures = graph.list_features(projects=\"myproject\")\n\n# Get all features regardless of project\nall_features = graph.list_features(only_current_project=False)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def list_features(\n    self,\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]:\n    \"\"\"List all feature keys in the graph, optionally filtered by project(s).\n\n    By default, filters features by the current project (first part of feature key).\n    This prevents operations from affecting features in other projects.\n\n    Args:\n        projects: Project name(s) to filter by. Can be:\n            - None: Use current project from MetaxyConfig (if only_current_project=True)\n            - str: Single project name\n            - list[str]: Multiple project names\n        only_current_project: If True, filter by current/specified project(s).\n            If False, return all features regardless of project.\n\n    Returns:\n        List of feature keys\n\n    Example:\n        ```py\n        # Get features for specific project\n        features = graph.list_features(projects=\"myproject\")\n\n        # Get all features regardless of project\n        all_features = graph.list_features(only_current_project=False)\n        ```\n    \"\"\"\n    if not only_current_project:\n        # Return all features (both class-based and definition-only)\n        return list(self.feature_definitions_by_key.keys())\n\n    # Normalize projects to list\n    project_list: list[str]\n    if projects is None:\n        # Try to get from config context\n        try:\n            from metaxy.config import MetaxyConfig\n\n            config = MetaxyConfig.get()\n            if config.project is None:\n                # No project configured - return all features\n                return list(self.feature_definitions_by_key.keys())\n            project_list = [config.project]\n        except RuntimeError:\n            # Config not initialized - in tests or non-CLI usage\n            # Return all features (can't determine project)\n            return list(self.feature_definitions_by_key.keys())\n    elif isinstance(projects, str):\n        project_list = [projects]\n    else:\n        project_list = projects\n\n    # Filter by project(s) using FeatureDefinition.project\n    return [key for key, defn in self.feature_definitions_by_key.items() if defn.project in project_list]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_plan","title":"metaxy.FeatureGraph.get_feature_plan","text":"<pre><code>get_feature_plan(key: CoercibleToFeatureKey) -&gt; FeaturePlan\n</code></pre> <p>Get a feature plan for a given feature key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get plan for. Accepts types that can be converted into a feature key.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeaturePlan</code>           \u2013            <p>FeaturePlan instance with feature spec and dependencies.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetaxyMissingFeatureDependency</code>             \u2013            <p>If any dependency is not in the graph.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_plan(self, key: CoercibleToFeatureKey) -&gt; FeaturePlan:\n    \"\"\"Get a feature plan for a given feature key.\n\n    Args:\n        key: Feature key to get plan for. Accepts types that can be converted into a feature key.\n\n    Returns:\n        FeaturePlan instance with feature spec and dependencies.\n\n    Raises:\n        MetaxyMissingFeatureDependency: If any dependency is not in the graph.\n    \"\"\"\n    from metaxy.utils.exceptions import MetaxyMissingFeatureDependency\n\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    definition = self.feature_definitions_by_key[validated_key]\n    spec = definition.spec\n\n    # Check all dependencies are present and collect their specs\n    dep_specs = []\n    for dep in spec.deps or []:\n        if dep.feature not in self.feature_definitions_by_key:\n            raise MetaxyMissingFeatureDependency(\n                f\"Feature '{validated_key.to_string()}' depends on '{dep.feature.to_string()}' \"\n                f\"which is not in the graph.\"\n            )\n        dep_specs.append(self.feature_definitions_by_key[dep.feature].spec)\n\n    return FeaturePlan(\n        feature=spec,\n        deps=dep_specs or None,\n        feature_deps=spec.deps,\n    )\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version_by_field","title":"metaxy.FeatureGraph.get_feature_version_by_field","text":"<pre><code>get_feature_version_by_field(\n    key: CoercibleToFeatureKey,\n) -&gt; dict[str, str]\n</code></pre> <p>Computes the field provenance map for a feature.</p> <p>Hash together field provenance entries with the feature code version.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get field versions for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>dict[str, str]: The provenance hash for each field in the feature plan. Keys are field names as strings.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version_by_field(self, key: CoercibleToFeatureKey) -&gt; dict[str, str]:\n    \"\"\"Computes the field provenance map for a feature.\n\n    Hash together field provenance entries with the feature code version.\n\n    Args:\n        key: Feature key to get field versions for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        dict[str, str]: The provenance hash for each field in the feature plan.\n            Keys are field names as strings.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    res = {}\n\n    plan = self.get_feature_plan(validated_key)\n\n    for k, v in plan.feature.fields_by_key.items():\n        res[k.to_string()] = self.get_field_version(FQFieldKey(field=k, feature=validated_key))\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version","title":"metaxy.FeatureGraph.get_feature_version","text":"<pre><code>get_feature_version(key: CoercibleToFeatureKey) -&gt; str\n</code></pre> <p>Computes the feature version as a single string.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get version for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Truncated SHA256 hash representing the feature version.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version(self, key: CoercibleToFeatureKey) -&gt; str:\n    \"\"\"Computes the feature version as a single string.\n\n    Args:\n        key: Feature key to get version for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        Truncated SHA256 hash representing the feature version.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    hasher = hashlib.sha256()\n    provenance_by_field = self.get_feature_version_by_field(validated_key)\n    for field_key in sorted(provenance_by_field):\n        hasher.update(field_key.encode())\n        hasher.update(provenance_by_field[field_key].encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_downstream_features","title":"metaxy.FeatureGraph.get_downstream_features","text":"<pre><code>get_downstream_features(\n    sources: Sequence[CoercibleToFeatureKey],\n) -&gt; list[FeatureKey]\n</code></pre> <p>Get all features downstream of sources, topologically sorted.</p> <p>Performs a depth-first traversal of the dependency graph to find all features that transitively depend on any of the source features.</p> <p>Parameters:</p> <ul> <li> <code>sources</code>               (<code>Sequence[CoercibleToFeatureKey]</code>)           \u2013            <p>List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of downstream feature keys in topological order (dependencies first).</p> </li> <li> <code>list[FeatureKey]</code>           \u2013            <p>Does not include the source features themselves.</p> </li> </ul> Example <pre><code># Build a DAG: a -&gt; b -&gt; d, a -&gt; c -&gt; d\nclass FeatureA(mx.BaseFeature, spec=mx.FeatureSpec(key=\"a\", id_columns=[\"id\"])):\n    id: str\n\n\nclass FeatureB(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"b\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n):\n    id: str\n\n\nclass FeatureC(\n    mx.BaseFeature, spec=mx.FeatureSpec(key=\"c\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n):\n    id: str\n\n\nclass FeatureD(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"d\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureB), mx.FeatureDep(feature=FeatureC)]\n    ),\n):\n    id: str\n\n\ngraph.get_downstream_features([\"a\"])\n# [FeatureKey(['b']), FeatureKey(['c']), FeatureKey(['d'])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_downstream_features(self, sources: Sequence[CoercibleToFeatureKey]) -&gt; list[FeatureKey]:\n    \"\"\"Get all features downstream of sources, topologically sorted.\n\n    Performs a depth-first traversal of the dependency graph to find all\n    features that transitively depend on any of the source features.\n\n    Args:\n        sources: List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.\n\n    Returns:\n        List of downstream feature keys in topological order (dependencies first).\n        Does not include the source features themselves.\n\n    Example:\n        ```py\n        # Build a DAG: a -&gt; b -&gt; d, a -&gt; c -&gt; d\n        class FeatureA(mx.BaseFeature, spec=mx.FeatureSpec(key=\"a\", id_columns=[\"id\"])):\n            id: str\n\n\n        class FeatureB(\n            mx.BaseFeature, spec=mx.FeatureSpec(key=\"b\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n        ):\n            id: str\n\n\n        class FeatureC(\n            mx.BaseFeature, spec=mx.FeatureSpec(key=\"c\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureA)])\n        ):\n            id: str\n\n\n        class FeatureD(\n            mx.BaseFeature,\n            spec=mx.FeatureSpec(\n                key=\"d\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=FeatureB), mx.FeatureDep(feature=FeatureC)]\n            ),\n        ):\n            id: str\n\n\n        graph.get_downstream_features([\"a\"])\n        # [FeatureKey(['b']), FeatureKey(['c']), FeatureKey(['d'])]\n        ```\n    \"\"\"\n    # Validate and coerce the source keys\n    validated_sources = ValidatedFeatureKeySequenceAdapter.validate_python(sources)\n\n    source_set = set(validated_sources)\n    visited = set()\n    post_order = []\n    source_set = set(sources)\n    visited = set()\n    post_order = []  # Reverse topological order\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS traversal.\"\"\"\n        if key in visited:\n            return\n        visited.add(key)\n\n        # Find all features that depend on this one\n        for feature_key, definition in self.feature_definitions_by_key.items():\n            if definition.spec.deps:\n                for dep in definition.spec.deps:\n                    if dep.feature == key:\n                        # This feature depends on 'key', so visit it\n                        visit(feature_key)\n\n        post_order.append(key)\n\n    # Visit all sources\n    for source in validated_sources:\n        visit(source)\n\n    # Remove sources from result, reverse to get topological order\n    result = [k for k in reversed(post_order) if k not in source_set]\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.topological_sort_features","title":"metaxy.FeatureGraph.topological_sort_features","text":"<pre><code>topological_sort_features(\n    feature_keys: Sequence[CoercibleToFeatureKey]\n    | None = None,\n    *,\n    descending: bool = False,\n) -&gt; list[FeatureKey]\n</code></pre> <p>Sort feature keys in topological order.</p> <p>Uses stable alphabetical ordering when multiple nodes are at the same level. This ensures deterministic output for diff comparisons and migrations.</p> <p>Implemented using depth-first search with post-order traversal.</p> <p>Parameters:</p> <ul> <li> <code>feature_keys</code>               (<code>Sequence[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of feature keys to sort. Each element can be string, sequence, FeatureKey, or BaseFeature class. If None, sorts all features (both Feature classes and standalone specs) in the graph.</p> </li> <li> <code>descending</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If False (default), dependencies appear before dependents. For a chain A -&gt; B -&gt; C, returns [A, B, C]. If True, dependents appear before dependencies. For a chain A -&gt; B -&gt; C, returns [C, B, A].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys sorted in topological order</p> </li> </ul> Example <pre><code>class VideoRaw(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/raw\", id_columns=[\"id\"])):\n    id: str\n\n\nclass VideoScene(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(key=\"video/scene\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=VideoRaw)]),\n):\n    id: str\n\n\ngraph.topological_sort_features([\"video/raw\", \"video/scene\"])\n# [FeatureKey(['video', 'raw']), FeatureKey(['video', 'scene'])]\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def topological_sort_features(\n    self,\n    feature_keys: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    descending: bool = False,\n) -&gt; list[FeatureKey]:\n    \"\"\"Sort feature keys in topological order.\n\n    Uses stable alphabetical ordering when multiple nodes are at the same level.\n    This ensures deterministic output for diff comparisons and migrations.\n\n    Implemented using depth-first search with post-order traversal.\n\n    Args:\n        feature_keys: List of feature keys to sort. Each element can be string, sequence,\n            FeatureKey, or BaseFeature class. If None, sorts all features\n            (both Feature classes and standalone specs) in the graph.\n        descending: If False (default), dependencies appear before dependents.\n            For a chain A -&gt; B -&gt; C, returns [A, B, C].\n            If True, dependents appear before dependencies.\n            For a chain A -&gt; B -&gt; C, returns [C, B, A].\n\n    Returns:\n        List of feature keys sorted in topological order\n\n    Example:\n        ```py\n        class VideoRaw(mx.BaseFeature, spec=mx.FeatureSpec(key=\"video/raw\", id_columns=[\"id\"])):\n            id: str\n\n\n        class VideoScene(\n            mx.BaseFeature,\n            spec=mx.FeatureSpec(key=\"video/scene\", id_columns=[\"id\"], deps=[mx.FeatureDep(feature=VideoRaw)]),\n        ):\n            id: str\n\n\n        graph.topological_sort_features([\"video/raw\", \"video/scene\"])\n        # [FeatureKey(['video', 'raw']), FeatureKey(['video', 'scene'])]\n        ```\n    \"\"\"\n    # Determine which features to sort\n    if feature_keys is None:\n        # Include all features\n        keys_to_sort = set(self.feature_definitions_by_key.keys())\n    else:\n        # Validate and coerce the feature keys\n        validated_keys = ValidatedFeatureKeySequenceAdapter.validate_python(feature_keys)\n        keys_to_sort = set(validated_keys)\n\n    visited = set()\n    result = []  # Topological order (dependencies first)\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS visit with post-order traversal.\"\"\"\n        if key in visited or key not in keys_to_sort:\n            return\n        visited.add(key)\n\n        # Get dependencies from feature definition\n        definition = self.feature_definitions_by_key.get(key)\n        if definition and definition.spec.deps:\n            # Sort dependencies alphabetically for deterministic ordering\n            sorted_deps = sorted(\n                (dep.feature for dep in definition.spec.deps),\n                key=lambda k: k.to_string().lower(),\n            )\n            for dep_key in sorted_deps:\n                if dep_key in keys_to_sort:\n                    visit(dep_key)\n\n        # Add to result after visiting dependencies (post-order)\n        result.append(key)\n\n    # Visit all keys in sorted order for deterministic traversal\n    for key in sorted(keys_to_sort, key=lambda k: k.to_string().lower()):\n        visit(key)\n\n    # Post-order DFS gives topological order (dependencies before dependents)\n    if descending:\n        return list(reversed(result))\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_project_version","title":"metaxy.FeatureGraph.get_project_version","text":"<pre><code>get_project_version(project: str) -&gt; str\n</code></pre> <p>Generate a project version for features belonging to a specific project.</p> <p>Uses feature_definition_version (spec + schema only), excluding external features. This makes the project version independent of external feature changes.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str</code>)           \u2013            <p>The project name to compute version for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A hash representing the project's feature definitions.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_project_version(self, project: str) -&gt; str:\n    \"\"\"Generate a project version for features belonging to a specific project.\n\n    Uses feature_definition_version (spec + schema only), excluding external features.\n    This makes the project version independent of external feature changes.\n\n    Args:\n        project: The project name to compute version for.\n\n    Returns:\n        A hash representing the project's feature definitions.\n    \"\"\"\n    project_features = sorted(\n        (\n            (key, defn)\n            for key, defn in self.feature_definitions_by_key.items()\n            if defn.project == project and not defn.is_external\n        ),\n        key=lambda x: x[0],\n    )\n    return self._compute_project_version(project_features)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.to_snapshot","title":"metaxy.FeatureGraph.to_snapshot","text":"<pre><code>to_snapshot(\n    *, project: str | None = None\n) -&gt; dict[str, SerializedFeature]\n</code></pre> <p>Serialize graph to snapshot format.</p> <p>Returns a dict mapping feature_key (string) to feature data dict, including the import path of the Feature class for reconstruction.</p> <p>External features are excluded from the snapshot as they should not be pushed to the metadata store.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Only include features from this project. If not provided, uses the graph's single project (via the <code>project</code> property).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, SerializedFeature]</code>           \u2013            <p>Dictionary mapping feature_key (string) to feature data dict.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If no project is provided and features span multiple projects.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def to_snapshot(self, *, project: str | None = None) -&gt; dict[str, SerializedFeature]:\n    \"\"\"Serialize graph to snapshot format.\n\n    Returns a dict mapping feature_key (string) to feature data dict,\n    including the import path of the Feature class for reconstruction.\n\n    External features are excluded from the snapshot as they should not be\n    pushed to the metadata store.\n\n    Args:\n        project: Only include features from this project. If not provided,\n            uses the graph's single project (via the `project` property).\n\n    Returns:\n        Dictionary mapping feature_key (string) to feature data dict.\n\n    Raises:\n        RuntimeError: If no project is provided and features span multiple projects.\n    \"\"\"\n    if project is None:\n        project = self.project\n\n    snapshot: dict[str, SerializedFeature] = {}\n\n    for feature_key, definition in self.feature_definitions_by_key.items():\n        # Skip external features - they should not be pushed to the metadata store\n        if definition.is_external:\n            continue\n\n        # Skip features from other projects\n        if definition.project != project:\n            continue\n\n        feature_key_str = feature_key.to_string()\n        feature_spec_dict = definition.spec.model_dump(mode=\"json\")\n        feature_schema_dict = definition.feature_schema\n        feature_version = self.get_feature_version(feature_key)\n        definition_version = definition.feature_definition_version\n        project = definition.project\n        class_path = definition.feature_class_path\n        assert class_path is not None, \"feature_class_path must be set for serialization\"\n\n        snapshot[feature_key_str] = {\n            \"feature_spec\": feature_spec_dict,\n            \"feature_schema\": feature_schema_dict,\n            FEATURE_VERSION_COL: feature_version,\n            FEATURE_TRACKING_VERSION_COL: definition_version,\n            \"feature_class_path\": class_path,\n            \"project\": project,\n        }\n\n    return snapshot\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.from_snapshot","title":"metaxy.FeatureGraph.from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(\n    snapshot_data: Mapping[str, Mapping[str, Any]],\n) -&gt; FeatureGraph\n</code></pre> <p>Reconstruct graph from snapshot by creating FeatureDefinition objects.</p> <p>This method creates FeatureDefinition objects directly from the snapshot data without any dynamic imports. The resulting graph contains all feature metadata needed for operations like migrations and comparisons.</p> <p>Parameters:</p> <ul> <li> <code>snapshot_data</code>               (<code>Mapping[str, Mapping[str, Any]]</code>)           \u2013            <p>Dict of feature_key -&gt; dict containing all required fields: - feature_spec (dict): The feature specification - feature_schema (dict): The JSON schema for the feature - feature_class_path (str): The import path of the feature class - project (str): The project name</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>New FeatureGraph with FeatureDefinition objects</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If required fields are missing from snapshot data</p> </li> </ul> Example <pre><code>snapshot_data = {}  # Loaded from metadata store\n# Load snapshot from metadata store\nhistorical_graph = FeatureGraph.from_snapshot(snapshot_data)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef from_snapshot(\n    cls,\n    snapshot_data: Mapping[str, Mapping[str, Any]],\n) -&gt; \"FeatureGraph\":\n    \"\"\"Reconstruct graph from snapshot by creating FeatureDefinition objects.\n\n    This method creates FeatureDefinition objects directly from the snapshot data\n    without any dynamic imports. The resulting graph contains all feature metadata\n    needed for operations like migrations and comparisons.\n\n    Args:\n        snapshot_data: Dict of feature_key -&gt; dict containing all required fields:\n            - feature_spec (dict): The feature specification\n            - feature_schema (dict): The JSON schema for the feature\n            - feature_class_path (str): The import path of the feature class\n            - project (str): The project name\n\n    Returns:\n        New FeatureGraph with FeatureDefinition objects\n\n    Raises:\n        KeyError: If required fields are missing from snapshot data\n\n    Example:\n        ```py\n        snapshot_data = {}  # Loaded from metadata store\n        # Load snapshot from metadata store\n        historical_graph = FeatureGraph.from_snapshot(snapshot_data)\n        ```\n    \"\"\"\n    graph = cls()\n\n    required_fields = (\"feature_spec\", \"feature_schema\", \"feature_class_path\", \"project\")\n\n    for feature_key_str, feature_data in snapshot_data.items():\n        # Validate all required fields are present\n        missing_fields = [f for f in required_fields if f not in feature_data]\n        if missing_fields:\n            raise KeyError(\n                f\"Feature '{feature_key_str}' snapshot is missing required fields: {missing_fields}. \"\n                f\"All snapshots must include: {required_fields}\"\n            )\n\n        definition = FeatureDefinition.from_stored_data(\n            feature_spec=feature_data[\"feature_spec\"],\n            feature_schema=feature_data[\"feature_schema\"],\n            feature_class_path=feature_data[\"feature_class_path\"],\n            project=feature_data[\"project\"],\n            source=\"snapshot\",\n        )\n        graph.add_feature_definition(definition)\n\n    return graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get","title":"metaxy.FeatureGraph.get  <code>classmethod</code>","text":"<pre><code>get() -&gt; FeatureGraph\n</code></pre> <p>Get the currently active graph.</p> <p>Returns the graph from the context variable if set, otherwise returns the default global graph.</p> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>Active FeatureGraph instance</p> </li> </ul> Example <pre><code>graph = mx.FeatureGraph.get_active()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef get(cls) -&gt; \"FeatureGraph\":\n    \"\"\"Get the currently active graph.\n\n    Returns the graph from the context variable if set, otherwise returns\n    the default global graph.\n\n    Returns:\n        Active FeatureGraph instance\n\n    Example:\n        ```py\n        graph = mx.FeatureGraph.get_active()\n        ```\n    \"\"\"\n    return _active_graph.get() or graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.set_active","title":"metaxy.FeatureGraph.set_active  <code>classmethod</code>","text":"<pre><code>set_active(reg: FeatureGraph) -&gt; None\n</code></pre> <p>Set the active graph for the current context.</p> <p>This sets the context variable that will be returned by get_active(). Typically used in application setup code or test fixtures.</p> <p>Parameters:</p> <ul> <li> <code>reg</code>               (<code>FeatureGraph</code>)           \u2013            <p>FeatureGraph to activate</p> </li> </ul> Example <pre><code>my_graph = mx.FeatureGraph()\nmx.FeatureGraph.set_active(my_graph)\nmx.FeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef set_active(cls, reg: \"FeatureGraph\") -&gt; None:\n    \"\"\"Set the active graph for the current context.\n\n    This sets the context variable that will be returned by get_active().\n    Typically used in application setup code or test fixtures.\n\n    Args:\n        reg: FeatureGraph to activate\n\n    Example:\n        ```py\n        my_graph = mx.FeatureGraph()\n        mx.FeatureGraph.set_active(my_graph)\n        mx.FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    _active_graph.set(reg)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.use","title":"metaxy.FeatureGraph.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Context manager to temporarily use this graph as active.</p> <p>This is the recommended way to use custom registries, especially in tests. The graph is automatically restored when the context exits.</p> <p>Yields:</p> <ul> <li> <code>FeatureGraph</code> (              <code>Self</code> )          \u2013            <p>This graph instance</p> </li> </ul> Example <pre><code>with graph.use():\n\n    class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test\", id_columns=[\"id\"])):\n        id: str\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Context manager to temporarily use this graph as active.\n\n    This is the recommended way to use custom registries, especially in tests.\n    The graph is automatically restored when the context exits.\n\n    Yields:\n        FeatureGraph: This graph instance\n\n    Example:\n        ```py\n        with graph.use():\n\n            class TestFeature(mx.BaseFeature, spec=mx.FeatureSpec(key=\"test\", id_columns=[\"id\"])):\n                id: str\n        ```\n    \"\"\"\n    token = _active_graph.set(self)\n    try:\n        yield self\n    finally:\n        _active_graph.reset(token)\n</code></pre>"},{"location":"reference/api/definitions/relationship/","title":"Lineage Relationships","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship","title":"metaxy.models.lineage.LineageRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for lineage relationship configurations with convenient constructors.</p> <p>This provides a cleaner API for creating lineage relationships while maintaining type safety through discriminated unions.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n  \"properties\": {\n    \"relationship\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"1:1\": \"#/$defs/IdentityRelationship\",\n          \"1:N\": \"#/$defs/ExpansionRelationship\",\n          \"N:1\": \"#/$defs/AggregationRelationship\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/IdentityRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/AggregationRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/ExpansionRelationship\"\n        }\n      ],\n      \"title\": \"Relationship\"\n    }\n  },\n  \"required\": [\n    \"relationship\"\n  ],\n  \"title\": \"LineageRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.identity","title":"metaxy.models.lineage.LineageRelationship.identity  <code>classmethod</code>","text":"<pre><code>identity() -&gt; Self\n</code></pre> <p>Create an identity (1:1) relationship.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:1 relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.identity()\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; Self:\n    \"\"\"Create an identity (1:1) relationship.\n\n    Returns:\n        Configured LineageRelationship for 1:1 relationship.\n\n    Example:\n        ```python\n        mx.LineageRelationship.identity()\n        ```\n    \"\"\"\n    return cls(relationship=IdentityRelationship())\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.aggregation","title":"metaxy.models.lineage.LineageRelationship.aggregation  <code>classmethod</code>","text":"<pre><code>aggregation(on: Sequence[str] | None = None) -&gt; Self\n</code></pre> <p>Create an aggregation (N:1) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Columns to group by for aggregation. If None, uses all target ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for N:1 relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef aggregation(cls, on: Sequence[str] | None = None) -&gt; Self:\n    \"\"\"Create an aggregation (N:1) relationship.\n\n    Args:\n        on: Columns to group by for aggregation. If None, uses all target ID columns.\n\n    Returns:\n        Configured LineageRelationship for N:1 relationship.\n\n    Example:\n        ```py\n        mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n        ```\n    \"\"\"\n    return cls(relationship=AggregationRelationship(on=on))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.expansion","title":"metaxy.models.lineage.LineageRelationship.expansion  <code>classmethod</code>","text":"<pre><code>expansion(\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self\n</code></pre> <p>Create an expansion (1:N) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. Required - must explicitly specify which columns link parent to child.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Pattern for generating child IDs. Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:N relationship.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef expansion(\n    cls,\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self:\n    \"\"\"Create an expansion (1:N) relationship.\n\n    Args:\n        on: Parent ID columns that identify the parent record. Child records with\n            the same parent IDs will share the same upstream provenance.\n            Required - must explicitly specify which columns link parent to child.\n        id_generation_pattern: Pattern for generating child IDs.\n            Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().\n\n    Returns:\n        Configured LineageRelationship for 1:N relationship.\n\n    Example:\n        ```py\n        mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n        ```\n    \"\"\"\n    return cls(relationship=ExpansionRelationship(on=on, id_generation_pattern=id_generation_pattern))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.get_aggregation_columns","title":"metaxy.models.lineage.LineageRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>Get columns to aggregate on for this relationship.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>Columns to group by for aggregation, or None if no aggregation needed.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(self, target_id_columns: Sequence[str]) -&gt; Sequence[str] | None:\n    \"\"\"Get columns to aggregate on for this relationship.\n\n    Args:\n        target_id_columns: The target feature's ID columns.\n\n    Returns:\n        Columns to group by for aggregation, or None if no aggregation needed.\n    \"\"\"\n    return self.relationship.get_aggregation_columns(target_id_columns)\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationshipType","title":"metaxy.models.lineage.LineageRelationshipType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of lineage relationship between features.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship","title":"metaxy.models.lineage.IdentityRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-one relationship where each child row maps to exactly one parent row.</p> <p>This is the default relationship type. Parent and child features share the same ID columns and have the same cardinality.</p> <p>Construct this relationship via <code>LineageRelationship.identity</code> classmethod.</p> Example <pre><code>mx.LineageRelationship.identity()\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality.\\n\\nConstruct this relationship via [`LineageRelationship.identity`][metaxy.models.lineage.LineageRelationship.identity] classmethod.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.identity()\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:1\",\n      \"default\": \"1:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"IdentityRelationship\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","title":"metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>No aggregation needed for identity relationships.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"No aggregation needed for identity relationships.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship","title":"metaxy.models.lineage.ExpansionRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-many relationship where one parent row expands to multiple child rows.</p> <p>Child features have more granular ID columns than the parent. Each parent row generates multiple child rows with additional ID columns.</p> <p>Construct this relationship via <code>LineageRelationship.expansion</code> classmethod.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. If not specified, will be inferred from the available columns.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>)           \u2013            <p>Optional pattern for generating child IDs. Can be \"sequential\", \"hash\", or a custom pattern. If not specified, the feature's load_input() method is responsible for ID generation.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.expansion(on=[\"video_id\"], id_generation_pattern=\"sequential\")\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.expansion`][metaxy.models.lineage.LineageRelationship.expansion] classmethod.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.expansion(on=[\\\"video_id\\\"], id_generation_pattern=\\\"sequential\\\")\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:N\",\n      \"default\": \"1:N\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"On\",\n      \"type\": \"array\"\n    },\n    \"id_generation_pattern\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n      \"title\": \"Id Generation Pattern\"\n    }\n  },\n  \"required\": [\n    \"on\"\n  ],\n  \"title\": \"ExpansionRelationship\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.on","title":"metaxy.models.lineage.ExpansionRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str]\n</code></pre> <p>Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.id_generation_pattern","title":"metaxy.models.lineage.ExpansionRelationship.id_generation_pattern  <code>pydantic-field</code>","text":"<pre><code>id_generation_pattern: str | None = None\n</code></pre> <p>Pattern for generating child IDs. If None, handled by load_input().</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","title":"metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None\n</code></pre> <p>Get aggregation columns for the joiner phase.</p> <p>For expansion relationships, returns None because aggregation happens during diff resolution, not during joining. The joiner should pass through all parent records without aggregation.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target (child) feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>None - no aggregation during join phase for expansion relationships.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"Get aggregation columns for the joiner phase.\n\n    For expansion relationships, returns None because aggregation\n    happens during diff resolution, not during joining. The joiner\n    should pass through all parent records without aggregation.\n\n    Args:\n        target_id_columns: The target (child) feature's ID columns.\n\n    Returns:\n        None - no aggregation during join phase for expansion relationships.\n    \"\"\"\n    # Expansion relationships don't aggregate during join phase\n    # Aggregation happens later during diff resolution\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship","title":"metaxy.models.lineage.AggregationRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>Many-to-one relationship where multiple parent rows aggregate to one child row.</p> <p>Parent features have more granular ID columns than the child. The child aggregates multiple parent rows by grouping on a subset of the parent's ID columns.</p> <p>Construct this relationship via <code>LineageRelationship.aggregation</code> classmethod.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>)           \u2013            <p>Columns to group by for aggregation. These should be a subset of the target feature's ID columns. If not specified, uses all target ID columns.</p> </li> </ul> Example <pre><code>mx.LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nConstruct this relationship via [`LineageRelationship.aggregation`][metaxy.models.lineage.LineageRelationship.aggregation] classmethod.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExample:\\n    ```python\\n    mx.LineageRelationship.aggregation(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    ```\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"N:1\",\n      \"default\": \"N:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n      \"title\": \"On\"\n    }\n  },\n  \"title\": \"AggregationRelationship\",\n  \"type\": \"object\"\n}\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.on","title":"metaxy.models.lineage.AggregationRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str] | None = None\n</code></pre> <p>Columns to group by for aggregation. Defaults to all target ID columns.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","title":"metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]\n</code></pre> <p>Get columns to aggregate on.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]:\n    \"\"\"Get columns to aggregate on.\"\"\"\n    return self.on if self.on is not None else target_id_columns\n</code></pre>"},{"location":"reference/api/metadata-stores/base/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata behind an interface called <code>MetadataStore</code>.</p> <p>Users can extend this class to implement support for arbitrary metadata storage such as databases, lakehouse formats, or really any kind of external system.</p> <p>Learn how to use metadata stores here.</p> <p>Here are some of the built-in metadata store types (1):</p> <ol> <li>the full list can be found here</li> </ol>"},{"location":"reference/api/metadata-stores/base/#databases","title":"Databases","text":"<ul> <li> <p>BigQuery</p> </li> <li> <p>ClickHouse</p> </li> <li> <p>DuckDB - if used with Motherduck</p> </li> <li> <p>LanceDB - if used with LanceDB Cloud</p> </li> </ul>"},{"location":"reference/api/metadata-stores/base/#storage","title":"Storage","text":"<ul> <li> <p>DeltaMetadataStore</p> </li> <li> <p>DuckDB - if used with a local file</p> </li> <li> <p>LanceDB - if used with a local file</p> </li> </ul>"},{"location":"reference/api/metadata-stores/base/#metadata-store-interface","title":"Metadata Store Interface","text":""},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore","title":"metaxy.MetadataStore","text":"<pre><code>MetadataStore(\n    *,\n    name: str | None = None,\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: Sequence[MetadataStore | str]\n    | None = None,\n    auto_create_tables: bool | None = None,\n    materialization_id: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for metadata storage backends.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for this store. For representation purposes only. Is typically included into <code>.display()</code>.</p> </li> <li> <code>hash_algorithm</code>               (<code>HashAlgorithm | None</code>, default:                   <code>None</code> )           \u2013            <p>Hash algorithm to use for the versioning engine.</p> </li> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use.</p> <ul> <li> <p>\"auto\": Prefer the store's native engine and fall back to Polars if needed</p> </li> <li> <p>\"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if at some point the metadata store has to switch to Polars</p> </li> <li> <p>\"polars\": Always use the Polars engine</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>Sequence[MetadataStore | str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. Used when upstream features are not in this store. <code>VersioningEngineMismatchError</code> is not raised when reading from fallback stores.</p> </li> <li> <code>auto_create_tables</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>Whether to automatically create tables when writing new features. If not provided, the global Metaxy configuration <code>auto_created_tables</code> option will be used (which can be set via <code>METAXY_AUTO_CREATE_TABLES</code> env var).</p> <p>Warning</p> <p>This is intended for development/testing purposes. Use proper database migration tools like Alembic to manage table infrastructure in production.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID. If provided, all metadata writes will include this ID in the <code>metaxy_materialization_id</code> column. Can be overridden per <code>MetadataStore.write</code> call.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If fallback stores use different hash algorithms or truncation lengths</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If the versioning engine is attempted to be switched to Polars and <code>versioning_engine</code> is set to <code>native</code>.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str | None = None,\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: Sequence[MetadataStore | str] | None = None,\n    auto_create_tables: bool | None = None,\n    materialization_id: str | None = None,\n):\n    \"\"\"\n    Initialize the metadata store.\n\n    Args:\n        name: Optional name for this store. For representation purposes only.\n            Is typically included into `.display()`.\n        hash_algorithm: Hash algorithm to use for the versioning engine.\n        versioning_engine: Which versioning engine to use.\n\n            - \"auto\": Prefer the store's native engine and fall back to Polars if needed\n\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if at some point the metadata store has to switch to Polars\n\n            - \"polars\": Always use the Polars engine\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            Used when upstream features are not in this store.\n            `VersioningEngineMismatchError` is not raised when reading from fallback stores.\n        auto_create_tables: Whether to automatically create tables when writing new features.\n            If not provided, the global Metaxy configuration `auto_created_tables` option will\n            be used (which can be set via `METAXY_AUTO_CREATE_TABLES` env var).\n\n            !!! warning\n                This is intended for development/testing purposes.\n                Use proper database migration tools like Alembic to manage table infrastructure in production.\n\n        materialization_id: Optional external orchestration ID.\n            If provided, all metadata writes will include this ID in the `metaxy_materialization_id` column.\n            Can be overridden per [`MetadataStore.write`][metaxy.MetadataStore.write] call.\n\n    Raises:\n        ValueError: If fallback stores use different hash algorithms or truncation lengths\n        VersioningEngineMismatchError: If the versioning engine is attempted to be switched\n            to Polars and `versioning_engine` is set to `native`.\n    \"\"\"\n    self._name = name\n    self._is_open = False\n    self._context_depth = 0\n    self._versioning_engine = versioning_engine\n    self._materialization_id = materialization_id\n    self._open_cm: AbstractContextManager[Self] | None = None\n    self._transaction_timestamp: datetime | None = None\n    self._soft_delete_in_progress: bool = False\n\n    if auto_create_tables is None:\n        from metaxy.config import MetaxyConfig\n\n        self.auto_create_tables = MetaxyConfig.get().auto_create_tables\n    else:\n        self.auto_create_tables = auto_create_tables\n\n    if hash_algorithm is None:\n        hash_algorithm = self._get_default_hash_algorithm()\n    self.hash_algorithm = hash_algorithm\n\n    from metaxy.metadata_store.fallback import FallbackStoreList\n\n    self.fallback_stores: FallbackStoreList = (\n        fallback_stores\n        if isinstance(fallback_stores, FallbackStoreList)\n        else FallbackStoreList(fallback_stores or [])\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.materialization_id","title":"metaxy.MetadataStore.materialization_id  <code>property</code>","text":"<pre><code>materialization_id: str | None\n</code></pre> <p>The external orchestration ID for this store instance.</p> <p>If set, all metadata writes include this ID in the <code>metaxy_materialization_id</code> column, allowing filtering of rows written during a specific materialization run.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.name","title":"metaxy.MetadataStore.name  <code>property</code>","text":"<pre><code>name: str | None\n</code></pre> <p>The configured name of this store, if any.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.qualified_class_name","title":"metaxy.MetadataStore.qualified_class_name  <code>property</code>","text":"<pre><code>qualified_class_name: str\n</code></pre> <p>The fully qualified class name (module.classname).</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.resolve_update","title":"metaxy.MetadataStore.resolve_update","text":"<pre><code>resolve_update(\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]]\n    | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    target_filters: Sequence[Expr] | None = None,\n    lazy: Literal[False] = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"]\n    | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; Increment\n</code></pre><pre><code>resolve_update(\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]]\n    | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    target_filters: Sequence[Expr] | None = None,\n    lazy: Literal[True],\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"]\n    | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>IntoFrame | Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>A dataframe with joined upstream metadata and <code>\"metaxy_provenance_by_field\"</code> column set. When provided, <code>MetadataStore</code> skips loading upstream feature metadata and provenance calculations.</p> <p>Required for root features</p> <p>Metaxy doesn't know how to populate input metadata for root features, so <code>samples</code> argument for must be provided for them.</p> <p>Tip</p> <p>For non-root features, use <code>samples</code> to customize the automatic upstream loading and field provenance calculation. For example, it can be used to requires processing for specific sample IDs.</p> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping from feature keys to lists of Narwhals filter expressions. Keys can be feature classes, FeatureKey objects, or string paths. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: <code>{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}</code></p> </li> <li> <code>global_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of Narwhals filter expressions applied to all features (both upstream and target). These filters are combined with any feature-specific filters from <code>filters</code>. Must reference columns that exist in ALL features. Useful for filtering by common columns like <code>sample_uid</code> across all features. Example: <code>[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]</code></p> </li> <li> <code>target_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of Narwhals filter expressions applied ONLY to the target feature (not to upstream features). Use this when filtering by columns that only exist in the target feature. Example: <code>[nw.col(\"height\").is_null()]</code></p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a <code>LazyIncrement</code> or a <code>Increment</code>.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>skip_comparison</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, skip the increment comparison logic and return all upstream samples in <code>increment.new</code>. The <code>changed</code> and <code>removed</code> frames will be empty.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> dataframe has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If <code>versioning_engine</code> has been set to <code>\"native\"</code> and a dataframe of a different implementation has been encountered during <code>resolve_update</code>.</p> </li> </ul> <p>Note</p> <p>This method automatically loads feature definitions from the metadata store before computing the update. This ensures that any external feature dependencies are resolved with their actual definitions from the store, preventing incorrect version calculations from stale external feature definitions.</p> <p>With a root feature</p> <pre><code>import narwhals as nw\nimport polars as pl\n\nsamples = pl.DataFrame(\n    {\n        \"id\": [\"x\", \"y\", \"z\"],\n        \"metaxy_provenance_by_field\": [\n            {\"part_1\": \"h1\", \"part_2\": \"h2\"},\n            {\"part_1\": \"h3\", \"part_2\": \"h4\"},\n            {\"part_1\": \"h5\", \"part_2\": \"h6\"},\n        ],\n    }\n)\nwith store.open(mode=\"w\"):\n    result = store.resolve_update(MyFeature, samples=nw.from_native(samples))\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[nw.Expr]] | None = None,\n    global_filters: Sequence[nw.Expr] | None = None,\n    target_filters: Sequence[nw.Expr] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: A dataframe with joined upstream metadata and `\"metaxy_provenance_by_field\"` column set.\n            When provided, `MetadataStore` skips loading upstream feature metadata and provenance calculations.\n\n            !!! info \"Required for root features\"\n                Metaxy doesn't know how to populate input metadata for root features,\n                so `samples` argument for **must** be provided for them.\n\n            !!! tip\n                For non-root features, use `samples` to customize the automatic upstream loading and field provenance calculation.\n                For example, it can be used to requires processing for specific sample IDs.\n\n            Setting this parameter during normal operations is not required.\n\n        filters: A mapping from feature keys to lists of Narwhals filter expressions.\n            Keys can be feature classes, FeatureKey objects, or string paths.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: `{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}`\n        global_filters: A list of Narwhals filter expressions applied to all features\n            (both upstream and target). These filters are combined with any feature-specific\n            filters from `filters`. Must reference columns that exist in ALL features.\n            Useful for filtering by common columns like `sample_uid` across all features.\n            Example: `[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]`\n        target_filters: A list of Narwhals filter expressions applied ONLY to the target\n            feature (not to upstream features). Use this when filtering by columns that\n            only exist in the target feature.\n            Example: `[nw.col(\"height\").is_null()]`\n        lazy: Whether to return a [`LazyIncrement`][metaxy.versioning.types.LazyIncrement] or a [`Increment`][metaxy.versioning.types.Increment].\n        versioning_engine: Override the store's versioning engine for this operation.\n        skip_comparison: If True, skip the increment comparison logic and return all\n            upstream samples in `increment.new`. The `changed` and `removed` frames will\n            be empty.\n\n    Raises:\n        ValueError: If no `samples` dataframe has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If `versioning_engine` has been set to `\"native\"`\n            and a dataframe of a different implementation has been encountered during `resolve_update`.\n\n    !!! note\n        This method automatically loads feature definitions from the metadata store\n        before computing the update. This ensures that any external feature dependencies\n        are resolved with their actual definitions from the store, preventing incorrect\n        version calculations from stale external feature definitions.\n\n    !!! example \"With a root feature\"\n\n        ```py\n        import narwhals as nw\n        import polars as pl\n\n        samples = pl.DataFrame(\n            {\n                \"id\": [\"x\", \"y\", \"z\"],\n                \"metaxy_provenance_by_field\": [\n                    {\"part_1\": \"h1\", \"part_2\": \"h2\"},\n                    {\"part_1\": \"h3\", \"part_2\": \"h4\"},\n                    {\"part_1\": \"h5\", \"part_2\": \"h6\"},\n                ],\n            }\n        )\n        with store.open(mode=\"w\"):\n            result = store.resolve_update(MyFeature, samples=nw.from_native(samples))\n        ```\n    \"\"\"\n    import narwhals as nw\n\n    import metaxy as mx\n    from metaxy.config import MetaxyConfig\n\n    # Sync external feature definitions from the store to replace any external feature placeholders.\n    # This ensures version hashes are computed correctly against actual stored definitions.\n    # it is acceptable to call this here automatically for three reasons:\n    # 1. `resolve_update` is typically only called once at the start of the workflow\n    # 2. `resolve_update` is already doing heavy computations so an extra little call won't hurt performance\n    # 3. it is extremely important to get the result right\n    if MetaxyConfig.get(_allow_default_config=True).sync:\n        mx.sync_external_features(self)\n\n    # Convert samples to Narwhals frame if not already\n    samples_nw: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None\n    if samples is not None:\n        if isinstance(samples, (nw.DataFrame, nw.LazyFrame)):\n            samples_nw = samples\n        else:\n            samples_nw = nw.from_native(samples)  # ty: ignore[invalid-assignment]\n\n    # Normalize filter keys to FeatureKey\n    normalized_filters: dict[FeatureKey, list[nw.Expr]] = {}\n    if filters:\n        for key, exprs in filters.items():\n            feature_key = self._resolve_feature_key(key)\n            normalized_filters[feature_key] = list(exprs)\n\n    # Convert global_filters and target_filters to lists for easy concatenation\n    global_filter_list = list(global_filters) if global_filters else []\n    target_filter_list = list(target_filters) if target_filters else []\n\n    feature_key = self._resolve_feature_key(feature)\n    if self._is_system_table(feature_key):\n        raise NotImplementedError(\"Delete operations are not yet supported for system tables.\")\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature_key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples_nw is None:\n        raise ValueError(\n            f\"Feature {feature_key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    # Combine feature-specific filters, global filters, and target filters for current feature\n    # target_filters are ONLY applied to the current feature, not to upstream features\n    current_feature_filters = [\n        *normalized_filters.get(feature_key, []),\n        *global_filter_list,\n        *target_filter_list,\n    ]\n\n    # Read current metadata with deduplication (with_sample_history=False by default)\n    # Use allow_fallback=False since we only want metadata from THIS store\n    # to determine what needs to be updated locally\n    try:\n        current_metadata: nw.LazyFrame[Any] | None = self.read(\n            feature_key,\n            filters=current_feature_filters if current_feature_filters else None,\n            allow_fallback=False,\n            with_feature_history=False,  # filters by current feature_version\n            with_sample_history=False,  # deduplicates by id_columns, keeping latest\n        )\n    except FeatureNotFoundError:\n        current_metadata = None\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples_nw is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples_nw\n        if current_feature_filters:\n            filtered_samples = samples_nw.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples_nw = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n\n        # For root features, add data_version columns if they don't exist\n        # (root features have no computation, so data_version equals provenance)\n        # Use collect_schema().names() to avoid PerformanceWarning on lazy frames\n        if METAXY_DATA_VERSION_BY_FIELD not in samples_nw.collect_schema().names():\n            samples_nw = samples_nw.with_columns(\n                nw.col(METAXY_PROVENANCE_BY_FIELD).alias(METAXY_DATA_VERSION_BY_FIELD),\n                nw.col(METAXY_PROVENANCE).alias(METAXY_DATA_VERSION),\n            )\n    else:\n        for upstream_spec in plan.deps or []:\n            # Combine feature-specific filters with global filters for upstream\n            upstream_filters = [\n                *normalized_filters.get(upstream_spec.key, []),\n                *global_filter_list,\n            ]\n            upstream_feature_metadata = self.read(\n                upstream_spec.key,\n                filters=upstream_filters if upstream_filters else None,\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = versioning_engine if versioning_engine is not None else self._versioning_engine\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (engine_mode == \"native\" and self.fallback_stores):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if samples_nw is not None and samples_nw.implementation != self.native_implementation():\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples_nw.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples_nw.implementation,\n                        message=f\"Provided `samples` have implementation {samples_nw.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples_nw:\n            samples_nw = switch_implementation_to_polars(samples_nw)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(plan=plan, implementation=implementation) as engine:\n        if skip_comparison:\n            # Skip comparison: return all upstream samples as added\n            if samples_nw is not None:\n                # Root features or user-provided samples: use samples directly\n                # Note: samples already has metaxy_provenance computed\n                added = samples_nw.lazy()\n                input_df = None  # Root features have no upstream input\n            else:\n                # Non-root features: load all upstream with provenance\n                added = engine.load_upstream_with_provenance(\n                    upstream=upstream_by_key,\n                    hash_algo=self.hash_algorithm,\n                    filters=filters_by_key,\n                )\n                input_df = added  # Input is the same as added when skipping comparison\n            changed = None\n            removed = None\n        else:\n            added, changed, removed, input_df = engine.resolve_increment_with_provenance(\n                current=current_metadata,\n                upstream=upstream_by_key,\n                hash_algorithm=self.hash_algorithm,\n                filters=filters_by_key,\n                sample=samples_nw.lazy() if samples_nw is not None else None,\n            )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            new=added if isinstance(added, nw.LazyFrame) else nw.from_native(added),\n            stale=changed if isinstance(changed, nw.LazyFrame) else nw.from_native(changed),\n            orphaned=removed if isinstance(removed, nw.LazyFrame) else nw.from_native(removed),\n            input=input_df if input_df is None or isinstance(input_df, nw.LazyFrame) else nw.from_native(input_df),\n        )\n    else:\n        return Increment(\n            new=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            stale=changed.collect() if isinstance(changed, nw.LazyFrame) else changed,\n            orphaned=removed.collect() if isinstance(removed, nw.LazyFrame) else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.compute_provenance","title":"metaxy.MetadataStore.compute_provenance","text":"<pre><code>compute_provenance(\n    feature: CoercibleToFeatureKey, df: FrameT\n) -&gt; FrameT\n</code></pre> <p>Compute provenance columns for a DataFrame with pre-joined upstream data.</p> <p>Note</p> <p>This method may be useful in very rare cases. Rely on <code>MetadataStore.resolve_update</code> instead.</p> <p>Use this method when you perform custom joins outside of Metaxy's auto-join system but still want Metaxy to compute provenance. The method computes metaxy_provenance_by_field, metaxy_provenance, metaxy_data_version_by_field, and metaxy_data_version columns based on the upstream metadata.</p> <p>Info</p> <p>The input DataFrame must contain the renamed metaxy_data_version_by_field columns from each upstream feature. The naming convention follows the pattern <code>metaxy_data_version_by_field__&lt;feature_key.to_column_suffix()&gt;</code>. For example, for an upstream feature with key <code>[\"video\", \"raw\"]</code>, the column should be named <code>metaxy_data_version_by_field__video_raw</code>.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The feature to compute provenance for.</p> </li> <li> <code>df</code>               (<code>FrameT</code>)           \u2013            <p>A DataFrame containing pre-joined upstream data with renamed metaxy_data_version_by_field columns from each upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FrameT</code>           \u2013            <p>The input DataFrame with provenance columns added. Returns the same</p> </li> <li> <code>FrameT</code>           \u2013            <p>frame type as the input, either an eager DataFrame or a LazyFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If the store is not open.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If required upstream <code>metaxy_data_version_by_field</code> columns are missing from the DataFrame.</p> </li> </ul> Example <pre><code>    # Read upstream metadata\n    video_df = store.read(VideoFeature).collect()\n    audio_df = store.read(AudioFeature).collect()\n\n    # Rename data_version_by_field columns to the expected convention\n    video_df = video_df.rename({\n        \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__video_raw\"\n    })\n    audio_df = audio_df.rename({\n        \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__audio_raw\"\n    })\n\n    # Perform custom join\n    joined = video_df.join(audio_df, on=\"sample_uid\", how=\"inner\")\n\n    # Compute provenance\n    with_provenance = store.compute_provenance(MyFeature, joined)\n\n    # Pass to resolve_update\n    increment = store.resolve_update(MyFeature, samples=with_provenance)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def compute_provenance(\n    self,\n    feature: CoercibleToFeatureKey,\n    df: FrameT,\n) -&gt; FrameT:\n    \"\"\"Compute provenance columns for a DataFrame with pre-joined upstream data.\n\n    !!! note\n        This method may be useful in very rare cases.\n        Rely on [`MetadataStore.resolve_update`][metaxy.metadata_store.base.MetadataStore.resolve_update] instead.\n\n    Use this method when you perform custom joins outside of Metaxy's auto-join\n    system but still want Metaxy to compute provenance. The method computes\n    metaxy_provenance_by_field, metaxy_provenance, metaxy_data_version_by_field,\n    and metaxy_data_version columns based on the upstream metadata.\n\n    !!! info\n        The input DataFrame must contain the renamed metaxy_data_version_by_field\n        columns from each upstream feature. The naming convention follows the pattern\n        `metaxy_data_version_by_field__&lt;feature_key.to_column_suffix()&gt;`. For example, for an\n        upstream feature with key `[\"video\", \"raw\"]`, the column should be named\n        `metaxy_data_version_by_field__video_raw`.\n\n    Args:\n        feature: The feature to compute provenance for.\n        df: A DataFrame containing pre-joined upstream data with renamed\n            metaxy_data_version_by_field columns from each upstream feature.\n\n    Returns:\n        The input DataFrame with provenance columns added. Returns the same\n        frame type as the input, either an eager DataFrame or a LazyFrame.\n\n    Raises:\n        StoreNotOpenError: If the store is not open.\n        ValueError: If required upstream `metaxy_data_version_by_field` columns\n            are missing from the DataFrame.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n\n            # Read upstream metadata\n            video_df = store.read(VideoFeature).collect()\n            audio_df = store.read(AudioFeature).collect()\n\n            # Rename data_version_by_field columns to the expected convention\n            video_df = video_df.rename({\n                \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__video_raw\"\n            })\n            audio_df = audio_df.rename({\n                \"metaxy_data_version_by_field\": \"metaxy_data_version_by_field__audio_raw\"\n            })\n\n            # Perform custom join\n            joined = video_df.join(audio_df, on=\"sample_uid\", how=\"inner\")\n\n            # Compute provenance\n            with_provenance = store.compute_provenance(MyFeature, joined)\n\n            # Pass to resolve_update\n            increment = store.resolve_update(MyFeature, samples=with_provenance)\n        ```\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature_key)\n\n    # Use native implementation if DataFrame matches, otherwise fall back to Polars\n    implementation = self.native_implementation()\n    if df.implementation != implementation:\n        implementation = nw.Implementation.POLARS\n        df = switch_implementation_to_polars(df)  # ty: ignore[no-matching-overload]\n\n    with self.create_versioning_engine(plan=plan, implementation=implementation) as engine:\n        # Validate required upstream columns exist\n        expected_columns = {\n            dep.feature: engine.get_renamed_data_version_by_field_col(dep.feature)\n            for dep in (plan.feature_deps or [])\n        }\n\n        df_columns = set(df.collect_schema().names())  # ty: ignore[invalid-argument-type]\n        missing_columns = [\n            f\"{col} (from upstream feature {key.to_string()})\"\n            for key, col in expected_columns.items()\n            if col not in df_columns\n        ]\n\n        if missing_columns:\n            raise ValueError(\n                f\"DataFrame is missing required upstream columns for computing \"\n                f\"provenance of feature {feature_key.to_string()}. \"\n                f\"Missing columns: {missing_columns}. \"\n                f\"Make sure to rename metaxy_data_version_by_field columns from \"\n                f\"each upstream feature using the pattern \"\n                f\"metaxy_data_version_by_field__&lt;feature_key.table_name&gt;.\"\n            )\n\n        return engine.compute_provenance_columns(df, hash_algo=self.hash_algorithm)  # ty: ignore[invalid-argument-type]\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.read","title":"metaxy.MetadataStore.read","text":"<pre><code>read(\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: Literal[False] = False,\n) -&gt; LazyFrame[Any]\n</code></pre><pre><code>read(\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: Literal[True],\n) -&gt; tuple[LazyFrame[Any], MetadataStore]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>By default, does not include:    - rows from historical feature versions (configured via <code>with_feature_history=False</code>)    - rows that have been overwritten by subsequent writes with the same feature version (configured via <code>with_sample_history=False</code>)    - soft-deleted with <code>metaxy_deleted_at</code> set to a non-null value (configured via <code>include_soft_deleted=False</code>)</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with with_feature_history=False)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to allow fallback to upstream stores if the requested feature is not found in the current store.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include rows from all historical feature versions. By default (False), only returns rows with the currently registered feature version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations per sample. By default (False), deduplicates samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> <li> <code>include_soft_deleted</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, include soft-deleted rows in the result. Previous historical materializations of the same feature version will be effectively removed from the output otherwise.</p> </li> <li> <code>with_store_info</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If <code>True</code>, return a tuple of (LazyFrame, MetadataStore) where the MetadataStore is the store that actually contained the feature (which may be a fallback store if allow_fallback=True).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | tuple[LazyFrame[Any], MetadataStore]</code>           \u2013            <p>Narwhals <code>LazyFrame</code> with metadata, or, if <code>with_store_info=True</code>, a tuple of (<code>LazyFrame</code>, <code>MetadataStore</code>) containing the metadata store which has been actually used to retrieve the feature (may be a fallback store).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existent Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and with_feature_history=False are provided</p> </li> </ul> <p>Info</p> <p>When this method is called with default arguments, it will return the latest (by <code>metaxy_created_at</code>) metadata for the current feature version excluding soft-deleted rows. Therefore, it's perfectly suitable for most use cases.</p> <p>Warning</p> <p>The order of rows is not guaranteed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n    include_soft_deleted: bool = False,\n    with_store_info: bool = False,\n) -&gt; nw.LazyFrame[Any] | tuple[nw.LazyFrame[Any], MetadataStore]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    By default, does not include:\n       - rows from historical feature versions (configured via `with_feature_history=False`)\n       - rows that have been overwritten by subsequent writes with the same feature version (configured via `with_sample_history=False`)\n       - soft-deleted with `metaxy_deleted_at` set to a non-null value (configured via `include_soft_deleted=False`)\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with with_feature_history=False)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: Whether to allow fallback to upstream stores if the requested feature is not found in the current store.\n        with_feature_history: If True, include rows from all historical feature versions.\n            By default (False), only returns rows with the currently registered feature version.\n        with_sample_history: If True, include all historical materializations per sample.\n            By default (False), deduplicates samples within `id_columns` groups ordered by `metaxy_created_at`.\n        include_soft_deleted: If `True`, include soft-deleted rows in the result. Previous historical materializations of the same feature version will be effectively removed from the output otherwise.\n        with_store_info: If `True`, return a tuple of (LazyFrame, MetadataStore) where\n            the MetadataStore is the store that actually contained the feature (which\n            may be a fallback store if allow_fallback=True).\n\n    Returns:\n        Narwhals `LazyFrame` with metadata, or, if `with_store_info=True`, a tuple of (`LazyFrame`, `MetadataStore`) containing the metadata store which has been actually used to retrieve the feature (may be a fallback store).\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existent Metaxy system data\n        ValueError: If both feature_version and with_feature_history=False are provided\n\n    !!! info\n        When this method is called with default arguments, it will return the latest (by `metaxy_created_at`)\n        metadata for the current feature version excluding soft-deleted rows. Therefore, it's perfectly suitable\n        for most use cases.\n\n    !!! warning\n        The order of rows is not guaranteed.\n    \"\"\"\n    import metaxy as mx\n    from metaxy.config import MetaxyConfig\n\n    self._check_open()\n\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Sync external features if auto-sync is enabled (default)\n    # This call is a no-op most of the time and is very lightweight when it's not\n    # Skip for system tables to avoid infinite recursion (sync_external_features reads system tables)\n    if not is_system_table and MetaxyConfig.get(_allow_default_config=True).sync:\n        mx.sync_external_features(self)\n\n    # If caller wants soft-deleted records, do not filter them out later\n    filter_deleted = not include_soft_deleted and not is_system_table\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and not with_feature_history:\n        raise ValueError(\n            \"Cannot specify both feature_version and with_feature_history=False. \"\n            \"Use with_feature_history=True with feature_version parameter.\"\n        )\n\n    # Separate system filters (applied before dedup) from user filters (applied after dedup)\n    # System filters like feature_version need to be applied early to reduce data volume\n    # User filters should see the deduplicated view of the data\n    system_filters: list[nw.Expr] = []\n    user_filters = list(filters) if filters else []\n\n    # Add feature_version filter only when needed (this is a system filter)\n    if not with_feature_history or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key) if not with_feature_history else feature_version\n        )\n        system_filters.append(version_filter)\n\n    # If user filters are provided, we need to read all columns since filters may\n    # reference columns not in the requested columns list. Column selection happens\n    # after filtering\n    if user_filters:\n        read_columns = None\n    elif columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [c for c in ALL_SYSTEM_COLUMNS if c not in columns_set]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        # Only pass system filters to _read_feature\n        # User filters will be applied after deduplication\n        lazy_frame = self._read_feature(\n            feature, filters=system_filters if system_filters else None, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self}. Invoke `metaxy push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where _read_feature returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self}. Invoke `metaxy push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table:\n        # Deduplicate first, then filter soft-deleted rows\n        if not with_sample_history:\n            id_cols = list(self._resolve_feature_plan(feature_key).feature.id_columns)\n            # Treat soft-deletes like hard deletes by ordering on the\n            # most recent lifecycle timestamp.\n            lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n                df=lazy_frame,\n                group_columns=id_cols,\n                timestamp_columns=[METAXY_DELETED_AT, METAXY_UPDATED_AT],\n            )\n\n        if filter_deleted:\n            lazy_frame = lazy_frame.filter(nw.col(METAXY_DELETED_AT).is_null())\n\n        # Apply user filters AFTER deduplication so they see the latest version of each row\n        for user_filter in user_filters:\n            lazy_frame = lazy_frame.filter(user_filter)\n\n    # For system tables, apply user filters directly (no dedup needed)\n    if lazy_frame is not None and is_system_table:\n        for user_filter in user_filters:\n            lazy_frame = lazy_frame.filter(user_filter)\n\n    if lazy_frame is not None:\n        # After dedup and user filters, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        if with_store_info:\n            return lazy_frame, self\n        return lazy_frame\n\n    # Try fallback stores (opened on demand)\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Open fallback store on demand for reading\n                with store:\n                    # Use full read to handle nested fallback chains\n                    if with_store_info:\n                        return store.read(\n                            feature,\n                            feature_version=feature_version,\n                            filters=filters,\n                            columns=columns,\n                            allow_fallback=True,\n                            with_feature_history=with_feature_history,\n                            with_sample_history=with_sample_history,\n                            include_soft_deleted=include_soft_deleted,\n                            with_store_info=True,\n                        )\n                    return store.read(\n                        feature,\n                        feature_version=feature_version,\n                        filters=filters,\n                        columns=columns,\n                        allow_fallback=True,\n                        with_feature_history=with_feature_history,\n                        with_sample_history=with_sample_history,\n                        include_soft_deleted=include_soft_deleted,\n                    )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\" + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.write","title":"metaxy.MetadataStore.write","text":"<pre><code>write(\n    feature: CoercibleToFeatureKey,\n    df: IntoFrame,\n    materialization_id: str | None = None,\n) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for this write. Overrides the store's default <code>materialization_id</code> if provided. Useful for tracking which orchestration run produced this metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> <p>Note:     - Must be called within a <code>MetadataStore.open(mode=\"w\")</code> context manager.</p> <pre><code>- Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.\n\n- Fallback stores are never used for writes.\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write(\n    self,\n    feature: CoercibleToFeatureKey,\n    df: IntoFrame,\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n        materialization_id: Optional external orchestration ID for this write.\n            Overrides the store's default `materialization_id` if provided.\n            Useful for tracking which orchestration run produced this metadata.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"w\")` context manager.\n\n        - Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.\n\n        - Fallback stores are never used for writes.\n\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, (nw.DataFrame, nw.LazyFrame)), f\"df must be a Narwhals DataFrame, got {type(df_nw)}\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self._write_feature(feature_key, df_nw)\n        return\n\n    # Use collect_schema().names() to avoid PerformanceWarning on lazy frames\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.collect_schema().names():\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\")\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadataStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(df_nw, feature, materialization_id=materialization_id)\n\n    self._validate_schema(df_nw)\n    self._write_feature(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.write_multi","title":"metaxy.MetadataStore.write_multi","text":"<pre><code>write_multi(\n    metadata: Mapping[Any, IntoFrame],\n    materialization_id: str | None = None,\n) -&gt; None\n</code></pre> <p>Write metadata for multiple features in reverse topological order.</p> <p>Processes features so that dependents are written before their dependencies. This ordering ensures that downstream features are written first, which can be useful for certain data consistency requirements or when features need to be processed in a specific order.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Mapping[Any, IntoFrame]</code>)           \u2013            <p>Mapping from feature keys to metadata DataFrames. Keys can be any type coercible to FeatureKey (string, sequence, FeatureKey, or BaseFeature class). Values must be DataFrames compatible with Narwhals, containing required system columns.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for all writes. Overrides the store's default <code>materialization_id</code> if provided. Applied to all feature writes in this batch.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If any DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Must be called within a <code>MetadataStore.open(mode=\"w\")</code> context manager.</li> <li>Empty mappings are handled gracefully (no-op).</li> <li>Each feature's metadata is written via <code>write</code>, so all   validation and system column handling from that method applies.</li> </ul> Example <pre><code>with store.open(mode=\"w\"):\n    store.write_multi(\n        {\n            ChildFeature: child_df,\n            ParentFeature: parent_df,\n        }\n    )\n# Features are written in reverse topological order:\n# ChildFeature first, then ParentFeature\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_multi(\n    self,\n    metadata: Mapping[Any, IntoFrame],\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for multiple features in reverse topological order.\n\n    Processes features so that dependents are written before their dependencies.\n    This ordering ensures that downstream features are written first, which can\n    be useful for certain data consistency requirements or when features need\n    to be processed in a specific order.\n\n    Args:\n        metadata: Mapping from feature keys to metadata DataFrames.\n            Keys can be any type coercible to FeatureKey (string, sequence,\n            FeatureKey, or BaseFeature class). Values must be DataFrames\n            compatible with Narwhals, containing required system columns.\n        materialization_id: Optional external orchestration ID for all writes.\n            Overrides the store's default `materialization_id` if provided.\n            Applied to all feature writes in this batch.\n\n    Raises:\n        MetadataSchemaError: If any DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"w\")` context manager.\n        - Empty mappings are handled gracefully (no-op).\n        - Each feature's metadata is written via `write`, so all\n          validation and system column handling from that method applies.\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```py\n        with store.open(mode=\"w\"):\n            store.write_multi(\n                {\n                    ChildFeature: child_df,\n                    ParentFeature: parent_df,\n                }\n            )\n        # Features are written in reverse topological order:\n        # ChildFeature first, then ParentFeature\n        ```\n    \"\"\"\n    if not metadata:\n        return\n\n    # Build mapping from resolved keys to dataframes in one pass\n    resolved_metadata = {self._resolve_feature_key(key): df for key, df in metadata.items()}\n\n    # Get reverse topological order (dependents first)\n    graph = current_graph()\n    sorted_keys = graph.topological_sort_features(list(resolved_metadata.keys()), descending=True)\n\n    # Write metadata in reverse topological order\n    for feature_key in sorted_keys:\n        self.write(\n            feature_key,\n            resolved_metadata[feature_key],\n            materialization_id=materialization_id,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.config_model","title":"metaxy.MetadataStore.config_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[MetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef config_model(cls) -&gt; type[MetadataStoreConfig]:\n    \"\"\"Return the configuration model class for this store type.\n\n    Subclasses must override this to return their specific config class.\n\n    Returns:\n        The config class type (e.g., DuckDBMetadataStoreConfig)\n\n    Note:\n        Subclasses override this with a more specific return type.\n        Type checkers may show a warning about incompatible override,\n        but this is intentional - each store returns its own config type.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.from_config","title":"metaxy.MetadataStore.from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: MetadataStoreConfig, **kwargs: Any\n) -&gt; Self\n</code></pre> <p>Create a store instance from a configuration object.</p> <p>This method creates a store by: 1. Converting the config to a dict 2. Resolving fallback store names to actual store instances 3. Calling the store's init with the config parameters</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetadataStoreConfig</code>)           \u2013            <p>Configuration object (should be the type returned by config_model())</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed directly to the store constructor (e.g., materialization_id for runtime parameters not in config)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>A new store instance configured according to the config object</p> </li> </ul> Example <pre><code>from metaxy.ext.metadata_stores.duckdb import (\n    DuckDBMetadataStore,\n    DuckDBMetadataStoreConfig,\n)\n\nconfig = DuckDBMetadataStoreConfig(\n    database=\"metadata.db\",\n    fallback_stores=[\"prod\"],\n)\n\nstore = DuckDBMetadataStore.from_config(config)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MetadataStoreConfig, **kwargs: Any) -&gt; Self:\n    \"\"\"Create a store instance from a configuration object.\n\n    This method creates a store by:\n    1. Converting the config to a dict\n    2. Resolving fallback store names to actual store instances\n    3. Calling the store's __init__ with the config parameters\n\n    Args:\n        config: Configuration object (should be the type returned by config_model())\n        **kwargs: Additional arguments passed directly to the store constructor\n            (e.g., materialization_id for runtime parameters not in config)\n\n    Returns:\n        A new store instance configured according to the config object\n\n    Example:\n        &lt;!-- skip next --&gt;\n        ```python\n        from metaxy.ext.metadata_stores.duckdb import (\n            DuckDBMetadataStore,\n            DuckDBMetadataStoreConfig,\n        )\n\n        config = DuckDBMetadataStoreConfig(\n            database=\"metadata.db\",\n            fallback_stores=[\"prod\"],\n        )\n\n        store = DuckDBMetadataStore.from_config(config)\n        ```\n    \"\"\"\n    from metaxy.metadata_store.fallback import FallbackStoreList\n\n    config_dict = config.model_dump(exclude_unset=True)\n    fallback_store_names = config_dict.pop(\"fallback_stores\", [])\n    store = cls(**config_dict, **kwargs)\n    if fallback_store_names:\n        store.fallback_stores = FallbackStoreList(\n            fallback_store_names,\n            config=MetaxyConfig.get(),\n            parent_hash_algorithm=store.hash_algorithm,\n        )\n    return store\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.native_implementation","title":"metaxy.MetadataStore.native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.create_versioning_engine","title":"metaxy.MetadataStore.create_versioning_engine","text":"<pre><code>create_versioning_engine(\n    plan: FeaturePlan, implementation: Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.open","title":"metaxy.MetadataStore.open  <code>abstractmethod</code>","text":"<pre><code>open(mode: AccessMode = 'r') -&gt; Iterator[Self]\n</code></pre> <p>Open/initialize the store for operations.</p> <p>Context manager that opens the store with specified access mode. Called internally by <code>__enter__</code>. Child classes should implement backend-specific connection setup/teardown here.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'r'</code> )           \u2013            <p>Access mode for this connection session.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Note <p>Users should prefer using <code>with store:</code> pattern except when write access mode is needed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef open(self, mode: AccessMode = \"r\") -&gt; Iterator[Self]:\n    \"\"\"Open/initialize the store for operations.\n\n    Context manager that opens the store with specified access mode.\n    Called internally by `__enter__`.\n    Child classes should implement backend-specific connection setup/teardown here.\n\n    Args:\n        mode: Access mode for this connection session.\n\n    Yields:\n        Self: The store instance with connection open\n\n    Note:\n        Users should prefer using `with store:` pattern except when write access mode is needed.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.__enter__","title":"metaxy.MetadataStore.__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = \"w\" if self.auto_create_tables else \"r\"\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)  # ty: ignore[invalid-assignment]\n    self._open_cm.__enter__()  # ty: ignore[possibly-missing-attribute]\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.validate_hash_algorithm","title":"metaxy.MetadataStore.validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(\n    check_fallback_stores: bool = True,\n) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.drop_feature_metadata","title":"metaxy.MetadataStore.drop_feature_metadata","text":"<pre><code>drop_feature_metadata(\n    feature: CoercibleToFeatureKey,\n) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>with store_with_data.open(mode=\"w\"):\n    assert store_with_data.has_feature(MyFeature)\n    store_with_data.drop_feature_metadata(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        with store_with_data.open(mode=\"w\"):\n            assert store_with_data.has_feature(MyFeature)\n            store_with_data.drop_feature_metadata(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    if self._is_system_table(feature_key):\n        raise NotImplementedError(f\"{self.__class__.__name__} does not support deletes for system tables\")\n    self._drop_feature(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.delete","title":"metaxy.MetadataStore.delete","text":"<pre><code>delete(\n    feature: CoercibleToFeatureKey,\n    filters: Sequence[Expr] | Expr | None,\n    *,\n    soft: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n) -&gt; None\n</code></pre> <p>Delete records matching provided filters.</p> <p>Performs a soft delete by default. This is achieved by setting metaxy_deleted_at to the current timestamp. Subsequent [[MetadataStore.read]] calls would ignore these records by default.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to delete from.</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | Expr | None</code>)           \u2013            <p>One or more Narwhals expressions or a sequence of expressions that determine which records to delete. If <code>None</code>, deletes all records (subject to <code>with_feature_history</code> and <code>with_sample_history</code> constraints).</p> </li> <li> <code>soft</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to perform a soft delete.</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, delete across all historical feature versions. If False (default), only current version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations. If False (default), deduplicate to latest rows.</p> </li> </ul> <p>Critical</p> <p>By default, deletions target historical records. Even when <code>with_feature_history</code> is <code>False</code>, records with the same feature version but an older <code>metaxy_created_at</code> would be targeted as well. Consider adding additional conditions to <code>filters</code> if you want to avoid that.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def delete(\n    self,\n    feature: CoercibleToFeatureKey,\n    filters: Sequence[nw.Expr] | nw.Expr | None,\n    *,\n    soft: bool = True,\n    with_feature_history: bool = False,\n    with_sample_history: bool = False,\n) -&gt; None:\n    \"\"\"Delete records matching provided filters.\n\n    Performs a soft delete by default. This is achieved by setting metaxy_deleted_at to the current timestamp.\n    Subsequent [[MetadataStore.read]] calls would ignore these records by default.\n\n    Args:\n        feature: Feature to delete from.\n        filters: One or more Narwhals expressions or a sequence of expressions that determine which records to delete.\n            If `None`, deletes all records (subject to `with_feature_history` and `with_sample_history` constraints).\n        soft: Whether to perform a soft delete.\n        with_feature_history: If True, delete across all historical feature versions. If False (default), only current version.\n        with_sample_history: If True, include all historical materializations. If False (default), deduplicate to latest rows.\n\n    !!! critical\n        By default, deletions target historical records. Even when `with_feature_history` is `False`,\n        records with the same feature version but an older `metaxy_created_at` would be targeted as\n        well. Consider adding additional conditions to `filters` if you want to avoid that.\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n\n    # Normalize filters to list\n    if filters is None:\n        filter_list: list[nw.Expr] = []\n    elif isinstance(filters, nw.Expr):\n        filter_list = [filters]\n    else:\n        filter_list = list(filters)\n\n    if soft:\n        # Soft delete: mark records with deletion timestamp, preserving original updated_at\n        lazy = self.read(\n            feature_key,\n            filters=filter_list,\n            include_soft_deleted=False,\n            with_feature_history=with_feature_history,\n            with_sample_history=with_sample_history,\n            allow_fallback=True,\n        )\n        with self._shared_transaction_timestamp(soft_delete=True) as ts:\n            soft_deletion_marked = lazy.with_columns(\n                nw.lit(ts).alias(METAXY_DELETED_AT),\n            )\n            self.write(feature_key, soft_deletion_marked.to_native())\n    else:\n        # Hard delete: add version filter if needed, then delegate to backend\n        if not with_feature_history and not self._is_system_table(feature_key):\n            version_filter = nw.col(METAXY_FEATURE_VERSION) == current_graph().get_feature_version(feature_key)\n            filter_list = [version_filter, *filter_list]\n\n        self._delete_feature(feature_key, filter_list, with_feature_history=with_feature_history)\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.read_feature_schema_from_store","title":"metaxy.MetadataStore.read_feature_schema_from_store","text":"<pre><code>read_feature_schema_from_store(\n    feature: CoercibleToFeatureKey,\n) -&gt; Schema\n</code></pre> <p>Read the schema for a feature from the store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read schema for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Schema</code>           \u2013            <p>Narwhals schema for the feature</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in the store</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_feature_schema_from_store(\n    self,\n    feature: CoercibleToFeatureKey,\n) -&gt; nw.Schema:\n    \"\"\"Read the schema for a feature from the store.\n\n    Args:\n        feature: Feature to read schema for\n\n    Returns:\n        Narwhals schema for the feature\n\n    Raises:\n        FeatureNotFoundError: If feature not found in the store\n    \"\"\"\n    lazy = self.read(\n        feature,\n        allow_fallback=False,\n    )\n    return lazy.collect_schema()\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.has_feature","title":"metaxy.MetadataStore.has_feature","text":"<pre><code>has_feature(\n    feature: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = False,\n) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self._read_feature(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.display","title":"metaxy.MetadataStore.display  <code>abstractmethod</code>","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return a human-readable display string for this store.</p> <p>Used in warnings, logs, and CLI output to identify the store.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef display(self) -&gt; str:\n    \"\"\"Return a human-readable display string for this store.\n\n    Used in warnings, logs, and CLI output to identify the store.\n\n    Returns:\n        Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.__repr__","title":"metaxy.MetadataStore.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return the display string with optional name prefix.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the display string with optional name prefix.\"\"\"\n    return self._format_display_with_name(self.display())\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.find_store_for_feature","title":"metaxy.MetadataStore.find_store_for_feature","text":"<pre><code>find_store_for_feature(\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; MetadataStore | None\n</code></pre> <p>Find the store that contains the given feature.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to find</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores when the feature is not found in the current store</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore | None</code>           \u2013            <p>The store containing the feature, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def find_store_for_feature(\n    self,\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; MetadataStore | None:\n    \"\"\"Find the store that contains the given feature.\n\n    Args:\n        feature_key: Feature to find\n        check_fallback: Whether to check fallback stores when the feature\n            is not found in the current store\n\n    Returns:\n        The store containing the feature, or None if not found\n    \"\"\"\n    self._check_open()\n\n    # Check if feature exists in this store\n    if self.has_feature(feature_key):\n        return self\n\n    # Try fallback stores if enabled (opened on demand)\n    if check_fallback:\n        for store in self.fallback_stores:\n            with store:\n                found = store.find_store_for_feature(feature_key, check_fallback=True)\n                if found is not None:\n                    return found\n\n    return None\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.get_store_metadata","title":"metaxy.MetadataStore.get_store_metadata","text":"<pre><code>get_store_metadata(\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; dict[str, Any]\n</code></pre> <p>Arbitrary key-value pairs with useful metadata for logging purposes (like a path in storage).</p> <p>This method should not expose sensitive information.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to get metadata for</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to check fallback stores when the feature is not found in the current store</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary with store-specific metadata. Contains:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>name</code>, <code>display</code>: The queried store (self)</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>resolved_from</code>: Where the feature was actually found (may be a fallback store), includes <code>name</code>, <code>display</code>, and store-specific fields like <code>table_name</code> or <code>uri</code></li> </ul> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def get_store_metadata(\n    self,\n    feature_key: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"Arbitrary key-value pairs with useful metadata for logging purposes (like a path in storage).\n\n    This method should not expose sensitive information.\n\n    Args:\n        feature_key: Feature to get metadata for\n        check_fallback: Whether to check fallback stores when the feature\n            is not found in the current store\n\n    Returns:\n        Dictionary with store-specific metadata. Contains:\n        - `name`, `display`: The queried store (self)\n        - `resolved_from`: Where the feature was actually found (may be a fallback store),\n            includes `name`, `display`, and store-specific fields like `table_name` or `uri`\n    \"\"\"\n    resolved_store = self.find_store_for_feature(feature_key, check_fallback=check_fallback)\n    result: dict[str, Any] = {\n        \"name\": self.name,\n        \"display\": self.display(),\n    }\n    if resolved_store is not None:\n        result[\"resolved_from\"] = resolved_store.get_store_info(feature_key)\n    return result\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.get_store_info","title":"metaxy.MetadataStore.get_store_info","text":"<pre><code>get_store_info(\n    feature_key: CoercibleToFeatureKey,\n) -&gt; dict[str, Any]\n</code></pre> <p>Build a dictionary with store identification and feature-specific metadata.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to get metadata for</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary containing:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>name</code>: The store name</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>type</code>: The fully qualified class name (module.classname)</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>display</code>: Human-readable store description</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li>Store-specific metadata from <code>_get_store_metadata_impl</code> (e.g., <code>table_name</code>, <code>uri</code>)</li> </ul> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def get_store_info(self, feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]:\n    \"\"\"Build a dictionary with store identification and feature-specific metadata.\n\n    Args:\n        feature_key: Feature to get metadata for\n\n    Returns:\n        Dictionary containing:\n        - `name`: The store name\n        - `type`: The fully qualified class name (module.classname)\n        - `display`: Human-readable store description\n        - Store-specific metadata from `_get_store_metadata_impl` (e.g., `table_name`, `uri`)\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"type\": self.qualified_class_name,\n        \"display\": self.display(),\n        **self._get_store_metadata_impl(feature_key),\n    }\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.calculate_input_progress","title":"metaxy.MetadataStore.calculate_input_progress","text":"<pre><code>calculate_input_progress(\n    lazy_increment: LazyIncrement,\n    feature_key: CoercibleToFeatureKey,\n) -&gt; float | None\n</code></pre> <p>Calculate progress percentage from lazy increment.</p> <p>Uses the <code>input</code> field from LazyIncrement to count total input units and compares with <code>added</code> to determine how many are missing.</p> <p>Progress represents the percentage of input units that have been processed at least once. Stale samples (in <code>changed</code>) are counted as processed since they have existing metadata, even though they may need re-processing due to upstream changes.</p> <p>Parameters:</p> <ul> <li> <code>lazy_increment</code>               (<code>LazyIncrement</code>)           \u2013            <p>The lazy increment containing input and added dataframes.</p> </li> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The feature key to look up lineage information.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float | None</code>           \u2013            <p>Progress percentage (0-100), or None if input is not available.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def calculate_input_progress(\n    self,\n    lazy_increment: LazyIncrement,\n    feature_key: CoercibleToFeatureKey,\n) -&gt; float | None:\n    \"\"\"Calculate progress percentage from lazy increment.\n\n    Uses the `input` field from LazyIncrement to count total input units\n    and compares with `added` to determine how many are missing.\n\n    Progress represents the percentage of input units that have been processed\n    at least once. Stale samples (in `changed`) are counted as processed since\n    they have existing metadata, even though they may need re-processing due to\n    upstream changes.\n\n    Args:\n        lazy_increment: The lazy increment containing input and added dataframes.\n        feature_key: The feature key to look up lineage information.\n\n    Returns:\n        Progress percentage (0-100), or None if input is not available.\n    \"\"\"\n    if lazy_increment.input is None:\n        return None\n\n    key = self._resolve_feature_key(feature_key)\n    graph = current_graph()\n    plan = graph.get_feature_plan(key)\n\n    # Get the columns that define input units from the feature plan\n    input_id_columns = plan.input_id_columns\n\n    # Count distinct input units using two separate queries\n    # We can't use concat because input and added may have different schemas\n    # (e.g., nullable vs non-nullable columns)\n    total_units: int = lazy_increment.input.select(input_id_columns).unique().select(nw.len()).collect().item()\n\n    if total_units == 0:\n        return None  # No input available from upstream\n\n    missing_units: int = lazy_increment.new.select(input_id_columns).unique().select(nw.len()).collect().item()\n\n    processed_units = total_units - missing_units\n    return (processed_units / total_units) * 100\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.MetadataStore.copy_metadata","title":"metaxy.MetadataStore.copy_metadata","text":"<pre><code>copy_metadata(\n    from_store: MetadataStore,\n    features: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    filters: Mapping[str, Sequence[Expr]] | None = None,\n    global_filters: Sequence[Expr] | None = None,\n    with_feature_history: bool = True,\n    with_sample_history: bool = False,\n) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened for reading)</p> </li> <li> <code>features</code>               (<code>Sequence[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>Features to copy. Can be:</p> <ul> <li> <p><code>None</code>: copies all features from the active graph</p> </li> <li> <p>Sequence of specific features to copy</p> </li> </ul> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>global_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions applied to all features. These filters are combined with any feature-specific filters from <code>filters</code>. Example: [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]</p> </li> <li> <code>with_feature_history</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), include rows from all historical feature versions. If False, only copy rows with the current feature_version.</p> </li> <li> <code>with_sample_history</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, include all historical materializations per sample. If False (default), deduplicate within <code>id_columns</code> groups by keeping latest.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If source or destination store is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Copy all features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        features=[mx.FeatureKey(\"my_feature\")],\n    )\n</code></pre> <pre><code># Copy with global filters applied to all features\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        global_filters=[nw.col(\"id\").is_in([\"a\", \"b\"])],\n    )\n</code></pre> <pre><code># Copy specific features with per-feature filters\nwith source_store, dest_store.open(\"w\"):\n    stats = dest_store.copy_metadata(\n        from_store=source_store,\n        features=[\n            mx.FeatureKey(\"feature_a\"),\n            mx.FeatureKey(\"feature_b\"),\n        ],\n        filters={\n            \"feature_a\": [nw.col(\"field_a\") &gt; 10],\n            \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n        },\n    )\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    global_filters: Sequence[nw.Expr] | None = None,\n    with_feature_history: bool = True,\n    with_sample_history: bool = False,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened for reading)\n        features: Features to copy. Can be:\n\n            - `None`: copies all features from the active graph\n\n            - Sequence of specific features to copy\n\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        global_filters: Sequence of Narwhals filter expressions applied to all features.\n            These filters are combined with any feature-specific filters from `filters`.\n            Example: [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]\n        with_feature_history: If True (default), include rows from all historical feature versions.\n            If False, only copy rows with the current feature_version.\n        with_sample_history: If True, include all historical materializations per sample.\n            If False (default), deduplicate within `id_columns` groups by keeping latest.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If source or destination store is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy all features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy specific features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                features=[mx.FeatureKey(\"my_feature\")],\n            )\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy with global filters applied to all features\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                global_filters=[nw.col(\"id\").is_in([\"a\", \"b\"])],\n            )\n        ```\n\n        &lt;!-- skip next --&gt;\n        ```py\n        # Copy specific features with per-feature filters\n        with source_store, dest_store.open(\"w\"):\n            stats = dest_store.copy_metadata(\n                from_store=source_store,\n                features=[\n                    mx.FeatureKey(\"feature_a\"),\n                    mx.FeatureKey(\"feature_b\"),\n                ],\n                filters={\n                    \"feature_a\": [nw.col(\"field_a\") &gt; 10],\n                    \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n                },\n            )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate both stores are open\n    if not self._is_open:\n        raise ValueError('Destination store must be opened with store.open(\"w\") before use')\n    if not from_store._is_open:\n        raise ValueError(\"Source store must be opened with store before use\")\n\n    return self._copy_metadata_impl(\n        from_store=from_store,\n        features=features,\n        filters=filters,\n        global_filters=global_filters,\n        with_feature_history=with_feature_history,\n        with_sample_history=with_sample_history,\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.types.AccessMode","title":"metaxy.metadata_store.types.AccessMode  <code>module-attribute</code>","text":"<pre><code>AccessMode = Literal['r', 'w']\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.VersioningEngineOptions","title":"metaxy.metadata_store.base.VersioningEngineOptions  <code>module-attribute</code>","text":"<pre><code>VersioningEngineOptions: TypeAlias = Literal[\n    \"auto\", \"native\", \"polars\"\n]\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#base-configuration-class","title":"Base Configuration Class","text":"<p>The following base configuration class is typically used by child metadata stores:</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig","title":"metaxy.metadata_store.base.MetadataStoreConfig  <code>pydantic-model</code>","text":"<pre><code>MetadataStoreConfig(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool\n    | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool\n    | list[str]\n    | tuple[str, ...]\n    | None = None,\n    _cli_settings_source: CliSettingsSource[Any]\n    | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool\n    | Literal[\"all\", \"no_enums\"]\n    | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]]\n    | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n)\n</code></pre> <p>               Bases: <code>BaseSettings</code></p> <p>Base configuration class for metadata stores.</p> <p>This class defines common configuration fields shared by all metadata store types. Store-specific config classes should inherit from this and add their own fields.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\n\n[stores.dev.config]\ndatabase = \"metadata.db\"\nhash_algorithm = \"md5\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Base configuration class for metadata stores.\\n\\nThis class defines common configuration fields shared by all metadata store types.\\nStore-specific config classes should inherit from this and add their own fields.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    database = \\\"metadata.db\\\"\\n    hash_algorithm = \\\"md5\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"MetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> Source code in <code>.venv/lib/python3.10/site-packages/pydantic_settings/main.py</code> <pre><code>def __init__(\n    __pydantic_self__,\n    _case_sensitive: bool | None = None,\n    _nested_model_default_partial_update: bool | None = None,\n    _env_prefix: str | None = None,\n    _env_file: DotenvType | None = ENV_FILE_SENTINEL,\n    _env_file_encoding: str | None = None,\n    _env_ignore_empty: bool | None = None,\n    _env_nested_delimiter: str | None = None,\n    _env_nested_max_split: int | None = None,\n    _env_parse_none_str: str | None = None,\n    _env_parse_enums: bool | None = None,\n    _cli_prog_name: str | None = None,\n    _cli_parse_args: bool | list[str] | tuple[str, ...] | None = None,\n    _cli_settings_source: CliSettingsSource[Any] | None = None,\n    _cli_parse_none_str: str | None = None,\n    _cli_hide_none_type: bool | None = None,\n    _cli_avoid_json: bool | None = None,\n    _cli_enforce_required: bool | None = None,\n    _cli_use_class_docs_for_groups: bool | None = None,\n    _cli_exit_on_error: bool | None = None,\n    _cli_prefix: str | None = None,\n    _cli_flag_prefix_char: str | None = None,\n    _cli_implicit_flags: bool | None = None,\n    _cli_ignore_unknown_args: bool | None = None,\n    _cli_kebab_case: bool | Literal['all', 'no_enums'] | None = None,\n    _cli_shortcuts: Mapping[str, str | list[str]] | None = None,\n    _secrets_dir: PathType | None = None,\n    **values: Any,\n) -&gt; None:\n    super().__init__(\n        **__pydantic_self__._settings_build_values(\n            values,\n            _case_sensitive=_case_sensitive,\n            _nested_model_default_partial_update=_nested_model_default_partial_update,\n            _env_prefix=_env_prefix,\n            _env_file=_env_file,\n            _env_file_encoding=_env_file_encoding,\n            _env_ignore_empty=_env_ignore_empty,\n            _env_nested_delimiter=_env_nested_delimiter,\n            _env_nested_max_split=_env_nested_max_split,\n            _env_parse_none_str=_env_parse_none_str,\n            _env_parse_enums=_env_parse_enums,\n            _cli_prog_name=_cli_prog_name,\n            _cli_parse_args=_cli_parse_args,\n            _cli_settings_source=_cli_settings_source,\n            _cli_parse_none_str=_cli_parse_none_str,\n            _cli_hide_none_type=_cli_hide_none_type,\n            _cli_avoid_json=_cli_avoid_json,\n            _cli_enforce_required=_cli_enforce_required,\n            _cli_use_class_docs_for_groups=_cli_use_class_docs_for_groups,\n            _cli_exit_on_error=_cli_exit_on_error,\n            _cli_prefix=_cli_prefix,\n            _cli_flag_prefix_char=_cli_flag_prefix_char,\n            _cli_implicit_flags=_cli_implicit_flags,\n            _cli_ignore_unknown_args=_cli_ignore_unknown_args,\n            _cli_kebab_case=_cli_kebab_case,\n            _cli_shortcuts=_cli_shortcuts,\n            _secrets_dir=_secrets_dir,\n        )\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.fallback_stores","title":"metaxy.metadata_store.base.MetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.hash_algorithm","title":"metaxy.metadata_store.base.MetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.versioning_engine","title":"metaxy.metadata_store.base.MetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"reference/api/metadata-stores/base/#configuration","title":"Configuration","text":"<p>The base <code>MetadataStoreConfig</code> class injects the following configuration options:</p> <p>Base configuration class for metadata stores.</p> <p>This class defines common configuration fields shared by all metadata store types. Store-specific config classes should inherit from this and add their own fields.</p> Example metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\"\n\n[stores.dev.config]\ndatabase = \"metadata.db\"\nhash_algorithm = \"md5\"\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"HashAlgorithm\": {\n      \"description\": \"Supported hash algorithms for field provenance calculation.\\n\\nThese algorithms are chosen for:\\n- Speed (non-cryptographic hashes preferred)\\n- Cross-database availability\\n- Good collision resistance for field provenance calculation\",\n      \"enum\": [\n        \"xxhash64\",\n        \"xxhash32\",\n        \"wyhash\",\n        \"sha256\",\n        \"md5\",\n        \"farmhash\"\n      ],\n      \"title\": \"HashAlgorithm\",\n      \"type\": \"string\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"description\": \"Base configuration class for metadata stores.\\n\\nThis class defines common configuration fields shared by all metadata store types.\\nStore-specific config classes should inherit from this and add their own fields.\\n\\nExample:\\n    ```toml title=\\\"metaxy.toml\\\"\\n    [stores.dev]\\n    type = \\\"metaxy.ext.metadata_stores.duckdb.DuckDBMetadataStore\\\"\\n\\n    [stores.dev.config]\\n    database = \\\"metadata.db\\\"\\n    hash_algorithm = \\\"md5\\\"\\n    ```\",\n  \"properties\": {\n    \"fallback_stores\": {\n      \"description\": \"List of fallback store names to search when features are not found in the current store.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Fallback Stores\",\n      \"type\": \"array\"\n    },\n    \"hash_algorithm\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/HashAlgorithm\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash algorithm for versioning. If None, uses store's default.\"\n    },\n    \"versioning_engine\": {\n      \"default\": \"auto\",\n      \"description\": \"Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.\",\n      \"enum\": [\n        \"auto\",\n        \"native\",\n        \"polars\"\n      ],\n      \"title\": \"Versioning Engine\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"MetadataStoreConfig\",\n  \"type\": \"object\"\n}\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nfallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=[]\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nhash_algorithm = \"...\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.fallback_stores","title":"metaxy.metadata_store.base.MetadataStoreConfig.fallback_stores  <code>pydantic-field</code>","text":"<pre><code>fallback_stores: list[str]\n</code></pre> <p>List of fallback store names to search when features are not found in the current store.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.hash_algorithm","title":"metaxy.metadata_store.base.MetadataStoreConfig.hash_algorithm  <code>pydantic-field</code>","text":"<pre><code>hash_algorithm: HashAlgorithm | None = None\n</code></pre> <p>Hash algorithm for versioning. If None, uses store's default.</p>"},{"location":"reference/api/metadata-stores/base/#metaxy.metadata_store.base.MetadataStoreConfig.versioning_engine","title":"metaxy.metadata_store.base.MetadataStoreConfig.versioning_engine  <code>pydantic-field</code>","text":"<pre><code>versioning_engine: Literal[\"auto\", \"native\", \"polars\"] = (\n    \"auto\"\n)\n</code></pre> <p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p>"},{"location":"reference/api/metadata-stores/exceptions/","title":"Metadata Store Exceptions","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions","title":"metaxy.metadata_store.exceptions","text":"<p>Exceptions for metadata store operations.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-classes","title":"Classes","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataStoreError","title":"metaxy.metadata_store.exceptions.MetadataStoreError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for metadata store errors.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FeatureNotFoundError","title":"metaxy.metadata_store.exceptions.FeatureNotFoundError","text":"<pre><code>FeatureNotFoundError(\n    message: str, *, keys: list[FeatureKey] | None = None\n)\n</code></pre> <p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a feature is not found in the store.</p> Source code in <code>src/metaxy/metadata_store/exceptions.py</code> <pre><code>def __init__(self, message: str, *, keys: list[FeatureKey] | None = None):\n    super().__init__(message)\n    self.keys = keys or []\n</code></pre>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.SystemDataNotFoundError","title":"metaxy.metadata_store.exceptions.SystemDataNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when system features are not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FieldNotFoundError","title":"metaxy.metadata_store.exceptions.FieldNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a field is not found for a feature.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataSchemaError","title":"metaxy.metadata_store.exceptions.MetadataSchemaError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when metadata DataFrame has invalid schema.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.DependencyError","title":"metaxy.metadata_store.exceptions.DependencyError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when upstream dependencies are missing or invalid.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.StoreNotOpenError","title":"metaxy.metadata_store.exceptions.StoreNotOpenError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when attempting to use a store that hasn't been opened.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","title":"metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a hash algorithm is not supported by the store or its components.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.TableNotFoundError","title":"metaxy.metadata_store.exceptions.TableNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a table does not exist and auto_create_tables is disabled.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.VersioningEngineMismatchError","title":"metaxy.metadata_store.exceptions.VersioningEngineMismatchError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when versioning_engine='native' is requested but data has wrong implementation.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-functions","title":"Functions","text":""}]}