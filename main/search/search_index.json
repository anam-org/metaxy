{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Metaxy <p>Warning</p> <p>Metaxy hasn't been publicly released yet, but you can try the latest dev release:</p> <pre><code>pip install --pre metaxy\n</code></pre> <p>Metaxy is a metadata layer for multi-modal Data and ML pipelines that manages and tracks metadata: sample versions, dependencies, and data lineage across complex computational graphs.</p> <p>It's agnostic to orchestration frameworks, compute engines, data and metadata storage.</p> <p>It has no strict infrastructure requirements and can run computations in external databases or locally.</p> <p>It can scale to handle large amounts of big metadata.</p>"},{"location":"#what-problem-exactly-does-metaxy-solve","title":"What problem exactly does Metaxy solve?","text":"<p>Data, ML and AI workloads processing large amounts of images, videos, audios, texts, or any other kind of data can be very expensive to run. In contrast to traditional data engineering, re-running the whole pipeline on changes is no longer an option. Therefore, it becomes crucially important to correctly implement incremental processing and sample-level versioning.</p> <p>Typically, a feature has to be re-computed in one of the following scenarios:</p> <ul> <li> <p>upstream data changes</p> </li> <li> <p>bug fixes or algorithmic changes</p> </li> </ul> <p>But correctly distinguishing these scenarios from cases where the feature should not be re-computed is surprisingly challenging. Here are some of the cases where it would be undesirable:</p> <ul> <li> <p>merging two consecutive steps into one (refactoring the graph topology)</p> </li> <li> <p>partial data updates, e.g. changing only the audio track inside a video file</p> </li> <li> <p>backfilling metadata from another source</p> </li> </ul> <p>Tracking and propagating these changes correctly to the right subset of samples and features can become incredibly complicated. Until now, a general solution for this problem did not exist, but this is not the case anymore.</p>"},{"location":"#metaxys-solution","title":"Metaxy's solution","text":"<p>Metaxy solves the first set of problems with a feature and field dependency system, and the second set with a migrations system.</p> <p>Metaxy builds a versioned graphs from feature definitions and tracks version changes.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Head to Quickstart (WIP!).</p>"},{"location":"#about-metaxy","title":"About Metaxy","text":"<p>Metaxy is:</p> <ul> <li> <p>\ud83e\udde9 composable --- bring your own everything!</p> <ul> <li>supports DuckDB, ClickHouse, and 20+ databases via Ibis</li> <li>supports lakehouse storage formats such as DeltaLake or DuckLake</li> <li>is agnostic to tabular compute engines: Polars, Spark, Pandas, and databases thanks to Narwhals</li> <li>we totally don't care how is the multi-modal data produced or where is it stored: Metaxy is responsible for yielding input metadata and writing output metadata</li> </ul> </li> <li> <p>\ud83e\udd38 flexible to work around restrictions consciously:</p> <ul> <li>features are defined as Pydantic models, leveraging Pydantic's type safety guarantees, rich validation system, and allowing inheritance patterns to stay DRY</li> <li>has a migrations system to compensate for reconciling field provenances and metadata when computations are not desired</li> </ul> </li> <li> <p>\ud83e\udea8 rock solid when it matters:</p> <ul> <li>field provenance is guaranteed to be consistent across DBs or in-memory compute engines. We really have tested this very well!</li> <li>changes to topology, feature versioning, or individual samples ruthlessly propagate downstream</li> <li>unique field-level dependency system prevents unnecessary recomputations for features that depend on partial data</li> <li>metadata is append-only to ensure data integrity and immutability. Users can perform cleanup if needed (Metaxy provides tools for this).</li> </ul> </li> <li> <p>\ud83d\udcc8 scalable:</p> <ul> <li>supports feature organization and discovery patterns such as packaging entry points. This enables collaboration across teams and projects.</li> <li>is built with performance in mind: all operations default to run in the DB, Metaxy does not stand in the way of metadata flow</li> </ul> </li> <li> <p>\ud83e\uddd1\u200d\ud83d\udcbb dev friendly:</p> <ul> <li>clean, intuitive Python API that stays out of your way when you don't need it</li> <li>feature discovery system for effortless dependency management</li> <li>comprehensive type hints and Pydantic integration for excellent IDE support</li> <li>first-class support for local development, testing, preview environments, CI/CD</li> <li>CLI tool for easy interaction, inspection and visualization of feature graphs, enriched with real metadata and stats</li> <li>integrations with popular tools such as SQLModel and Dagster.</li> <li>testing helpers that you're going to appreciate</li> </ul> </li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>Itching to write some Metaxy code? Continue to Quickstart (WIP!).</p> </li> <li> <p>Learn more about feature definitions or versioning</p> </li> <li> <p>View complete, end-to-end examples</p> </li> <li> <p>Explore Metaxy integrations</p> </li> <li> <p>Use Metaxy from the command line</p> </li> <li> <p>Learn how to configure Metaxy</p> </li> <li> <p>Get lost in our API Reference</p> </li> </ul>"},{"location":"examples/","title":"Metaxy Examples","text":"<p>Here you can find complete example Metaxy projects. Each example is a self-contained Python project that demonstrates a specific feature or use case of Metaxy.</p> <p>Success</p> <p>Examples are continuously tested in CI and are guaranteed to work.</p> <ul> <li>One-to-Many Expansion</li> </ul>"},{"location":"examples/one-to-many/","title":"One-to-Many Expansion","text":"<p> View Example Source on GitHub</p> <p>This example demonstrates how to implement <code>1:N</code> transformations with Metaxy. In such relationships a single parent sample can map into multiple child samples.</p> <p>In Metaxy they can be modeled with LineageRelationship.expansion lineage type.</p> <p>We will use a hypothetical video chunking pipeline as an example. We are also going to demonstrate that other Metaxy features such as fields mapping work with non-standard lineage types.</p>"},{"location":"examples/one-to-many/#the-pipeline","title":"The Pipeline","text":"<p>We are going to define a typical video processing pipeline with three features:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        video_raw[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/raw&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_chunk[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/chunk&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio&lt;br/&gt;\u2022 frames&lt;/div&gt;\"]\n        video_faces[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;video/faces&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces&lt;/div&gt;\"]\n        video_raw --&gt; video_chunk\n        video_chunk --&gt; video_faces</code></pre>"},{"location":"examples/one-to-many/#defining-features-video","title":"Defining features: <code>Video</code>","text":"<p>Each video-like feature in our pipeline is going to have two fields: <code>audio</code> and <code>frames</code>.</p> <p>Let's set the code version of <code>audio</code> to <code>\"1\"</code> in order to change it in the future. <code>frames</code> field will have a default version.</p> src/example_one_to_many/features.py<pre><code>import metaxy as mx\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video/raw\",\n        id_columns=[\"video_id\"],\n        fields=[\n            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n            \"frames\",\n        ],\n    ),\n):\n    video_id: str\n    path: str  # where the video is stored\n</code></pre>"},{"location":"examples/one-to-many/#defining-features-videochunk","title":"Defining features: <code>VideoChunk</code>","text":"<p><code>VideoChunk</code> represents a piece of the upstream <code>Video</code> feature. Since each <code>Video</code> sample can be split into multiple chunks, we need to tell Metaxy how to map each chunk to its parent video.</p> src/example_one_to_many/features.py<pre><code>class VideoChunk(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"chunk\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[mx.FeatureDep(feature=Video)],\n        fields=[\"audio\", \"frames\"],\n        lineage=mx.LineageRelationship.expansion(on=[\"video_id\"]),\n    ),\n):\n    video_id: str  # points to the parent video\n    video_chunk_id: str\n    path: str  # where the video chunk is stored\n</code></pre> <p>We do not specify custom versions on its fields. Metaxy will automatically assign field-level dependencies by matching on field names: <code>VideoChunk.frames</code> depends on <code>Video.frames</code> and <code>VideoChunk.audio</code> depends on <code>Video.audio</code>.</p>"},{"location":"examples/one-to-many/#defining-features-facerecognition","title":"Defining features: <code>FaceRecognition</code>","text":"<p><code>FaceRecognition</code> processes video chunks and only depends on the <code>frames</code> field. This can be expressed with a <code>SpecificFieldsMapping</code>.</p> src/example_one_to_many/features.py<pre><code>class FaceRecognition(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=[\"video\", \"faces\"],\n        id_columns=[\"video_chunk_id\"],\n        deps=[\n            mx.FeatureDep(\n                feature=VideoChunk,\n                fields_mapping=mx.FieldsMapping.specific(\n                    mapping={mx.FieldKey(\"faces\"): {mx.FieldKey(\"frames\")}}\n                ),\n            )\n        ],\n        fields=[\"faces\"],\n    ),\n):\n    video_chunk_id: str\n    num_faces: int  # number of faces detected\n</code></pre> <p>This completes the feature definitions. Let's proceed to running the pipeline.</p>"},{"location":"examples/one-to-many/#walkthrough","title":"Walkthrough","text":"<p>Here is a toy pipeline for computing the feature graph described above:</p> <code>pipeline.py</code> pipeline.py<pre><code>import random\n\nimport narwhals as nw\nimport polars as pl\nfrom example_one_to_many.features import FaceRecognition, Video, VideoChunk\nfrom example_one_to_many.utils import split_video_into_chunks\n\nfrom metaxy import init_metaxy\n\n\ndef main():\n    cfg = init_metaxy()\n    store = cfg.get_store(\"dev\")\n\n    # let's pretend somebody has already created the videos for us\n    samples = pl.DataFrame(\n        {\n            \"video_id\": [1, 2, 3],\n            \"path\": [\"video1.mp4\", \"video2.mp4\", \"video3.mp4\"],\n            \"metaxy_provenance_by_field\": [\n                {\"audio\": \"v1\", \"frames\": \"v1\"},\n                {\"audio\": \"v2\", \"frames\": \"v2\"},\n                {\"audio\": \"v3\", \"frames\": \"v3\"},\n            ],\n        }\n    )\n\n    with store:\n        # showcase: resolve incremental update for a root feature\n        diff = store.resolve_update(Video, samples=nw.from_native(samples))\n        if len(diff.added) &gt; 0:\n            print(f\"Found {len(diff.added)} new videos\")\n            store.write_metadata(Video, diff.added)\n\n    # now we are going to resolve the videos that have to be split to chunks\n    with store:\n        diff = store.resolve_update(VideoChunk)\n        # the DataFrame dimensions matches Video (with ID column renamed)\n\n        print(\n            f\"Found {len(diff.added)} videos and {len(diff.changed)} videos that need chunking\"\n        )\n\n        for row_dict in pl.concat(\n            [diff.added.to_polars(), diff.changed.to_polars()]\n        ).iter_rows(named=True):\n            print(f\"Processing video: {row_dict}\")\n            # let's split each video to 3-5 chunks randomly\n\n            video_id = row_dict[\"video_id\"]\n            path = row_dict[\"path\"]\n\n            provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n            provenance = row_dict[\"metaxy_provenance\"]\n\n            # pretend we split the video into chunks\n            chunk_paths = split_video_into_chunks(path)\n\n            # Generate chunk IDs based on the parent video ID\n            chunk_ids = [f\"{video_id}_{i}\" for i in range(len(chunk_paths))]\n\n            # write the chunks to the store\n            # CRUSIAL: all the chunks **must share the same provenance values**\n            chunk_df = pl.DataFrame(\n                {\n                    \"video_id\": [video_id] * len(chunk_paths),\n                    \"video_chunk_id\": chunk_ids,\n                    \"path\": chunk_paths,\n                    \"metaxy_provenance_by_field\": [provenance_by_field]\n                    * len(chunk_paths),\n                    \"metaxy_provenance\": [provenance] * len(chunk_paths),\n                }\n            )\n            print(f\"Writing {len(chunk_paths)} chunks for video {video_id}\")\n            store.write_metadata(VideoChunk, nw.from_native(chunk_df))\n\n    # now process face recognition on video chunks\n    with store:\n        diff = store.resolve_update(FaceRecognition)\n        print(\n            f\"Found {len(diff.added)} video chunks and {len(diff.changed)} video chunks that need face recognition\"\n        )\n\n        if len(diff.added) &gt; 0:\n            # simulate face detection on each chunk\n            face_data = []\n            for row_dict in pl.concat(\n                [diff.added.to_polars(), diff.changed.to_polars()]\n            ).iter_rows(named=True):\n                video_chunk_id = row_dict[\"video_chunk_id\"]\n                provenance_by_field = row_dict[\"metaxy_provenance_by_field\"]\n                provenance = row_dict[\"metaxy_provenance\"]\n\n                # simulate detecting random number of faces\n                num_faces = random.randint(0, 10)\n\n                face_data.append(\n                    {\n                        \"video_chunk_id\": video_chunk_id,\n                        \"num_faces\": num_faces,\n                        \"metaxy_provenance_by_field\": provenance_by_field,\n                        \"metaxy_provenance\": provenance,\n                    }\n                )\n\n            face_df = pl.DataFrame(face_data)\n            print(f\"Writing face recognition results for {len(face_data)} chunks\")\n            store.write_metadata(FaceRecognition, nw.from_native(face_df))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/one-to-many/#step-1-launch-initial-run","title":"Step 1: Launch Initial Run","text":"<p>Run the pipeline to create videos, chunks, and face recognition results:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 4 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 3 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 5 chunks for video 3\nFound 12 video chunks and 0 video chunks that need face recognition\nWriting face recognition results for 12 chunks\n</code></pre> <p>All three features have been materialized. Note that the <code>VideoChunk</code> feature may dynamically create as many samples as needed: Metaxy doesn't need to know anything about this in advance, except the relationship type.</p>"},{"location":"examples/one-to-many/#step-2-verify-idempotency","title":"Step 2: Verify Idempotency","text":"<p>Run the pipeline again without any changes:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 0 videos and 0 videos that need chunking\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Nothing needs recomputation - the system correctly detects no changes.</p>"},{"location":"examples/one-to-many/#step-3-change-audio-code-version","title":"Step 3: Change Audio Code Version","text":"<p>Now let's bump the code version on the <code>audio</code> field of <code>Video</code> feature:</p> <code>patches/01_update_video_code_version.patch</code> patches/01_update_video_code_version.patch<pre><code>--- a/src/example_one_to_many/features.py\n+++ b/src/example_one_to_many/features.py\n@@ -9,6 +9,6 @@ class Video(\n         id_columns=[\"video_id\"],\n         fields=[\n-            mx.FieldSpec(key=\"audio\", code_version=\"1\"),\n+            mx.FieldSpec(key=\"audio\", code_version=\"2\"),\n             \"frames\",\n         ],\n     ),\n</code></pre> <p>This represents updating the audio processing algorithm, and therefore the audio data.</p>"},{"location":"examples/one-to-many/#step-4-observe-field-level-tracking","title":"Step 4: Observe Field-Level Tracking","text":"<p>Run the pipeline again after the code change:</p> <pre><code>python pipeline.py\n</code></pre> <p>Output:</p> <pre><code>Found 3 new videos\nFound 3 videos and 0 videos that need chunking\nProcessing video: {'video_id': 1, ...}\nWriting 3 chunks for video 1\nProcessing video: {'video_id': 2, ...}\nWriting 5 chunks for video 2\nProcessing video: {'video_id': 3, ...}\nWriting 4 chunks for video 3\nFound 0 video chunks and 0 video chunks that need face recognition\n</code></pre> <p>Key observation:</p> <ul> <li><code>VideoChunk</code> has been recomputed since the <code>audio</code> field on it has been affected by the upstream change</li> <li><code>FaceRecognition</code> did not require a recompute, because it only depends on the <code>frames</code> field (which did not change)</li> </ul>"},{"location":"examples/one-to-many/#conclusion","title":"Conclusion","text":"<p>Metaxy provides a convenient API for modeling <code>1:N</code> relationships: LineageRelationship.expansion. Other Metaxy features such as field-level versioning continue to work seamlessly when declaring <code>1:N</code> relationships.</p>"},{"location":"examples/one-to-many/#related-materials","title":"Related materials","text":"<p>Learn more about:</p> <ul> <li>Features and Fields</li> <li>Relationships</li> <li>Fields Mapping</li> </ul>"},{"location":"integrations/","title":"Metaxy Integrations","text":""},{"location":"integrations/#orchestration","title":"Orchestration","text":"<ul> <li> <p> Dagster</p> <p> Orchestration \u2022 Data Platform</p> <p>Integrate Metaxy with Dagster using ConfigurableResource to manage metadata stores in your data pipelines.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#metadata-stores","title":"Metadata Stores","text":"<p>Learn more about metadata stores here.</p> <ul> <li> <p>Delta Lake Delta Lake</p> <p> Storage Only</p> <p>Store metadata in Delta Lake format with support for local filesystem and remote object stores (S3, Azure, GCS).</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>LanceDB LanceDB</p> <p> Database</p> <p>Columnar database optimized for vector search and multimodal data with embedded (local) and external (object store or LanceDB Cloud) deployments.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>DuckDB DuckDB</p> <p> Database</p> <p>Store metadata in DuckDB with support for local files, in-memory databases, and MotherDuck cloud.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>ClickHouse ClickHouse</p> <p> Database</p> <p>Leverage ClickHouse for large metadata volumes and high-throughput setups.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p>Google BigQuery BigQuery</p> <p> Database</p> <p>Use Google BigQuery as a scalable serverless metadata store on GCP.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/#plugins","title":"Plugins","text":"<ul> <li> <p>SQLAlchemy SQLAlchemy</p> <p> ORM \u2022 Database</p> <p>Retrieve SQLAlchemy URLs and <code>MetaData</code> for the current Metaxy project from Metaxy <code>MetadataStore</code> objects.</p> <p> Integration docs</p> <p> API docs</p> </li> <li> <p> SQLModel SQLModel</p> <p> ORM \u2022 Database</p> <p>Adds <code>SQLModel</code> capabilities to <code>metaxy.BaseFeature</code> class.</p> <p> Integration docs</p> <p> API docs</p> </li> </ul>"},{"location":"integrations/metadata-stores/","title":"Metadata Stores","text":"<p>Metadata stores may come in two flavors.</p>"},{"location":"integrations/metadata-stores/#database-backed","title":"Database-Backed","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible. (1)</p> <ol> <li> Typically the entire <code>MetadataStore.resolve_update</code> can be executed externally!</li> </ol> <p>These metadata stores can be found here.</p> <p>Example</p> <p>ClickHouse is an excellent choice for a production metadata store.</p> <p>Tip</p> <p>Some of them such as LanceDB or DuckDB can also act as local compute engines.</p>"},{"location":"integrations/metadata-stores/#storage-only","title":"Storage Only","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>The available storage-only stores can be found here.</p> <p>Example</p> <p>DeltaLake is an excellent choice for a storage-only metadata store.</p>"},{"location":"integrations/metadata-stores/#choosing-the-right-metadata-store","title":"Choosing the Right Metadata Store","text":"<p>Compute-backed stores are typically more performant, but require additional infrastructure and maintenance.</p> <p>For production environments that need to handle big metadata volumes, consider database-backed stores.</p> <p>For development, testing, branch deployments, and other scenarios where you want to keep things simple, consider using a storage-only store.</p> <p>Warning</p> <p>Not all metadata stores support parallel writes. For example, DuckDB requires application level work-arounds.</p>"},{"location":"integrations/metadata-stores/#reference","title":"Reference","text":"<ul> <li>Learn more about using metadata stores</li> </ul>"},{"location":"integrations/metadata-stores/databases/","title":"Database-Backed Metadata Stores","text":"<p>These metadata stores provide external compute resources. The most common example of such stores is databases. Metaxy delegates all versioning computations and operations to external compute as much as possible (typically the entire <code>MetadataStore.resolve_update</code> can be executed externally).</p>"},{"location":"integrations/metadata-stores/databases/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>Metaxy + BigQuery</li> <li>Metaxy + ClickHouse</li> <li>Metaxy + DuckDB</li> <li>Ibis Integration</li> <li>Metaxy + LanceDB</li> </ul>"},{"location":"integrations/metadata-stores/databases/bigquery/","title":"Metaxy + BigQuery","text":"<p>Metaxy implements <code>BigQueryMetadataStore</code>. It uses BigQuery as metadata storage and versioning engine.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[bigquery]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/metadata-stores/databases/bigquery/api/","title":"BigQuery Metadata Store API","text":""},{"location":"integrations/metadata-stores/databases/bigquery/api/#metaxy.metadata_store.bigquery","title":"metaxy.metadata_store.bigquery","text":"<p>BigQuery metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/bigquery/api/#metaxy.metadata_store.bigquery.BigQueryMetadataStore","title":"metaxy.metadata_store.bigquery.BigQueryMetadataStore","text":"<pre><code>BigQueryMetadataStore(project_id: str | None = None, dataset_id: str | None = None, *, credentials_path: str | None = None, credentials: Any | None = None, location: str | None = None, connection_params: dict[str, Any] | None = None, fallback_stores: list[MetadataStore] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>BigQuery metadata store using Ibis backend.</p> Warning <p>It's on the user to set up infrastructure for Metaxy correctly. Make sure to have large tables partitioned as appropriate for your use case.</p> Note <p>BigQuery automatically optimizes queries on partitioned tables. When tables are partitioned (e.g., by date or ingestion time with _PARTITIONTIME), BigQuery will automatically prune partitions based on WHERE clauses in queries, without needing explicit configuration in the metadata store. Make sure to use appropriate <code>filters</code> when calling BigQueryMetadataStore.read_metadata.</p> Basic Connection <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n)\n</code></pre> With Service Account <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    credentials_path=\"/path/to/service-account.json\",\n)\n</code></pre> With Location Configuration <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    location=\"EU\",  # Specify data location\n)\n</code></pre> With Custom Hash Algorithm <pre><code>store = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"my_dataset\",\n    hash_algorithm=HashAlgorithm.SHA256,  # Use SHA256 instead of default FARMHASH\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>project_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud project ID containing the dataset. Can also be set via GOOGLE_CLOUD_PROJECT environment variable.</p> </li> <li> <code>dataset_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>BigQuery dataset name for storing metadata tables. If not provided, uses the default dataset for the project.</p> </li> <li> <code>credentials_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to service account JSON file. Alternative to passing credentials object directly.</p> </li> <li> <code>credentials</code>               (<code>Any | None</code>, default:                   <code>None</code> )           \u2013            <p>Google Cloud credentials object. If not provided, uses default credentials from environment.</p> </li> <li> <code>location</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Default location for BigQuery resources (e.g., \"US\", \"EU\"). If not specified, BigQuery determines based on dataset location.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional Ibis BigQuery connection parameters. Overrides individual parameters if provided.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-bigquery not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither project_id nor connection_params provided</p> </li> </ul> Note <p>Authentication priority: 1. Explicit credentials or credentials_path 2. Application Default Credentials (ADC) 3. Google Cloud SDK credentials</p> <p>BigQuery automatically handles partition pruning when querying partitioned tables. If your tables are partitioned (e.g., by date or ingestion time), BigQuery will automatically optimize queries with appropriate WHERE clauses on the partition column.</p> Example <pre><code># Using environment authentication\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n)\n\n# Using service account\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    credentials_path=\"/path/to/key.json\",\n)\n\n# With location specification\nstore = BigQueryMetadataStore(\n    project_id=\"my-project\",\n    dataset_id=\"ml_metadata\",\n    location=\"EU\",\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/bigquery.py</code> <pre><code>def __init__(\n    self,\n    project_id: str | None = None,\n    dataset_id: str | None = None,\n    *,\n    credentials_path: str | None = None,\n    credentials: Any | None = None,\n    location: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [BigQuery](https://cloud.google.com/bigquery) metadata store.\n\n    Args:\n        project_id: Google Cloud project ID containing the dataset.\n            Can also be set via GOOGLE_CLOUD_PROJECT environment variable.\n        dataset_id: BigQuery dataset name for storing metadata tables.\n            If not provided, uses the default dataset for the project.\n        credentials_path: Path to service account JSON file.\n            Alternative to passing credentials object directly.\n        credentials: Google Cloud credentials object.\n            If not provided, uses default credentials from environment.\n        location: Default location for BigQuery resources (e.g., \"US\", \"EU\").\n            If not specified, BigQuery determines based on dataset location.\n        connection_params: Additional Ibis BigQuery connection parameters.\n            Overrides individual parameters if provided.\n        fallback_stores: Ordered list of read-only fallback stores.\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]\n\n    Raises:\n        ImportError: If ibis-bigquery not installed\n        ValueError: If neither project_id nor connection_params provided\n\n    Note:\n        Authentication priority:\n        1. Explicit credentials or credentials_path\n        2. Application Default Credentials (ADC)\n        3. Google Cloud SDK credentials\n\n        BigQuery automatically handles partition pruning when querying partitioned tables.\n        If your tables are partitioned (e.g., by date or ingestion time), BigQuery will\n        automatically optimize queries with appropriate WHERE clauses on the partition column.\n\n    Example:\n        ```py\n        # Using environment authentication\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n        )\n\n        # Using service account\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            credentials_path=\"/path/to/key.json\",\n        )\n\n        # With location specification\n        store = BigQueryMetadataStore(\n            project_id=\"my-project\",\n            dataset_id=\"ml_metadata\",\n            location=\"EU\",\n        )\n        ```\n    \"\"\"\n    # Build connection parameters if not provided\n    if connection_params is None:\n        connection_params = self._build_connection_params(\n            project_id=project_id,\n            dataset_id=dataset_id,\n            credentials_path=credentials_path,\n            credentials=credentials,\n            location=location,\n        )\n\n    # Validate we have minimum required parameters\n    if \"project_id\" not in connection_params and project_id is None:\n        raise ValueError(\n            \"Must provide either project_id or connection_params with project_id. \"\n            \"Example: project_id='my-project'\"\n        )\n\n    # Store parameters for display\n    self.project_id = project_id or connection_params.get(\"project_id\")\n    self.dataset_id = dataset_id or connection_params.get(\"dataset_id\", \"\")\n\n    # Initialize Ibis store with BigQuery backend\n    super().__init__(\n        backend=\"bigquery\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/api/#metaxy.metadata_store.bigquery.BigQueryMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/bigquery/api/#metaxy.metadata_store.bigquery.BigQueryMetadataStore.display","title":"metaxy.metadata_store.bigquery.BigQueryMetadataStore.display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/bigquery.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    dataset_info = f\"/{self.dataset_id}\" if self.dataset_id else \"\"\n    return f\"BigQueryMetadataStore(project={self.project_id}{dataset_info})\"\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/api/#metaxy.metadata_store.bigquery.BigQueryMetadataStore.config_model","title":"metaxy.metadata_store.bigquery.BigQueryMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[BigQueryMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/bigquery.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[BigQueryMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return BigQueryMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/","title":"BigQuery Configuration","text":""},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#project_id","title":"<code>project_id</code>","text":"<p>Google Cloud project ID containing the dataset.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# project_id = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# project_id = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__PROJECT_ID=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#dataset_id","title":"<code>dataset_id</code>","text":"<p>BigQuery dataset name for storing metadata tables.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# dataset_id = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# dataset_id = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATASET_ID=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#credentials_path","title":"<code>credentials_path</code>","text":"<p>Path to service account JSON file.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# credentials_path = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# credentials_path = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS_PATH=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#credentials","title":"<code>credentials</code>","text":"<p>Google Cloud credentials object.</p> <p>Type: <code>Optional[Any]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# credentials = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# credentials = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CREDENTIALS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/bigquery/configuration/#location","title":"<code>location</code>","text":"<p>Default location for BigQuery resources (e.g., 'US', 'EU').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# location = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# location = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LOCATION=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/","title":"Metaxy + ClickHouse","text":"<p>Metaxy implements <code>ClickHouseMetadataStore</code>. It uses ClickHouse as metadata storage and versioning engine.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[clickhouse]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/#struct-columns-and-json-storage","title":"Struct Columns and JSON Storage","text":"<p>Metaxy uses struct columns (<code>metaxy_provenance_by_field</code>, <code>metaxy_data_version_by_field</code>) to track field-level versioning. In Python world this corresponds to <code>dict[str, str]</code>.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#how-clickhouse-handles-structs","title":"How ClickHouse Handles Structs","text":"<p>ClickHouse offers multiple approaches for structured data:</p> Type Description Use Case <code>JSON</code> Native JSON with typed subcolumns Improved query performance on JSON paths <code>Map(K, V)</code> Native key-value map When schema is <code>dict[K, V]</code>, even better performance"},{"location":"integrations/metadata-stores/databases/clickhouse/#alembic-migrations","title":"Alembic Migrations","text":"<p>For Alembic migrations, use <code>clickhouse-connect</code>:</p> <pre><code>pip install clickhouse-connect\n</code></pre> Alternative Community Driver <p>Alternatively, use the community <code>clickhouse-sqlalchemy</code> driver.</p> <p>Alembic Integration</p> <p>See Alembic setup guide for additional instructions on how to use Alembic with Metaxy.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> <li>Introduction to ClickHouse</li> <li>ClickHouse Connect</li> </ul>"},{"location":"integrations/metadata-stores/databases/clickhouse/api/","title":"ClickHouse Metadata Store API","text":""},{"location":"integrations/metadata-stores/databases/clickhouse/api/#metaxy.metadata_store.clickhouse","title":"metaxy.metadata_store.clickhouse","text":"<p>ClickHouse metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/clickhouse/api/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","title":"metaxy.metadata_store.clickhouse.ClickHouseMetadataStore","text":"<pre><code>ClickHouseMetadataStore(connection_string: str | None = None, *, connection_params: dict[str, Any] | None = None, fallback_stores: list[MetadataStore] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>ClickHouse metadata storeusing Ibis backend.</p> Connection Parameters <pre><code>store = ClickHouseMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\n        \"host\": \"localhost\",\n        \"port\": 9000,\n        \"database\": \"default\",\n        \"user\": \"default\",\n        \"password\": \"\"\n    },\n    hash_algorithm=HashAlgorithm.XXHASH64\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ClickHouse connection string.</p> <p>Format: <code>clickhouse://[user[:password]@]host[:port]/database[?param=value]</code></p> <p>Examples:     <pre><code>- \"clickhouse://localhost:9000/default\"\n- \"clickhouse://user:pass@host:9000/db\"\n- \"clickhouse://host:9000/db?secure=true\"\n</code></pre></p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Alternative to connection_string, specify params as dict:</p> <ul> <li> <p>host: Server host</p> </li> <li> <p>port: Server port (default: <code>9000</code>)</p> </li> <li> <p>database: Database name</p> </li> <li> <p>user: Username</p> </li> <li> <p>password: Password</p> </li> <li> <p>secure: Use secure connection (default: <code>False</code>)</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If ibis-clickhouse not installed</p> </li> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor connection_params provided</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/clickhouse.py</code> <pre><code>def __init__(\n    self,\n    connection_string: str | None = None,\n    *,\n    connection_params: dict[str, Any] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [ClickHouse](https://clickhouse.com/) metadata store.\n\n    Args:\n        connection_string: ClickHouse connection string.\n\n            Format: `clickhouse://[user[:password]@]host[:port]/database[?param=value]`\n\n            Examples:\n                ```\n                - \"clickhouse://localhost:9000/default\"\n                - \"clickhouse://user:pass@host:9000/db\"\n                - \"clickhouse://host:9000/db?secure=true\"\n                ```\n\n        connection_params: Alternative to connection_string, specify params as dict:\n\n            - host: Server host\n\n            - port: Server port (default: `9000`)\n\n            - database: Database name\n\n            - user: Username\n\n            - password: Password\n\n            - secure: Use secure connection (default: `False`)\n\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Raises:\n        ImportError: If ibis-clickhouse not installed\n        ValueError: If neither connection_string nor connection_params provided\n    \"\"\"\n    if connection_string is None and connection_params is None:\n        raise ValueError(\n            \"Must provide either connection_string or connection_params. \"\n            \"Example: connection_string='clickhouse://localhost:9000/default'\"\n        )\n\n    # Initialize Ibis store with ClickHouse backend\n    super().__init__(\n        connection_string=connection_string,\n        backend=\"clickhouse\" if connection_string is None else None,\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/api/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/clickhouse/api/#metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.config_model","title":"metaxy.metadata_store.clickhouse.ClickHouseMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[ClickHouseMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/clickhouse.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[ClickHouseMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return ClickHouseMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/","title":"ClickHouse Configuration","text":""},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/clickhouse/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/","title":"Metaxy + DuckDB","text":"<p>Metaxy implements <code>DuckDBMetadataStore</code>. It uses DuckDB as metadata storage and versioning engine.</p> <p>Warning</p> <p>DuckDB does not (currently) support concurrent writes. If multiple writers are a requirement (e.g. with distributed data processing), consider either using DuckLake with a <code>PostgreSQL</code> catalog, or refer to DuckDB's documentation to learn about implementing application-side work-arounds.</p> <p>Tip</p> <p>The Delta Lake metadata store might be a better alternative for concurrent writes.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[duckdb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/#extensions","title":"Extensions","text":"<p>DuckDB extensions can be loaded automatically:</p> <pre><code>store = DuckDBMetadataStore(\"metadata.db\", extensions=[\"hashfuncs\", \"spatial\"])\n</code></pre> <p><code>hashfuncs</code> is typically used by the versioning engine.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/metadata-stores/databases/duckdb/api/","title":"DuckDB Metadata Store API","text":""},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb","title":"metaxy.metadata_store.duckdb","text":"<p>DuckDB metadata store - thin wrapper around IbisMetadataStore.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore","text":"<pre><code>DuckDBMetadataStore(database: str | Path, *, config: dict[str, str] | None = None, extensions: Sequence[ExtensionInput] | None = None, fallback_stores: list[MetadataStore] | None = None, ducklake: DuckLakeConfigInput | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>IbisMetadataStore</code></p> <p>DuckDB metadata store using Ibis backend.</p> Local File <pre><code>store = DuckDBMetadataStore(\"metadata.db\")\n</code></pre> In-memory database <pre><code># In-memory database\nstore = DuckDBMetadataStore(\":memory:\")\n</code></pre> MotherDuck <pre><code># MotherDuck\nstore = DuckDBMetadataStore(\"md:my_database\")\n</code></pre> With extensions <pre><code># With extensions\nstore = DuckDBMetadataStore(\n    \"metadata.db\",\n    hash_algorithm=HashAlgorithm.XXHASH64,\n    extensions=[\"hashfuncs\"]\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>database</code>               (<code>str | Path</code>)           \u2013            <p>Database connection string or path. - File path: <code>\"metadata.db\"</code> or <code>Path(\"metadata.db\")</code></p> <ul> <li> <p>In-memory: <code>\":memory:\"</code></p> </li> <li> <p>MotherDuck: <code>\"md:my_database\"</code> or <code>\"md:my_database?motherduck_token=...\"</code></p> </li> <li> <p>S3: <code>\"s3://bucket/path/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>HTTPS: <code>\"https://example.com/database.duckdb\"</code> (read-only via ATTACH)</p> </li> <li> <p>Any valid DuckDB connection string</p> </li> </ul> </li> <li> <code>config</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})</p> </li> <li> <code>extensions</code>               (<code>Sequence[ExtensionInput] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of DuckDB extensions to install and load on open. Supports strings (community repo), mapping-like objects with <code>name</code>/<code>repository</code> keys, or metaxy.metadata_store.duckdb.ExtensionSpec instances.</p> </li> </ul> Optional DuckLake attachment configuration. Provide either a <p>mapping with 'metadata_backend' and 'storage_backend' entries or a DuckLakeAttachmentConfig instance. When supplied, the DuckDB connection is configured to ATTACH the DuckLake catalog after open(). fallback_stores: Ordered list of read-only fallback stores.</p> <p>**kwargs: Passed to metaxy.metadata_store.ibis.IbisMetadataStore`</p> Warning <p>Parent directories are NOT created automatically. Ensure paths exist before initializing the store.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def __init__(\n    self,\n    database: str | Path,\n    *,\n    config: dict[str, str] | None = None,\n    extensions: Sequence[ExtensionInput] | None = None,\n    fallback_stores: list[\"MetadataStore\"] | None = None,\n    ducklake: DuckLakeConfigInput | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize [DuckDB](https://duckdb.org/) metadata store.\n\n    Args:\n        database: Database connection string or path.\n            - File path: `\"metadata.db\"` or `Path(\"metadata.db\")`\n\n            - In-memory: `\":memory:\"`\n\n            - MotherDuck: `\"md:my_database\"` or `\"md:my_database?motherduck_token=...\"`\n\n            - S3: `\"s3://bucket/path/database.duckdb\"` (read-only via ATTACH)\n\n            - HTTPS: `\"https://example.com/database.duckdb\"` (read-only via ATTACH)\n\n            - Any valid DuckDB connection string\n\n        config: Optional DuckDB configuration settings (e.g., {'threads': '4', 'memory_limit': '4GB'})\n        extensions: List of DuckDB extensions to install and load on open.\n            Supports strings (community repo), mapping-like objects with\n            ``name``/``repository`` keys, or [metaxy.metadata_store.duckdb.ExtensionSpec][] instances.\n\n    ducklake: Optional DuckLake attachment configuration. Provide either a\n        mapping with 'metadata_backend' and 'storage_backend' entries or a\n        DuckLakeAttachmentConfig instance. When supplied, the DuckDB\n        connection is configured to ATTACH the DuckLake catalog after open().\n        fallback_stores: Ordered list of read-only fallback stores.\n\n        **kwargs: Passed to [metaxy.metadata_store.ibis.IbisMetadataStore][]`\n\n    Warning:\n        Parent directories are NOT created automatically. Ensure paths exist\n        before initializing the store.\n    \"\"\"\n    database_str = str(database)\n\n    # Build connection params for Ibis DuckDB backend\n    # Ibis DuckDB backend accepts config params directly (not nested under 'config')\n    connection_params = {\"database\": database_str}\n    if config:\n        connection_params.update(config)\n\n    self.database = database_str\n    base_extensions: list[NormalisedExtension] = _normalise_extensions(\n        extensions or []\n    )\n\n    self._ducklake_config: DuckLakeAttachmentConfig | None = None\n    self._ducklake_attachment: DuckLakeAttachmentManager | None = None\n    if ducklake is not None:\n        attachment_config, manager = build_ducklake_attachment(ducklake)\n        ensure_extensions_with_plugins(base_extensions, attachment_config.plugins)\n        self._ducklake_config = attachment_config\n        self._ducklake_attachment = manager\n\n    self.extensions = base_extensions\n\n    # Auto-add hashfuncs extension if not present (needed for default XXHASH64)\n    # But we'll fall back to MD5 if hashfuncs is not available\n    extension_names: list[str] = []\n    for ext in self.extensions:\n        if isinstance(ext, str):\n            extension_names.append(ext)\n        elif isinstance(ext, ExtensionSpec):\n            extension_names.append(ext.name)\n        else:\n            # After _normalise_extensions, this should not happen\n            # But keep defensive check for type safety\n            raise TypeError(\n                f\"Extension must be str or ExtensionSpec after normalization; got {type(ext)}\"\n            )\n    if \"hashfuncs\" not in extension_names:\n        self.extensions.append(\"hashfuncs\")\n\n    # Initialize Ibis store with DuckDB backend\n    super().__init__(\n        backend=\"duckdb\",\n        connection_params=connection_params,\n        fallback_stores=fallback_stores,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-attributes","title":"Attributes","text":""},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.sqlalchemy_url","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.sqlalchemy_url  <code>property</code>","text":"<pre><code>sqlalchemy_url: str\n</code></pre> <p>Get SQLAlchemy-compatible connection URL for DuckDB.</p> <p>Constructs a DuckDB SQLAlchemy URL from the database parameter.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SQLAlchemy-compatible URL string (e.g., \"duckdb:///path/to/db.db\")</p> </li> </ul> Example <pre><code>store = DuckDBMetadataStore(\":memory:\")\nprint(store.sqlalchemy_url)  # duckdb:///:memory:\n\nstore = DuckDBMetadataStore(\"metadata.db\")\nprint(store.sqlalchemy_url)  # duckdb:///metadata.db\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment  <code>property</code>","text":"<pre><code>ducklake_attachment: DuckLakeAttachmentManager\n</code></pre> <p>DuckLake attachment manager (raises if not configured).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment_config","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.ducklake_attachment_config  <code>property</code>","text":"<pre><code>ducklake_attachment_config: DuckLakeAttachmentConfig\n</code></pre> <p>DuckLake attachment configuration (raises if not configured).</p>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.open","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.open","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open DuckDB connection with specified access mode.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode (READ or WRITE). Defaults to READ. READ mode sets read_only=True for concurrent access.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:\n    \"\"\"Open DuckDB connection with specified access mode.\n\n    Args:\n        mode: Access mode (READ or WRITE). Defaults to READ.\n            READ mode sets read_only=True for concurrent access.\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    # Setup: Configure connection params based on mode\n    if mode == \"read\":\n        self.connection_params[\"read_only\"] = True\n    else:\n        # Remove read_only if present (switching to WRITE)\n        self.connection_params.pop(\"read_only\", None)\n\n    # Call parent context manager to establish connection\n    with super().open(mode):\n        try:\n            # Configure DuckLake if needed (only on first entry)\n            if self._ducklake_attachment is not None and self._context_depth == 1:\n                duckdb_conn = self._duckdb_raw_connection()\n                self._ducklake_attachment.configure(duckdb_conn)\n\n            yield self\n        finally:\n            # Cleanup is handled by parent's finally block\n            pass\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.preview_ducklake_sql","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.preview_ducklake_sql","text":"<pre><code>preview_ducklake_sql() -&gt; list[str]\n</code></pre> <p>Return DuckLake attachment SQL if configured.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>def preview_ducklake_sql(self) -&gt; list[str]:\n    \"\"\"Return DuckLake attachment SQL if configured.\"\"\"\n    return self.ducklake_attachment.preview_sql()\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckDBMetadataStore.config_model","title":"metaxy.metadata_store.duckdb.DuckDBMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[DuckDBMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/duckdb.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[DuckDBMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return DuckDBMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.ExtensionSpec","title":"metaxy.metadata_store.duckdb.ExtensionSpec  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DuckDB extension specification accepted by DuckDBMetadataStore.</p> <p>Supports additional keys for forward compatibility.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"DuckDB extension specification accepted by DuckDBMetadataStore.\\n\\nSupports additional keys for forward compatibility.\",\n  \"properties\": {\n    \"name\": {\n      \"title\": \"Name\",\n      \"type\": \"string\"\n    },\n    \"repository\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Repository\"\n    }\n  },\n  \"required\": [\n    \"name\"\n  ],\n  \"title\": \"ExtensionSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>name</code>                 (<code>str</code>)             </li> <li> <code>repository</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store.duckdb.DuckLakeConfigInput","title":"metaxy.metadata_store.duckdb.DuckLakeConfigInput  <code>module-attribute</code>","text":"<pre><code>DuckLakeConfigInput = DuckLakeAttachmentConfig | Mapping[str, Any]\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig","title":"metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration payload used to attach DuckLake to a DuckDB connection.</p> Show JSON schema: <pre><code>{\n  \"additionalProperties\": true,\n  \"description\": \"Configuration payload used to attach DuckLake to a DuckDB connection.\",\n  \"properties\": {\n    \"metadata_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Metadata Backend\",\n      \"type\": \"object\"\n    },\n    \"storage_backend\": {\n      \"additionalProperties\": true,\n      \"title\": \"Storage Backend\",\n      \"type\": \"object\"\n    },\n    \"alias\": {\n      \"default\": \"ducklake\",\n      \"title\": \"Alias\",\n      \"type\": \"string\"\n    },\n    \"plugins\": {\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Plugins\",\n      \"type\": \"array\"\n    },\n    \"attach_options\": {\n      \"additionalProperties\": true,\n      \"title\": \"Attach Options\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"metadata_backend\",\n    \"storage_backend\"\n  ],\n  \"title\": \"DuckLakeAttachmentConfig\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>arbitrary_types_allowed</code>: <code>True</code></li> <li><code>extra</code>: <code>allow</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metadata_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>storage_backend</code>                 (<code>DuckLakeBackend</code>)             </li> <li> <code>alias</code>                 (<code>str</code>)             </li> <li> <code>plugins</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>attach_options</code>                 (<code>dict[str, Any]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_coerce_backends</code>                 \u2192                   <code>metadata_backend</code>,                   <code>storage_backend</code> </li> <li> <code>_coerce_alias</code>                 \u2192                   <code>alias</code> </li> <li> <code>_coerce_plugins</code>                 \u2192                   <code>plugins</code> </li> <li> <code>_coerce_attach_options</code>                 \u2192                   <code>attach_options</code> </li> </ul>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.metadata_sql_parts","title":"metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.metadata_sql_parts","text":"<pre><code>metadata_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed metadata SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef metadata_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed metadata SQL components for DuckLake attachments.\"\"\"\n    return resolve_metadata_backend(self.metadata_backend, self.alias)\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/api/#metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.storage_sql_parts","title":"metaxy.metadata_store._ducklake_support.DuckLakeAttachmentConfig.storage_sql_parts","text":"<pre><code>storage_sql_parts() -&gt; tuple[str, str]\n</code></pre> <p>Pre-computed storage SQL components for DuckLake attachments.</p> Source code in <code>src/metaxy/metadata_store/_ducklake_support.py</code> <pre><code>@computed_field(return_type=tuple[str, str])\ndef storage_sql_parts(self) -&gt; tuple[str, str]:\n    \"\"\"Pre-computed storage SQL components for DuckLake attachments.\"\"\"\n    return resolve_storage_backend(self.storage_backend, self.alias)\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/","title":"DuckDB Configuration","text":""},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#connection_string","title":"<code>connection_string</code>","text":"<p>Ibis connection string (e.g., 'clickhouse://host:9000/db').</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_string = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_STRING=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#connection_params","title":"<code>connection_params</code>","text":"<p>Backend-specific connection parameters.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connection_params = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECTION_PARAMS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#table_prefix","title":"<code>table_prefix</code>","text":"<p>Optional prefix for all table names.</p> <p>Type: <code>str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# table_prefix = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__TABLE_PREFIX=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>If True, create tables on open. For development/testing only.</p> <p>Type: <code>bool | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# auto_create_tables = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__AUTO_CREATE_TABLES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#database","title":"<code>database</code>","text":"<p>Database path (:memory:, file path, or md:database).</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# database = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# database = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DATABASE=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#config","title":"<code>config</code>","text":"<p>DuckDB configuration settings (e.g., {'threads': '4'}).</p> <p>Type: <code>dict[str, str | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# config = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# config = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONFIG=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#extensions","title":"<code>extensions</code>","text":"<p>DuckDB extensions to install and load on open.</p> <p>Type: <code>collections.abc.Sequence[str | metaxy.metadata_store.duckdb.ExtensionSpec | collections.abc.Mapping[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# extensions = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# extensions = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__EXTENSIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#ducklake","title":"<code>ducklake</code>","text":"<p>DuckLake attachment configuration.</p>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#metadata_backend","title":"<code>metadata_backend</code>","text":"<p>Type: <code>metaxy.metadata_store._ducklake_support.SupportsDuckLakeParts | dict[str, Any</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# metadata_backend = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# metadata_backend = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__METADATA_BACKEND=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#storage_backend","title":"<code>storage_backend</code>","text":"<p>Type: <code>metaxy.metadata_store._ducklake_support.SupportsDuckLakeParts | dict[str, Any</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# storage_backend = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# storage_backend = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__STORAGE_BACKEND=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#alias","title":"<code>alias</code>","text":"<p>Type: <code>str</code> | Default: <code>\"ducklake\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\nalias = \"ducklake\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ALIAS=ducklake\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#plugins","title":"<code>plugins</code>","text":"<p>Type: <code>tuple[str, ...]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# plugins = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# plugins = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__PLUGINS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/duckdb/configuration/#attach_options","title":"<code>attach_options</code>","text":"<p>Type: <code>dict[str, Any]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config.ducklake]\n# Optional\n# attach_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config.ducklake]\n# Optional\n# attach_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DUCKLAKE__ATTACH_OPTIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/","title":"Ibis Integration","text":"<p>Metaxy uses Ibis as a portable dataframe abstraction for SQL-based metadata stores. The <code>IbisMetadataStore</code> is the base class for all SQL-backed stores.</p>"},{"location":"integrations/metadata-stores/databases/ibis/#available-backends","title":"Available Backends","text":"<p>The following metadata stores are built on Ibis:</p> <ul> <li>DuckDB</li> <li>ClickHouse</li> <li>BigQuery</li> </ul>"},{"location":"integrations/metadata-stores/databases/ibis/#reference","title":"Reference","text":"<ul> <li>API</li> </ul>"},{"location":"integrations/metadata-stores/databases/ibis/api/","title":"Ibis Metadata Store API","text":""},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis","title":"metaxy.metadata_store.ibis","text":"<p>Ibis-based metadata store for SQL databases.</p> <p>Supports any SQL database that Ibis supports: - DuckDB, PostgreSQL, MySQL (local/embedded) - ClickHouse, Snowflake, BigQuery (cloud analytical) - And 20+ other backends</p>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore","title":"metaxy.metadata_store.ibis.IbisMetadataStore","text":"<pre><code>IbisMetadataStore(versioning_engine: VersioningEngineOptions = 'auto', connection_string: str | None = None, *, backend: str | None = None, connection_params: dict[str, Any] | None = None, table_prefix: str | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code>, <code>ABC</code></p> <p>Generic SQL metadata store using Ibis.</p> <p>Supports any Ibis backend that supports struct types, such as: DuckDB, PostgreSQL, ClickHouse, and others.</p> Warning <p>Backends without native struct support (e.g., SQLite) are NOT supported.</p> <p>Storage layout: - Each feature gets its own table: {feature}__{key} - System tables: metaxy__system__feature_versions, metaxy__system__migrations - Uses Ibis for cross-database compatibility</p> <p>Note: Uses MD5 hash by default for cross-database compatibility. DuckDBMetadataStore overrides this with dynamic algorithm detection. For other backends, override the calculator instance variable with backend-specific implementations.</p> Example <pre><code># ClickHouse\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# PostgreSQL\nstore = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\n\n# DuckDB (use DuckDBMetadataStore instead for better hash support)\nstore = IbisMetadataStore(\"duckdb:///metadata.db\")\n\nwith store:\n    store.write_metadata(MyFeature, df)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use. - \"auto\": Prefer the store's native engine, fall back to Polars if needed - \"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible - \"polars\": Always use the Polars engine</p> </li> <li> <code>connection_string</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis connection string (e.g., \"clickhouse://host:9000/db\") If provided, backend and connection_params are ignored.</p> </li> <li> <code>backend</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\") Used with connection_params for more control.</p> </li> <li> <code>connection_params</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Backend-specific connection parameters e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}</p> </li> <li> <code>table_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional prefix applied to all feature and system table names. Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL identifier when combined with the generated table name.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither connection_string nor backend is provided</p> </li> <li> <code>ImportError</code>             \u2013            <p>If Ibis or required backend driver not installed</p> </li> </ul> Example <pre><code># Using connection string\nstore = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n# Using backend + params\nstore = IbisMetadataStore(\n    backend=\"clickhouse\",\n    connection_params={\"host\": \"localhost\", \"port\": 9000}\n    )\n</code></pre> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def __init__(\n    self,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    connection_string: str | None = None,\n    *,\n    backend: str | None = None,\n    connection_params: dict[str, Any] | None = None,\n    table_prefix: str | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize Ibis metadata store.\n\n    Args:\n        versioning_engine: Which versioning engine to use.\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n            - \"polars\": Always use the Polars engine\n        connection_string: Ibis connection string (e.g., \"clickhouse://host:9000/db\")\n            If provided, backend and connection_params are ignored.\n        backend: Ibis backend name (e.g., \"clickhouse\", \"postgres\", \"duckdb\")\n            Used with connection_params for more control.\n        connection_params: Backend-specific connection parameters\n            e.g., {\"host\": \"localhost\", \"port\": 9000, \"database\": \"default\"}\n        table_prefix: Optional prefix applied to all feature and system table names.\n            Useful for logically separating environments (e.g., \"prod_\"). Must form a valid SQL\n            identifier when combined with the generated table name.\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n\n    Raises:\n        ValueError: If neither connection_string nor backend is provided\n        ImportError: If Ibis or required backend driver not installed\n\n    Example:\n        ```py\n        # Using connection string\n        store = IbisMetadataStore(\"clickhouse://user:pass@host:9000/db\")\n\n        # Using backend + params\n        store = IbisMetadataStore(\n            backend=\"clickhouse\",\n            connection_params={\"host\": \"localhost\", \"port\": 9000}\n            )\n        ```\n    \"\"\"\n    import ibis\n\n    self.connection_string = connection_string\n    self.backend = backend\n    self.connection_params = connection_params or {}\n    self._conn: ibis.BaseBackend | None = None\n    self._table_prefix = table_prefix or \"\"\n\n    super().__init__(\n        **kwargs,\n        versioning_engine=versioning_engine,\n        versioning_engine_cls=IbisVersioningEngine,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore-attributes","title":"Attributes","text":""},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.ibis_conn","title":"metaxy.metadata_store.ibis.IbisMetadataStore.ibis_conn  <code>property</code>","text":"<pre><code>ibis_conn: BaseBackend\n</code></pre> <p>Get Ibis backend connection.</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.conn","title":"metaxy.metadata_store.ibis.IbisMetadataStore.conn  <code>property</code>","text":"<pre><code>conn: BaseBackend\n</code></pre> <p>Get connection (alias for ibis_conn for consistency).</p> <p>Returns:</p> <ul> <li> <code>BaseBackend</code>           \u2013            <p>Active Ibis backend connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.sqlalchemy_url","title":"metaxy.metadata_store.ibis.IbisMetadataStore.sqlalchemy_url  <code>property</code>","text":"<pre><code>sqlalchemy_url: str\n</code></pre> <p>Get SQLAlchemy-compatible connection URL for tools like Alembic.</p> <p>Returns the connection string if available. If the store was initialized with backend + connection_params instead of a connection string, raises an error since constructing a proper URL is backend-specific.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SQLAlchemy-compatible URL string</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If connection_string is not available</p> </li> </ul> Example <pre><code>store = IbisMetadataStore(\"postgresql://user:pass@host:5432/db\")\nprint(store.sqlalchemy_url)  # postgresql://user:pass@host:5432/db\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.get_table_name","title":"metaxy.metadata_store.ibis.IbisMetadataStore.get_table_name","text":"<pre><code>get_table_name(key: FeatureKey) -&gt; str\n</code></pre> <p>Generate the storage table name for a feature or system table.</p> <p>Applies the configured table_prefix (if any) to the feature key's table name. Subclasses can override this method to implement custom naming logic.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to convert to storage table name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Storage table name with optional prefix applied.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def get_table_name(\n    self,\n    key: FeatureKey,\n) -&gt; str:\n    \"\"\"Generate the storage table name for a feature or system table.\n\n    Applies the configured table_prefix (if any) to the feature key's table name.\n    Subclasses can override this method to implement custom naming logic.\n\n    Args:\n        key: Feature key to convert to storage table name.\n\n    Returns:\n        Storage table name with optional prefix applied.\n    \"\"\"\n    base_name = key.table_name\n\n    return f\"{self._table_prefix}{base_name}\" if self._table_prefix else base_name\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.open","title":"metaxy.metadata_store.ibis.IbisMetadataStore.open","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open connection to database via Ibis.</p> <p>Subclasses should override this to add backend-specific initialization (e.g., loading extensions) and must call this method via super().open(mode).</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode. Subclasses may use this to set backend-specific connection parameters (e.g., <code>read_only</code> for DuckDB).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:\n    \"\"\"Open connection to database via Ibis.\n\n    Subclasses should override this to add backend-specific initialization\n    (e.g., loading extensions) and must call this method via super().open(mode).\n\n    Args:\n        mode: Access mode. Subclasses may use this to set backend-specific connection\n            parameters (e.g., `read_only` for DuckDB).\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    import ibis\n\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # Setup: Connect to database\n            if self.connection_string:\n                # Use connection string\n                self._conn = ibis.connect(self.connection_string)\n            else:\n                # Use backend + params\n                # Get backend-specific connect function\n                assert self.backend is not None, (\n                    \"backend must be set if connection_string is None\"\n                )\n                backend_module = getattr(ibis, self.backend)\n                self._conn = backend_module.connect(**self.connection_params)\n\n            # Mark store as open and validate\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            # Teardown: Close connection\n            if self._conn is not None:\n                # Ibis connections may not have explicit close method\n                # but setting to None releases resources\n                self._conn = None\n            self._is_open = False\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.write_metadata_to_store","title":"metaxy.metadata_store.ibis.IbisMetadataStore.write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame, **kwargs: Any) -&gt; None\n</code></pre> <p>Internal write implementation using Ibis.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TableNotFoundError</code>             \u2013            <p>If table doesn't exist and auto_create_tables is False</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation using Ibis.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n        **kwargs: Backend-specific parameters (currently unused)\n\n    Raises:\n        TableNotFoundError: If table doesn't exist and auto_create_tables is False\n    \"\"\"\n    if df.implementation == nw.Implementation.IBIS:\n        df_to_insert = df.to_native()  # Ibis expression\n    else:\n        from metaxy._utils import collect_to_polars\n\n        df_to_insert = collect_to_polars(df)  # Polars DataFrame\n\n    table_name = self.get_table_name(feature_key)\n\n    try:\n        self.conn.insert(table_name, obj=df_to_insert)  # type: ignore[attr-defined]  # pyright: ignore[reportAttributeAccessIssue]\n    except Exception as e:\n        import ibis.common.exceptions\n\n        if not isinstance(e, ibis.common.exceptions.TableNotFound):\n            raise\n        if self.auto_create_tables:\n            # Warn about auto-create (first time only)\n            if self._should_warn_auto_create_tables:\n                import warnings\n\n                warnings.warn(\n                    f\"AUTO_CREATE_TABLES is enabled - automatically creating table '{table_name}'. \"\n                    \"Do not use in production! \"\n                    \"Use proper database migration tools like Alembic for production deployments.\",\n                    UserWarning,\n                    stacklevel=4,\n                )\n\n            # Note: create_table(table_name, obj=df) both creates the table AND inserts the data\n            # No separate insert needed - the data from df is already written\n            self.conn.create_table(table_name, obj=df_to_insert)\n        else:\n            raise TableNotFoundError(\n                f\"Table '{table_name}' does not exist for feature {feature_key.to_string()}. \"\n                f\"Enable auto_create_tables=True to automatically create tables, \"\n                f\"or use proper database migration tools like Alembic to create the table first.\"\n            ) from e\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata_in_store","title":"metaxy.metadata_store.ibis.IbisMetadataStore.read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: CoercibleToFeatureKey, *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, **kwargs: Any) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version (applied as SQL WHERE clause)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions (converted to SQL WHERE clauses)</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version (applied as SQL WHERE clause)\n        filters: List of Narwhals filter expressions (converted to SQL WHERE clauses)\n        columns: Optional list of columns to select\n        **kwargs: Backend-specific parameters (currently unused)\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n    \"\"\"\n    feature_key = self._resolve_feature_key(feature)\n    table_name = self.get_table_name(feature_key)\n\n    # Check if table exists\n    existing_tables = self.conn.list_tables()\n    if table_name not in existing_tables:\n        return None\n\n    # Get Ibis table reference\n    table = self.conn.table(table_name)\n\n    # Wrap Ibis table with Narwhals (stays lazy in SQL)\n    nw_lazy: nw.LazyFrame[Any] = nw.from_native(table, eager_only=False)\n\n    # Apply feature_version filter (stays in SQL via Narwhals)\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters (stays in SQL)\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns (stays in SQL)\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Return Narwhals LazyFrame wrapping Ibis table (stays lazy in SQL)\n    return nw_lazy\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.display","title":"metaxy.metadata_store.ibis.IbisMetadataStore.display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    from metaxy.metadata_store.utils import sanitize_uri\n\n    backend_info = self.connection_string or f\"{self.backend}\"\n    # Sanitize connection strings that may contain credentials\n    sanitized_info = sanitize_uri(backend_info)\n    return f\"{self.__class__.__name__}(backend={sanitized_info})\"\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.get_store_metadata","title":"metaxy.metadata_store.ibis.IbisMetadataStore.get_store_metadata","text":"<pre><code>get_store_metadata(feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]\n</code></pre> <p>Return store metadata including table name.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get metadata for.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary with <code>table_name</code> key.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>def get_store_metadata(self, feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]:\n    \"\"\"Return store metadata including table name.\n\n    Args:\n        feature_key: Feature key to get metadata for.\n\n    Returns:\n        Dictionary with `table_name` key.\n    \"\"\"\n    resolved_key = self._resolve_feature_key(feature_key)\n    return {\"table_name\": self.get_table_name(resolved_key)}\n</code></pre>"},{"location":"integrations/metadata-stores/databases/ibis/api/#metaxy.metadata_store.ibis.IbisMetadataStore.config_model","title":"metaxy.metadata_store.ibis.IbisMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[IbisMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/ibis.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[IbisMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return IbisMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/","title":"Metaxy + LanceDB","text":"<p>Metaxy implements <code>LanceDBMetadataStore</code>. LanceDB keeps one Lance table per feature, writes metadata in append mode, and uses the in-memory Polars versioning engine for provenance calculations. LanceDB handles schema evolution, transactions, and compaction automatically.</p> <p>It runs embedded (local directory) or against external storage (object stores, HTTP endpoints, LanceDB Cloud), so you can use the same store for local development and remote compute.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#installation","title":"Installation","text":"<p>The backend relies on <code>lancedb</code>, which is shipped with Metaxy's <code>lancedb</code> extras.</p> <pre><code>pip install 'metaxy[lancedb]'\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-targets","title":"Storage Targets","text":"<p>Point <code>uri</code> at any supported URI (<code>s3://</code>, <code>gs://</code>, <code>az://</code>, <code>db://</code>, ...) and forward credentials with the platform's native mechanism (environment variables, IAM roles, workload identity, etc.). LanceDB supports local filesystem, S3, GCS, Azure, LanceDB Cloud, and remote HTTP/HTTPS endpoints.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#storage-layout","title":"Storage Layout","text":"<p>All tables are stored within a single LanceDB database at the configured URI location. Each feature gets its own Lance table.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/metadata-stores/databases/lancedb/api/","title":"LanceDB Metadata Store API","text":""},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb","title":"metaxy.metadata_store.lancedb","text":"<p>LanceDB metadata store implementation.</p>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore","text":"<pre><code>LanceDBMetadataStore(uri: str | Path, *, fallback_stores: list[MetadataStore] | None = None, connect_kwargs: dict[str, Any] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>LanceDB metadata store for vector and structured data.</p> <p>LanceDB is a columnar database optimized for vector search and multimodal data. Each feature is stored in its own Lance table within the database directory. Uses Polars components for data processing (no native SQL execution).</p> <p>Storage layout:</p> <ul> <li> <p>Each feature gets its own table: <code>{namespace}__{feature_name}</code></p> </li> <li> <p>Tables are stored as Lance format in the directory specified by the URI</p> </li> <li> <p>LanceDB handles schema evolution, transactions, and compaction automatically</p> </li> </ul> Local Directory <pre><code>from pathlib import Path\nfrom metaxy.metadata_store.lancedb import LanceDBMetadataStore\n\n# Local filesystem\nstore = LanceDBMetadataStore(Path(\"/path/to/featuregraph\"))\n</code></pre> Object Storage (S3, GCS, Azure) <pre><code># object store (requires credentials)\nstore = LanceDBMetadataStore(\"s3:///path/to/featuregraph\")\n</code></pre> LanceDB Cloud <pre><code>import os\n\n# Option 1: Environment variable\nos.environ[\"LANCEDB_API_KEY\"] = \"your-api-key\"\nstore = LanceDBMetadataStore(\"db://my-database\")\n\n# Option 2: Explicit credentials\nstore = LanceDBMetadataStore(\n    \"db://my-database\",\n    connect_kwargs={\"api_key\": \"your-api-key\", \"region\": \"us-east-1\"}\n)\n</code></pre> <p>The database directory is created automatically if it doesn't exist (local paths only). Tables are created on-demand when features are first written.</p> <p>Parameters:</p> <ul> <li> <code>uri</code>               (<code>str | Path</code>)           \u2013            <p>Directory path or URI for LanceDB tables. Supports:</p> <ul> <li> <p>Local path: <code>\"./metadata\"</code> or <code>Path(\"/data/metaxy/lancedb\")</code></p> </li> <li> <p>Object stores: <code>s3://</code>, <code>gs://</code>, <code>az://</code> (requires cloud credentials)</p> </li> <li> <p>LanceDB Cloud: <code>\"db://database-name\"</code> (requires API key)</p> </li> <li> <p>Remote HTTP/HTTPS: Any URI supported by LanceDB</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. When reading features not found in this store, Metaxy searches fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.</p> </li> <li> <code>connect_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra keyword arguments passed directly to lancedb.connect(). Useful for LanceDB Cloud credentials (api_key, region) when you cannot rely on environment variables.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to metaxy.metadata_store.base.MetadataStore (e.g., hash_algorithm, hash_truncation_length, prefer_native)</p> </li> </ul> Note <p>Unlike SQL stores, LanceDB doesn't require explicit table creation. Tables are created automatically when writing metadata.</p> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>def __init__(\n    self,\n    uri: str | Path,\n    *,\n    fallback_stores: list[MetadataStore] | None = None,\n    connect_kwargs: dict[str, Any] | None = None,\n    **kwargs: Any,\n):\n    \"\"\"\n    Initialize [LanceDB](https://lancedb.com/docs/) metadata store.\n\n    The database directory is created automatically if it doesn't exist (local paths only).\n    Tables are created on-demand when features are first written.\n\n    Args:\n        uri: Directory path or URI for LanceDB tables. Supports:\n\n            - **Local path**: `\"./metadata\"` or `Path(\"/data/metaxy/lancedb\")`\n\n            - **Object stores**: `s3://`, `gs://`, `az://` (requires cloud credentials)\n\n            - **LanceDB Cloud**: `\"db://database-name\"` (requires API key)\n\n            - **Remote HTTP/HTTPS**: Any URI supported by LanceDB\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            When reading features not found in this store, Metaxy searches\n            fallback stores in order. Useful for local dev \u2192 staging \u2192 production chains.\n        connect_kwargs: Extra keyword arguments passed directly to\n            [lancedb.connect()](https://lancedb.github.io/lancedb/python/python/#lancedb.connect).\n            Useful for LanceDB Cloud credentials (api_key, region) when you cannot\n            rely on environment variables.\n        **kwargs: Passed to [metaxy.metadata_store.base.MetadataStore][]\n            (e.g., hash_algorithm, hash_truncation_length, prefer_native)\n\n    Note:\n        Unlike SQL stores, LanceDB doesn't require explicit table creation.\n        Tables are created automatically when writing metadata.\n    \"\"\"\n    self.uri: str = str(uri)\n    self._conn: Any | None = None\n    self._connect_kwargs = connect_kwargs or {}\n    super().__init__(\n        fallback_stores=fallback_stores,\n        auto_create_tables=True,\n        versioning_engine_cls=PolarsVersioningEngine,\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore-attributes","title":"Attributes","text":""},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.conn","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.conn  <code>property</code>","text":"<pre><code>conn: Any\n</code></pre> <p>Get LanceDB connection.</p> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Active LanceDB connection</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.open","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.open","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open LanceDB connection.</p> <p>For local filesystem paths, creates the directory if it doesn't exist. For remote URIs (S3, LanceDB Cloud, etc.), connects directly. Tables are created on-demand when features are first written.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode (READ or WRITE). Accepted for consistency but not used by LanceDB (LanceDB handles concurrent access internally).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ConnectionError</code>             \u2013            <p>If remote connection fails (e.g., invalid credentials)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:\n    \"\"\"Open LanceDB connection.\n\n    For local filesystem paths, creates the directory if it doesn't exist.\n    For remote URIs (S3, LanceDB Cloud, etc.), connects directly.\n    Tables are created on-demand when features are first written.\n\n    Args:\n        mode: Access mode (READ or WRITE). Accepted for consistency but not used\n            by LanceDB (LanceDB handles concurrent access internally).\n\n    Yields:\n        Self: The store instance\n\n    Raises:\n        ConnectionError: If remote connection fails (e.g., invalid credentials)\n    \"\"\"\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            import lancedb\n\n            if is_local_path(self.uri):\n                Path(self.uri).mkdir(parents=True, exist_ok=True)\n\n            self._conn = lancedb.connect(self.uri, **self._connect_kwargs)\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            self._conn = None\n            self._is_open = False\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.write_metadata_to_store","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame, **kwargs: Any) -&gt; None\n</code></pre> <p>Append metadata to Lance table.</p> <p>Creates the table if it doesn't exist, otherwise appends to existing table. Uses LanceDB's native Polars/Arrow integration for efficient storage.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>Narwhals Frame with metadata (already validated by base class)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Append metadata to Lance table.\n\n    Creates the table if it doesn't exist, otherwise appends to existing table.\n    Uses LanceDB's native Polars/Arrow integration for efficient storage.\n\n    Args:\n        feature_key: Feature key to write to\n        df: Narwhals Frame with metadata (already validated by base class)\n    \"\"\"\n    # Convert Narwhals frame to Polars DataFrame\n    df_polars = collect_to_polars(df)\n\n    table_name = self._table_name(feature_key)\n\n    # LanceDB supports both Polars DataFrames and Arrow tables directly\n    # Try Polars first (native integration), fall back to Arrow if needed\n    try:\n        if self._table_exists(table_name):\n            table = self._get_table(table_name)\n            # Use Polars DataFrame directly - LanceDB handles conversion\n            table.add(df_polars)  # type: ignore[attr-defined]\n        else:\n            # Create table from Polars DataFrame - LanceDB handles schema\n            self.conn.create_table(table_name, data=df_polars)  # type: ignore[attr-defined]\n    except TypeError as exc:\n        if not self._should_fallback_to_arrow(exc):\n            raise\n        # Defensive fallback: Modern LanceDB (&gt;=0.3) accepts Polars DataFrames natively,\n        # but fall back to Arrow if an older version or edge case doesn't support it.\n        # This ensures compatibility across LanceDB versions.\n        logger.debug(\"Falling back to Arrow format for LanceDB write: %s\", exc)\n        arrow_table = df_polars.to_arrow()\n        if self._table_exists(table_name):\n            table = self._get_table(table_name)\n            table.add(arrow_table)  # type: ignore[attr-defined]\n        else:\n            self.conn.create_table(table_name, data=arrow_table)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.read_metadata_in_store","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: CoercibleToFeatureKey, *, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, **kwargs: Any) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from Lance table.</p> <p>Loads data from Lance, converts to Polars, and returns as Narwhals LazyFrame. Applies filters and column selection in memory.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (unused)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if table not found</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"Read metadata from Lance table.\n\n    Loads data from Lance, converts to Polars, and returns as Narwhals LazyFrame.\n    Applies filters and column selection in memory.\n\n    Args:\n        feature: Feature to read\n        filters: List of Narwhals filter expressions\n        columns: Optional list of columns to select\n        **kwargs: Backend-specific parameters (unused)\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if table not found\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    table_name = self._table_name(feature_key)\n    if not self._table_exists(table_name):\n        return None\n\n    table = self._get_table(table_name)\n    # https://github.com/lancedb/lancedb/issues/1539\n    # Fall back to eager Arrow conversion until LanceDB issue #1539 is resolved.\n    arrow_table = table.to_arrow()\n    pl_lazy = pl.DataFrame(arrow_table).lazy()\n    nw_lazy = nw.from_native(pl_lazy)\n\n    if filters:\n        nw_lazy = nw_lazy.filter(*filters)\n\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    return nw_lazy\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.display","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Human-readable representation with sanitized credentials.</p> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Human-readable representation with sanitized credentials.\"\"\"\n    path = sanitize_uri(self.uri)\n    return f\"LanceDBMetadataStore(path={path})\"\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/api/#metaxy.metadata_store.lancedb.LanceDBMetadataStore.config_model","title":"metaxy.metadata_store.lancedb.LanceDBMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[LanceDBMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/lancedb.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[LanceDBMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return LanceDBMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/configuration/","title":"LanceDB Configuration","text":""},{"location":"integrations/metadata-stores/databases/lancedb/configuration/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/configuration/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/configuration/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/configuration/#uri","title":"<code>uri</code>","text":"<p>Directory path or URI for LanceDB tables.</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# uri = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# uri = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__URI=...\n</code></pre>"},{"location":"integrations/metadata-stores/databases/lancedb/configuration/#connect_kwargs","title":"<code>connect_kwargs</code>","text":"<p>Extra keyword arguments passed to lancedb.connect().</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# connect_kwargs = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# connect_kwargs = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__CONNECT_KWARGS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/","title":"Storage","text":""},{"location":"integrations/metadata-stores/storage/#storage-only","title":"Storage Only","text":"<p>These metadata stores only provide storage and rely on local (also referred to as embedded) compute.</p> <p>Example</p> <p>DeltaLake is an excellent choice for a storage-only metadata store.</p>"},{"location":"integrations/metadata-stores/storage/#available-metadata-stores","title":"Available Metadata Stores","text":"<ul> <li>Metaxy + Delta Lake</li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/","title":"Metaxy + Delta Lake","text":"<p>Metaxy implements <code>DeltaMetadataStore</code> that stores metadata in Delta Lake (also known as a LakeHouse format) and uses an in-memory Polars versioning engine.</p> <p>It supports the local filesystem and remote object stores.</p>"},{"location":"integrations/metadata-stores/storage/delta/#installation","title":"Installation","text":"<pre><code>pip install 'metaxy[delta]'\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/#using-object-stores","title":"Using Object Stores","text":"<p>Point <code>root_path</code> at any supported URI (<code>s3://</code>, <code>abfss://</code>, <code>gs://</code>, ...) and forward credentials with <code>storage_options</code>. The dict is passed verbatim to <code>deltalake</code>.</p> <p>Learn more in the API docs.</p>"},{"location":"integrations/metadata-stores/storage/delta/#storage-layout","title":"Storage Layout","text":"<p>It's possible to control how feature keys map to DeltaLake table locations with the <code>layout</code> parameter:</p> <ul> <li><code>nested</code> (default) places every feature in its own directory: <code>your/feature/key.delta</code></li> <li><code>flat</code> stores all of them in the same directory: <code>your__feature_key.delta</code></li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/metadata-stores/storage/delta/api/","title":"Delta Lake Metadata Store API","text":""},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta","title":"metaxy.metadata_store.delta","text":"<p>Delta Lake metadata store implemented with delta-rs.</p>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore","title":"metaxy.metadata_store.delta.DeltaMetadataStore","text":"<pre><code>DeltaMetadataStore(root_path: str | Path, *, storage_options: dict[str, Any] | None = None, fallback_stores: list[MetadataStore] | None = None, layout: Literal['flat', 'nested'] = 'nested', delta_write_options: dict[str, Any] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>Delta Lake metadata store backed by delta-rs.</p> <p>It stores feature metadata in Delta Lake tables located under <code>root_path</code>. It uses the Polars versioning engine for provenance calculations.</p> <p>Example:</p> <pre><code>```py\nfrom metaxy.metadata_store.delta import DeltaMetadataStore\n\nstore = DeltaMetadataStore(\n    root_path=\"s3://my-bucket/metaxy\",\n    storage_options={\"AWS_REGION\": \"us-west-2\"},\n)\n```\n</code></pre> <p>Parameters:</p> <ul> <li> <code>root_path</code>               (<code>str | Path</code>)           \u2013            <p>Base directory or URI where feature tables are stored. Supports local paths (<code>/path/to/dir</code>), <code>s3://</code> URLs, and other object store URIs.</p> </li> <li> <code>storage_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Storage backend options passed to delta-rs. Example: <code>{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}</code> See https://delta-io.github.io/delta-rs/ for details on supported options.</p> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores.</p> </li> <li> <code>layout</code>               (<code>Literal['flat', 'nested']</code>, default:                   <code>'nested'</code> )           \u2013            <p>Directory layout for feature tables. Options:</p> <ul> <li> <p><code>\"nested\"</code>: Feature tables stored in nested directories <code>{part1}/{part2}.delta</code></p> </li> <li> <p><code>\"flat\"</code>: Feature tables stored as <code>{part1}__{part2}.delta</code></p> </li> </ul> </li> <li> <code>delta_write_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional options passed to deltalake.write_deltalake() - see https://delta-io.github.io/delta-rs/upgrade-guides/guide-1.0.0/#write_deltalake-api. Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Forwarded to metaxy.metadata_store.base.MetadataStore.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def __init__(\n    self,\n    root_path: str | Path,\n    *,\n    storage_options: dict[str, Any] | None = None,\n    fallback_stores: list[MetadataStore] | None = None,\n    layout: Literal[\"flat\", \"nested\"] = \"nested\",\n    delta_write_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize Delta Lake metadata store.\n\n    Args:\n        root_path: Base directory or URI where feature tables are stored.\n            Supports local paths (`/path/to/dir`), `s3://` URLs, and other object store URIs.\n        storage_options: Storage backend options passed to delta-rs.\n            Example: `{\"AWS_REGION\": \"us-west-2\", \"AWS_ACCESS_KEY_ID\": \"...\", ...}`\n            See https://delta-io.github.io/delta-rs/ for details on supported options.\n        fallback_stores: Ordered list of read-only fallback stores.\n        layout: Directory layout for feature tables. Options:\n\n            - `\"nested\"`: Feature tables stored in nested directories `{part1}/{part2}.delta`\n\n            - `\"flat\"`: Feature tables stored as `{part1}__{part2}.delta`\n\n        delta_write_options: Additional options passed to deltalake.write_deltalake() - see https://delta-io.github.io/delta-rs/upgrade-guides/guide-1.0.0/#write_deltalake-api.\n            Overrides default {\"schema_mode\": \"merge\"}. Example: {\"max_workers\": 4}\n        **kwargs: Forwarded to [metaxy.metadata_store.base.MetadataStore][metaxy.metadata_store.base.MetadataStore].\n    \"\"\"\n    self.storage_options = storage_options or {}\n    if layout not in (\"flat\", \"nested\"):\n        raise ValueError(f\"Invalid layout: {layout}. Must be 'flat' or 'nested'.\")\n    self.layout = layout\n    self.delta_write_options = delta_write_options or {}\n\n    root_str = str(root_path)\n    self._is_remote = not is_local_path(root_str)\n\n    if self._is_remote:\n        # Remote path (S3, Azure, GCS, etc.)\n        self._root_uri = root_str.rstrip(\"/\")\n    else:\n        # Local path (including file:// and local:// URLs)\n        if root_str.startswith(\"file://\"):\n            # Strip file:// prefix\n            root_str = root_str[7:]\n        elif root_str.startswith(\"local://\"):\n            # Strip local:// prefix\n            root_str = root_str[8:]\n        local_path = Path(root_str).expanduser().resolve()\n        self._root_uri = str(local_path)\n\n    super().__init__(\n        fallback_stores=fallback_stores,\n        versioning_engine_cls=PolarsVersioningEngine,\n        versioning_engine=\"polars\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore-attributes","title":"Attributes","text":""},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.default_delta_write_options","title":"metaxy.metadata_store.delta.DeltaMetadataStore.default_delta_write_options  <code>cached</code> <code>property</code>","text":"<pre><code>default_delta_write_options: dict[str, Any]\n</code></pre> <p>Default write options for Delta Lake operations.</p> <p>Merges base defaults with user-provided delta_write_options. Base defaults: mode=\"append\", schema_mode=\"merge\", storage_options.</p>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore-functions","title":"Functions","text":""},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.open","title":"metaxy.metadata_store.delta.DeltaMetadataStore.open","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open the Delta Lake store.</p> <p>Delta-rs opens connections lazily per operation, so no connection state management needed.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode for this connection session (accepted for consistency but not used).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:  # noqa: ARG002\n    \"\"\"Open the Delta Lake store.\n\n    Delta-rs opens connections lazily per operation, so no connection state management needed.\n\n    Args:\n        mode: Access mode for this connection session (accepted for consistency but not used).\n\n    Yields:\n        Self: The store instance with connection open\n    \"\"\"\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # Mark store as open and validate\n            # Note: Delta auto-creates tables on first write, no need to pre-create them\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            self._is_open = False\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.write_metadata_to_store","title":"metaxy.metadata_store.delta.DeltaMetadataStore.write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame, **kwargs: Any) -&gt; None\n</code></pre> <p>Append metadata to the Delta table for a feature.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>DataFrame with metadata (already validated)</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Append metadata to the Delta table for a feature.\n\n    Args:\n        feature_key: Feature key to write to\n        df: DataFrame with metadata (already validated)\n        **kwargs: Backend-specific parameters (currently unused)\n    \"\"\"\n    table_uri = self._feature_uri(feature_key)\n\n    # Delta Lake auto-creates tables on first write, no need to check existence\n    # Convert to Polars and collect lazy frames\n    df_polars = switch_implementation_to_polars(df)\n\n    # Collect lazy frames, keep eager frames as-is\n    if isinstance(df_polars, nw.LazyFrame):\n        df_native = df_polars.collect().to_native()\n    else:\n        df_native = df_polars.to_native()\n\n    assert isinstance(df_native, pl.DataFrame)\n\n    # Cast Enum columns to String to avoid delta-rs Utf8View incompatibility\n    # (delta-rs parquet writer cannot handle Utf8View dictionary values)\n    df_native = df_native.with_columns(pl.selectors.by_dtype(pl.Enum).cast(pl.Utf8))\n\n    # Prepare write parameters for Polars write_delta\n    # Extract mode and storage_options as top-level parameters\n    write_opts = self.default_delta_write_options.copy()\n    mode = write_opts.pop(\"mode\", \"append\")\n    storage_options = write_opts.pop(\"storage_options\", None)\n\n    # Write using Polars DataFrame.write_delta\n    df_native.write_delta(\n        table_uri,\n        mode=mode,\n        storage_options=storage_options,\n        delta_write_options=write_opts or None,\n    )\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.read_metadata_in_store","title":"metaxy.metadata_store.delta.DeltaMetadataStore.read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: CoercibleToFeatureKey, *, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, **kwargs: Any) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata stored in Delta for a single feature using lazy evaluation.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"Read metadata stored in Delta for a single feature using lazy evaluation.\n\n    Args:\n        feature: Feature to read metadata for\n        filters: List of Narwhals filter expressions\n        columns: Subset of columns to return\n        **kwargs: Backend-specific parameters (currently unused)\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    table_uri = self._feature_uri(feature_key)\n    if not self._table_exists(table_uri):\n        return None\n\n    # Use scan_delta for lazy evaluation\n    lf = pl.scan_delta(\n        table_uri,\n        storage_options=self.storage_options or None,\n    )\n\n    # Convert to Narwhals\n    nw_lazy = nw.from_native(lf)\n\n    # Apply filters (unpack list, skip if empty)\n    if filters:\n        nw_lazy = nw_lazy.filter(*filters)\n\n    # Apply column selection\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    return nw_lazy\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.display","title":"metaxy.metadata_store.delta.DeltaMetadataStore.display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return human-readable representation of the store.</p> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Return human-readable representation of the store.\"\"\"\n    details = [f\"path={self._root_uri}\"]\n    details.append(f\"layout={self.layout}\")\n    return f\"DeltaMetadataStore({', '.join(details)})\"\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.get_store_metadata","title":"metaxy.metadata_store.delta.DeltaMetadataStore.get_store_metadata","text":"<pre><code>get_store_metadata(feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]\n</code></pre> <p>Arbitrary key-value pairs with useful metadata like path in storage.</p> <p>Useful for logging purposes. This method should not expose sensitive information.</p> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>def get_store_metadata(self, feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]:\n    return {\"uri\": self._feature_uri(self._resolve_feature_key(feature_key))}\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/api/#metaxy.metadata_store.delta.DeltaMetadataStore.config_model","title":"metaxy.metadata_store.delta.DeltaMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[DeltaMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/delta.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[DeltaMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return DeltaMetadataStoreConfig\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/","title":"Delta Lake Configuration","text":""},{"location":"integrations/metadata-stores/storage/delta/configuration/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#root_path","title":"<code>root_path</code>","text":"<p>Base directory or URI where feature tables are stored.</p> <p>Type: <code>str | pathlib.Path</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# root_path = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# root_path = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__ROOT_PATH=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#storage_options","title":"<code>storage_options</code>","text":"<p>Storage backend options passed to delta-rs.</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# storage_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# storage_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__STORAGE_OPTIONS=...\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#layout","title":"<code>layout</code>","text":"<p>Directory layout for feature tables ('nested' or 'flat').</p> <p>Type: <code>Literal['flat', 'nested']</code> | Default: <code>\"nested\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nlayout = \"nested\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__LAYOUT=nested\n</code></pre>"},{"location":"integrations/metadata-stores/storage/delta/configuration/#delta_write_options","title":"<code>delta_write_options</code>","text":"<p>Options passed to deltalake.write_deltalake().</p> <p>Type: <code>dict[str, Any | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# delta_write_options = {}\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# delta_write_options = {}\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__DELTA_WRITE_OPTIONS=...\n</code></pre>"},{"location":"integrations/orchestration/","title":"Metaxy + Orchestrators","text":"<p>Metaxy can manage feature metadata, but it's not opinionated about how and where to materialize the data itself.</p> <p>This task is typically handled by an orchestrator.</p>"},{"location":"integrations/orchestration/#dagster","title":"Dagster","text":"<p>Metaxy has been built with the Dagster integration in mind from the very beginning. Dagster is an excellent choice for managing Metaxy jobs.</p>"},{"location":"integrations/orchestration/dagster/","title":"Metaxy + Dagster","text":"<p>Metaxy's dependency system has been originally inspired by Dagster.</p> <p>Because of this, Metaxy code can be naturally composed with Dagster code, Metaxy concepts map directly into Dagster concepts, and the provided <code>@metaxify</code> decorator makes this process effortless.</p> <p>The only step that has to be taken in order to inject Metaxy into Dagster assets is to associate the Dagster asset with the Metaxy feature.</p> <p>Unleash the full power of <code>@metaxify</code> on Dagster!</p> <p>Example</p> <p>Set the well-known <code>\"metaxy/feature\"</code> key (1):</p> <ol> <li> point it to... the Metaxy feature key!</li> </ol> <pre><code>import dagster as dg\nimport metaxy.ext.dagster as mxd\n\n@mxd.metaxify()\n@dg.asset(metadata={\"metaxy/feature\": \"my/metaxy/feature\"})\ndef my_asset():\n  ...\n</code></pre> <p><code>@metaxify</code> will take care of injecting information (such as asset dependencies or metadata) from the Metaxy feature to the Dagster asset. Learn more about <code>@metaxify</code> (with example screenshots) here.</p>"},{"location":"integrations/orchestration/dagster/#whats-in-the-box","title":"What's in the box","text":"<p>This integration provides:</p> <ul> <li> <p><code>metaxify</code> - a decorator that enriches Dagster asset definitions with Metaxy information such as upstream dependencies, description, metadata, code version, table schema, column lineage, and so on. More info and some screenshots here.</p> </li> <li> <p><code>MetaxyIOManager</code> - an IO manager that reads and writes Dagster assets that are Metaxy features and logs useful runtime metadata.</p> </li> <li> <p><code>MetaxyStoreFromConfigResource</code> - a resource that provides access to <code>MetadataStore</code></p> </li> <li> <p><code>generate_materialize_results</code> / <code>generate_observe_results</code> - generators for yielding <code>dagster.MaterializeResult</code> or <code>dagster.ObserveResult</code> events from Dagster assets (and multi-assets), with automatic topological ordering, partition filtering, logging row counts, and setting Dagster data versions.</p> </li> <li> <p><code>observable_metaxy_asset</code> - a decorator that creates observable source assets for monitoring external Metaxy features.</p> </li> </ul>"},{"location":"integrations/orchestration/dagster/#quick-start","title":"Quick Start","text":""},{"location":"integrations/orchestration/dagster/#1-define-metaxy-features","title":"1. Define Metaxy Features","text":"defs.py<pre><code># Upstream feature\nupstream_spec = mx.FeatureSpec(\n    key=\"audio/embeddings\",\n    id_columns=[\"audio_id\"],\n    fields=[\"embedding\"],\n)\n\n\nclass AudioEmbeddings(mx.BaseFeature, spec=upstream_spec):\n    audio_id: str\n\n\n# Downstream feature that depends on upstream\ndownstream_spec = mx.FeatureSpec(\n    key=\"audio/clusters\",\n    id_columns=[\"audio_id\"],\n    deps=[AudioEmbeddings],\n)\n\n\nclass AudioClusters(mx.BaseFeature, spec=downstream_spec):\n    audio_id: str\n    mean: float\n    std: float\n</code></pre>"},{"location":"integrations/orchestration/dagster/#2-define-dagster-assets","title":"2. Define Dagster Assets","text":"<p>Root Asset</p> <p>Let's define an asset that doesn't have any upstream Metaxy features.</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/embeddings\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_embeddings(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    # somehow, acquire root source data\n    samples = pl.DataFrame(\n        {\n            \"audio_id\": [\"a1\", \"a2\", \"a3\"],\n            \"metaxy_provenance_by_field\": [\n                {\"embedding\": \"hash1\"},\n                {\"embedding\": \"hash2\"},\n                {\"embedding\": \"hash3\"},\n            ],\n        }\n    )\n\n    # resolve the increment with Metaxy\n\n    with store:\n        increment = store.resolve_update(\"audio/embeddings\", samples=samples)\n\n    # Compute embeddings...\n\n    df = ...  # at this point this dataframe should have `mean` and `std` columns set\n\n    # either write embeddings metadata via Metaxy\n    # or return a dataframe to write it via MetaxyIOManager\n\n    return df\n</code></pre> <p>Downstream Asset</p> defs.py<pre><code>@mxd.metaxify\n@dg.asset(\n    metadata={\"metaxy/feature\": \"audio/clusters\"},\n    io_manager_key=\"metaxy_io_manager\",\n)\ndef audio_clusters(\n    store: dg.ResourceParam[mx.MetadataStore],\n):\n    with store:\n        # Get IDs that need recomputation\n        update = store.resolve_update(AudioClusters)\n    ...\n</code></pre> <p>Non-Metaxy Downstream Asset</p> <pre><code>@dg.asset(\n    ins={\n        \"clusters\": dg.AssetIn(\n            key=[\"audio\", \"clusters\"],\n        )\n    },\n)\ndef cluster_report(clusters: nw.LazyFrame):\n    # clusters is a narwhals LazyFrame loaded via MetaxyIOManager\n    df = clusters.collect().to_polars()\n    # Generate a report...\n    return {\"total_clusters\": df.select(\"cluster_id\").n_unique()}\n</code></pre>"},{"location":"integrations/orchestration/dagster/#3-create-dagster-definitions","title":"3. Create Dagster Definitions","text":"defs.py<pre><code>store = mxd.MetaxyStoreFromConfigResource(name=\"dev\")\nmetaxy_io_manager = mxd.MetaxyIOManager(store=store)\n\n\n@dg.definitions\ndef definitions():\n    mx.init_metaxy()  # (1)!\n\n    return dg.Definitions(\n        assets=[\n            audio_embeddings,\n            audio_clusters,\n            cluster_report,\n        ],\n        resources={\n            \"store\": store,\n            \"metaxy_io_manager\": metaxy_io_manager,\n        },\n    )\n</code></pre> <ol> <li>This loads Metaxy configuration and feature definitions</li> </ol>"},{"location":"integrations/orchestration/dagster/#4-start-dagster","title":"4. Start Dagster","text":"<pre><code>dg dev -f defs.py\n</code></pre> <p>Materialize your assets and let Metaxy take care of state and versioning!</p>"},{"location":"integrations/orchestration/dagster/#observable-source-assets","title":"Observable Source Assets","text":"<p>Use <code>observable_metaxy_asset</code> to create observable source assets that monitor external Metaxy features. This is useful when Metaxy features are populated outside of Dagster (e.g., by external pipelines) and you want Dagster to track their data versions.</p> <p>Basic Observable Asset</p> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\n@mxd.observable_metaxy_asset(key=\"dagster/asset/key\", feature=\"external/feature\")\ndef external_data(context, store: dg.ResourceParam[mx.MetadataStore], lazy_df: nw.LazyFrame):\n    # build a custom metadata dict\n    metadata = ...\n    return metadata\n</code></pre> <p>The observation automatically tracks:</p> <ul> <li>Data version: Uses <code>mean(metaxy_created_at)</code> to detect both additions and deletions</li> <li>Row count: Logged as <code>dagster/row_count</code> metadata</li> </ul>"},{"location":"integrations/orchestration/dagster/#reference","title":"Reference","text":"<ul> <li>API</li> <li>Dagster docs</li> </ul>"},{"location":"integrations/orchestration/dagster/api/","title":"Dagster Integration API","text":"<p>Integration docs</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster","title":"metaxy.ext.dagster","text":""},{"location":"integrations/orchestration/dagster/api/#decorators","title":"Decorators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.metaxify.metaxify","title":"metaxy.ext.dagster.metaxify.metaxify","text":"<pre><code>metaxify(_asset: _T | None = None, *, key: CoercibleToAssetKey | None = None, key_prefix: CoercibleToAssetKeyPrefix | None = None, inject_metaxy_kind: bool = True, inject_code_version: bool = True, set_description: bool = True, inject_column_schema: bool = True, inject_column_lineage: bool = True)\n</code></pre> <p>Inject Metaxy metadata into a Dagster <code>AssetsDefinition</code> or <code>AssetSpec</code>.</p> <p>Affects assets with <code>metaxy/feature</code> metadata set.</p> <p>Learn more about <code>@metaxify</code> and see example screenshots here.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToAssetKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit asset key that overrides all other key resolution logic. Cannot be used with <code>key_prefix</code> or with multi-asset definitions that produce multiple outputs.</p> </li> <li> <code>key_prefix</code>               (<code>CoercibleToAssetKeyPrefix | None</code>, default:                   <code>None</code> )           \u2013            <p>Prefix to prepend to the resolved asset key. Also applied to upstream dependency keys. Cannot be used with <code>key</code>.</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds. Currently, kinds count is limited by 3, and <code>metaxify</code> will skip kind injection if there are already 3 kinds on the asset.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version into the asset's code version. The version is appended in the format <code>metaxy:&lt;version&gt;</code>.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set the asset description from the feature class docstring if the asset doesn't already have a description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject Pydantic field definitions as Dagster column schema. Field types are converted to strings, and field descriptions are used as column descriptions.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column-level lineage into the asset metadata under <code>dagster/column_lineage</code>. Uses Pydantic model fields to track column provenance via <code>FeatureDep.rename</code>, <code>FeatureSpec.lineage</code>, and direct pass-through.</p> </li> </ul> <p>Tip</p> <p>Multiple Dagster assets can contribute to the same Metaxy feature by setting the same <code>\"metaxy/feature\"</code> metadata. This is a perfectly valid setup since Metaxy writes are append-only.</p> <p>Example</p> <pre><code>import dagster as dg\nimport metaxy as mx\nimport metaxy.ext.dagster as mxd\n\n@mxd.metaxify()\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\"\n    },\n)\ndef my_asset(store: mx.MetadataStore):\n    with store:\n        increment = store.resolve_update(\"my/feature/key\")\n    ...\n</code></pre> With <code>@multi_asset</code> <p>Multiple Metaxy features can be produced by the same <code>@multi_asset</code>. (1)</p> <ol> <li>Typically, they are produced independently of each other</li> </ol> <pre><code>@mxd.metaxify()\n@dg.multi_asset(\n    specs=[\n        dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"feature/a\"}),\n        dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"feature/b\"}),\n    ]\n)\ndef my_multi_asset():\n    ...\n</code></pre> With <code>dagster.AssetSpec</code> <pre><code>asset_spec = dg.AssetSpec(\n    key=\"my_asset\",\n    metadata={\"metaxy/feature\": \"my/feature/key\"},\n)\nasset_spec = mxd.metaxify()(asset_spec)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/metaxify.py</code> <pre><code>def __init__(\n    self,\n    _asset: \"_T | None\" = None,\n    *,\n    key: CoercibleToAssetKey | None = None,\n    key_prefix: CoercibleToAssetKeyPrefix | None = None,\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n) -&gt; None:\n    # Actual initialization happens in __new__, but we set defaults here for type checkers\n    self.key = dg.AssetKey.from_coercible(key) if key is not None else None\n    self.key_prefix = (\n        dg.AssetKey.from_coercible(key_prefix) if key_prefix is not None else None\n    )\n    self.inject_metaxy_kind = inject_metaxy_kind\n    self.inject_code_version = inject_code_version\n    self.set_description = set_description\n    self.inject_column_schema = inject_column_schema\n    self.inject_column_lineage = inject_column_lineage\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.observable.observable_metaxy_asset","title":"metaxy.ext.dagster.observable.observable_metaxy_asset","text":"<pre><code>observable_metaxy_asset(feature: CoercibleToFeatureKey, *, store_resource_key: str = 'store', inject_metaxy_kind: bool = True, inject_code_version: bool = True, set_description: bool = True, **observable_kwargs: Any)\n</code></pre> <p>Decorator to create an observable source asset for a Metaxy feature.</p> <p>The observation reads the feature's metadata from the store, counts rows, and uses <code>mean(metaxy_created_at)</code> as the data version to track changes. Using mean ensures that both additions and deletions are detected.</p> <p>The decorated function receives <code>(context, store, lazy_df)</code> and can return a dict of additional metadata to include in the observation.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to observe.</p> </li> <li> <code>store_resource_key</code>               (<code>str</code>, default:                   <code>'store'</code> )           \u2013            <p>Resource key for the MetadataStore (default: <code>\"store\"</code>).</p> </li> <li> <code>inject_metaxy_kind</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject <code>\"metaxy\"</code> kind into asset kinds.</p> </li> <li> <code>inject_code_version</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the Metaxy feature code version.</p> </li> <li> <code>set_description</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to set description from feature class docstring.</p> </li> <li> <code>**observable_kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to <code>@observable_source_asset</code> (key, group_name, tags, metadata, description, partitions_def, etc.)</p> </li> </ul> Example <pre><code>import metaxy.ext.dagster as mxd\nfrom myproject.features import ExternalFeature\n\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data(context, store, lazy_df):\n    pass\n\n# With custom metadata - return a dict\n@mxd.observable_metaxy_asset(feature=ExternalFeature)\ndef external_data_with_metrics(context, store, lazy_df):\n    # Run aggregations in the database\n    total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n    return {\"custom/total\": total}\n</code></pre> Note <p><code>observable_source_asset</code> does not support <code>deps</code>. Upstream Metaxy feature dependencies from the feature spec are not propagated to the SourceAsset.</p> Source code in <code>src/metaxy/ext/dagster/observable.py</code> <pre><code>def observable_metaxy_asset(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    store_resource_key: str = \"store\",\n    # metaxify kwargs\n    inject_metaxy_kind: bool = True,\n    inject_code_version: bool = True,\n    set_description: bool = True,\n    # observable_source_asset kwargs\n    **observable_kwargs: Any,\n):\n    \"\"\"Decorator to create an observable source asset for a Metaxy feature.\n\n    The observation reads the feature's metadata from the store, counts rows,\n    and uses `mean(metaxy_created_at)` as the data version to track changes.\n    Using mean ensures that both additions and deletions are detected.\n\n    The decorated function receives `(context, store, lazy_df)` and can return\n    a dict of additional metadata to include in the observation.\n\n    Args:\n        feature: The Metaxy feature to observe.\n        store_resource_key: Resource key for the MetadataStore (default: `\"store\"`).\n        inject_metaxy_kind: Whether to inject `\"metaxy\"` kind into asset kinds.\n        inject_code_version: Whether to inject the Metaxy feature code version.\n        set_description: Whether to set description from feature class docstring.\n        **observable_kwargs: Passed to `@observable_source_asset`\n            (key, group_name, tags, metadata, description, partitions_def, etc.)\n\n    Example:\n        ```python\n        import metaxy.ext.dagster as mxd\n        from myproject.features import ExternalFeature\n\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data(context, store, lazy_df):\n            pass\n\n        # With custom metadata - return a dict\n        @mxd.observable_metaxy_asset(feature=ExternalFeature)\n        def external_data_with_metrics(context, store, lazy_df):\n            # Run aggregations in the database\n            total = lazy_df.select(nw.col(\"value\").sum()).collect().item(0, 0)\n            return {\"custom/total\": total}\n        ```\n\n    Note:\n        `observable_source_asset` does not support `deps`. Upstream Metaxy feature\n        dependencies from the feature spec are not propagated to the SourceAsset.\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n\n    def decorator(fn: Callable[..., Any]) -&gt; dg.SourceAsset:\n        # Build an AssetSpec from kwargs and enrich with metaxify\n        # Merge user metadata with metaxy/feature\n        user_metadata = observable_kwargs.pop(\"metadata\", None) or {}\n        spec = dg.AssetSpec(\n            key=observable_kwargs.pop(\"key\", None) or fn.__name__,\n            group_name=observable_kwargs.pop(\"group_name\", None),\n            tags=observable_kwargs.pop(\"tags\", None),\n            metadata={\n                **user_metadata,\n                DAGSTER_METAXY_FEATURE_METADATA_KEY: feature_key.to_string(),\n            },\n            description=observable_kwargs.pop(\"description\", None),\n        )\n        enriched = metaxify(\n            inject_metaxy_kind=inject_metaxy_kind,\n            inject_code_version=inject_code_version,\n            set_description=set_description,\n        )(spec)\n\n        def _observe(context: dg.AssetExecutionContext) -&gt; dg.ObserveResult:\n            store: mx.MetadataStore = getattr(context.resources, store_resource_key)\n\n            with store:\n                lazy_df = store.read_metadata(feature_key)\n                stats = compute_stats_from_lazy_frame(lazy_df)\n\n                # Call the user's function - it can return additional metadata\n                extra_metadata = fn(context, store, lazy_df) or {}\n\n            metadata: dict[str, Any] = {\"dagster/row_count\": stats.row_count}\n            metadata.update(extra_metadata)\n\n            return dg.ObserveResult(\n                data_version=stats.data_version,\n                metadata=metadata,\n            )\n\n        # Apply observable_source_asset decorator\n        return dg.observable_source_asset(\n            key=enriched.key,\n            description=enriched.description,\n            group_name=enriched.group_name,\n            tags=dict(enriched.tags) if enriched.tags else None,\n            metadata=dict(enriched.metadata) if enriched.metadata else None,\n            required_resource_keys={store_resource_key},\n            **observable_kwargs,\n        )(_observe)\n\n    return decorator\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#io-manager","title":"IO Manager","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager","title":"metaxy.ext.dagster.MetaxyIOManager","text":"<p>               Bases: <code>ConfigurableIOManager</code></p> <p>MetaxyIOManager is a Dagster IOManager that reads and writes data to/from Metaxy's <code>MetadataStore</code>.</p> <p>It automatically attaches Metaxy feature and store metadata to Dagster materialization events and handles partitioned assets.</p> <p>Always set <code>\"metaxy/feature\"</code> Dagster metadata</p> <p>This IOManager is using <code>\"metaxy/feature\"</code> Dagster metadata key to map Dagster assets into Metaxy features. It expects it to be set on the assets being loaded or materialized.</p> Example <pre><code>import dagster as dg\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n    }\n)\ndef my_asset():\n    ...\n</code></pre> <p>Defining Partitioned Assets</p> <p>To tell Metaxy which column to use when filtering partitioned assets, set <code>\"partition_by\"</code> Dagster metadata key.</p> Example <pre><code>import dagster as dg\n\n@dg.asset(\n    metadata={\n        \"metaxy/feature\": \"my/feature/key\",\n        \"partition_by\": \"date\",\n    }\n)\ndef my_partitioned_asset():\n    ...\n</code></pre> <p>This key is commonly used to configure partitioning behavior by various Dagster IO managers.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.load_input","title":"metaxy.ext.dagster.MetaxyIOManager.load_input","text":"<pre><code>load_input(context: InputContext) -&gt; LazyFrame[Any]\n</code></pre> <p>Load feature metadata from <code>MetadataStore</code>.</p> <p>Reads metadata for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. For partitioned assets, filters to the current partition using the column specified in <code>\"partition_by\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InputContext</code>)           \u2013            <p>Dagster input context containing asset metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>A narwhals LazyFrame with the feature metadata.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def load_input(self, context: \"dg.InputContext\") -&gt; nw.LazyFrame[Any]:\n    \"\"\"Load feature metadata from [`MetadataStore`][metaxy.MetadataStore].\n\n    Reads metadata for the feature specified in the asset's `\"metaxy/feature\"` metadata.\n    For partitioned assets, filters to the current partition using the column specified\n    in `\"partition_by\"` metadata.\n\n    Args:\n        context: Dagster input context containing asset metadata.\n\n    Returns:\n        A narwhals LazyFrame with the feature metadata.\n    \"\"\"\n    with self.metadata_store:\n        context.log.debug(\n            f\"Reading metadata for Metaxy feature {self._feature_key_from_context(context).to_string()} from {self.metadata_store.display()}\"\n        )\n\n        # Build partition filter if applicable\n        partition_col = (\n            context.definition_metadata.get(DAGSTER_METAXY_PARTITION_KEY)\n            if context.has_asset_partitions\n            else None\n        )\n        partition_key = (\n            context.asset_partition_key if context.has_asset_partitions else None\n        )\n        filters = build_partition_filter(\n            partition_col,  # pyright: ignore[reportArgumentType]\n            partition_key,\n        )\n\n        return self.metadata_store.read_metadata(\n            feature=self._feature_key_from_context(context),\n            filters=filters,\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyIOManager.handle_output","title":"metaxy.ext.dagster.MetaxyIOManager.handle_output","text":"<pre><code>handle_output(context: OutputContext, obj: MetaxyOutput) -&gt; None\n</code></pre> <p>Write feature metadata to <code>MetadataStore</code>.</p> <p>Writes the output dataframe to the metadata store for the feature specified in the asset's <code>\"metaxy/feature\"</code> metadata. Also logs metadata about the feature and store to Dagster's materialization events.</p> <p>If <code>obj</code> is <code>None</code>, only metadata logging is performed (no data is written).</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>OutputContext</code>)           \u2013            <p>Dagster output context containing asset metadata.</p> </li> <li> <code>obj</code>               (<code>MetaxyOutput</code>)           \u2013            <p>A narwhals-compatible dataframe to write, or None to skip writing.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/io_manager.py</code> <pre><code>def handle_output(self, context: \"dg.OutputContext\", obj: MetaxyOutput) -&gt; None:\n    \"\"\"Write feature metadata to [`MetadataStore`][metaxy.MetadataStore].\n\n    Writes the output dataframe to the metadata store for the feature specified\n    in the asset's `\"metaxy/feature\"` metadata. Also logs metadata about the\n    feature and store to Dagster's materialization events.\n\n    If `obj` is `None`, only metadata logging is performed (no data is written).\n\n    Args:\n        context: Dagster output context containing asset metadata.\n        obj: A narwhals-compatible dataframe to write, or None to skip writing.\n    \"\"\"\n    assert DAGSTER_METAXY_FEATURE_METADATA_KEY in context.definition_metadata, (\n        f'Missing `\"{DAGSTER_METAXY_FEATURE_METADATA_KEY}\"` key in asset metadata'\n    )\n    key = self._feature_key_from_context(context)\n    feature = mx.get_feature_by_key(key)\n\n    if obj is not None:\n        context.log.debug(\n            f'Writing metadata for Metaxy feature \"{key.to_string()}\" into {self.metadata_store.display()}'\n        )\n        with self.metadata_store.open(\"write\"):\n            self.metadata_store.write_metadata(feature=feature, df=obj)\n        context.log.debug(\n            f'Metadata written for Metaxy feature \"{key.to_string()}\" into {self.metadata_store.display()}'\n        )\n    else:\n        context.log.debug(\n            f'The output corresponds to Metaxy feature \"{key.to_string()}\" stored in {self.metadata_store.display()}'\n        )\n\n    self._log_output_metadata(context)\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-types","title":"Dagster Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.dagster_type.feature_to_dagster_type","title":"metaxy.ext.dagster.dagster_type.feature_to_dagster_type","text":"<pre><code>feature_to_dagster_type(feature: CoercibleToFeatureKey, *, name: str | None = None, description: str | None = None, inject_column_schema: bool = True, inject_column_lineage: bool = True, metadata: Mapping[str, Any] | None = None) -&gt; DagsterType\n</code></pre> <p>Build a Dagster type from a Metaxy feature.</p> <p>Creates a <code>dagster.DagsterType</code> that validates outputs are <code>MetaxyOutput</code> (i.e., narwhals-compatible dataframes or <code>None</code>) and includes metadata derived from the feature's Pydantic model fields.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature to create a type for. Can be a feature class, feature key, or string that can be coerced to a feature key.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom name for the DagsterType. Defaults to the feature's table name (e.g., \"project__feature_name\").</p> </li> <li> <code>description</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom description. Defaults to the feature class docstring or a generated description.</p> </li> <li> <code>inject_column_schema</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject the column schema as metadata. The schema is derived from Pydantic model fields.</p> </li> <li> <code>inject_column_lineage</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to inject column lineage as metadata. The lineage is derived from feature dependencies.</p> </li> <li> <code>metadata</code>               (<code>Mapping[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional custom metadata to inject into the DagsterType.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DagsterType</code>           \u2013            <p>A DagsterType configured for the Metaxy feature with appropriate</p> </li> <li> <code>DagsterType</code>           \u2013            <p>type checking and metadata.</p> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>import dagster as dg\nimport polars as pl\nimport metaxy.ext.dagster as mxd\nfrom myproject.features import MyFeature  # Your Metaxy feature class\n\n@mxd.metaxify(feature=MyFeature)\n@dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\ndef my_asset():\n    return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n</code></pre> <p>See also</p> <ul> <li><code>metaxify</code>: Decorator for injecting   Metaxy metadata into Dagster assets.</li> <li><code>MetaxyOutput</code>: The type alias for valid   Metaxy outputs.</li> </ul> Source code in <code>src/metaxy/ext/dagster/dagster_type.py</code> <pre><code>def feature_to_dagster_type(\n    feature: mx.CoercibleToFeatureKey,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    inject_column_schema: bool = True,\n    inject_column_lineage: bool = True,\n    metadata: Mapping[str, Any] | None = None,\n) -&gt; dg.DagsterType:\n    \"\"\"Build a Dagster type from a Metaxy feature.\n\n    Creates a `dagster.DagsterType` that validates outputs are\n    [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput] (i.e., narwhals-compatible\n    dataframes or `None`) and includes metadata derived from the feature's Pydantic\n    model fields.\n\n    Args:\n        feature: The Metaxy feature to create a type for. Can be a feature class,\n            feature key, or string that can be coerced to a feature key.\n        name: Optional custom name for the DagsterType. Defaults to the feature's\n            table name (e.g., \"project__feature_name\").\n        description: Optional custom description. Defaults to the feature class\n            docstring or a generated description.\n        inject_column_schema: Whether to inject the column schema as metadata.\n            The schema is derived from Pydantic model fields.\n        inject_column_lineage: Whether to inject column lineage as metadata.\n            The lineage is derived from feature dependencies.\n        metadata: Optional custom metadata to inject into the DagsterType.\n\n    Returns:\n        A DagsterType configured for the Metaxy feature with appropriate\n        type checking and metadata.\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        import dagster as dg\n        import polars as pl\n        import metaxy.ext.dagster as mxd\n        from myproject.features import MyFeature  # Your Metaxy feature class\n\n        @mxd.metaxify(feature=MyFeature)\n        @dg.asset(dagster_type=mxd.feature_to_dagster_type(MyFeature))\n        def my_asset():\n            return pl.DataFrame({\"id\": [1, 2, 3], \"value\": [\"a\", \"b\", \"c\"]})\n        ```\n\n    !!! info \"See also\"\n        - [`metaxify`][metaxy.ext.dagster.metaxify.metaxify]: Decorator for injecting\n          Metaxy metadata into Dagster assets.\n        - [`MetaxyOutput`][metaxy.ext.dagster.MetaxyOutput]: The type alias for valid\n          Metaxy outputs.\n    \"\"\"\n    from metaxy.ext.dagster.io_manager import MetaxyOutput\n\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_cls = mx.get_feature_by_key(feature_key)\n\n    # Determine name\n    type_name = name or feature_key.table_name\n\n    # Determine description\n    if description is None:\n        if feature_cls.__doc__:\n            import inspect\n\n            description = inspect.cleandoc(feature_cls.__doc__)\n        else:\n            description = f\"Metaxy feature '{feature_key.to_string()}'.\"\n\n    # Build metadata - start with custom metadata if provided\n    final_metadata: dict[str, Any] = dict(metadata) if metadata else {}\n    final_metadata[DAGSTER_METAXY_INFO_METADATA_KEY] = build_feature_info_metadata(\n        feature_key\n    )\n    if inject_column_schema:\n        column_schema = build_column_schema(feature_cls)\n        if column_schema is not None:\n            final_metadata[DAGSTER_COLUMN_SCHEMA_METADATA_KEY] = column_schema\n\n    if inject_column_lineage:\n        column_lineage = build_column_lineage(feature_cls)\n        if column_lineage is not None:\n            final_metadata[DAGSTER_COLUMN_LINEAGE_METADATA_KEY] = column_lineage\n\n    dagster_type = dg.DagsterType(\n        type_check_fn=_create_type_check_fn(feature_key),\n        name=type_name,\n        description=description,\n        typing_type=MetaxyOutput,\n        metadata=final_metadata,\n    )\n\n    return dagster_type\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#dagster-event-generators","title":"Dagster Event Generators","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_materialize_results","title":"metaxy.ext.dagster.utils.generate_materialize_results","text":"<pre><code>generate_materialize_results(context: AssetExecutionContext, store: MetadataStore | MetaxyStoreFromConfigResource, specs: Iterable[AssetSpec] | None = None) -&gt; Iterator[MaterializeResult[None]]\n</code></pre> <p>Generate <code>dagster.MaterializeResult</code> events for assets in topological order.</p> <p>Yields a <code>MaterializeResult</code> for each asset spec, sorted by their associated Metaxy features in topological order (dependencies before dependents). Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext</code>)           \u2013            <p>The Dagster asset execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional, concrete Dagster asset specs. If missing, specs will be taken from the context.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>MaterializeResult[None]</code>           \u2013            <p>Materialization result for each asset in topological order.</p> </li> </ul> Example <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n@metaxify\n@dg.multi_asset(specs=specs)\ndef my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    # ... compute and write data ...\n    yield from generate_materialize_results(context, store)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>def generate_materialize_results(\n    context: dg.AssetExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.MaterializeResult[None]]:\n    \"\"\"Generate `dagster.MaterializeResult` events for assets in topological order.\n\n    Yields a `MaterializeResult` for each asset spec, sorted by their associated\n    Metaxy features in topological order (dependencies before dependents).\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster asset execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Optional, concrete Dagster asset specs.\n            If missing, specs will be taken from the context.\n\n    Yields:\n        Materialization result for each asset in topological order.\n\n    Example:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n        @metaxify\n        @dg.multi_asset(specs=specs)\n        def my_multi_asset(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            # ... compute and write data ...\n            yield from generate_materialize_results(context, store)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    specs = specs or context.assets_def.specs\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_filters = get_partition_filter(context, asset_spec)\n\n        with store:\n            try:\n                lazy_df = store.read_metadata(key, filters=partition_filters)\n            except FeatureNotFoundError:\n                context.log.exception(\n                    f\"Feature {key.to_string()} not found in store, skipping materialization result\"\n                )\n                continue\n\n            stats = compute_stats_from_lazy_frame(lazy_df)\n\n            # Build runtime metadata using shared function\n            metadata = build_runtime_feature_metadata(key, store, lazy_df, context)\n\n            # Get materialized-in-run count if materialization_id is set\n            if store.materialization_id is not None:\n                mat_filters = partition_filters + [\n                    nw.col(METAXY_MATERIALIZATION_ID) == store.materialization_id\n                ]\n                mat_df = store.read_metadata(key, filters=mat_filters)\n                metadata[\"metaxy/materialized_in_run\"] = (\n                    mat_df.select(nw.len()).collect().item(0, 0)\n                )\n\n        yield dg.MaterializeResult(\n            value=None,\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.generate_observe_results","title":"metaxy.ext.dagster.utils.generate_observe_results","text":"<pre><code>generate_observe_results(context: AssetExecutionContext, store: MetadataStore | MetaxyStoreFromConfigResource, specs: Iterable[AssetSpec] | None = None) -&gt; Iterator[ObserveResult]\n</code></pre> <p>Generate <code>dagster.ObserveResult</code> events for assets in topological order.</p> <p>Yields an <code>ObserveResult</code> for each asset spec that has <code>\"metaxy/feature\"</code> metadata key set, sorted by their associated Metaxy features in topological order. Each result includes the row count as <code>\"dagster/row_count\"</code> metadata.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>AssetExecutionContext</code>)           \u2013            <p>The Dagster asset execution context.</p> </li> <li> <code>store</code>               (<code>MetadataStore | MetaxyStoreFromConfigResource</code>)           \u2013            <p>The Metaxy metadata store to read from.</p> </li> <li> <code>specs</code>               (<code>Iterable[AssetSpec] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional, concrete Dagster asset specs. If missing, this function will take the current specs from the context.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>ObserveResult</code>           \u2013            <p>Observation result for each asset in topological order.</p> </li> </ul> Example <pre><code>specs = [\n    dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n    dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n]\n\n@metaxify\n@dg.multi_observable_source_asset(specs=specs)\ndef my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n    yield from generate_observe_results(context, store)\n</code></pre> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>def generate_observe_results(\n    context: dg.AssetExecutionContext,\n    store: mx.MetadataStore | MetaxyStoreFromConfigResource,\n    specs: Iterable[dg.AssetSpec] | None = None,\n) -&gt; Iterator[dg.ObserveResult]:\n    \"\"\"Generate `dagster.ObserveResult` events for assets in topological order.\n\n    Yields an `ObserveResult` for each asset spec that has `\"metaxy/feature\"` metadata key set, sorted by their associated\n    Metaxy features in topological order.\n    Each result includes the row count as `\"dagster/row_count\"` metadata.\n\n    Args:\n        context: The Dagster asset execution context.\n        store: The Metaxy metadata store to read from.\n        specs: Optional, concrete Dagster asset specs.\n            If missing, this function will take the current specs from the context.\n\n    Yields:\n        Observation result for each asset in topological order.\n\n    Example:\n        ```python\n        specs = [\n            dg.AssetSpec(\"output_a\", metadata={\"metaxy/feature\": \"my/feature/a\"}),\n            dg.AssetSpec(\"output_b\", metadata={\"metaxy/feature\": \"my/feature/b\"}),\n        ]\n\n        @metaxify\n        @dg.multi_observable_source_asset(specs=specs)\n        def my_observable_assets(context: dg.AssetExecutionContext, store: mx.MetadataStore):\n            yield from generate_observe_results(context, store)\n        ```\n    \"\"\"\n    # Build mapping from feature key to asset spec\n    spec_by_feature_key: dict[mx.FeatureKey, dg.AssetSpec] = {}\n    specs = specs or context.assets_def.specs\n\n    for spec in specs:\n        if feature_key_raw := spec.metadata.get(DAGSTER_METAXY_FEATURE_METADATA_KEY):\n            feature_key = mx.coerce_to_feature_key(feature_key_raw)\n            spec_by_feature_key[feature_key] = spec\n\n    # Sort by topological order of feature keys\n    graph = mx.FeatureGraph.get_active()\n    sorted_keys = graph.topological_sort_features(list(spec_by_feature_key.keys()))\n\n    for key in sorted_keys:\n        asset_spec = spec_by_feature_key[key]\n        partition_filters = get_partition_filter(context, asset_spec)\n\n        with store:\n            try:\n                lazy_df = store.read_metadata(key, filters=partition_filters)\n            except FeatureNotFoundError:\n                context.log.exception(\n                    f\"Feature {key.to_string()} not found in store, skipping observation result\"\n                )\n                continue\n\n            stats = compute_stats_from_lazy_frame(lazy_df)\n            metadata = build_runtime_feature_metadata(key, store, lazy_df, context)\n\n        yield dg.ObserveResult(\n            asset_key=asset_spec.key,\n            metadata=metadata,\n            data_version=stats.data_version,\n        )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.build_feature_info_metadata","title":"metaxy.ext.dagster.utils.build_feature_info_metadata","text":"<pre><code>build_feature_info_metadata(feature: CoercibleToFeatureKey) -&gt; dict[str, Any]\n</code></pre> <p>Build feature info metadata dict for Dagster assets.</p> <p>Creates a dictionary with information about the Metaxy feature that can be used as Dagster asset metadata under the <code>\"metaxy/feature_info\"</code> key.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>The Metaxy feature (class, key, or string).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A nested dictionary containing:</p> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>feature</code>: Feature information</li> <li><code>project</code>: The project name</li> <li><code>spec</code>: The full feature spec as a dict (via <code>model_dump()</code>)</li> <li><code>version</code>: The feature version string</li> <li><code>type</code>: The feature class module path</li> </ul> </li> <li> <code>dict[str, Any]</code>           \u2013            <ul> <li><code>metaxy</code>: Metaxy library information</li> <li><code>version</code>: The metaxy library version</li> </ul> </li> </ul> <p>Tip</p> <p>This is automatically injected by <code>@metaxify</code></p> Example <pre><code>from metaxy.ext.dagster.utils import build_feature_info_metadata\n\ninfo = build_feature_info_metadata(MyFeature)\n# {\n#     \"feature\": {\n#         \"project\": \"my_project\",\n#         \"spec\": {...},  # Full FeatureSpec model_dump()\n#         \"version\": \"my__feature@abc123\",\n#         \"type\": \"myproject.features\",\n#     },\n#     \"metaxy\": {\n#         \"version\": \"0.1.0\",\n#     },\n# }\n</code></pre> Source code in <code>src/metaxy/ext/dagster/utils.py</code> <pre><code>def build_feature_info_metadata(\n    feature: mx.CoercibleToFeatureKey,\n) -&gt; dict[str, Any]:\n    \"\"\"Build feature info metadata dict for Dagster assets.\n\n    Creates a dictionary with information about the Metaxy feature that can be\n    used as Dagster asset metadata under the `\"metaxy/feature_info\"` key.\n\n    Args:\n        feature: The Metaxy feature (class, key, or string).\n\n    Returns:\n        A nested dictionary containing:\n\n        - `feature`: Feature information\n            - `project`: The project name\n            - `spec`: The full feature spec as a dict (via `model_dump()`)\n            - `version`: The feature version string\n            - `type`: The feature class module path\n        - `metaxy`: Metaxy library information\n            - `version`: The metaxy library version\n\n    !!! tip\n        This is automatically injected by [`@metaxify`][metaxy.ext.dagster.metaxify.metaxify]\n\n    Example:\n        ```python\n        from metaxy.ext.dagster.utils import build_feature_info_metadata\n\n        info = build_feature_info_metadata(MyFeature)\n        # {\n        #     \"feature\": {\n        #         \"project\": \"my_project\",\n        #         \"spec\": {...},  # Full FeatureSpec model_dump()\n        #         \"version\": \"my__feature@abc123\",\n        #         \"type\": \"myproject.features\",\n        #     },\n        #     \"metaxy\": {\n        #         \"version\": \"0.1.0\",\n        #     },\n        # }\n        ```\n    \"\"\"\n    feature_key = mx.coerce_to_feature_key(feature)\n    feature_cls = mx.get_feature_by_key(feature_key)\n\n    return {\n        \"feature\": {\n            \"project\": feature_cls.project,\n            \"spec\": feature_cls.spec().model_dump(mode=\"json\"),\n            \"version\": feature_cls.feature_version(),\n            \"type\": feature_cls.__module__,\n        },\n        \"metaxy\": {\n            \"version\": mx.__version__,\n            \"plugins\": mx.MetaxyConfig.get().plugins,\n        },\n    }\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#resources","title":"Resources","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource","text":"<p>               Bases: <code>ConfigurableResource[MetadataStore]</code></p> <p>This resource creates a <code>metaxy.MetadataStore</code> based on the current Metaxy configuration (<code>metaxy.toml</code>).</p> <p>If <code>name</code> is not provided, the default store will be used. It can be set with <code>store = \"my_name\"</code> in <code>metaxy.toml</code> or with<code>$METAXY_STORE</code> environment variable.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource-functions","title":"Functions","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","title":"metaxy.ext.dagster.MetaxyStoreFromConfigResource.create_resource","text":"<pre><code>create_resource(context: InitResourceContext) -&gt; MetadataStore\n</code></pre> <p>Create a MetadataStore from the Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>InitResourceContext</code>)           \u2013            <p>Dagster resource initialization context.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore</code>           \u2013            <p>A MetadataStore configured with the Dagster run ID as the materialization ID.</p> </li> </ul> Source code in <code>src/metaxy/ext/dagster/resources.py</code> <pre><code>def create_resource(self, context: dg.InitResourceContext) -&gt; mx.MetadataStore:\n    \"\"\"Create a MetadataStore from the Metaxy configuration.\n\n    Args:\n        context: Dagster resource initialization context.\n\n    Returns:\n        A MetadataStore configured with the Dagster run ID as the materialization ID.\n    \"\"\"\n    assert context.run is not None\n    return mx.MetaxyConfig.get().get_store(\n        self.name, materialization_id=context.run.run_id\n    )\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#helpers","title":"Helpers","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.utils.FeatureStats","title":"metaxy.ext.dagster.utils.FeatureStats","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Statistics about a feature's metadata for Dagster events.</p>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.selection.select_metaxy_assets","title":"metaxy.ext.dagster.selection.select_metaxy_assets","text":"<pre><code>select_metaxy_assets(*, project: str | None = None, feature: CoercibleToFeatureKey | None = None) -&gt; AssetSelection\n</code></pre> <p>Select Metaxy assets by project and/or feature.</p> <p>This helper creates an <code>AssetSelection</code> that filters assets tagged by <code>@metaxify</code>.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by project name. If None, uses <code>MetaxyConfig.get().project</code>.</p> </li> <li> <code>feature</code>               (<code>CoercibleToFeatureKey | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature key. If provided, further narrows the selection.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AssetSelection</code>           \u2013            <p>An <code>AssetSelection</code> that can be used with <code>dg.define_asset_job</code>,</p> </li> <li> <code>AssetSelection</code>           \u2013            <p><code>dg.materialize</code>, or <code>AssetSelection</code> operations like <code>|</code> and <code>&amp;</code>.</p> </li> </ul> Select all Metaxy assets in current project <pre><code>import metaxy.ext.dagster as mxd\n\nall_metaxy = mxd.select_metaxy_assets()\n</code></pre> Select assets for a specific project <pre><code>prod_assets = mxd.select_metaxy_assets(project=\"production\")\n</code></pre> Select a specific feature's assets <pre><code>feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n</code></pre> Use with asset jobs <pre><code>metaxy_job = dg.define_asset_job(\n    name=\"materialize_metaxy\",\n    selection=mxd.select_metaxy_assets(),\n)\n</code></pre> Combine with other selections <pre><code># All metaxy assets plus some other assets\ncombined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n# Metaxy assets that are also in a specific group\nfiltered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n</code></pre> Source code in <code>src/metaxy/ext/dagster/selection.py</code> <pre><code>def select_metaxy_assets(\n    *,\n    project: str | None = None,\n    feature: mx.CoercibleToFeatureKey | None = None,\n) -&gt; dg.AssetSelection:\n    \"\"\"Select Metaxy assets by project and/or feature.\n\n    This helper creates an `AssetSelection` that filters assets tagged by `@metaxify`.\n\n    Args:\n        project: Filter by project name. If None, uses `MetaxyConfig.get().project`.\n        feature: Filter by specific feature key. If provided, further narrows the selection.\n\n    Returns:\n        An `AssetSelection` that can be used with `dg.define_asset_job`,\n        `dg.materialize`, or `AssetSelection` operations like `|` and `&amp;`.\n\n    Example: Select all Metaxy assets in current project\n        ```python\n        import metaxy.ext.dagster as mxd\n\n        all_metaxy = mxd.select_metaxy_assets()\n        ```\n\n    Example: Select assets for a specific project\n        ```python\n        prod_assets = mxd.select_metaxy_assets(project=\"production\")\n        ```\n\n    Example: Select a specific feature's assets\n        ```python\n        feature_assets = mxd.select_metaxy_assets(feature=\"my/feature/key\")\n        ```\n\n    Example: Use with asset jobs\n        ```python\n        metaxy_job = dg.define_asset_job(\n            name=\"materialize_metaxy\",\n            selection=mxd.select_metaxy_assets(),\n        )\n        ```\n\n    Example: Combine with other selections\n        ```python\n        # All metaxy assets plus some other assets\n        combined = mxd.select_metaxy_assets() | dg.AssetSelection.keys(\"other_asset\")\n\n        # Metaxy assets that are also in a specific group\n        filtered = mxd.select_metaxy_assets() &amp; dg.AssetSelection.groups(\"my_group\")\n        ```\n    \"\"\"\n    resolved_project = project if project is not None else mx.MetaxyConfig.get().project\n\n    selection = dg.AssetSelection.tag(DAGSTER_METAXY_PROJECT_TAG_KEY, resolved_project)\n\n    if feature is not None:\n        feature_key = mx.coerce_to_feature_key(feature)\n        selection = selection &amp; dg.AssetSelection.tag(\n            DAGSTER_METAXY_FEATURE_METADATA_KEY, str(feature_key)\n        )\n\n    return selection\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#types","title":"Types","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.MetaxyOutput","title":"metaxy.ext.dagster.MetaxyOutput  <code>module-attribute</code>","text":"<pre><code>MetaxyOutput = IntoFrame | None\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#constants","title":"Constants","text":""},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_FEATURE_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_FEATURE_METADATA_KEY = 'metaxy/feature'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_KIND","title":"metaxy.ext.dagster.DAGSTER_METAXY_KIND  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_KIND = 'metaxy'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_INFO_METADATA_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_INFO_METADATA_KEY = 'metaxy/info'\n</code></pre>"},{"location":"integrations/orchestration/dagster/api/#metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY","title":"metaxy.ext.dagster.DAGSTER_METAXY_PARTITION_KEY  <code>module-attribute</code>","text":"<pre><code>DAGSTER_METAXY_PARTITION_KEY = 'partition_by'\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/","title":"Metaxify","text":"<p>The <code>@metaxify</code> decorator can be used to automatically enrich Dagster assets definitions with information taken from Metaxy features.</p> <p>It's highly recommended to <code>@metaxify</code> all your Dagster assets that produce Metaxy features. It's also recommended to use it in combination with <code>MetaxyIOManager</code>.</p> <p><code>@metaxify</code> modifies most of the attributes available on the asset spec.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#deps","title":"Deps","text":"<p>Upstream Metaxy features are injected into <code>deps</code>.</p> Dagster UICode <p></p> <p> <code>models/landmarker_v1</code> is an upstream non-metaxy Dagster asset.</p> <pre><code>spec = FeatureSpec(..., deps=[Chunk, BodyPose])  # (1)!\n</code></pre> <ol> <li> <code>Chunk</code> and <code>BodyPose</code> are Metaxy feature classes</li> </ol>"},{"location":"integrations/orchestration/dagster/metaxify/#code-version","title":"Code Version","text":"<p>Metaxy's feature spec code version is appended to the asset's code version in the format <code>metaxy:&lt;version&gt;</code>.</p> <p></p>"},{"location":"integrations/orchestration/dagster/metaxify/#description","title":"Description","text":"<p>The Metaxy feature class docstring is used if the asset spec doesn't have a description set.</p> Dagster UICode <p></p> <pre><code>class AudioFeature(...):\n    \"\"\"Scene chunk audio with optional waveform visualization.\"\"\"\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#metadata","title":"Metadata","text":"<p><code>@metaxify</code> injects static metadata into the asset spec.</p> <p>All standard metadata types are supported. Additionally, <code>metaxy/info</code> is added. It contains the Metaxy feature spec, Metaxy project, Metaxy version and enabled Metaxy plugins.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#column-schema","title":"Column Schema","text":"<p>Pydantic fields schema is injected into the asset metadata under <code>dagster/column_schema</code>. Field types are converted to strings, and field descriptions are used as column descriptions. If the asset already has a column schema defined, Metaxy columns are appended (user-defined columns take precedence for columns with the same name).</p> <p>Warning</p> <p>Pydantic feature schema may not match the corresponding table schema in the metadata store. This will be improved in the future.</p> Dagster UICode <p></p> <pre><code>class AudioFeature(...):\n    # some fields are inherited\n    duration: float = Field(description=\"duration in seconds\")\n    sample_rate: int = Field(description=\"sample rate in Hz\")\n    channels: int = Field(description=\"number of audio channels\")\n    codec: str = Field(description=\"audio codec\"))\n</code></pre>"},{"location":"integrations/orchestration/dagster/metaxify/#column-lineage","title":"Column Lineage","text":"<p>Column lineage is injected into the asset metadata under <code>dagster/column_lineage</code>.</p> <p>Tracks which upstream columns each downstream column depends on by analyzing:</p> <ul> <li> <p>Direct pass-through: Columns with the same name in both upstream and downstream features.</p> </li> <li> <p><code>FeatureDep.rename</code>: Renamed columns trace back to their original upstream column names.</p> </li> <li> <p><code>FeatureSpec.lineage</code>: ID column relationships based on lineage type (identity, aggregation, expansion).</p> </li> </ul> <p>Column lineage is derived from Pydantic model fields on the feature class. If the asset already has column lineage defined, Metaxy lineage is merged with user-defined lineage (user-defined dependencies are appended to Metaxy-detected dependencies for each column).</p>"},{"location":"integrations/orchestration/dagster/metaxify/#kinds","title":"Kinds","text":"<p><code>\"metaxy\"</code> kind is injected into asset kinds if <code>inject_metaxy_kind</code> is <code>True</code> and there are less than 3 kinds currently.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#tags","title":"Tags","text":"<p><code>metaxy/feature</code> and <code>metaxy/project</code> are injected into the asset tags.</p>"},{"location":"integrations/orchestration/dagster/metaxify/#arbitrary-asset-attributes","title":"Arbitrary Asset Attributes","text":"<p>All keys from <code>\"dagster/attributes\"</code> in the feature spec metadata (such as <code>group_name</code>, <code>owners</code>, <code>tags</code>) are applied to the Dagster asset spec (with replacement).</p>"},{"location":"integrations/plugins/sqlalchemy/","title":"Metaxy + SQLAlchemy","text":"<p>Metaxy provides helpers for integrating with SQLAlchemy. These helpers allow to construct <code>sqlalchemy.MetaData</code> objects for user-defined feature tables and for Metaxy system tables.</p> <p>This integration is convenient for setting up Alembic migrations.</p> <p>SQLModel Features</p> <p>Check out our SQLModel integration for metaclass features that combine Metaxy features with SQLModel ORM models. This is the recommended way to use Metaxy with SQLAlchemy.</p>"},{"location":"integrations/plugins/sqlalchemy/#alembic-integration","title":"Alembic Integration","text":"<p>Alembic is a database migration toolkit for SQLAlchemy.</p> <p>The two helper functions: <code>filter_feature_sqla_metadata</code> and <code>get_system_slqa_metadata</code> can be used to retrieve an SQLAlchemy URL and <code>MetaData</code> object for a given <code>IbisMetadataStore</code>.</p> <p><code>filter_feature_sqla_metadata</code> returns table metadata for the user-defined tables, while <code>get_system_slqa_metadata</code> returns metadata for Metaxy's system tables.</p> <p>Call <code>init_metaxy</code> first</p> <p>You must call <code>init_metaxy</code> before using <code>filter_feature_sqla_metadata</code> to ensure all features are loaded into the feature graph.</p> <p>Here is an example Alembic <code>env.py</code> that uses the Metaxy SQLAlchemy integration:</p> env.py<pre><code>from alembic import context\nfrom metaxy import init_metaxy\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n\n# Alembic Config object\nconfig = context.config\n\nmetaxy_cfg = init_metaxy()\nstore = metaxy_cfg.get_store(\"my_store\")\n\n# import your SQLAlchemy metadata from somewhere\nmy_metadata = ...\n\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n\n# Configure Alembic\nconfig.set_main_option(\"sqlalchemy.url\", url)\n\n\n# continue with the standard Alembic workflow\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#multi-store-setup","title":"Multi-Store Setup","text":"<p>You can configure separate metadata stores for different environments:</p> metaxy.toml<pre><code>[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"dev_metadata.duckdb\" }\n\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\nconfig = { database = \"prod_metadata.duckdb\" }\n</code></pre> <p>Then create multiple Alembic directories and register them with Alembic:</p> alembic.ini<pre><code>[dev]\nscript_location = alembic/dev\n\n[prod]\nscript_location = alembic/prod\n</code></pre> <p>Separate Alembic Version Tables</p> <p>When using multiple Alembic environments (e.g., system tables vs feature tables), configure separate version tables to avoid conflicts. Set up separate script locations in <code>alembic.ini</code>:</p> alembic.ini<pre><code>[dev:metaxy_system]\nscript_location = alembic/dev/system\n\n[dev:metaxy_features]\nscript_location = alembic/dev/features\n</code></pre> <p>Then pass <code>version_table</code> to <code>context.configure()</code> in each env.py:</p> alembic/dev/system/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_system\",\n)\n</code></pre> alembic/dev/features/env.py<pre><code>context.configure(\n    url=url,\n    target_metadata=target_metadata,\n    version_table=\"alembic_version_metaxy_features\",\n)\n</code></pre> <p>Each environment now tracks migrations independently:</p> <ul> <li><code>alembic_version_metaxy_system</code> for system tables</li> <li><code>alembic_version_metaxy_features</code> for feature tables</li> </ul> <p>Create and run migrations separately:</p> <pre><code>alembic -n dev:metaxy_system revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_features revision --autogenerate -m \"initialize\"\nalembic -n dev:metaxy_system upgrade head\nalembic -n dev:metaxy_features upgrade head\n</code></pre> <p>The two environments now can be managed independently:</p> devprod alembic/dev/env.py<pre><code>from metaxy import init_metaxy\nconfig = init_metaxy()\nstore = config.get_store(\"dev\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n dev upgrade head\n</code></pre> alembic/prod/env.py<pre><code>from metaxy import init_metaxy\nconfig = init_metaxy()\nstore = config.get_store(\"prod\")\nurl, target_metadata = filter_feature_sqla_metadata(my_metadata, store)\n</code></pre> <p>The <code>-n</code> argument can be used to specify the target Alembic directory:</p> <pre><code>alembic -n prod upgrade head\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/#alembic-sqlmodel","title":"Alembic + SQLModel","text":"<p>To add <code>SQLModel</code> into the mix, make sure to use the SQLModel integration and pass <code>sqlmodel.SQLModel.metadata</code> into <code>filter_feature_sqla_metadata</code>.</p>"},{"location":"integrations/plugins/sqlalchemy/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/api/","title":"SQLAlchemy API Reference","text":""},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy","title":"metaxy.ext.sqlalchemy","text":"<p>SQLAlchemy integration for metaxy.</p> <p>This module provides SQLAlchemy table definitions and helpers for metaxy. These can be used with migration tools like Alembic.</p> <p>The main functions return tuples of (sqlalchemy_url, metadata) for easy integration with migration tools:</p> <ul> <li><code>get_system_slqa_metadata</code>: Get URL and system table metadata for a store</li> <li><code>filter_feature_sqla_metadata</code>: Get URL and feature table metadata for a store</li> </ul>"},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy-classes","title":"Classes","text":""},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy.SQLAlchemyConfig","title":"metaxy.ext.sqlalchemy.SQLAlchemyConfig","text":"<p>               Bases: <code>PluginConfig</code></p> <p>Configuration for SQLAlchemy integration.</p> <p>This plugin provides helpers for working with SQLAlchemy metadata and table definitions.</p>"},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy-functions","title":"Functions","text":""},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","title":"metaxy.ext.sqlalchemy.filter_feature_sqla_metadata","text":"<pre><code>filter_feature_sqla_metadata(store: IbisMetadataStore, source_metadata: MetaData, project: str | None = None, filter_by_project: bool = True, inject_primary_key: bool | None = None, inject_index: bool | None = None) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and feature table metadata for a metadata store.</p> <p>This function filters the source metadata to include only feature tables belonging to the specified project, and returns the connection URL for the store.</p> <p>This function must be called after init_metaxy() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.               If False, include all features.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> <li> <code>ImportError</code>             \u2013            <p>If source_metadata is None and SQLModel is not installed</p> </li> </ul> <p>Example: Basic Usage</p> <pre><code>```py\nfrom metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\nfrom metaxy import init_metaxy\nfrom metaxy.config import MetaxyConfig\n\n# Load features first\ninit_metaxy()\n\n# Get store instance\nconfig = MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\n# With custom metadata\nfrom sqlalchemy import MetaData\nmy_metadata = MetaData()\n# ... define tables in my_metadata ...\n\n# apply the filter function\nurl, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n```\n</code></pre> <p>Example: With SQLModel</p> <pre><code>```py\n# With SQLModel\nfrom sqlmodel import SQLModel\nurl, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>def filter_feature_sqla_metadata(\n    store: IbisMetadataStore,\n    source_metadata: MetaData,\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and feature table metadata for a metadata store.\n\n    This function filters the source metadata to include only feature tables\n    belonging to the specified project, and returns the connection URL for the store.\n\n    This function must be called after init_metaxy() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance\n        source_metadata: Source SQLAlchemy MetaData to filter.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n                          If False, include all features.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n        ImportError: If source_metadata is None and SQLModel is not installed\n\n    Example: Basic Usage\n\n        ```py\n        from metaxy.ext.sqlalchemy import filter_feature_sqla_metadata\n        from metaxy import init_metaxy\n        from metaxy.config import MetaxyConfig\n\n        # Load features first\n        init_metaxy()\n\n        # Get store instance\n        config = MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        # With custom metadata\n        from sqlalchemy import MetaData\n        my_metadata = MetaData()\n        # ... define tables in my_metadata ...\n\n        # apply the filter function\n        url, metadata = filter_feature_sqla_metadata(store, source_metadata=my_metadata)\n        ```\n\n    Example: With SQLModel\n\n        ```py\n        # With SQLModel\n        from sqlmodel import SQLModel\n        url, metadata = filter_feature_sqla_metadata(store, SQLModel.metadata)\n        ```\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store)\n    metadata = _get_features_metadata(\n        source_metadata=source_metadata,\n        store=store,\n        project=project,\n        filter_by_project=filter_by_project,\n        inject_primary_key=inject_primary_key,\n        inject_index=inject_index,\n    )\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/api/#metaxy.ext.sqlalchemy.get_system_slqa_metadata","title":"metaxy.ext.sqlalchemy.get_system_slqa_metadata","text":"<pre><code>get_system_slqa_metadata(store: IbisMetadataStore) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.</p> <p>This function retrieves both the connection URL and system table metadata for a store, with the store's <code>table_prefix</code> automatically applied to table names.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, system_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> Source code in <code>src/metaxy/ext/sqlalchemy/plugin.py</code> <pre><code>def get_system_slqa_metadata(\n    store: IbisMetadataStore,\n) -&gt; tuple[str, MetaData]:\n    \"\"\"Get SQLAlchemy URL and Metaxy system tables metadata for a metadata store.\n\n    This function retrieves both the connection URL and system table metadata\n    for a store, with the store's `table_prefix` automatically applied to table names.\n\n    Args:\n        store: IbisMetadataStore instance\n\n    Returns:\n        Tuple of (sqlalchemy_url, system_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n    \"\"\"\n    url = _get_store_sqlalchemy_url(store)\n    metadata = _get_system_metadata(table_prefix=store._table_prefix)\n    return url, metadata\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/configuration/","title":"SQLAlchemy Configuration","text":""},{"location":"integrations/plugins/sqlalchemy/configuration/#enable","title":"<code>enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__ENABLE=false\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/configuration/#inject_primary_key","title":"<code>inject_primary_key</code>","text":"<p>Automatically inject composite primary key constraints on user-defined feature tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_PRIMARY_KEY=false\n</code></pre>"},{"location":"integrations/plugins/sqlalchemy/configuration/#inject_index","title":"<code>inject_index</code>","text":"<p>Automatically inject composite index on user-defined feature tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlalchemy]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLALCHEMY_EXT__SQLALCHEMY__INJECT_INDEX=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/","title":"Metaxy + SQLModel","text":"<p>The SQLModel integration enables Metaxy features to also act as SQLAlchemy ORM models. It exposes user-defined feature tables to SQLAlchemy.</p> <p>It is the primary way to use Metaxy with database-backed metadata stores.</p> <p>Database Migrations</p> <p>For database migration management with Alembic, see the SQLAlchemy integration guide.</p>"},{"location":"integrations/plugins/sqlmodel/#installation","title":"Installation","text":"<p>The SQLModel integration requires the sqlmodel package:</p> <pre><code>pip install 'metaxy[sqlmodel]'\n</code></pre> <p>and has to be enabled explicitly:</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = true\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_ENABLE=true\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/#usage","title":"Usage","text":"<p>The SQLModel integration provides <code>BaseSQLModelFeature</code> which combines the functionality of a Metaxy feature and an SQLModel table.</p> <pre><code>import metaxy as mx\nimport metaxy.ext.sqlmodel as mxsql\nfrom sqlmodel import Field\n\n\nclass VideoFeature(\n    mxsql.BaseSQLModelFeature,\n    table=True,\n    spec=mx.FeatureSpec(\n        key=FeatureKey([\"video\"]),\n        id_columns=[\"video_id\"],\n        fields=[\n            \"frames\",\n            \"duration\",\n        ],\n    ),\n):\n    # User-defined metadata columns\n    video_id: str\n    path: str\n    duration: float\n</code></pre> <p>Do Not Use Server-Generated IDs</p> <p>ID columns should not be server-generated because they are typically used to determine data locations such as object storage keys, so they have to be defined before metadata is inserted into the database</p> <p>Automatic Table Naming</p> <p>When <code>__tablename__</code> is not specified, it is automatically generated from the feature key. For <code>FeatureKey([\"video\", \"processing\"])</code>, it becomes <code>\"video__processing\"</code>. This behavior can be disabled in the plugin configuration.</p>"},{"location":"integrations/plugins/sqlmodel/#database-migrations","title":"Database Migrations","text":"<p>When using SQLModel features with Alembic or other migration tools, use <code>filter_feature_sqlmodel_metadata()</code> to transform table names and filter metadata.</p> <p>Table Name Transformation</p> <p>Pass <code>SQLModel.metadata</code> to <code>filter_feature_sqlmodel_metadata()</code> and it will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <pre><code>from sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom metaxy.config import MetaxyConfig\nfrom metaxy import init_metaxy\n\ninit_metaxy()\nconfig = MetaxyConfig.get()\nstore = config.get_store()\n\n# Transform SQLModel metadata with table_prefix\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nfrom alembic import context\n\ncontext.configure(url=url, target_metadata=target_metadata)\n</code></pre> <p>The <code>filter_feature_sqlmodel_metadata()</code> function:</p> <ul> <li>Transforms table names by adding the store's <code>table_prefix</code></li> <li>Filters tables by project (configurable)</li> <li>Returns the SQLAlchemy URL for the store</li> <li>Optionally injects primary key and index constraints</li> </ul> <p>See the SQLAlchemy integration guide for complete Alembic setup examples.</p> <p>Separate Alembic Version Tables</p> <p>When managing both system tables and feature tables with Alembic, use separate version tables to avoid conflicts. See the Multi-Store Setup section for configuration details.</p>"},{"location":"integrations/plugins/sqlmodel/#reference","title":"Reference","text":"<ul> <li>Configuration</li> <li>API</li> </ul>"},{"location":"integrations/plugins/sqlmodel/api/","title":"SQLModel API Reference","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel","title":"metaxy.ext.sqlmodel","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel-classes","title":"Classes","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.SQLModelPluginConfig","title":"metaxy.ext.sqlmodel.SQLModelPluginConfig","text":"<p>               Bases: <code>PluginConfig</code></p> <p>Configuration for SQLModel integration.</p> <p>This plugin enhances SQLModel-based features with automatic table name inference and optional primary key injection.</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature","title":"metaxy.ext.sqlmodel.BaseSQLModelFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>SQLModel</code>, <code>BaseFeature</code></p> <p>Base class for <code>Metaxy</code> features that are also <code>SQLModel</code> tables.</p> <p>Example</p> <pre><code>from metaxy.integrations.sqlmodel import BaseSQLModelFeature\nfrom metaxy import FeatureSpec, FeatureKey, FieldSpec, FieldKey\nfrom sqlmodel import Field\n\nclass VideoFeature(\n    BaseSQLModelFeature,\n    table=True,\n    spec=FeatureSpec(\n        key=FeatureKey([\"video\"]),\n        id_columns=[\"uid\"],\n        fields=[\n            FieldSpec(\n                key=FieldKey([\"video_file\"]),\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n\n    uid: str = Field(primary_key=True)\n    path: str\n    duration: float\n\n    # Now you can use both Metaxy and SQLModel features:\n    # - VideoFeature.feature_version() -&gt; Metaxy versioning\n    # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Base class for `Metaxy` features that are also `SQLModel` tables.\\n\\n!!! example\\n\\n    ```py\\n    from metaxy.integrations.sqlmodel import BaseSQLModelFeature\\n    from metaxy import FeatureSpec, FeatureKey, FieldSpec, FieldKey\\n    from sqlmodel import Field\\n\\n    class VideoFeature(\\n        BaseSQLModelFeature,\\n        table=True,\\n        spec=FeatureSpec(\\n            key=FeatureKey([\\\"video\\\"]),\\n            id_columns=[\\\"uid\\\"],\\n            fields=[\\n                FieldSpec(\\n                    key=FieldKey([\\\"video_file\\\"]),\\n                    code_version=\\\"1\\\",\\n                ),\\n            ],\\n        ),\\n    ):\\n\\n        uid: str = Field(primary_key=True)\\n        path: str\\n        duration: float\\n\\n        # Now you can use both Metaxy and SQLModel features:\\n        # - VideoFeature.feature_version() -&gt; Metaxy versioning\\n        # - session.exec(select(VideoFeature)) -&gt; SQLModel queries\\n    ```\",\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"default\": null,\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_snapshot_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph snapshot\",\n      \"title\": \"Metaxy Snapshot Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    },\n    \"metaxy_feature_spec_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the complete feature specification.\",\n      \"title\": \"Metaxy Feature Spec Version\"\n    }\n  },\n  \"title\": \"BaseSQLModelFeature\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>default</code>: <code>{'frozen': False}</code></li> </ul> <p>Fields:</p> <ul> <li> <code>metaxy_provenance</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_provenance_by_field</code>                 (<code>dict[str, str]</code>)             </li> <li> <code>metaxy_feature_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_feature_spec_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_snapshot_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version_by_field</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>metaxy_created_at</code>                 (<code>AwareDatetime | None</code>)             </li> <li> <code>metaxy_materialization_id</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_id_columns_exist</code> </li> </ul>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature-attributes","title":"Attributes","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_provenance","title":"metaxy_provenance  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance: str | None = None\n</code></pre> <p>Hash of metaxy_provenance_by_field</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_provenance_by_field","title":"metaxy_provenance_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance_by_field: dict[str, str] = None\n</code></pre> <p>Field-level provenance hashes (maps field names to hashes)</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_feature_version","title":"metaxy_feature_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_feature_version: str | None = None\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions)</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_feature_spec_version","title":"metaxy_feature_spec_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_feature_spec_version: str | None = None\n</code></pre> <p>Hash of the complete feature specification.</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_snapshot_version","title":"metaxy_snapshot_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_snapshot_version: str | None = None\n</code></pre> <p>Hash of the entire feature graph snapshot</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_data_version","title":"metaxy_data_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version: str | None = None\n</code></pre> <p>Hash of metaxy_data_version_by_field</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_data_version_by_field","title":"metaxy_data_version_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version_by_field: dict[str, str] | None = None\n</code></pre> <p>Field-level data version hashes (maps field names to version hashes)</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_created_at","title":"metaxy_created_at  <code>pydantic-field</code>","text":"<pre><code>metaxy_created_at: AwareDatetime | None = None\n</code></pre> <p>Timestamp when the metadata row was created (UTC)</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.metaxy_materialization_id","title":"metaxy_materialization_id  <code>pydantic-field</code>","text":"<pre><code>metaxy_materialization_id: str | None = None\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID)</p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature-functions","title":"Functions","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.table_name","title":"table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.feature_version","title":"feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.feature_spec_version","title":"feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.full_definition_version","title":"full_definition_version  <code>classmethod</code>","text":"<pre><code>full_definition_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature definition including Pydantic schema.</p> <p>This method computes a hash of the entire feature class definition, including: - Pydantic model schema - Project name</p> <p>Used in the <code>metaxy_full_definition_version</code> column of system tables.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete definition</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef full_definition_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature definition including Pydantic schema.\n\n    This method computes a hash of the entire feature class definition, including:\n    - Pydantic model schema\n    - Project name\n\n    Used in the `metaxy_full_definition_version` column of system tables.\n\n    Returns:\n        SHA256 hex digest of the complete definition\n    \"\"\"\n    import json\n\n    hasher = hashlib.sha256()\n\n    # Hash the Pydantic schema (includes field types, descriptions, validators, etc.)\n    schema = cls.model_json_schema()\n    schema_json = json.dumps(schema, sort_keys=True)\n    hasher.update(schema_json.encode())\n\n    # Hash the feature specification\n    hasher.update(cls.feature_spec_version().encode())\n\n    # Hash the project name\n    hasher.update(cls.project.encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.provenance_by_field","title":"provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.load_input","title":"load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.BaseSQLModelFeature.resolve_data_version_diff","title":"resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: Any, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>Any</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: Any,\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.versioning.types import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.SQLModelFeatureMeta","title":"metaxy.ext.sqlmodel.SQLModelFeatureMeta","text":"<p>               Bases: <code>MetaxyMeta</code>, <code>SQLModelMetaclass</code></p>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.SQLModelFeatureMeta-functions","title":"Functions","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.SQLModelFeatureMeta.__new__","title":"__new__","text":"<pre><code>__new__(cls_name: str, bases: tuple[type[Any], ...], namespace: dict[str, Any], *, spec: FeatureSpecWithIDColumns | None = None, inject_primary_key: bool | None = None, inject_index: bool | None = None, **kwargs: Any) -&gt; type[Any]\n</code></pre> <p>Create a new SQLModel + Metaxy Feature class.</p> <p>Parameters:</p> <ul> <li> <code>cls_name</code>               (<code>str</code>)           \u2013            <p>Name of the class being created</p> </li> <li> <code>bases</code>               (<code>tuple[type[Any], ...]</code>)           \u2013            <p>Base classes</p> </li> <li> <code>namespace</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Class namespace (attributes and methods)</p> </li> <li> <code>spec</code>               (<code>FeatureSpecWithIDColumns | None</code>, default:                   <code>None</code> )           \u2013            <p>Metaxy FeatureSpec (required for concrete features)</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, automatically create composite primary key including id_columns + (metaxy_created_at, metaxy_data_version).</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, automatically create composite index including id_columns + (metaxy_created_at, metaxy_data_version).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments (e.g., table=True for SQLModel)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Any]</code>           \u2013            <p>New class that is both a SQLModel table and a Metaxy feature</p> </li> </ul> Source code in <code>src/metaxy/ext/sqlmodel/plugin.py</code> <pre><code>def __new__(\n    cls,\n    cls_name: str,\n    bases: tuple[type[Any], ...],\n    namespace: dict[str, Any],\n    *,\n    spec: FeatureSpecWithIDColumns | None = None,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n    **kwargs: Any,\n) -&gt; type[Any]:\n    \"\"\"Create a new SQLModel + Metaxy Feature class.\n\n    Args:\n        cls_name: Name of the class being created\n        bases: Base classes\n        namespace: Class namespace (attributes and methods)\n        spec: Metaxy FeatureSpec (required for concrete features)\n        inject_primary_key: If True, automatically create composite primary key\n            including id_columns + (metaxy_created_at, metaxy_data_version).\n        inject_index: If True, automatically create composite index\n            including id_columns + (metaxy_created_at, metaxy_data_version).\n        **kwargs: Additional keyword arguments (e.g., table=True for SQLModel)\n\n    Returns:\n        New class that is both a SQLModel table and a Metaxy feature\n    \"\"\"\n    # Override frozen config for SQLModel - instances need to be mutable for ORM\n    if \"model_config\" not in namespace:\n        from pydantic import ConfigDict\n\n        namespace[\"model_config\"] = ConfigDict(frozen=False)\n\n    # Check plugin config for defaults\n    sqlmodel_config = MetaxyConfig.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    if inject_primary_key is None:\n        inject_primary_key = sqlmodel_config.inject_primary_key\n    if inject_index is None:\n        inject_index = sqlmodel_config.inject_index\n\n    # If this is a concrete table (table=True) with a spec\n    if kwargs.get(\"table\") and spec is not None:\n        # Forbid custom __tablename__ since it won't work with metadata store's get_table_name()\n        if \"__tablename__\" in namespace:\n            raise ValueError(\n                f\"Cannot define custom __tablename__ in {cls_name}. \"\n                \"The table name is automatically derived from the feature key. \"\n                \"If you need a different table name, adjust the feature key instead.\"\n            )\n\n        # Prevent user-defined fields from shadowing system-managed columns\n        conflicts = {\n            attr_name\n            for attr_name in namespace\n            if attr_name in RESERVED_SQLMODEL_FIELD_NAMES\n        }\n\n        # Also guard against explicit sa_column_kwargs targeting system columns\n        for attr_name, attr_value in namespace.items():\n            sa_column_kwargs = getattr(attr_value, \"sa_column_kwargs\", None)\n            if isinstance(sa_column_kwargs, dict):\n                column_name = sa_column_kwargs.get(\"name\")\n                if column_name in ALL_SYSTEM_COLUMNS:\n                    conflicts.add(attr_name)\n\n        if conflicts:\n            reserved = \", \".join(sorted(ALL_SYSTEM_COLUMNS))\n            conflict_list = \", \".join(sorted(conflicts))\n            raise ValueError(\n                \"Cannot define SQLModel field(s) \"\n                f\"{conflict_list} because they map to reserved Metaxy system columns. \"\n                f\"Reserved columns: {reserved}\"\n            )\n\n        # Automatically set __tablename__ from the feature key\n        namespace[\"__tablename__\"] = spec.key.table_name\n\n        # Inject table args (info metadata + optional constraints)\n        cls._inject_table_args(\n            namespace, spec, cls_name, inject_primary_key, inject_index\n        )\n\n    # Call super().__new__ which follows MRO: MetaxyMeta -&gt; SQLModelMetaclass -&gt; ...\n    # MetaxyMeta will consume the spec parameter and pass remaining kwargs to SQLModelMetaclass\n    new_class = super().__new__(\n        cls, cls_name, bases, namespace, spec=spec, **kwargs\n    )\n\n    return new_class\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel-functions","title":"Functions","text":""},{"location":"integrations/plugins/sqlmodel/api/#metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","title":"metaxy.ext.sqlmodel.filter_feature_sqlmodel_metadata","text":"<pre><code>filter_feature_sqlmodel_metadata(store: IbisMetadataStore, source_metadata: MetaData, project: str | None = None, filter_by_project: bool = True, inject_primary_key: bool | None = None, inject_index: bool | None = None) -&gt; tuple[str, MetaData]\n</code></pre> <p>Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.</p> <p>This function transforms SQLModel table names to include the store's table_prefix, ensuring that table names in the metadata match what's expected in the database.</p> <p>You can pass <code>SQLModel.metadata</code> directly - this function will transform table names by adding the store's <code>table_prefix</code>. The returned metadata will have prefixed table names that match the actual database tables.</p> <p>This function must be called after init_metaxy() to ensure features are loaded.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>IbisMetadataStore</code>)           \u2013            <p>IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)</p> </li> <li> <code>source_metadata</code>               (<code>MetaData</code>)           \u2013            <p>Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).             Tables are looked up in this metadata by their unprefixed names.</p> </li> <li> <code>project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name to filter by. If None, uses MetaxyConfig.get().project</p> </li> <li> <code>filter_by_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, only include features for the specified project.</p> </li> <li> <code>inject_primary_key</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite primary key constraints.                If False, do not inject. If None, uses config default.</p> </li> <li> <code>inject_index</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, inject composite index.          If False, do not inject. If None, uses config default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, MetaData]</code>           \u2013            <p>Tuple of (sqlalchemy_url, filtered_metadata)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store's sqlalchemy_url is empty</p> </li> </ul> <p>Example:</p> <pre><code>```py\nfrom sqlmodel import SQLModel\nfrom metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\nfrom metaxy import init_metaxy\nfrom metaxy.config import MetaxyConfig\n\n# Load features first\ninit_metaxy()\n\n# Get store instance\nconfig = MetaxyConfig.get()\nstore = config.get_store(\"my_store\")\n\n# Filter SQLModel metadata with prefix transformation\nurl, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n# Use with Alembic env.py\nfrom alembic import context\nurl, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\ncontext.configure(url=url, target_metadata=target_metadata)\n```\n</code></pre> Source code in <code>src/metaxy/ext/sqlmodel/plugin.py</code> <pre><code>def filter_feature_sqlmodel_metadata(\n    store: \"IbisMetadataStore\",\n    source_metadata: \"MetaData\",\n    project: str | None = None,\n    filter_by_project: bool = True,\n    inject_primary_key: bool | None = None,\n    inject_index: bool | None = None,\n) -&gt; tuple[str, \"MetaData\"]:\n    \"\"\"Get SQLAlchemy URL and filtered SQLModel feature metadata for a metadata store.\n\n    This function transforms SQLModel table names to include the store's table_prefix,\n    ensuring that table names in the metadata match what's expected in the database.\n\n    You can pass `SQLModel.metadata` directly - this function will transform table names\n    by adding the store's `table_prefix`. The returned metadata will have prefixed table\n    names that match the actual database tables.\n\n    This function must be called after init_metaxy() to ensure features are loaded.\n\n    Args:\n        store: IbisMetadataStore instance (provides table_prefix and sqlalchemy_url)\n        source_metadata: Source SQLAlchemy MetaData to filter (typically SQLModel.metadata).\n                        Tables are looked up in this metadata by their unprefixed names.\n        project: Project name to filter by. If None, uses MetaxyConfig.get().project\n        filter_by_project: If True, only include features for the specified project.\n        inject_primary_key: If True, inject composite primary key constraints.\n                           If False, do not inject. If None, uses config default.\n        inject_index: If True, inject composite index.\n                     If False, do not inject. If None, uses config default.\n\n    Returns:\n        Tuple of (sqlalchemy_url, filtered_metadata)\n\n    Raises:\n        ValueError: If store's sqlalchemy_url is empty\n\n    Example:\n\n        ```py\n        from sqlmodel import SQLModel\n        from metaxy.ext.sqlmodel import filter_feature_sqlmodel_metadata\n        from metaxy import init_metaxy\n        from metaxy.config import MetaxyConfig\n\n        # Load features first\n        init_metaxy()\n\n        # Get store instance\n        config = MetaxyConfig.get()\n        store = config.get_store(\"my_store\")\n\n        # Filter SQLModel metadata with prefix transformation\n        url, metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n\n        # Use with Alembic env.py\n        from alembic import context\n        url, target_metadata = filter_feature_sqlmodel_metadata(store, SQLModel.metadata)\n        context.configure(url=url, target_metadata=target_metadata)\n        ```\n    \"\"\"\n\n    from sqlalchemy import MetaData\n\n    config = MetaxyConfig.get()\n\n    if project is None:\n        project = config.project\n\n    # Check plugin config for defaults\n    sqlmodel_config = config.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    if inject_primary_key is None:\n        inject_primary_key = sqlmodel_config.inject_primary_key\n    if inject_index is None:\n        inject_index = sqlmodel_config.inject_index\n\n    # Get SQLAlchemy URL from store\n    if not store.sqlalchemy_url:\n        raise ValueError(\"IbisMetadataStore has an empty `sqlalchemy_url`.\")\n    url = store.sqlalchemy_url\n\n    # Create new metadata with transformed table names\n    filtered_metadata = MetaData()\n\n    # Get the FeatureGraph to look up feature classes by key\n    from metaxy.models.feature import FeatureGraph\n\n    feature_graph = FeatureGraph.get_active()\n\n    # Iterate over tables in source metadata\n    for table_name, original_table in source_metadata.tables.items():\n        # Check if this table has Metaxy feature metadata\n        if metaxy_system_info := original_table.info.get(\"metaxy-system\"):\n            metaxy_info = MetaxyTableInfo.model_validate(metaxy_system_info)\n            feature_key = metaxy_info.feature_key\n        else:\n            continue\n        # Look up the feature class from the FeatureGraph\n        feature_cls = feature_graph.features_by_key.get(feature_key)\n        if feature_cls is None:\n            # Skip tables for features that aren't registered\n            continue\n\n        # Filter by project if requested\n        if filter_by_project:\n            feature_project = getattr(feature_cls, \"project\", None)\n            if feature_project != project:\n                continue\n\n        # Compute prefixed name using store's table_prefix\n        prefixed_name = store.get_table_name(feature_key)\n\n        # Copy table to new metadata with prefixed name\n        new_table = original_table.to_metadata(filtered_metadata, name=prefixed_name)\n\n        # Inject constraints if requested\n        if inject_primary_key or inject_index:\n            from metaxy.ext.sqlalchemy.plugin import _inject_constraints\n\n            spec = feature_cls.spec()\n            _inject_constraints(\n                table=new_table,\n                spec=spec,\n                inject_primary_key=inject_primary_key,\n                inject_index=inject_index,\n            )\n\n    return url, filtered_metadata\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/configuration/","title":"SQLModel Configuration","text":""},{"location":"integrations/plugins/sqlmodel/configuration/#enable","title":"<code>enable</code>","text":"<p>Whether to enable the plugin.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\nenable = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__ENABLE=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/configuration/#inject_primary_key","title":"<code>inject_primary_key</code>","text":"<p>Automatically inject composite primary key constraints on SQLModel tables. The key is composed of ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_primary_key = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_PRIMARY_KEY=false\n</code></pre>"},{"location":"integrations/plugins/sqlmodel/configuration/#inject_index","title":"<code>inject_index</code>","text":"<p>Automatically inject composite index on SQLModel tables. The index covers ID columns, <code>metaxy_created_at</code>, and <code>metaxy_data_version</code>.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>[tool.metaxy.ext.sqlmodel]\ninject_index = false\n</code></pre> <pre><code>export METAXY_EXT__SQLMODEL_EXT__SQLMODEL__INJECT_INDEX=false\n</code></pre>"},{"location":"learn/data-versioning/","title":"Versioning","text":"<p>Metaxy calculates a few types of versions at feature, field, and sample levels.</p> <p>Metaxy's versioning system is declarative, static, deterministic and idempotent.</p>"},{"location":"learn/data-versioning/#versioning_1","title":"Versioning","text":"<p>Feature and field versions are defined by the feature graph topology and the user-provided code versions of fields. Sample versions are defined by upstream sample versions and the code versions of the fields defined on the sample's feature.</p> <p>All versions are computed ahead of time: feature and field versions can be immediately derived from code (and we keep historical graph snapshots for them), and calculating sample versions requires access to the metadata store.</p> <p>Metaxy uses hashing algorithms to compute all versions. The algorithm and the hash length can be configured.</p> <p>Here is how these versions are calculated, from bottom to top.</p>"},{"location":"learn/data-versioning/#definitions","title":"Definitions","text":"<p>These versions can be computed from Metaxy definitions (e.g. Python code or historical snapshots of the feature graph). We don't need to access the metadata store in order to calculate them.</p>"},{"location":"learn/data-versioning/#field-level","title":"Field Level","text":"<ul> <li>Field Code Version is defined on the field and is provided by the user (defaults to <code>\"__metaxy_initial__\"</code>)</li> </ul> <p>Code Version Value</p> <p>The value can be arbitrary, but in the future we might implement something around semantic versioning.</p> <ul> <li>Field Version is computed from the code version of this field, the fully qualified field path and from the field versions of its parent fields (if any exist, for example, fields on root features do not have dependencies).</li> </ul>"},{"location":"learn/data-versioning/#feature-level","title":"Feature Level","text":"<ul> <li>Feature Version: is computed from the Field Versions of all fields defined on the feature and the key of the feature.</li> <li>Feature Code Version is computed from the Field Code Versions of all fields defined on the feature. Unlike Feature Version, this version does not change when dependencies change. The value of this version is determined entirely by user input.</li> </ul>"},{"location":"learn/data-versioning/#graph-level","title":"Graph Level","text":"<ul> <li>Snapshot Version: is computed from the Feature Versions of all features defined on the graph.</li> </ul> <p>Why Do We Need Snapshot Version?</p> <p>This value is used to uniquely encode versioned feature graph topology in historical snapshots.</p>"},{"location":"learn/data-versioning/#samples","title":"Samples","text":"<p>These versions are sample-level and require access to the metadata store in order to compute them.</p> <ul> <li>Provenance By Field is computed from the upstream Provenance By Fields (with respect to defined field-level dependencies and the code versions of the current fields. This is a dictionary mapping sample field names to their respective versions. This is how this looks like in the metadata store (database):</li> </ul> sample_uid provenance_by_field video_001 <code>{\"audio\": \"a7f3c2d8\", \"frames\": \"b9e1f4a2\"}</code> video_002 <code>{\"audio\": \"d4b8e9c1\", \"frames\": \"f2a6d7b3\"}</code> video_003 <code>{\"audio\": \"c9f2a8e4\", \"frames\": \"e7d3b1c5\"}</code> video_004 <code>{\"audio\": \"b1e4f9a7\", \"frames\": \"a8c2e6d9\"}</code> <ul> <li>Sample Version is derived from the Provenance By Field by simply hashing it.</li> </ul> <p>This is the end game of the versioning system. It ensures that only the necessary samples are recomputed when a feature version changes. It acts as source of truth for resolving incremental updates for feature metadata.</p>"},{"location":"learn/data-versioning/#practical-example","title":"Practical Example","text":"<p>Consider a video processing pipeline with these features:</p> <pre><code>from metaxy import (\n    Feature,\n    FeatureDep,\n    FeatureSpec,\n    FieldDep,\n    FieldSpec,\n)\n\n\nclass Video(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/video\",\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n            ),\n        ],\n    ),\n):\n    \"\"\"Video metadata feature (root).\"\"\"\n\n    frames: int\n    duration: float\n    size: int\n\n\nclass Crop(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/crop\",\n        deps=[FeatureDep(feature=Video)],\n        fields=[\n            FieldSpec(\n                key=\"audio\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n            FieldSpec(\n                key=\"frames\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass  # omit columns for the sake of simplicity\n\n\nclass FaceDetection(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/face_detection\",\n        deps=[\n            FeatureDep(\n                feature=Crop,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"faces\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Crop,\n                        fields=[\"frames\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n\n\nclass SpeechToText(\n    Feature,\n    spec=FeatureSpec(\n        key=\"example/stt\",\n        deps=[\n            FeatureDep(\n                feature=Video,\n            )\n        ],\n        fields=[\n            FieldSpec(\n                key=\"transcription\",\n                code_version=\"1\",\n                deps=[\n                    FieldDep(\n                        feature=Video,\n                        fields=[\"audio\"],\n                    )\n                ],\n            ),\n        ],\n    ),\n):\n    pass\n</code></pre> <p>Running <code>metaxy graph render --format mermaid</code> produces this graph:</p> <pre><code>---\ntitle: Feature Graph\n---\nflowchart TB\n    %% Snapshot version: 8468950d\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n        example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: bc9ca835)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 22742381)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: 794116a9)&lt;/small&gt;&lt;/div&gt;\"]\n        example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 3ac04df8)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 audio &lt;small&gt;(v: 76c8bdc9)&lt;/small&gt;&lt;br/&gt;\u2022 frames &lt;small&gt;(v: abc79017)&lt;/small&gt;&lt;/div&gt;\"]\n        example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: 1ac83b07)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 faces &lt;small&gt;(v: 2d75f0bd)&lt;/small&gt;&lt;/div&gt;\"]\n        example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;small&gt;(v: c83a754a)&lt;/small&gt;&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;\u2022 transcription &lt;small&gt;(v: ac412b3c)&lt;/small&gt;&lt;/div&gt;\"]\n        example_video --&gt; example_crop\n        example_crop --&gt; example_face_detection\n        example_video --&gt; example_stt</code></pre>"},{"location":"learn/data-versioning/#tracking-definitions-changes","title":"Tracking Definitions Changes","text":"<p>Imagine the <code>audio</code> field of the <code>Video</code> feature changes (perhaps denoising was applied):</p> <pre><code>         key=\"example/video\",\n         fields=[\n             FieldSpec(\n                 key=\"audio\",\n-                code_version=\"1\",\n+                code_version=\"2\",\n             ),\n</code></pre> <p>Run <code>metaxy graph diff</code> to see what changed:</p> <pre><code>---\ntitle: Merged Graph Diff\n---\nflowchart TB\n    %%{init: {'flowchart': {'htmlLabels': true, 'curve': 'basis'}, 'themeVariables': {'fontSize': '14px'}}}%%\n\n    example_video[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/video&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;bc9ca8&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;6db302&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;227423&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;09c839&lt;/font&gt;)&lt;br/&gt;- frames (794116)&lt;/div&gt;\"]\n    style example_video stroke:#FFA500,stroke-width:3px\n    example_crop[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/crop&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;3ac04d&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;54dc7f&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;audio&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;76c8bd&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;f3130c&lt;/font&gt;)&lt;br/&gt;- frames (abc790)&lt;/div&gt;\"]\n    style example_crop stroke:#FFA500,stroke-width:3px\n    example_face_detection[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/face_detection&lt;/b&gt;&lt;br/&gt;1ac83b&lt;br/&gt;&lt;font\ncolor=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- faces (2d75f0)&lt;/div&gt;\"]\n    example_stt[\"&lt;div style=\"text-align:left\"&gt;&lt;b&gt;example/stt&lt;/b&gt;&lt;br/&gt;&lt;font color=\"#CC0000\"&gt;c83a75&lt;/font&gt; \u2192 &lt;font\ncolor=\"#00AA00\"&gt;066d34&lt;/font&gt;&lt;br/&gt;&lt;font color=\"#999\"&gt;---&lt;/font&gt;&lt;br/&gt;- &lt;font color=\"#FFAA00\"&gt;transcription&lt;/font&gt; (&lt;font\ncolor=\"#CC0000\"&gt;ac412b&lt;/font&gt; \u2192 &lt;font color=\"#00AA00\"&gt;058410&lt;/font&gt;)&lt;/div&gt;\"]\n    style example_stt stroke:#FFA500,stroke-width:3px\n\n    example_video --&gt; example_crop\n    example_crop --&gt; example_face_detection\n    example_video --&gt; example_stt</code></pre> <p>Notice:</p> <ul> <li><code>Video</code>, <code>Crop</code>, and <code>SpeechToText</code> changed (highlighted)</li> <li><code>FaceDetection</code> remained unchanged (depends only on <code>frames</code>, not <code>audio</code>)</li> <li>Audio field versions changed throughout the graph</li> <li>Frame field versions stayed the same</li> </ul>"},{"location":"learn/data-versioning/#incremental-computation","title":"Incremental Computation","text":"<p>The metadata store's <code>calculate_provenance_by_field()</code> method:</p> <ol> <li>Joins upstream feature metadata</li> <li>Computes sample versions</li> <li>Compares against existing metadata</li> <li>Returns diff: added, changed, removed samples</li> </ol> <p>Typically, steps 1-3 can be run directly in the database. Analytical databases such as ClickHouse or Snowflake can efficiently handle these operations.</p> <p>The Python pipeline then processes only the delta</p> <pre><code>with store:  # MetadataStore\n    # Metaxy computes provenance_by_field and identifies changes\n    diff = store.resolve_update(MyFeature)\n\n    # Process only changed samples\n</code></pre> <p>The <code>diff</code> object has attributes for new upstream samples, samples with new versions, and samples that have been removed from upstream metadata.</p> <p>This approach avoids expensive recomputation when nothing changed, while ensuring correctness when dependencies update.</p>"},{"location":"learn/feature-definitions/","title":"Feature System","text":"<p>Metaxy has a declarative (defined statically at class level), expressive, flexible feature system. It has been inspired by Dagster's Software-Defined Assets and Nix.</p> <p>Features represent tabular metadata, typically containing references to external multi-modal data such as files, images, or videos. But it can be just pure metadata as well.</p> <p>I will highlight data and metadata with bold so it really stands out.</p> <p>Metaxy is responsible for providing correct metadata to users. During incremental processing, Metaxy will automatically resolve added, changed and deleted metadata rows and calculate the right sample versions for them. Metaxy does not interact with data directly, the user is responsible for writing it, typically using metadata to identify sample locations in storage (it's a good idea to inject the sample version into the data sample identifier). Metaxy is designed to be used with systems that do not overwrite existing metadata (Metaxy only appends metadata) and therefore data as well (while we cannot enforce that since the user is responsible for writing the data, it's easily achievable by including the sample version into the data sample identifier).</p> <p>I hope we can stop using bold for data and metadata from now on, hopefully we've made our point.</p> <p>Include sample version in your data path</p> <p>Include the sample version in your data path to ensure strong consistency guarantees. I mean it. Really do it!</p> <p>Features live on a global <code>FeatureGraph</code> object (typically users do not need to interact with it directly). Features are bound to a specific Metaxy project, but can be moved between projects over time. Features must have unique (across all projects) <code>FeatureKey</code> associated with them.</p>"},{"location":"learn/feature-definitions/#feature-definitions","title":"Feature Definitions","text":"<p>Metaxy provides a <code>BaseFeature</code> class that can be extended to create user-defined features. It's a Pydantic model.</p> <pre><code>from metaxy import BaseFeature, FeatureSpec\n\n\nclass VideoFeature(\n    BaseFeature, spec=FeatureSpec(key=\"/raw/video\", id_columns=[\"video_id\"])\n):\n    path: str\n</code></pre> <p>Metaxy must know how to uniquely identify feature samples and join metadata tables, therefore, you need to attach one or more ID columns to your <code>FeatureSpec</code>.</p> <p>That's it! Since it's a root feature, it doesn't have any dependencies. Easy.</p> <p>You may now use <code>VideoFeature.spec()</code> class method to access the original feature spec: it's bound to the class.</p> <p>Now let's define a child feature.</p> <pre><code>class Transcript(\n    BaseFeature,\n    spec=FeatureSpec(key=\"/processed/transcript\", id_columns=[\"video_id\"] deps=[VideoFeature]),\n):\n    transcript_path: str\n    speakers_json_path: str\n    num_speakers: int\n</code></pre> <p>Hurray! You get the idea.</p>"},{"location":"learn/feature-definitions/#field-level-dependencies","title":"Field-Level Dependencies","text":"<p>A core (I'll be straight: a killer) feature of Metaxy is the concept of field-level dependencies. These are used to define dependencies between logical fields of features.</p> <p>A field is not to be confused with metadata column (Pydantic fields). Fields are completely independent from them.</p> <p>Columns refer to metadata and are stored in metadata stores (such as databases) supported by Metaxy.</p> <p>Fields refer to data and are logical -- users are free to define them as they see fit. Fields are supposed to represent parts of data that users care about. For example, a <code>Video</code> feature -- an <code>.mp4</code> file -- may have <code>frames</code> and <code>audio</code> fields.</p> <p>Downstream features can depend on specific fields of upstream features. This enables fine-grained control over field provenance, avoiding unnecessary reprocessing.</p> <p>At this point, careful readers have probably noticed that the <code>Transcript</code> feature from the example above should not depend on the full video: it only needs the audio track in order to generate the transcript. Let's express that with Metaxy:</p> <pre><code>from metaxy import FieldDep, FieldSpec\n\nvideo_spec = FeatureSpec(key=\"/raw/video\", fields=[\"audio\", \"frames\"])\n\n\nclass VideoFeature(BaseFeature, spec=video_spec):\n    path: str\n\n\ntranscript_spec = TranscriptFeatureSpec(\n    key=\"/raw/transcript\",\n    id_columns=[\"video_id\"],\n    fields=[\n        FieldSpec(\n            key=\"text\",\n            deps=[FieldDep(feature=VideoFeature, fields=[\"audio\"])],\n        )\n    ],\n)\n\n\nclass TranscriptFeature(BaseTranscriptFeature, spec=transcript_spec):\n    path: str\n</code></pre> <p>Voil\u00e0!</p> <p>Use boilerplate-free API</p> <p>Metaxy allows passing simplified types to some of the models like <code>FeatureSpec</code> or <code>FeatureKey</code>. See syntactic sugar for more details.</p> <p>The Data Versioning docs explain more about how Metaxy calculates versions for different components of a feature graph.</p>"},{"location":"learn/feature-definitions/#attaching-user-defined-metadata","title":"Attaching user-defined metadata","text":"<p>Users can attach arbitrary JSON-like metadata dictionary to feature specs, typically used for declaring ownership, providing information to third-party tooling, or documentation purposes. This metadata does not influence graph topology or the versioning system.</p>"},{"location":"learn/feature-definitions/#fully-qualified-field-key","title":"Fully Qualified Field Key","text":"<p>A fully qualified field key (FQFK) is an identifier that uniquely identifies a field within the whole feature graph. It consists of the feature key and the field key, separated by a colon, for example: <code>/raw/video:frames</code>, <code>/raw/video:audio/english</code>.</p>"},{"location":"learn/feature-discovery/","title":"Feature Discovery","text":"<p>Warning</p> <p>This page is WIP</p>"},{"location":"learn/feature-discovery/#config-based-discovery","title":"Config-Based Discovery","text":"<p>Specify paths for modules containing Metaxy features in Metaxy configuration:</p> metaxy.tomlpyproject.toml <pre><code>project = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"my-project\"\nentrypoints = [\n    \"myapp.features.video\",\n    \"myapp.features.audio\",\n]\n</code></pre>"},{"location":"learn/filters/","title":"Specifying Filters As Text","text":"<p>There are a few occasions with Metaxy where users may want to define custom filter expressions via text, mainly being CLI arguments or configuration files. For this purpose, Metaxy implements <code>parse_filter_string</code>, which converts SQL-like <code>WHERE</code> clauses into Narwhals filter expressions.</p> <p>The following syntax is supported:</p> <ul> <li> <p>Comparisons: <code>=</code>, <code>!=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></p> </li> <li> <p>Logical operators: <code>AND</code>, <code>OR</code>, <code>NOT</code></p> </li> <li> <p>Parentheses for grouping</p> </li> <li> <p>Column references (identifiers or dotted paths)</p> </li> <li> <p>Literals: strings (<code>'value'</code>), numbers, booleans (<code>TRUE</code>/<code>FALSE</code>), and <code>NULL</code></p> </li> <li> <p>Implicit boolean columns (e.g., <code>NOT is_active</code>)</p> </li> </ul> <p>Example usage:</p> <pre><code>from metaxy.models.filter_expression import parse_filter_string\n\ndf = ...  # a Narwhals frame\n\n# Parse a SQL WHERE clause into a backend-agnostic Narwhals expression\nexpr = parse_filter_string(\"(age &gt; 25 OR age &lt; 18) AND status != 'deleted'\")\n\ndf = df.filter(expr)\n</code></pre>"},{"location":"learn/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata stored in external systems such as databases, files, or object stores, through a unified interface: <code>MetadataStore</code>.</p> <p>Metadata stores expose methods for reading, writing, deleting metadata, and the most important one: resolve_update for receiving a metadata increment. Metaxy intentionally does not support mutating metadata in-place for performance reasons. Deletes are not required during normal operations, but they are still supported since users would want to eventually delete stale metadata and data.</p> <p>Metadata reads/writes are not guaranteed to be ACID: Metaxy is designed to interact with analytical databases which lack ACID guarantees by definition and design (for performance reasons). However, Metaxy guarantees to never attempt to retrieve the same sample version twice, so as long as users do not write it twice (or have deduplication configured inside the metadata store) we should be all good.</p> <p>When resolving incremental updates for a feature, Metaxy attempts to perform all computations such as sample version calculations within the metadata store. This includes joining upstream features, hashing their versions, and filtering out samples that have already been processed.</p> <p>There are 3 cases where this is done in-memory instead (with the help of polars-hash):</p> <ol> <li>The metadata store does not have a compute engine at all: for example, DeltaLake is just a storage format.</li> <li>The user explicitly requested to keep the computations in-memory (<code>MetadataStore(..., prefer_native=False)</code>)</li> <li>When having to use a fallback store to retrieve one of the parent features.</li> </ol> <p>All 3 cases cannot be accidental and require preconfigured settings or explicit user action. In the third case, Metaxy will also issue a warning just in case the user has accidentally configured a fallback store in production.</p>"},{"location":"learn/metadata-stores/#metadata-store-implementations","title":"Metadata Store Implementations","text":"<p>Metaxy provides ready <code>MetadataStore</code> [implementations](../integrations/metadata-stores] for popular databases and storage systems.</p> <p>Warning</p> <p>Metaxy does not handle infrastructure setup. Make sure to have large tables partitioned as appropriate for your use case.</p>"},{"location":"learn/metadata-stores/#configuration","title":"Configuration","text":"<p>Learn about configuring metadata stores here</p>"},{"location":"learn/relationship/","title":"Lineage Relationships","text":"<p>Metaxy supports a few common mappings from parent to child samples out of the box. These include:</p> <ul> <li> <p><code>1:1</code> mapping with <code>LineageRelationship.identity</code> (the default one)</p> </li> <li> <p><code>1:N</code> mapping with <code>LineageRelationship.expansion</code></p> </li> <li> <p><code>N:1</code> mapping with <code>LineageRelationship.aggregation</code></p> </li> </ul> <p>Tip</p> <p>Always use these classmethods to create instances of lineage relationships. They use Pydantic's discriminated unions under the hood to ensure correct type construction.</p>"},{"location":"learn/relationship/#examples","title":"Examples","text":"<ul> <li>1:N example demonstrates how to use <code>1:N</code> lineage relationships in Metaxy.</li> </ul>"},{"location":"learn/syntactic-sugar/","title":"Syntactic Sugar","text":""},{"location":"learn/syntactic-sugar/#type-coercion-for-input-types","title":"Type Coercion For Input Types","text":"<p>Internally, Metaxy uses strongly typed Pydantic models to represent feature keys, their fields, and the dependencies between them.</p> <p>To avoid boilerplate, Metaxy also has syntactic sugar for construction of these classes. Different ways to provide them are automatically coerced into canonical internal models. This is fully typed and only affects constructor arguments, so accessing attributes on Metaxy models will always return only the canonical types.</p> <p>Some examples:</p> <pre><code>from metaxy import BaseFeature as FeatureKey\n\nkey = FeatureKey(\"prefix/feature\")\nkey = FeatureKey([\"prefix\", \"feature\"])\nsame_key = FeatureKey(key)\n</code></pre> <p>Metaxy really loves you, the user!</p>"},{"location":"learn/syntactic-sugar/#keys","title":"Keys","text":"<p>Both <code>FeatureKey</code> and <code>FieldKey</code> accept:</p> <ul> <li>String format: <code>FeatureKey(\"prefix/feature\")</code></li> <li>Sequence format: <code>FeatureKey([\"prefix\", \"feature\"])</code></li> <li>Same type: <code>FeatureKey(another_feature_key)</code> -- for full Inception mode</li> </ul> <p>All formats produce equivalent keys, internally represented as a sequence of parts.</p>"},{"location":"learn/syntactic-sugar/#feature-dep","title":"Feature Dep","text":"<p><code>FeatureDep</code> accepts types coercible to <code>FeatureKey</code> and additionally subclasses of <code>BaseFeature</code>:</p> <pre><code>dep = FeatureDep(feature=MyFeature)\n</code></pre>"},{"location":"learn/syntactic-sugar/#feature-spec","title":"Feature Spec","text":"<p><code>FeatureSpec</code> has some syntactic sugar implemented as well.</p>"},{"location":"learn/syntactic-sugar/#deps","title":"Deps","text":"<p>The <code>deps</code> argument accepts a sequence of types coercible to <code>FeatureDep</code>:</p> <pre><code>spec = FeatureSpec(\n    ...,\n    deps=[\n        MyFeature,\n        FeatureDep(feature=[\"my/feature/key\"]),\n        [\"another/key\"],\n        \"very/nice\",\n    ],\n)\n</code></pre>"},{"location":"learn/syntactic-sugar/#fields","title":"Fields","text":"<p><code>fields</code> elements can omit the full <code>FieldsSpec</code> and be strings (field keys) instead:</p> <pre><code>spec = FeatureSpec(\n    ..., fields=[\"my/field\", FieldSpec(key=\"field/with/version\", code_version=\"v1.2.3\")]\n)\n</code></pre>"},{"location":"learn/syntactic-sugar/#fields-mapping","title":"Fields Mapping","text":"<p>Metaxy uses a bunch of common sense heuristics automatically find parent fields by matching on their names. This is enabled by default. For example, using the same field names in upstream and downstream features will automatically create a dependency between these fields:</p> <pre><code>class Parent(BaseFeature, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n\nclass Child(Parent, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n</code></pre> <p>is equivalent to:</p> <pre><code>class Child(Parent, spec=FeatureSpec(fields=[\"my_field\"], ...):\n    ...\n\nclass Grandchild(Child, spec=FeatureSpec(fields=[FieldSpec(key=\"my_field\", deps=[FieldDep(feature=Parent.spec().key, field=\"my_field\")])], ...):\n    ...\n</code></pre>"},{"location":"learn/system-columns/","title":"System Column Registry","text":"<p>Metaxy reserves a set of system-managed columns that it attaches to user-defined feature metadata tables. These columns are part of the platform contract and are used by the metadata store, versioning engine, and migration tooling.</p> <p>All system column names start with the <code>metaxy_</code> prefix.</p>"},{"location":"learn/system-columns/#canonical-column-names","title":"Canonical column names","text":"Canonical name Explanation Level Type <code>metaxy_provenance_by_field</code> Derived from upstream data versions and code version per field sample struct <code>metaxy_provenance</code> Hash of <code>metaxy_provenance_by_field</code> sample string <code>metaxy_data_version_by_field</code> Defaults to <code>metaxy_provenance_by_field</code>, can be user-defined sample struct <code>metaxy_data_version</code> Hash of <code>metaxy_data_version_by_field</code> sample string <code>metaxy_feature_version</code> Derived from versions of relevant upstream fields feature string <code>metaxy_snapshot_version</code> Derived from the entire Metaxy feature graph graph string <code>metaxy_feature_spec_version</code> Derived from the part of the feature spec responsible for versioning sample string <code>metaxy_full_definition_version</code> Hash of the entire feature Pydanitc model schema and the Metaxy project string true <code>metaxy_created_at</code> Timestamp when the metadata row was created sample string <code>metaxy_materialization_id</code> External orchestration run ID (e.g., Dagster, Airflow) for tracking run string"},{"location":"learn/testing/","title":"Testing Metaxy Features","text":"<p>This guide covers patterns for testing your features when using Metaxy.</p>"},{"location":"learn/testing/#graph-isolation","title":"Graph Isolation","text":"<p>By default, Metaxy uses a single global feature graph where all features register themselves automatically. During testing, you might want to construct your own, clean and isolated graphs.</p>"},{"location":"learn/testing/#using-isolated-graphs","title":"Using Isolated Graphs","text":"<p>Always use isolated graphs in tests:</p> <pre><code>@pytest.fixture(autouse=True)\ndef graph():\n    with FeatureGraph().use():\n        yield graph\n\n\ndef test_my_feature(graph: FeatureGraph):\n    class MyFeature(Feature, spec=...):\n        pass\n\n    # Test operations here\n\n    # inspect the graph object if needed\n</code></pre> <p>The context manager ensures all feature registrations within the block use the test graph instead of the global one. Multiple graphs can exist at the same time, but only one will be used for feature registration.</p>"},{"location":"learn/testing/#graph-context-management","title":"Graph Context Management","text":"<p>The active graph uses context variables to support multiple graphs:</p> <pre><code># Default global graph (used in production)\ngraph = FeatureGraph()\n\n# Get active graph\nactive = FeatureGraph.get_active()\n\n# Use custom graph temporarily\nwith custom_graph.use():\n    # All operations use custom_graph\n    pass\n</code></pre> <p>This enables:</p> <ul> <li>Isolated testing: Each test gets its own feature registry</li> <li>Migration testing: Load historical graphs for migration scenarios</li> <li>Multi-environment testing: Test different feature configurations</li> </ul>"},{"location":"learn/testing/#testing-metadata-store-operations","title":"Testing Metadata Store Operations","text":""},{"location":"learn/testing/#testing-with-different-backends","title":"Testing with Different Backends","text":"<p>Use parametrized tests to verify behavior across backends:</p> <pre><code>import pytest\n\n\n@pytest.mark.parametrize(\n    \"store_cls\",\n    [\n        InMemoryMetadataStore,\n        DuckDBMetadataStore,\n    ],\n)\ndef test_store_behavior(store_cls, tmp_path):\n    # Use tmp_path for file-based stores\n    store_kwargs = {}\n    if store_cls != InMemoryMetadataStore:\n        store_kwargs[\"path\"] = tmp_path / \"test.db\"\n\n    with store_cls(**store_kwargs) as store:\n        # Test your feature operations\n        pass\n</code></pre>"},{"location":"learn/testing/#suppressing-auto_create_tables-warnings","title":"Suppressing AUTO_CREATE_TABLES Warnings","text":"<p>When testing with <code>auto_create_tables=True</code>, Metaxy emits warnings to remind you not to use this in production. These warnings are important for production safety, but can clutter test output.</p> <p>To suppress these warnings in your test suite, use pytest's <code>filterwarnings</code> configuration:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nenv = [\n  \"METAXY_AUTO_CREATE_TABLES=1\", # Enable auto-creation in tests\n]\nfilterwarnings = [\n  \"ignore:AUTO_CREATE_TABLES is enabled:UserWarning\", # Suppress the warning\n]\n</code></pre> <p>The warning is still emitted (important for production awareness), but pytest filters it from test output.</p> <p>Testing the Warning Itself</p> <p>If you need to verify that the warning is actually emitted, use <code>pytest.warns()</code>:</p> <pre><code>import pytest\n\n\ndef test_auto_create_tables_warning():\n    with pytest.warns(\n        UserWarning, match=r\"AUTO_CREATE_TABLES is enabled.*do not use in production\"\n    ):\n        with DuckDBMetadataStore(\":memory:\", auto_create_tables=True) as store:\n            pass  # Warning is emitted and captured\n</code></pre> <p>This works even with <code>filterwarnings</code> configured, because <code>pytest.warns()</code> explicitly captures and verifies the warning.</p>"},{"location":"learn/testing/#programmatic-metaxy-configuration","title":"Programmatic Metaxy Configuration","text":"<p>When configuring Metaxy programmatically in Python code, use the following pattern (we also showcase plugin configuration):</p> <pre><code>from metaxy.config import MetaxyConfig\nfrom metaxy.ext.sqlmodel import SQLModelPluginConfig\n\nwith MetaxyConfig(\n    ext={\n        \"sqlmodel\": SQLModelPluginConfig(\n            enable=True,\n            infer_db_table_names=True,\n        )\n    }\n).use() as config:\n    sqlmodel_config = MetaxyConfig.get_plugin(\"sqlmodel\", SQLModelPluginConfig)\n    # your code here\n</code></pre> <p>When testing Metaxy code, it's best if this setup is performed via <code>pytest</code> fixtures.</p> <p>The plugin configuration is accessed via a dictionary where keys are plugin names and values are plugin-specific configuration objects.</p>"},{"location":"overview/feature-dependencies/","title":"Feature Dependencies","text":"<p>Back to quickstart</p> <p>Now let's add a downstream feature. We can use <code>deps</code> field on <code>FeatureSpec</code> in order to do that.</p> features.py, hl_lines=<pre><code>import metaxy as mx\nfrom pydantic import Field\n\n\nclass CroppedVideo(\n    Video,  # inheritance is a good way to automatically get matching DB columns\n    spec=mx.FeatureSpec(\n        key=\"cropped_video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            \"audio\",\n            \"frames\",\n        ],\n        deps=[Video],\n    ),\n):\n    # additional columns\n    height: int = Field(description=\"Height of the video in pixels\")\n    width: int = Field(description=\"Width of the video in pixels\")\n</code></pre>"},{"location":"overview/quickstart/","title":"Quickstart","text":""},{"location":"overview/quickstart/#installation","title":"Installation","text":"<p>Install Metaxy with <code>deltalake</code> --- an easy way to setup a <code>MetadataStore</code> locally:</p> <pre><code>pip install 'metaxy[delta]'\n</code></pre>"},{"location":"overview/quickstart/#create-a-minimal-metaxy-config","title":"Create a minimal Metaxy config","text":"metaxy.toml<pre><code>entrypoints = [\"features.py\"]\n\n[stores.dev]\ntype = \"metaxy.metadata_store.deltalake.DeltaMetadataStore\"\nconfig = { path = \"/tmp/metaxy.delta\" }\n</code></pre>"},{"location":"overview/quickstart/#define-a-root-feature","title":"Define a root feature","text":"<p>Every Metaxy project must define at least one root feature. Such features do not have upstream dependencies and act as inputs to the feature graph.</p> features.py<pre><code>import metaxy as mx\nfrom pydantic import Field\n\n\nclass Video(\n    mx.BaseFeature,\n    spec=mx.FeatureSpec(\n        key=\"video\",\n        id_columns=[\"video_id\"],\n        fields=[\n            \"audio\",\n            \"frames\",\n        ],\n    ),\n):\n    # define DB columns\n    video_id: str = Field(description=\"Unique identifier for the video\")\n    path: str = Field(description=\"Path to the video file\")\n    duration: float = Field(description=\"Duration of the video in seconds\")\n</code></pre>"},{"location":"overview/quickstart/#create-feature-materialization-script","title":"Create feature materialization script","text":"<p>Use <code>MetadataStore.resolve_update</code> to compute an increment for materialization:</p> script.py<pre><code>import metaxy as mx\n\nfrom .features import Video\n\n# discover and load Metaxy features\ncfg = mx.init_metaxy()\n\n# instantiate the MetadataStore\nstore = cfg.get_store(\"dev\")\n\n# somehow prepare a DataFrame with incoming metadata\n# this can be a Pandas, Polars, Ibis, or any other DataFrame supported by Narwhals\nsamples = ...\n\nwith store:\n    increment = store.resolve_update(Video, samples=samples)\n</code></pre>"},{"location":"overview/quickstart/#3-run-user-defined-computation-over-the-increment","title":"3. Run user-defined computation over the increment","text":"<p>Metaxy is not involved in this step at all.</p> script.py<pre><code>if (len(increment.added) + len(increment.changed)) &gt; 0:\n    # run your computation, this can be done in a distributed manner\n    results = run_custom_pipeline(diff, ...)\n</code></pre>"},{"location":"overview/quickstart/#4-record-metadata-for-processed-samples","title":"4. Record metadata for processed samples","text":"script.py<pre><code>with store.open(\"write\"):\n    store.write_metadata(VoiceDetection, results)\n</code></pre> <p>We have now successfully recorded the metadata for the computed samples! Processed samples will no longer be returned by <code>MetadataStore.resolve_update</code> during future pipeline runs.</p> <p>No Write Time Uniqueness Checks!</p> <p>Metaxy doesn't enforce deduplication or uniqueness checks at write time for performance reasons. While <code>MetadataStore.resolve_update</code> is guaranteed to never return the same versioned sample twice, it's up to the user to ensure that samples are not written multiple times to the metadata store. Configuring deduplication or uniqueness checks in the store (database) is a good idea. For example, the SQLModel integration can inject a composite primary key on <code>metaxy_data_version</code>, <code>metaxy_created_at</code> and the user-defined ID columns. However, Metaxy only uses the latest version (by <code>metaxy_created_at</code>) at read time.</p>"},{"location":"overview/quickstart/#next-steps","title":"Next Steps","text":"<p>Continue to next section to learn how to add more features and define feature dependencies.</p>"},{"location":"overview/quickstart/#additional-info","title":"Additional info","text":"<ul> <li> <p>Learn more about feature definitions or versioning</p> </li> <li> <p>Explore Metaxy integrations</p> </li> <li> <p>Use Metaxy from the command line</p> </li> <li> <p>Learn how to configure Metaxy</p> </li> <li> <p>Get lost in our API Reference</p> </li> </ul>"},{"location":"reference/cli/","title":"CLI Commands","text":"<p>This section provides a comprehensive reference for all Metaxy CLI commands.</p> <p>Warning</p> <p>Some CLI commands are experimental (read: half-baked)</p> <p>Metaxy CLI.</p> <p>Auto-discovers configuration (<code>metaxy.toml</code> or <code>pyproject.toml</code>) in current or parent directories. Feature definitions are collected via feature discovery.</p>"},{"location":"reference/cli/#table-of-contents","title":"Table of Contents","text":"<ul> <li><code>shell</code></li> <li><code>migrations</code><ul> <li><code>generate</code></li> <li><code>apply</code></li> <li><code>status</code></li> <li><code>list</code></li> <li><code>explain</code></li> <li><code>describe</code></li> </ul> </li> <li><code>graph</code><ul> <li><code>push</code></li> <li><code>history</code></li> <li><code>describe</code></li> <li><code>render</code></li> </ul> </li> <li><code>graph-diff</code><ul> <li><code>render</code></li> </ul> </li> <li><code>list</code><ul> <li><code>features</code></li> </ul> </li> <li><code>metadata</code><ul> <li><code>status</code></li> <li><code>drop</code></li> </ul> </li> </ul> <pre><code>metaxy COMMAND\n</code></pre> <p>Commands:</p> <ul> <li><code>graph</code>: Manage feature graphs</li> <li><code>graph-diff</code>: Compare and visualize graph snapshots</li> <li><code>list</code>: List Metaxy entities</li> <li><code>metadata</code>: Manage Metaxy metadata</li> <li><code>migrations</code>: Metadata migration commands</li> <li><code>shell</code>: Start interactive shell.</li> </ul> <p>Arguments:</p> <p>Parameters:</p> <ul> <li><code>--config-file</code>: Global option. Path to the Metaxy configuration file. Defaults to auto-discovery.  [env: METAXY_CONFIG_FILE]</li> <li><code>--project</code>: Global option. Metaxy project to work with. Some commands may forbid setting this argument.  [env: METAXY_PROJECT]</li> <li><code>--all-projects, --no-all-projects</code>: Global option. Operate on all available Metaxy projects. Some commands may forbid setting this argument.  [env: METAXY_ALL_PROJECTS] [default: --no-all-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-shell","title":"metaxy shell","text":"<p>Start interactive shell.</p> <pre><code>metaxy shell\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations","title":"metaxy migrations","text":"<p>Metadata migration commands</p>"},{"location":"reference/cli/#metaxy-migrations-generate","title":"metaxy migrations generate","text":"<p>Generate migration from detected feature changes.</p> <p>Two migration types are supported:</p> <ul> <li> <p>diff : Compares the latest snapshot in the store (or specified   from_snapshot) with the current active graph to detect changes. Only affected   features are included.</p> </li> <li> <p>full: Creates a migration that includes ALL features in the current graph.   Each operation will have a 'features' list with all feature keys.</p> </li> </ul> <pre><code>metaxy migrations generate --op LIST[STR] [OPTIONS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>--name</code>: Migration name (creates {timestamp}_{name} ID)</li> <li><code>--store</code>: Store name (defaults to default)</li> <li><code>--from-snapshot</code>: Compare from this historical snapshot version (defaults to latest)</li> <li><code>--OP</code>: Operation class path to use (can be repeated). Example: metaxy.migrations.ops.DataVersionReconciliation  [required]</li> <li><code>--type</code>: Migration type: 'diff' (compare different graph snapshots) or 'full' (operates on a single graph snapshot)  [choices: diff, full] [default: diff]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-apply","title":"metaxy migrations apply","text":"<p>Apply migration(s) from YAML files.</p> <p>Reads migration definitions from .metaxy/migrations/ directory (git). Follows parent chain to ensure correct order. Tracks execution state in database (events).</p> <pre><code>metaxy migrations apply [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to apply (applies all unapplied if not specified)</li> <li><code>STORE, --store</code>: Metadata store to use.</li> <li><code>--dry-run, --no-dry-run</code>: Preview changes without executing  [default: --no-dry-run]</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-status","title":"metaxy migrations status","text":"<p>Show migrations and execution status.</p> <p>Reads migration definitions from YAML files (git). Shows execution status from database events. Displays the parent chain in order.</p> <pre><code>metaxy migrations status\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-list","title":"metaxy migrations list","text":"<p>List all migrations in chain order as defined in code.</p> <p>Displays a simple table showing migration ID, creation time, and operations.</p> <pre><code>metaxy migrations list\n</code></pre>"},{"location":"reference/cli/#metaxy-migrations-explain","title":"metaxy migrations explain","text":"<p>Show detailed diff for a migration.</p> <p>Reads migration from YAML file. Computes and displays the GraphDiff between the two snapshots on-demand.</p> <pre><code>metaxy migrations explain [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-ID, --migration-id</code>: Migration ID to explain (explains latest if not specified)</li> </ul>"},{"location":"reference/cli/#metaxy-migrations-describe","title":"metaxy migrations describe","text":"<p>Show verbose description of migration(s).</p> <p>Displays detailed information about what the migration will do: - Migration metadata (ID, parent, snapshots, created timestamp) - Operations to execute - Affected features with row counts - Execution status if already run</p> <pre><code>metaxy migrations describe [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>MIGRATION-IDS, --migration-ids, --empty-migration-ids</code>: Migration IDs to describe (default: all migrations in order)  [default: []]</li> <li><code>STORE, --store</code>: Metadata store to use.</li> </ul>"},{"location":"reference/cli/#metaxy-graph","title":"metaxy graph","text":"<p>Manage feature graphs</p>"},{"location":"reference/cli/#metaxy-graph-push","title":"metaxy graph push","text":"<p>Serialize all Metaxy features to the metadata store.</p> <p>This is intended to be invoked in a CD pipeline before running Metaxy code in production.</p> <pre><code>metaxy graph push [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-t, --tags</code>: Arbitrary key-value pairs to attach to the pushed snapshot. Example: <code>--tags.git_commit abc123def</code>.</li> </ul>"},{"location":"reference/cli/#metaxy-graph-history","title":"metaxy graph history","text":"<p>Show history of recorded graph snapshots.</p> <p>Displays all recorded graph snapshots from the metadata store, showing snapshot versions, when they were recorded, and feature counts.</p> <pre><code>metaxy graph history [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>LIMIT, --limit</code>: Limit number of snapshots to show (defaults to all)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-describe","title":"metaxy graph describe","text":"<p>Describe a graph snapshot.</p> <p>Shows detailed information about a graph snapshot including: - Feature count (optionally filtered by project) - Graph depth (longest dependency chain) - Root features (features with no dependencies) - Leaf features (features with no dependents) - Project breakdown (if multi-project)</p> <pre><code>metaxy graph describe [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>SNAPSHOT, --snapshot</code>: Snapshot version to describe (defaults to current graph from code)</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> </ul>"},{"location":"reference/cli/#metaxy-graph-render","title":"metaxy graph render","text":"<p>Render feature graph visualization.</p> <p>Visualize the feature graph in different formats: - terminal: Terminal rendering with two types:   - graph (default): Hierarchical tree view   - cards: Panel/card-based view with dependency edges - mermaid: Mermaid flowchart markup - graphviz: Graphviz DOT format</p> <pre><code>metaxy graph render [OPTIONS] [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>-f, --format</code>: Output format: terminal, mermaid, or graphviz  [default: terminal]</li> <li><code>-t, --type</code>: Terminal rendering type: graph or cards (only for --format terminal)  [choices: graph, cards] [default: graph]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>--snapshot</code>: Snapshot version to render (default: current graph from code)</li> <li><code>--store</code>: Metadata store to use (for loading historical snapshots)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>--show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-graph-diff","title":"metaxy graph-diff","text":"<p>Compare and visualize graph snapshots</p>"},{"location":"reference/cli/#metaxy-graph-diff-render","title":"metaxy graph-diff render","text":"<p>Render merged graph visualization comparing two snapshots.</p> <p>Shows all features color-coded by status (added/removed/changed/unchanged). Uses the unified rendering system - same renderers as 'metaxy graph render'.</p> <p>Special snapshot literals: - \"latest\": Most recent snapshot in the store - \"current\": Current graph state from code</p> <p>Output formats: - terminal: Hierarchical tree view (default) - cards: Panel/card-based view - mermaid: Mermaid flowchart diagram - graphviz: Graphviz DOT format</p> <pre><code>metaxy graph-diff render [OPTIONS] FROM-SNAPSHOT [ARGS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>FROM-SNAPSHOT</code>: First snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [required]</li> <li><code>TO-SNAPSHOT, --to-snapshot</code>: Second snapshot to compare (can be \"latest\", \"current\", or snapshot hash)  [default: current]</li> <li><code>STORE, --store</code>: Metadata store to use (defaults to configured default store)</li> <li><code>-f, --format</code>: Output format: terminal, cards, mermaid, graphviz, json, or yaml  [choices: terminal, cards, mermaid, graphviz, json, yaml] [default: terminal]</li> <li><code>-o, --output</code>: Output file path (default: stdout)</li> <li><code>--minimal, --no-minimal</code>: Minimal output: only feature keys and dependencies  [default: --no-minimal]</li> <li><code>--verbose, --no-verbose</code>: Verbose output: show all available information  [default: --no-verbose]</li> <li><code>--show-fields, --no-show-fields</code>: Show field-level details within features  [default: --show-fields]</li> <li><code>--show-feature-versions, --no-show-feature-versions</code>: Show feature version hashes  [default: --show-feature-versions]</li> <li><code>--show-field-versions, --no-show-field-versions</code>: Show field version hashes (requires --show-fields)  [default: --show-field-versions]</li> <li><code>--show-code-versions, --no-show-code-versions</code>: Show feature and field code versions  [default: --no-show-code-versions]</li> <li><code>--show-snapshot-version, --no-show-snapshot-version</code>: Show graph snapshot version in output  [default: --show-snapshot-version]</li> <li><code>--hash-length</code>: Number of characters to show for version hashes (0 for full)  [default: 8]</li> <li><code>--direction</code>: Graph layout direction: TB (top-bottom) or LR (left-right)  [default: TB]</li> <li><code>--feature</code>: Focus on a specific feature (e.g., 'video/files' or 'video__files')</li> <li><code>--up</code>: Number of dependency levels to render upstream (default: all)</li> <li><code>--down</code>: Number of dependency levels to render downstream (default: all)</li> <li><code>--project</code>: Filter nodes by project (show only features from this project)</li> <li><code>--show-projects, --no-show-projects</code>: Show project names in feature nodes  [default: --show-projects]</li> </ul>"},{"location":"reference/cli/#metaxy-list","title":"metaxy list","text":"<p>List Metaxy entities</p>"},{"location":"reference/cli/#metaxy-list-features","title":"metaxy list features","text":"<p>List Metaxy features.</p> <pre><code>metaxy list features\n</code></pre>"},{"location":"reference/cli/#metaxy-metadata","title":"metaxy metadata","text":"<p>Manage Metaxy metadata</p>"},{"location":"reference/cli/#metaxy-metadata-status","title":"metaxy metadata status","text":"<p>Check metadata completeness and freshness for specified features.</p> <pre><code>metaxy metadata status [OPTIONS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>--feature, --empty-feature</code>: Feature key (e.g., 'my_feature' or 'namespace/feature'). Can be repeated.</li> <li><code>--all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: --no-all-features]</li> <li><code>--store</code>: Metadata store name (defaults to configured default store).</li> <li><code>--snapshot-id</code>: Check metadata against a specific snapshot version.</li> <li><code>--assert-in-sync, --no-assert-in-sync</code>: Exit with error if any feature needs updates or metadata is missing.  [default: --no-assert-in-sync]</li> <li><code>--verbose, --no-verbose</code>: Show additional details about samples needing updates.  [default: --no-verbose]</li> <li><code>--format</code>:   [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/cli/#metaxy-metadata-drop","title":"metaxy metadata drop","text":"<p>Drop metadata from a store.</p> <p>Removes metadata for specified features. This is destructive and requires --confirm.</p> <pre><code>metaxy metadata drop [OPTIONS]\n</code></pre> <p>Parameters:</p> <ul> <li><code>--feature, --empty-feature</code>: Feature key (e.g., 'my_feature' or 'namespace/feature'). Can be repeated.</li> <li><code>--all-features, --no-all-features</code>: Apply to all features in the project's feature graph.  [default: --no-all-features]</li> <li><code>--store</code>: Store name to drop metadata from (defaults to configured default store).</li> <li><code>--confirm, --no-confirm</code>: Confirm the drop operation (required to prevent accidental deletion).  [default: --no-confirm]</li> <li><code>--format</code>: Output format: 'plain' (default) or 'json'.  [choices: plain, json] [default: plain]</li> </ul>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Metaxy can be configured using TOML configuration files, environment variables, or programmatically.</p>"},{"location":"reference/configuration/#configuration-priority","title":"Configuration Priority","text":"<p>When the same setting is defined in multiple places, Metaxy uses the following priority order (highest to lowest):</p> <ol> <li>Explicit arguments - Values passed directly to <code>MetaxyConfig()</code></li> <li>Environment variables - Values from <code>METAXY_*</code> environment variables</li> <li>Configuration files - Values from <code>metaxy.toml</code> or <code>pyproject.toml</code></li> </ol> <p>Configuration files are discovered automatically by searching in the current or parent directories.</p>"},{"location":"reference/configuration/#templating-environment-variables","title":"Templating Environment Variables","text":"<p>Metaxy supports templating environment variables in configuration files using the <code>${VARIABLE_NAME}</code> syntax.</p> <p>Example</p> metaxy.toml<pre><code>[stores.branch.config]\nroot_path = \"s3://my-bucket/${BRANCH_NAME}\"\n</code></pre>"},{"location":"reference/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/configuration/#store","title":"<code>store</code>","text":"<p>Default metadata store to use</p> <p>Type: <code>str</code> | Default: <code>\"dev\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>store = \"dev\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n</code></pre> <pre><code>export METAXY_STORE=dev\n</code></pre>"},{"location":"reference/configuration/#stores","title":"<code>stores</code>","text":"<p>Named store configurations</p> <p>Type: dict[str, metaxy.config.StoreConfig]</p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# stores = {}\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# stores = {}\n</code></pre> <pre><code>export METAXY_STORES=...\n</code></pre>"},{"location":"reference/configuration/#migrations_dir","title":"<code>migrations_dir</code>","text":"<p>Directory where migration files are stored</p> <p>Type: <code>str</code> | Default: <code>\".metaxy/migrations\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>migrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>[tool.metaxy]\nmigrations_dir = \".metaxy/migrations\"\n</code></pre> <pre><code>export METAXY_MIGRATIONS_DIR=.metaxy/migrations\n</code></pre>"},{"location":"reference/configuration/#entrypoints","title":"<code>entrypoints</code>","text":"<p>List of Python module paths to load for feature discovery</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# entrypoints = []\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# entrypoints = []\n</code></pre> <pre><code>export METAXY_ENTRYPOINTS=...\n</code></pre>"},{"location":"reference/configuration/#theme","title":"<code>theme</code>","text":"<p>Graph rendering theme for CLI visualization</p> <p>Type: <code>str</code> | Default: <code>\"default\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>theme = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\ntheme = \"default\"\n</code></pre> <pre><code>export METAXY_THEME=default\n</code></pre>"},{"location":"reference/configuration/#hash_truncation_length","title":"<code>hash_truncation_length</code>","text":"<p>Truncate hash values to this length (minimum 8 characters).</p> <p>Type: <code>int | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code># Optional\n# hash_truncation_length = null\n</code></pre> <pre><code>[tool.metaxy]\n# Optional\n# hash_truncation_length = null\n</code></pre> <pre><code>export METAXY_HASH_TRUNCATION_LENGTH=...\n</code></pre>"},{"location":"reference/configuration/#auto_create_tables","title":"<code>auto_create_tables</code>","text":"<p>Auto-create tables when opening stores (development/testing only). WARNING: Do not use in production. Use proper database migration tools like Alembic.</p> <p>Type: <code>bool</code> | Default: <code>False</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>auto_create_tables = false\n</code></pre> <pre><code>[tool.metaxy]\nauto_create_tables = false\n</code></pre> <pre><code>export METAXY_AUTO_CREATE_TABLES=false\n</code></pre>"},{"location":"reference/configuration/#project","title":"<code>project</code>","text":"<p>Project name for metadata isolation. Used to scope operations to enable multiple independent projects in a shared metadata store. Does not modify feature keys or table names. Project names must be valid alphanumeric strings with dashes, underscores, and cannot contain forward slashes (<code>/</code>) or double underscores (<code>__</code>)</p> <p>Type: <code>str</code> | Default: <code>\"default\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>project = \"default\"\n</code></pre> <pre><code>[tool.metaxy]\nproject = \"default\"\n</code></pre> <pre><code>export METAXY_PROJECT=default\n</code></pre>"},{"location":"reference/configuration/#store-configuration","title":"Store Configuration","text":"<p>The <code>stores</code> field configures metadata store backends. Each store is defined by:</p> <ul> <li><code>type</code>: Full import path to the store class (e.g., <code>metaxy.metadata_store.duckdb.DuckDBMetadataStore</code>)</li> <li><code>config</code>: Dictionary of store-specific configuration options</li> </ul>"},{"location":"reference/configuration/#example-multiple-stores-with-fallback-stores","title":"Example: Multiple Stores with Fallback Stores","text":"metaxy.tomlpyproject.toml <pre><code># Default store to use\nstore = \"dev\"\n\n# Development store (in-memory) with fallback to production\n[stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n# Production store\n[stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre> <pre><code>[tool.metaxy]\nstore = \"dev\"\n\n[tool.metaxy.stores.dev]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.dev.config]\ndb_path = \":memory:\"\nfallback_stores = [\"prod\"]\n\n[tool.metaxy.stores.prod]\ntype = \"metaxy.metadata_store.duckdb.DuckDBMetadataStore\"\n[tool.metaxy.stores.prod.config]\ndb_path = \"s3://my-bucket/metadata.duckdb\"\n</code></pre>"},{"location":"reference/configuration/#configuring-metadata-stores","title":"Configuring Metadata Stores","text":"<p>Configuration options for metadata stores can be found at the respective store documentation page.</p>"},{"location":"reference/configuration/#configuring-metaxy-plugins","title":"Configuring Metaxy Plugins","text":"<p>Configuration options for Metaxy plugins can be found at the respective plugin documentation page.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#metaxy","title":"<code>metaxy</code>","text":"<p>The top-level <code>metaxy</code> module provides the main public API for Metaxy.</p>"},{"location":"reference/api/#initialization","title":"Initialization","text":""},{"location":"reference/api/#metaxy.init_metaxy","title":"metaxy.init_metaxy","text":"<pre><code>init_metaxy(config_file: Path | None = None, search_parents: bool = True) -&gt; MetaxyConfig\n</code></pre> <p>Main user-facing initialization function for Metaxy. It loads the configuration and features.</p> <p>Features are discovered from installed Python packages metadata.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file.</p> <p>Will be auto-discovered in current or parent directories if not provided.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set this parameter</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to search parent directories for configuration files. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code> (              <code>MetaxyConfig</code> )          \u2013            <p>The initialized Metaxy configuration.</p> </li> </ul> Source code in <code>src/metaxy/__init__.py</code> <pre><code>def init_metaxy(\n    config_file: Path | None = None, search_parents: bool = True\n) -&gt; MetaxyConfig:\n    \"\"\"Main user-facing initialization function for Metaxy. It loads the configuration and features.\n\n    Features are [discovered](../../learn/feature-discovery.md) from installed Python packages metadata.\n\n    Args:\n        config_file (Path | None, optional): Path to the configuration file.\n\n            Will be auto-discovered in current or parent directories if not provided.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set this parameter\n\n        search_parents (bool, optional): Whether to search parent directories for configuration files. Defaults to True.\n\n    Returns:\n        MetaxyConfig: The initialized Metaxy configuration.\n    \"\"\"\n    cfg = MetaxyConfig.load(\n        config_file=config_file,\n        search_parents=search_parents,\n    )\n    load_features(cfg.entrypoints)\n    return cfg\n</code></pre>"},{"location":"reference/api/#metadata-stores","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata through the MetadaStore interface.</p>"},{"location":"reference/api/#dependency-specification","title":"Dependency Specification","text":"<p>Metaxy has a declarative feature specification system that allows users to express dependencies between their features and their versioned fields.</p>"},{"location":"reference/api/config/","title":"Configuration","text":"<p>This is the Python SDK for Metaxy's configuration. See config file reference to learn how to configure Metaxy via <code>TOML</code> files.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig","title":"metaxy.MetaxyConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Main Metaxy configuration.</p> <p>Loads from (in order of precedence):</p> <ol> <li> <p>Init arguments</p> </li> <li> <p>Environment variables (METAXY_*)</p> </li> <li> <p>Config file (<code>metaxy.toml</code> or <code>[tool.metaxy]</code> in <code>pyproject.toml</code> )</p> </li> </ol> <p>Environment variables can be templated with <code>${MY_VAR:-default}</code> syntax.</p> Accessing current configuration <pre><code>config = MetaxyConfig.load()\n</code></pre> Getting a configured metadata store <pre><code>store = config.get_store(\"prod\")\n</code></pre> Templating environment variables metaxy.toml<pre><code>[stores.branch.config]\nroot_path = \"s3://my-bucket/${BRANCH_NAME}\"\n</code></pre> <p>The default store is <code>\"dev\"</code>; <code>METAXY_STORE</code> can be used to override it.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-attributes","title":"Attributes","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.plugins","title":"metaxy.MetaxyConfig.plugins  <code>property</code>","text":"<pre><code>plugins: list[str]\n</code></pre> <p>Returns all enabled plugin names from ext configuration.</p>"},{"location":"reference/api/config/#metaxy.MetaxyConfig-functions","title":"Functions","text":""},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_project","title":"metaxy.MetaxyConfig.validate_project  <code>classmethod</code>","text":"<pre><code>validate_project(v: str) -&gt; str\n</code></pre> <p>Validate project name follows naming rules.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"project\")\n@classmethod\ndef validate_project(cls, v: str) -&gt; str:\n    \"\"\"Validate project name follows naming rules.\"\"\"\n    if not v:\n        raise ValueError(\"project name cannot be empty\")\n    if \"/\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain forward slashes (/). \"\n            f\"Forward slashes are reserved for FeatureKey separation\"\n        )\n    if \"__\" in v:\n        raise ValueError(\n            f\"project name '{v}' cannot contain double underscores (__). \"\n            f\"Double underscores are reserved for table name generation\"\n        )\n    import re\n\n    if not re.match(r\"^[a-zA-Z0-9_-]+$\", v):\n        raise ValueError(\n            f\"project name '{v}' must contain only alphanumeric characters, underscores, and hyphens\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_plugin","title":"metaxy.MetaxyConfig.get_plugin  <code>classmethod</code>","text":"<pre><code>get_plugin(name: str, plugin_cls: type[PluginConfigT]) -&gt; PluginConfigT\n</code></pre> <p>Get the plugin config from the global Metaxy config.</p> <p>Unlike <code>get()</code>, this method does not warn when the global config is not initialized. This is intentional because plugins may call this at import time to read their configuration, and returning default plugin config is always safe.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get_plugin(cls, name: str, plugin_cls: type[PluginConfigT]) -&gt; PluginConfigT:\n    \"\"\"Get the plugin config from the global Metaxy config.\n\n    Unlike `get()`, this method does not warn when the global config is not\n    initialized. This is intentional because plugins may call this at import\n    time to read their configuration, and returning default plugin config\n    is always safe.\n    \"\"\"\n    ext = cls.get(_allow_default_config=True).ext\n    if name in ext:\n        existing = ext[name]\n        if isinstance(existing, plugin_cls):\n            # Already the correct type\n            plugin = existing\n        else:\n            # Convert from generic PluginConfig or dict to specific plugin class\n            plugin = plugin_cls.model_validate(existing.model_dump())\n    else:\n        # Return default config if plugin not configured\n        plugin = plugin_cls()\n    return plugin\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.validate_hash_truncation_length","title":"metaxy.MetaxyConfig.validate_hash_truncation_length  <code>classmethod</code>","text":"<pre><code>validate_hash_truncation_length(v: int | None) -&gt; int | None\n</code></pre> <p>Validate hash truncation length is at least 8 if set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@field_validator(\"hash_truncation_length\")\n@classmethod\ndef validate_hash_truncation_length(cls, v: int | None) -&gt; int | None:\n    \"\"\"Validate hash truncation length is at least 8 if set.\"\"\"\n    if v is not None and v &lt; 8:\n        raise ValueError(\n            f\"hash_truncation_length must be at least 8 characters, got {v}\"\n        )\n    return v\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.settings_customise_sources","title":"metaxy.MetaxyConfig.settings_customise_sources  <code>classmethod</code>","text":"<pre><code>settings_customise_sources(settings_cls: type[BaseSettings], init_settings: PydanticBaseSettingsSource, env_settings: PydanticBaseSettingsSource, dotenv_settings: PydanticBaseSettingsSource, file_secret_settings: PydanticBaseSettingsSource) -&gt; tuple[PydanticBaseSettingsSource, ...]\n</code></pre> <p>Customize settings sources: init \u2192 env \u2192 TOML.</p> <p>Priority (first wins): 1. Init arguments 2. Environment variables 3. TOML file</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls: type[BaseSettings],\n    init_settings: PydanticBaseSettingsSource,\n    env_settings: PydanticBaseSettingsSource,\n    dotenv_settings: PydanticBaseSettingsSource,\n    file_secret_settings: PydanticBaseSettingsSource,\n) -&gt; tuple[PydanticBaseSettingsSource, ...]:\n    \"\"\"Customize settings sources: init \u2192 env \u2192 TOML.\n\n    Priority (first wins):\n    1. Init arguments\n    2. Environment variables\n    3. TOML file\n    \"\"\"\n    toml_settings = TomlConfigSettingsSource(settings_cls)\n    return (init_settings, env_settings, toml_settings)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get","title":"metaxy.MetaxyConfig.get  <code>classmethod</code>","text":"<pre><code>get(*, _allow_default_config: bool = False) -&gt; MetaxyConfig\n</code></pre> <p>Get the current Metaxy configuration.</p> <p>Parameters:</p> <ul> <li> <code>_allow_default_config</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Internal parameter. When True, returns default config without warning if global config is not set. Used by methods like <code>get_plugin</code> that may be called at import time.</p> </li> </ul> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef get(cls, *, _allow_default_config: bool = False) -&gt; \"MetaxyConfig\":\n    \"\"\"Get the current Metaxy configuration.\n\n    Args:\n        _allow_default_config: Internal parameter. When True, returns default\n            config without warning if global config is not set. Used by methods\n            like `get_plugin` that may be called at import time.\n    \"\"\"\n    cfg = _metaxy_config.get()\n    if cfg is None:\n        if not _allow_default_config:\n            warnings.warn(\n                UserWarning(\n                    \"Global Metaxy configuration not initialized. It can be set with MetaxyConfig.set(config) typically after loading it from a toml file. Returning default configuration (with environment variables and other pydantic settings sources resolved, project='default').\"\n                ),\n                stacklevel=2,\n            )\n        return cls(project=\"default\")\n    else:\n        return cfg\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.set","title":"metaxy.MetaxyConfig.set  <code>classmethod</code>","text":"<pre><code>set(config: Self | None) -&gt; None\n</code></pre> <p>Set the current Metaxy configuration.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef set(cls, config: Self | None) -&gt; None:\n    \"\"\"Set the current Metaxy configuration.\"\"\"\n    _metaxy_config.set(config)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.is_set","title":"metaxy.MetaxyConfig.is_set  <code>classmethod</code>","text":"<pre><code>is_set() -&gt; bool\n</code></pre> <p>Check if the current Metaxy configuration is set.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef is_set(cls) -&gt; bool:\n    \"\"\"Check if the current Metaxy configuration is set.\"\"\"\n    return _metaxy_config.get() is not None\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.reset","title":"metaxy.MetaxyConfig.reset  <code>classmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the current Metaxy configuration to None.</p> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef reset(cls) -&gt; None:\n    \"\"\"Reset the current Metaxy configuration to None.\"\"\"\n    _metaxy_config.set(None)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.use","title":"metaxy.MetaxyConfig.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Use this configuration temporarily, restoring previous config on exit.</p> Example <pre><code>config = MetaxyConfig(project=\"test\")\nwith config.use():\n    # Code here uses test config\n    assert MetaxyConfig.get().project == \"test\"\n# Previous config restored\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Use this configuration temporarily, restoring previous config on exit.\n\n    Example:\n        ```py\n        config = MetaxyConfig(project=\"test\")\n        with config.use():\n            # Code here uses test config\n            assert MetaxyConfig.get().project == \"test\"\n        # Previous config restored\n        ```\n    \"\"\"\n    previous = _metaxy_config.get()\n    _metaxy_config.set(self)\n    try:\n        yield self\n    finally:\n        _metaxy_config.set(previous)\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.load","title":"metaxy.MetaxyConfig.load  <code>classmethod</code>","text":"<pre><code>load(config_file: str | Path | None = None, *, search_parents: bool = True, auto_discovery_start: Path | None = None) -&gt; MetaxyConfig\n</code></pre> <p>Load config with auto-discovery and parent directory search.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>               (<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional config file path.</p> <p>Tip</p> <p><code>METAXY_CONFIG</code> environment variable can be used to set this parameter</p> </li> <li> <code>search_parents</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Search parent directories for config file</p> </li> <li> <code>auto_discovery_start</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to start search from. Defaults to current working directory.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetaxyConfig</code>           \u2013            <p>Loaded config (TOML + env vars merged)</p> </li> </ul> Example <pre><code># Auto-discover with parent search\nconfig = MetaxyConfig.load()\n\n# Explicit file\nconfig = MetaxyConfig.load(\"custom.toml\")\n\n# Auto-discover without parent search\nconfig = MetaxyConfig.load(search_parents=False)\n\n# Auto-discover from a specific directory\nconfig = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    config_file: str | Path | None = None,\n    *,\n    search_parents: bool = True,\n    auto_discovery_start: Path | None = None,\n) -&gt; \"MetaxyConfig\":\n    \"\"\"Load config with auto-discovery and parent directory search.\n\n    Args:\n        config_file: Optional config file path.\n\n            !!! tip\n                `METAXY_CONFIG` environment variable can be used to set this parameter\n\n        search_parents: Search parent directories for config file\n        auto_discovery_start: Directory to start search from.\n            Defaults to current working directory.\n\n    Returns:\n        Loaded config (TOML + env vars merged)\n\n    Example:\n        ```py\n        # Auto-discover with parent search\n        config = MetaxyConfig.load()\n\n        # Explicit file\n        config = MetaxyConfig.load(\"custom.toml\")\n\n        # Auto-discover without parent search\n        config = MetaxyConfig.load(search_parents=False)\n\n        # Auto-discover from a specific directory\n        config = MetaxyConfig.load(auto_discovery_start=Path(\"/path/to/project\"))\n        ```\n    \"\"\"\n    # Search for config file if not explicitly provided\n\n    if config_from_env := os.getenv(\"METAXY_CONFIG\"):\n        config_file = Path(config_from_env)\n\n    if config_file is None and search_parents:\n        config_file = cls._discover_config_with_parents(auto_discovery_start)\n\n    # For explicit file, temporarily patch the TomlConfigSettingsSource\n    # to use that file, then use normal instantiation\n    # This ensures env vars still work\n\n    if config_file:\n        # Create a custom settings source class for this file\n        toml_path = Path(config_file)\n\n        class CustomTomlSource(TomlConfigSettingsSource):\n            def __init__(self, settings_cls: type[BaseSettings]):\n                # Skip auto-discovery, use explicit file\n                super(TomlConfigSettingsSource, self).__init__(settings_cls)\n                self.toml_file = toml_path\n                self.toml_data = self._load_toml()\n\n        # Customize sources to use custom TOML file\n        original_method = cls.settings_customise_sources\n\n        @classmethod  # type: ignore[misc]\n        def custom_sources(\n            cls_inner,\n            settings_cls,\n            init_settings,\n            env_settings,\n            dotenv_settings,\n            file_secret_settings,\n        ):\n            toml_settings = CustomTomlSource(settings_cls)\n            return (init_settings, env_settings, toml_settings)\n\n        # Temporarily replace method\n        cls.settings_customise_sources = custom_sources  # type: ignore[assignment]\n        config = cls()\n        cls.settings_customise_sources = original_method  # type: ignore[method-assign]\n    else:\n        # Use default sources (auto-discovery + env vars)\n        config = cls()\n\n    cls.set(config)\n\n    # Load plugins after config is set (plugins may access MetaxyConfig.get())\n    config._load_plugins()\n\n    return config\n</code></pre>"},{"location":"reference/api/config/#metaxy.MetaxyConfig.get_store","title":"metaxy.MetaxyConfig.get_store","text":"<pre><code>get_store(name: str | None = None, *, expected_type: Literal[None] = None, **kwargs: Any) -&gt; MetadataStore\n</code></pre><pre><code>get_store(name: str | None = None, *, expected_type: type[StoreTypeT], **kwargs: Any) -&gt; StoreTypeT\n</code></pre> <pre><code>get_store(name: str | None = None, *, expected_type: type[StoreTypeT] | None = None, **kwargs: Any) -&gt; MetadataStore | StoreTypeT\n</code></pre> <p>Instantiate metadata store by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Store name (uses config.store if None)</p> </li> <li> <code>expected_type</code>               (<code>type[StoreTypeT] | None</code>, default:                   <code>None</code> )           \u2013            <p>Expected type of the store. If the actual store type does not match the expected type, a <code>TypeError</code> is raised.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the store constructor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MetadataStore | StoreTypeT</code>           \u2013            <p>Instantiated metadata store</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If store name not found in config, or if fallback stores have different hash algorithms than the parent store</p> </li> <li> <code>ImportError</code>             \u2013            <p>If store class cannot be imported</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the actual store type does not match the expected type</p> </li> </ul> Example <pre><code>config = MetaxyConfig.load()\nstore = config.get_store(\"prod\")\n\n# Use default store\nstore = config.get_store()\n</code></pre> Source code in <code>src/metaxy/config.py</code> <pre><code>def get_store(\n    self,\n    name: str | None = None,\n    *,\n    expected_type: type[StoreTypeT] | None = None,\n    **kwargs: Any,\n) -&gt; \"MetadataStore | StoreTypeT\":\n    \"\"\"Instantiate metadata store by name.\n\n    Args:\n        name: Store name (uses config.store if None)\n        expected_type: Expected type of the store.\n            If the actual store type does not match the expected type, a `TypeError` is raised.\n        **kwargs: Additional keyword arguments to pass to the store constructor.\n\n    Returns:\n        Instantiated metadata store\n\n    Raises:\n        ValueError: If store name not found in config, or if fallback stores\n            have different hash algorithms than the parent store\n        ImportError: If store class cannot be imported\n        TypeError: If the actual store type does not match the expected type\n\n    Example:\n        ```py\n        config = MetaxyConfig.load()\n        store = config.get_store(\"prod\")\n\n        # Use default store\n        store = config.get_store()\n        ```\n    \"\"\"\n    from metaxy.versioning.types import HashAlgorithm\n\n    if len(self.stores) == 0:\n        raise ValueError(\n            \"No Metaxy stores available. They should be configured in metaxy.toml|pyproject.toml or via environment variables.\"\n        )\n\n    name = name or self.store\n\n    if name not in self.stores:\n        raise ValueError(\n            f\"Store '{name}' not found in config. \"\n            f\"Available stores: {list(self.stores.keys())}\"\n        )\n\n    store_config = self.stores[name]\n\n    # Get store class (already imported by Pydantic's ImportString)\n    store_class = store_config.type\n\n    if expected_type is not None and not issubclass(store_class, expected_type):\n        raise TypeError(f\"Store '{name}' is not of type '{expected_type.__name__}'\")\n\n    # Extract configuration and prepare for typed config model\n    config_copy = store_config.config.copy()\n\n    # Get hash_algorithm from config (if specified) and convert to enum\n    configured_hash_algorithm = config_copy.get(\"hash_algorithm\")\n    if configured_hash_algorithm is not None:\n        # Convert string to enum if needed\n        if isinstance(configured_hash_algorithm, str):\n            configured_hash_algorithm = HashAlgorithm(configured_hash_algorithm)\n            config_copy[\"hash_algorithm\"] = configured_hash_algorithm\n    else:\n        # Don't set a default here - let the store choose its own default\n        configured_hash_algorithm = None\n\n    # Get the store's config model class and create typed config\n    config_model_cls = store_class.config_model()\n\n    # Get auto_create_tables from global config only if the config model supports it\n    if (\n        \"auto_create_tables\" not in config_copy\n        and self.auto_create_tables is not None\n        and \"auto_create_tables\" in config_model_cls.model_fields\n    ):\n        # Use global setting from MetaxyConfig if not specified per-store\n        config_copy[\"auto_create_tables\"] = self.auto_create_tables\n\n    # Separate kwargs into config fields and extra constructor args\n    config_fields = set(config_model_cls.model_fields.keys())\n    extra_kwargs = {}\n    for key, value in kwargs.items():\n        if key in config_fields:\n            config_copy[key] = value\n        else:\n            extra_kwargs[key] = value\n\n    typed_config = config_model_cls.model_validate(config_copy)\n\n    # Instantiate using from_config() - fallback stores are resolved via MetaxyConfig.get()\n    # Use self.use() to ensure this config is available for fallback resolution\n    with self.use():\n        store = store_class.from_config(typed_config, **extra_kwargs)\n\n    # Verify the store actually uses the hash algorithm we configured\n    # (in case a store subclass overrides the default or ignores the parameter)\n    # Only check if we explicitly configured a hash algorithm\n    if (\n        configured_hash_algorithm is not None\n        and store.hash_algorithm != configured_hash_algorithm\n    ):\n        raise ValueError(\n            f\"Store '{name}' ({store_class.__name__}) was configured with \"\n            f\"hash_algorithm='{configured_hash_algorithm.value}' but is using \"\n            f\"'{store.hash_algorithm.value}'. The store class may have overridden \"\n            f\"the hash algorithm. All stores must use the same hash algorithm.\"\n        )\n\n    if expected_type is not None and not isinstance(store, expected_type):\n        raise TypeError(f\"Store '{name}' is not of type '{expected_type.__name__}'\")\n\n    return store\n</code></pre>"},{"location":"reference/api/config/#metaxy.StoreConfig","title":"metaxy.StoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for a single metadata store.</p> Example <pre><code>config = StoreConfig(\n    type=\"metaxy_delta.DeltaMetadataStore\",\n    config={\n        \"root_path\": \"s3://bucket/metadata\",\n        \"region\": \"us-west-2\",\n        \"fallback_stores\": [\"prod\"],\n    }\n)\n</code></pre>"},{"location":"reference/api/constants/","title":"Constants","text":""},{"location":"reference/api/constants/#metaxy.models.constants","title":"Constants","text":"<p>Shared constants for system-managed column names.</p> <p>All system columns use the metaxy_ prefix to avoid conflicts with user columns.</p>"},{"location":"reference/api/constants/#metaxy.models.constants-attributes","title":"Attributes","text":""},{"location":"reference/api/constants/#metaxy.models.constants.DEFAULT_CODE_VERSION","title":"metaxy.models.constants.DEFAULT_CODE_VERSION  <code>module-attribute</code>","text":"<pre><code>DEFAULT_CODE_VERSION = '__metaxy_initial__'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMN_PREFIX","title":"metaxy.models.constants.SYSTEM_COLUMN_PREFIX  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMN_PREFIX = 'metaxy_'\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD","title":"metaxy.models.constants.METAXY_PROVENANCE_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE_BY_FIELD = f'{SYSTEM_COLUMN_PREFIX}provenance_by_field'\n</code></pre> <p>Field-level provenance hashes (struct column mapping field names to hashes).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_PROVENANCE","title":"metaxy.models.constants.METAXY_PROVENANCE  <code>module-attribute</code>","text":"<pre><code>METAXY_PROVENANCE = f'{SYSTEM_COLUMN_PREFIX}provenance'\n</code></pre> <p>Hash of<code>metaxy_provenance_by_field</code> -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_VERSION","title":"metaxy.models.constants.METAXY_FEATURE_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_VERSION = f'{SYSTEM_COLUMN_PREFIX}feature_version'\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_SNAPSHOT_VERSION","title":"metaxy.models.constants.METAXY_SNAPSHOT_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_SNAPSHOT_VERSION = f'{SYSTEM_COLUMN_PREFIX}snapshot_version'\n</code></pre> <p>Hash of the entire feature graph snapshot (recorded during deployment).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FEATURE_SPEC_VERSION","title":"metaxy.models.constants.METAXY_FEATURE_SPEC_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FEATURE_SPEC_VERSION = f'{SYSTEM_COLUMN_PREFIX}feature_spec_version'\n</code></pre> <p>Hash of the complete feature specification.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_FULL_DEFINITION_VERSION","title":"metaxy.models.constants.METAXY_FULL_DEFINITION_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_FULL_DEFINITION_VERSION = f'{SYSTEM_COLUMN_PREFIX}full_definition_version'\n</code></pre> <p>Hash of the complete feature definition including Pydantic schema, feature spec, and project.</p> <p>This comprehensive hash captures ALL aspects of a feature definition: - Pydantic model schema (field types, descriptions, validators, serializers, etc.) - Feature specification (dependencies, fields, code_versions, metadata) - Project name</p> <p>Used in system tables to detect when ANY part of a feature changes.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD","title":"metaxy.models.constants.METAXY_DATA_VERSION_BY_FIELD  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION_BY_FIELD = f'{SYSTEM_COLUMN_PREFIX}data_version_by_field'\n</code></pre> <p>Field-level data version hashes (struct column mapping field names to version hashes).</p> <p>Similar to provenance_by_field, but can be user-overridden to implement custom versioning (e.g., content hashes, timestamps, semantic versions).</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_DATA_VERSION","title":"metaxy.models.constants.METAXY_DATA_VERSION  <code>module-attribute</code>","text":"<pre><code>METAXY_DATA_VERSION = f'{SYSTEM_COLUMN_PREFIX}data_version'\n</code></pre> <p>Hash of metaxy_data_version_by_field -- a single string value.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_CREATED_AT","title":"metaxy.models.constants.METAXY_CREATED_AT  <code>module-attribute</code>","text":"<pre><code>METAXY_CREATED_AT = f'{SYSTEM_COLUMN_PREFIX}created_at'\n</code></pre> <p>Timestamp when the metadata row was created.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.METAXY_MATERIALIZATION_ID","title":"metaxy.models.constants.METAXY_MATERIALIZATION_ID  <code>module-attribute</code>","text":"<pre><code>METAXY_MATERIALIZATION_ID = f'{SYSTEM_COLUMN_PREFIX}materialization_id'\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID, Airflow Run ID) for tracking pipeline executions.</p>"},{"location":"reference/api/constants/#metaxy.models.constants.ALL_SYSTEM_COLUMNS","title":"metaxy.models.constants.ALL_SYSTEM_COLUMNS  <code>module-attribute</code>","text":"<pre><code>ALL_SYSTEM_COLUMNS = frozenset({METAXY_PROVENANCE_BY_FIELD, METAXY_PROVENANCE, METAXY_FEATURE_VERSION, METAXY_SNAPSHOT_VERSION, METAXY_DATA_VERSION_BY_FIELD, METAXY_DATA_VERSION, METAXY_CREATED_AT, METAXY_MATERIALIZATION_ID})\n</code></pre> <p>All Metaxy-managed column names that are injected into feature tables.</p>"},{"location":"reference/api/constants/#metaxy.models.constants._DROPPABLE_COLUMNS","title":"metaxy.models.constants._DROPPABLE_COLUMNS  <code>module-attribute</code>","text":"<pre><code>_DROPPABLE_COLUMNS = frozenset({METAXY_FEATURE_VERSION, METAXY_SNAPSHOT_VERSION, METAXY_CREATED_AT, METAXY_DATA_VERSION_BY_FIELD, METAXY_DATA_VERSION, METAXY_MATERIALIZATION_ID})\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE","title":"metaxy.models.constants.SYSTEM_COLUMNS_WITH_LINEAGE  <code>module-attribute</code>","text":"<pre><code>SYSTEM_COLUMNS_WITH_LINEAGE: frozenset[str] = frozenset({METAXY_PROVENANCE_BY_FIELD, METAXY_PROVENANCE, METAXY_DATA_VERSION_BY_FIELD, METAXY_DATA_VERSION})\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants-functions","title":"Functions","text":""},{"location":"reference/api/constants/#metaxy.models.constants.is_system_column","title":"metaxy.models.constants.is_system_column","text":"<pre><code>is_system_column(name: str) -&gt; bool\n</code></pre> <p>Check whether a column name is a system-managed column.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Column name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the column is a system column, False otherwise</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_system_column(\"metaxy_feature_version\")\nTrue\n&gt;&gt;&gt; is_system_column(\"my_column\")\nFalse\n</code></pre> Source code in <code>src/metaxy/models/constants.py</code> <pre><code>def is_system_column(name: str) -&gt; bool:\n    \"\"\"Check whether a column name is a system-managed column.\n\n    Args:\n        name: Column name to check\n\n    Returns:\n        True if the column is a system column, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_system_column(\"metaxy_feature_version\")\n        True\n        &gt;&gt;&gt; is_system_column(\"my_column\")\n        False\n    \"\"\"\n    return name in ALL_SYSTEM_COLUMNS\n</code></pre>"},{"location":"reference/api/constants/#metaxy.models.constants.is_droppable_system_column","title":"metaxy.models.constants.is_droppable_system_column","text":"<pre><code>is_droppable_system_column(name: str) -&gt; bool\n</code></pre> <p>Check whether a column should be dropped when joining upstream features.</p> <p>Droppable columns (feature_version, snapshot_version) are recalculated for each feature, so keeping them from upstream would cause conflicts.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Column name to check</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the column should be dropped during joins, False otherwise</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_droppable_system_column(\"metaxy_feature_version\")\nTrue\n&gt;&gt;&gt; is_droppable_system_column(\"metaxy_provenance_by_field\")\nFalse\n</code></pre> Source code in <code>src/metaxy/models/constants.py</code> <pre><code>def is_droppable_system_column(name: str) -&gt; bool:\n    \"\"\"Check whether a column should be dropped when joining upstream features.\n\n    Droppable columns (feature_version, snapshot_version) are recalculated for\n    each feature, so keeping them from upstream would cause conflicts.\n\n    Args:\n        name: Column name to check\n\n    Returns:\n        True if the column should be dropped during joins, False otherwise\n\n    Examples:\n        &gt;&gt;&gt; is_droppable_system_column(\"metaxy_feature_version\")\n        True\n        &gt;&gt;&gt; is_droppable_system_column(\"metaxy_provenance_by_field\")\n        False\n    \"\"\"\n    return name in _DROPPABLE_COLUMNS\n</code></pre>"},{"location":"reference/api/types/","title":"Types","text":""},{"location":"reference/api/types/#versioning-engine","title":"Versioning Engine","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement","title":"metaxy.versioning.types.LazyIncrement","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing lazy dataframes.</p> <p>Contains three sets of samples: - added: New samples from upstream not present in current metadata - changed: Samples with different provenance - removed: Samples in current metadata but not in upstream state</p>"},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.LazyIncrement.collect","title":"metaxy.versioning.types.LazyIncrement.collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>Collect all lazy frames to eager DataFrames.</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; Increment:\n    \"\"\"Collect all lazy frames to eager DataFrames.\"\"\"\n    return Increment(\n        added=self.added.collect(),\n        changed=self.changed.collect(),\n        removed=self.removed.collect(),\n    )\n</code></pre>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment","title":"metaxy.versioning.types.Increment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of an incremental update containing eager dataframes.</p> <p>Contains three sets of samples: - added: New samples from upstream not present in current metadata - changed: Samples with different provenance - removed: Samples in current metadata but not in upstream state</p>"},{"location":"reference/api/types/#metaxy.versioning.types.Increment-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.versioning.types.Increment.collect","title":"metaxy.versioning.types.Increment.collect","text":"<pre><code>collect() -&gt; Increment\n</code></pre> <p>No-op for eager Increment (already collected).</p> Source code in <code>src/metaxy/versioning/types.py</code> <pre><code>def collect(self) -&gt; \"Increment\":\n    \"\"\"No-op for eager Increment (already collected).\"\"\"\n    return self\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm","title":"metaxy.HashAlgorithm","text":"<p>               Bases: <code>Enum</code></p> <p>Supported hash algorithms for field provenance calculation.</p> <p>These algorithms are chosen for: - Speed (non-cryptographic hashes preferred) - Cross-database availability - Good collision resistance for field provenance calculation</p>"},{"location":"reference/api/types/#metaxy.HashAlgorithm-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH64","title":"metaxy.HashAlgorithm.XXHASH64  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH64 = 'xxhash64'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.XXHASH32","title":"metaxy.HashAlgorithm.XXHASH32  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>XXHASH32 = 'xxhash32'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.WYHASH","title":"metaxy.HashAlgorithm.WYHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>WYHASH = 'wyhash'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.SHA256","title":"metaxy.HashAlgorithm.SHA256  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SHA256 = 'sha256'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.MD5","title":"metaxy.HashAlgorithm.MD5  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MD5 = 'md5'\n</code></pre>"},{"location":"reference/api/types/#metaxy.HashAlgorithm.FARMHASH","title":"metaxy.HashAlgorithm.FARMHASH  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FARMHASH = 'farmhash'\n</code></pre>"},{"location":"reference/api/types/#keys","title":"Keys","text":"<p>Types for working with feature and field keys.</p>"},{"location":"reference/api/types/#canonical-keys","title":"Canonical Keys","text":""},{"location":"reference/api/types/#metaxy.FeatureKey","title":"metaxy.FeatureKey","text":"<pre><code>FeatureKey(parts: str)\n</code></pre><pre><code>FeatureKey(parts: Sequence[str])\n</code></pre><pre><code>FeatureKey(parts: FeatureKey)\n</code></pre> <pre><code>FeatureKey(parts: str | Sequence[str] | FeatureKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Feature key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFeatureKey(\"a/b/c\")  # String format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey([\"a\", \"b\", \"c\"])  # List format\n# FeatureKey(parts=['a', 'b', 'c'])\n\nFeatureKey(FeatureKey([\"a\", \"b\", \"c\"]))  # FeatureKey copy\n# FeatureKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    parts: str | Sequence[str] | FeatureKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.FeatureKey.parts","title":"metaxy.FeatureKey.parts  <code>property</code>","text":"<pre><code>parts: tuple[str, ...]\n</code></pre> <p>Backward compatibility property for accessing root as parts.</p>"},{"location":"reference/api/types/#metaxy.FeatureKey.table_name","title":"metaxy.FeatureKey.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/types/#metaxy.FeatureKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FeatureKey.to_string","title":"metaxy.FeatureKey.to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.to_struct_key","title":"metaxy.FeatureKey.to_struct_key","text":"<pre><code>to_struct_key() -&gt; str\n</code></pre> <p>Convert to a name that can be used as struct key in databases</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_struct_key(self) -&gt; str:\n    \"\"\"Convert to a name that can be used as struct key in databases\"\"\"\n    return \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__repr__","title":"metaxy.FeatureKey.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__str__","title":"metaxy.FeatureKey.__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__lt__","title":"metaxy.FeatureKey.__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__le__","title":"metaxy.FeatureKey.__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__gt__","title":"metaxy.FeatureKey.__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__ge__","title":"metaxy.FeatureKey.__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__iter__","title":"metaxy.FeatureKey.__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__getitem__","title":"metaxy.FeatureKey.__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__len__","title":"metaxy.FeatureKey.__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__contains__","title":"metaxy.FeatureKey.__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__reversed__","title":"metaxy.FeatureKey.__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.model_dump","title":"metaxy.FeatureKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__hash__","title":"metaxy.FeatureKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.__eq__","title":"metaxy.FeatureKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FeatureKey.to_column_suffix","title":"metaxy.FeatureKey.to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey","title":"metaxy.FieldKey","text":"<pre><code>FieldKey(parts: str)\n</code></pre><pre><code>FieldKey(parts: Sequence[str])\n</code></pre><pre><code>FieldKey(parts: FieldKey)\n</code></pre> <pre><code>FieldKey(parts: str | Sequence[str] | FieldKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    parts: str | Sequence[str] | FieldKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey-attributes","title":"Attributes","text":""},{"location":"reference/api/types/#metaxy.FieldKey.parts","title":"metaxy.FieldKey.parts  <code>property</code>","text":"<pre><code>parts: tuple[str, ...]\n</code></pre> <p>Backward compatibility property for accessing root as parts.</p>"},{"location":"reference/api/types/#metaxy.FieldKey.table_name","title":"metaxy.FieldKey.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/types/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/types/#metaxy.FieldKey.to_string","title":"metaxy.FieldKey.to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.to_struct_key","title":"metaxy.FieldKey.to_struct_key","text":"<pre><code>to_struct_key() -&gt; str\n</code></pre> <p>Convert to a name that can be used as struct key in databases</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_struct_key(self) -&gt; str:\n    \"\"\"Convert to a name that can be used as struct key in databases\"\"\"\n    return \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.to_column_suffix","title":"metaxy.FieldKey.to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__repr__","title":"metaxy.FieldKey.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__str__","title":"metaxy.FieldKey.__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__lt__","title":"metaxy.FieldKey.__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__le__","title":"metaxy.FieldKey.__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__gt__","title":"metaxy.FieldKey.__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__ge__","title":"metaxy.FieldKey.__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__iter__","title":"metaxy.FieldKey.__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__getitem__","title":"metaxy.FieldKey.__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__len__","title":"metaxy.FieldKey.__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__contains__","title":"metaxy.FieldKey.__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__reversed__","title":"metaxy.FieldKey.__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.model_dump","title":"metaxy.FieldKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__hash__","title":"metaxy.FieldKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/types/#metaxy.FieldKey.__eq__","title":"metaxy.FieldKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/types/#type-annotations","title":"Type Annotations","text":"<p>These are typically used to annotate function parameters. Most APIs in Metaxy accepts them and perform type coercion into canonical types.</p>"},{"location":"reference/api/types/#metaxy.CoercibleToFeatureKey","title":"metaxy.CoercibleToFeatureKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFeatureKey: TypeAlias = str | Sequence[str] | FeatureKey | type['BaseFeature']\n</code></pre>"},{"location":"reference/api/types/#metaxy.CoercibleToFieldKey","title":"metaxy.CoercibleToFieldKey  <code>module-attribute</code>","text":"<pre><code>CoercibleToFieldKey: TypeAlias = str | Sequence[str] | FieldKey\n</code></pre>"},{"location":"reference/api/types/#pydantic-type-annotations","title":"Pydantic Type Annotations","text":"<p>These types are used for type coercion into canonical types with Pydantic.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKey","title":"metaxy.ValidatedFeatureKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKey: TypeAlias = FeatureKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKey","title":"metaxy.ValidatedFieldKey  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKey: TypeAlias = FieldKey\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequence","title":"metaxy.ValidatedFeatureKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequence: TypeAlias = Sequence[ValidatedFeatureKey]\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequence","title":"metaxy.ValidatedFieldKeySequence  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequence: TypeAlias = Sequence[ValidatedFieldKey]\n</code></pre>"},{"location":"reference/api/types/#adapters","title":"Adapters","text":"<p>These can perform type coercsion into canonical types in non-pydantic code.</p>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeyAdapter","title":"metaxy.ValidatedFeatureKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeyAdapter: TypeAdapter[ValidatedFeatureKey] = TypeAdapter(ValidatedFeatureKey)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFeatureKeySequenceAdapter","title":"metaxy.ValidatedFeatureKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFeatureKeySequenceAdapter: TypeAdapter[ValidatedFeatureKeySequence] = TypeAdapter(ValidatedFeatureKeySequence)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeyAdapter","title":"metaxy.ValidatedFieldKeyAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeyAdapter: TypeAdapter[ValidatedFieldKey] = TypeAdapter(ValidatedFieldKey)\n</code></pre>"},{"location":"reference/api/types/#metaxy.ValidatedFieldKeySequenceAdapter","title":"metaxy.ValidatedFieldKeySequenceAdapter  <code>module-attribute</code>","text":"<pre><code>ValidatedFieldKeySequenceAdapter: TypeAdapter[ValidatedFieldKeySequence] = TypeAdapter(ValidatedFieldKeySequence)\n</code></pre>"},{"location":"reference/api/types/#other-types","title":"Other Types","text":""},{"location":"reference/api/types/#metaxy.models.types.SnapshotPushResult","title":"metaxy.models.types.SnapshotPushResult","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Result of recording a feature graph snapshot.</p> <p>Attributes:</p> <ul> <li> <code>snapshot_version</code>               (<code>str</code>)           \u2013            <p>The deterministic hash of the graph snapshot</p> </li> <li> <code>already_pushed</code>               (<code>bool</code>)           \u2013            <p>True if this snapshot_version was already pushed previously</p> </li> <li> <code>updated_features</code>               (<code>list[str]</code>)           \u2013            <p>List of feature keys with updated information (changed full_definition_version)</p> </li> </ul>"},{"location":"reference/api/types/#metaxy.IDColumns","title":"metaxy.IDColumns  <code>module-attribute</code>","text":"<pre><code>IDColumns: TypeAlias = Sequence[str]\n</code></pre>"},{"location":"reference/api/definitions/","title":"Definitions","text":"<p>Metaxy's dependency specification system allows users to express dependencies between their features and their fields.</p>"},{"location":"reference/api/definitions/#featurespec","title":"<code>FeatureSpec</code>","text":"<p>FeatureSpec is the core of Metaxy's dependency specification system: it stores all the information about the parents, field mappings, and other metadata associated with a feature.</p>"},{"location":"reference/api/definitions/#feature","title":"Feature","text":"<p>A feature in Metaxy is used to model user-defined metadata. It must have a <code>FeatureSpec</code> instance associated with it. A <code>Feature</code> class is typically associated with a single table in the MetadataStore.</p>"},{"location":"reference/api/definitions/#fieldspec","title":"FieldSpec","text":"<p>A [field] in Metaxy is a logical slices of the data represented by feature metadata. Users are free to define their own fields as is suitable for them.</p> <p>Dependencies between fields are modeled with FieldDep and can be automatic (via field mappings) or explicitly set by users.</p>"},{"location":"reference/api/definitions/#graph","title":"Graph","text":"<p>All features live on a FeatureGraph object. The users don't typically interact with it outside of advanced use cases.</p>"},{"location":"reference/api/definitions/feature-spec/","title":"Feature Spec","text":"<p>Feature specs act as source of truth for all metadata related to features: their dependencies, fields, code versions, and so on.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec","title":"metaxy.FeatureSpec  <code>pydantic-model</code>","text":"<pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[FeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre><pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[CoercibleToFeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <pre><code>FeatureSpec(*, key: CoercibleToFeatureKey, id_columns: IDColumns, deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None, fields: Sequence[str | FieldSpec] | None = None, lineage: LineageRelationship | None = None, metadata: Mapping[str, JsonValue] | None = None, **kwargs: Any)\n</code></pre> <p>               Bases: <code>FrozenBaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"FeatureDep\": {\n      \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n\\n    # SQL filters\\n    FeatureDep(\\n        feature=\\\"upstream\\\",\\n        filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"]\\n    )\\n    ```\",\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\",\n          \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n        },\n        \"columns\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Columns\"\n        },\n        \"rename\": {\n          \"anyOf\": [\n            {\n              \"additionalProperties\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"object\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"title\": \"Rename\"\n        },\n        \"fields_mapping\": {\n          \"$ref\": \"#/$defs/FieldsMapping\"\n        },\n        \"filters\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"SQL-like filter strings applied to this dependency.\",\n          \"title\": \"Filters\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FeatureDep\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    },\n    \"JsonValue\": {},\n    \"LineageRelationship\": {\n      \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n      \"properties\": {\n        \"relationship\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"1:1\": \"#/$defs/IdentityRelationship\",\n              \"1:N\": \"#/$defs/ExpansionRelationship\",\n              \"N:1\": \"#/$defs/AggregationRelationship\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/IdentityRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/AggregationRelationship\"\n            },\n            {\n              \"$ref\": \"#/$defs/ExpansionRelationship\"\n            }\n          ],\n          \"title\": \"Relationship\"\n        }\n      },\n      \"required\": [\n        \"relationship\"\n      ],\n      \"title\": \"LineageRelationship\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"id_columns\": {\n      \"description\": \"Columns that uniquely identify a sample in this feature.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"Id Columns\",\n      \"type\": \"array\"\n    },\n    \"deps\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FeatureDep\"\n      },\n      \"title\": \"Deps\",\n      \"type\": \"array\"\n    },\n    \"fields\": {\n      \"items\": {\n        \"properties\": {\n          \"key\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"code_version\": {\n            \"default\": \"__metaxy_initial__\",\n            \"title\": \"Code Version\",\n            \"type\": \"string\"\n          },\n          \"deps\": {\n            \"anyOf\": [\n              {\n                \"$ref\": \"#/$defs/SpecialFieldDep\"\n              },\n              {\n                \"items\": {\n                  \"$ref\": \"#/$defs/FieldDep\"\n                },\n                \"type\": \"array\"\n              }\n            ],\n            \"title\": \"Deps\"\n          }\n        },\n        \"title\": \"FieldSpec\",\n        \"type\": \"object\"\n      },\n      \"title\": \"Fields\",\n      \"type\": \"array\"\n    },\n    \"lineage\": {\n      \"$ref\": \"#/$defs/LineageRelationship\",\n      \"description\": \"Lineage relationship of this feature.\"\n    },\n    \"metadata\": {\n      \"additionalProperties\": {\n        \"$ref\": \"#/$defs/JsonValue\"\n      },\n      \"description\": \"Metadata attached to this feature.\",\n      \"title\": \"Metadata\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"key\",\n    \"id_columns\"\n  ],\n  \"title\": \"FeatureSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FeatureKey</code>)             </li> <li> <code>id_columns</code>                 (<code>tuple[str, ...]</code>)             </li> <li> <code>deps</code>                 (<code>list[FeatureDep]</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldSpec]</code>)             </li> <li> <code>lineage</code>                 (<code>LineageRelationship</code>)             </li> <li> <code>metadata</code>                 (<code>dict[str, JsonValue]</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>validate_unique_field_keys</code> </li> <li> <code>validate_id_columns</code> </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    *,\n    key: CoercibleToFeatureKey,\n    id_columns: IDColumns,\n    deps: list[FeatureDep] | list[CoercibleToFeatureDep] | None = None,\n    fields: Sequence[str | FieldSpec] | None = None,\n    lineage: LineageRelationship | None = None,\n    metadata: Mapping[str, JsonValue] | None = None,\n    **kwargs: Any,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.id_columns","title":"metaxy.FeatureSpec.id_columns  <code>pydantic-field</code>","text":"<pre><code>id_columns: tuple[str, ...]\n</code></pre> <p>Columns that uniquely identify a sample in this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.lineage","title":"metaxy.FeatureSpec.lineage  <code>pydantic-field</code>","text":"<pre><code>lineage: LineageRelationship\n</code></pre> <p>Lineage relationship of this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.metadata","title":"metaxy.FeatureSpec.metadata  <code>pydantic-field</code>","text":"<pre><code>metadata: dict[str, JsonValue]\n</code></pre> <p>Metadata attached to this feature.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.code_version","title":"metaxy.FeatureSpec.code_version  <code>cached</code> <code>property</code>","text":"<pre><code>code_version: str\n</code></pre> <p>Hash of this feature's field code_versions only (no dependencies).</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.feature_spec_version","title":"metaxy.FeatureSpec.feature_spec_version  <code>property</code>","text":"<pre><code>feature_spec_version: str\n</code></pre> <p>Compute SHA256 hash of the complete feature specification.</p> <p>This property provides a deterministic hash of ALL specification properties, including key, deps, fields, and any metadata/tags. Used for audit trail and tracking specification changes.</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the specification</p> </li> </ul> Example <pre><code>spec = FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]))],\n)\nspec.feature_spec_version\n# 'abc123...'  # 64-character hex string\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.table_name","title":"metaxy.FeatureSpec.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.key.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_unique_field_keys","title":"metaxy.FeatureSpec.validate_unique_field_keys  <code>pydantic-validator</code>","text":"<pre><code>validate_unique_field_keys() -&gt; Self\n</code></pre> <p>Validate that all fields have unique keys.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_unique_field_keys(self) -&gt; Self:\n    \"\"\"Validate that all fields have unique keys.\"\"\"\n    seen_keys: set[tuple[str, ...]] = set()\n    for field in self.fields:\n        # Convert to tuple for hashability in case it's a plain list\n        key_tuple = tuple(field.key)\n        if key_tuple in seen_keys:\n            raise ValueError(\n                f\"Duplicate field key found: {field.key}. \"\n                f\"All fields must have unique keys.\"\n            )\n        seen_keys.add(key_tuple)\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureSpec.validate_id_columns","title":"metaxy.FeatureSpec.validate_id_columns  <code>pydantic-validator</code>","text":"<pre><code>validate_id_columns() -&gt; Self\n</code></pre> <p>Validate that id_columns is non-empty if specified.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>@pydantic.model_validator(mode=\"after\")\ndef validate_id_columns(self) -&gt; Self:\n    \"\"\"Validate that id_columns is non-empty if specified.\"\"\"\n    if self.id_columns is not None and len(self.id_columns) == 0:\n        raise ValueError(\n            \"id_columns must be non-empty if specified. Use None for default.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#feature-dependencies","title":"Feature Dependencies","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep","title":"metaxy.FeatureDep  <code>pydantic-model</code>","text":"<pre><code>FeatureDep(*, feature: str | Sequence[str] | FeatureKey | type[BaseFeature], columns: tuple[str, ...] | None = None, rename: dict[str, str] | None = None, fields_mapping: FieldsMapping | None = None, filters: Sequence[str] | None = None)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Feature dependency specification with optional column selection and renaming.</p> <p>Attributes:</p> <ul> <li> <code>key</code>           \u2013            <p>The feature key to depend on. Accepts string (\"a/b/c\"), list ([\"a\", \"b\", \"c\"]), FeatureKey instance, or BaseFeature class.</p> </li> <li> <code>columns</code>               (<code>tuple[str, ...] | None</code>)           \u2013            <p>Optional tuple of column names to select from upstream feature. - None (default): Keep all columns from upstream - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.) - Tuple of names: Keep only specified columns (plus system columns)</p> </li> <li> <code>rename</code>               (<code>dict[str, str] | None</code>)           \u2013            <p>Optional mapping of old column names to new names. Applied after column selection.</p> </li> <li> <code>fields_mapping</code>               (<code>FieldsMapping</code>)           \u2013            <p>Optional field mapping configuration for automatic field dependency resolution. When provided, fields without explicit deps will automatically map to matching upstream fields. Defaults to using <code>[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]</code>.</p> </li> <li> <code>filters</code>               (<code>tuple[Expr, ...]</code>)           \u2013            <p>Optional SQL-like filter strings applied to this dependency. Automatically parsed into Narwhals expressions (accessible via the <code>filters</code> property). Filters are automatically applied by FeatureDepTransformer after renames during all FeatureDep operations (including resolve_update and version computation).</p> </li> </ul> <p>Examples:</p> <pre><code># Keep all columns with default field mapping\nFeatureDep(feature=\"upstream\")\n\n# Keep all columns with suffix matching\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.default(match_suffix=True))\n\n# Keep all columns with all fields mapping\nFeatureDep(feature=\"upstream\", fields_mapping=FieldsMapping.all())\n\n# Keep only specific columns\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\")\n)\n\n# Rename columns to avoid conflicts\nFeatureDep(\n    feature=\"upstream/feature\",\n    rename={\"old_name\": \"new_name\"}\n)\n\n# Select and rename\nFeatureDep(\n    feature=\"upstream/feature\",\n    columns=(\"col1\", \"col2\"),\n    rename={\"col1\": \"upstream_col1\"}\n)\n\n# SQL filters\nFeatureDep(\n    feature=\"upstream\",\n    filters=[\"age &gt;= 25\", \"status = 'active'\"]\n)\n</code></pre> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"FieldsMapping\": {\n      \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n      \"properties\": {\n        \"mapping\": {\n          \"discriminator\": {\n            \"mapping\": {\n              \"all\": \"#/$defs/AllFieldsMapping\",\n              \"default\": \"#/$defs/DefaultFieldsMapping\",\n              \"none\": \"#/$defs/NoneFieldsMapping\",\n              \"specific\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            \"propertyName\": \"type\"\n          },\n          \"oneOf\": [\n            {\n              \"$ref\": \"#/$defs/AllFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/NoneFieldsMapping\"\n            },\n            {\n              \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n            }\n          ],\n          \"title\": \"Mapping\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"FieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Feature dependency specification with optional column selection and renaming.\\n\\nAttributes:\\n    key: The feature key to depend on. Accepts string (\\\"a/b/c\\\"), list ([\\\"a\\\", \\\"b\\\", \\\"c\\\"]),\\n        FeatureKey instance, or BaseFeature class.\\n    columns: Optional tuple of column names to select from upstream feature.\\n        - None (default): Keep all columns from upstream\\n        - Empty tuple (): Keep only system columns (sample_uid, provenance_by_field, etc.)\\n        - Tuple of names: Keep only specified columns (plus system columns)\\n    rename: Optional mapping of old column names to new names.\\n        Applied after column selection.\\n    fields_mapping: Optional field mapping configuration for automatic field dependency resolution.\\n        When provided, fields without explicit deps will automatically map to matching upstream fields.\\n        Defaults to using `[FieldsMapping.default()][metaxy.models.fields_mapping.DefaultFieldsMapping]`.\\n    filters: Optional SQL-like filter strings applied to this dependency. Automatically parsed into\\n        Narwhals expressions (accessible via the `filters` property). Filters are automatically\\n        applied by FeatureDepTransformer after renames during all FeatureDep operations (including\\n        resolve_update and version computation).\\n\\nExamples:\\n    ```py\\n    # Keep all columns with default field mapping\\n    FeatureDep(feature=\\\"upstream\\\")\\n\\n    # Keep all columns with suffix matching\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.default(match_suffix=True))\\n\\n    # Keep all columns with all fields mapping\\n    FeatureDep(feature=\\\"upstream\\\", fields_mapping=FieldsMapping.all())\\n\\n    # Keep only specific columns\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\")\\n    )\\n\\n    # Rename columns to avoid conflicts\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        rename={\\\"old_name\\\": \\\"new_name\\\"}\\n    )\\n\\n    # Select and rename\\n    FeatureDep(\\n        feature=\\\"upstream/feature\\\",\\n        columns=(\\\"col1\\\", \\\"col2\\\"),\\n        rename={\\\"col1\\\": \\\"upstream_col1\\\"}\\n    )\\n\\n    # SQL filters\\n    FeatureDep(\\n        feature=\\\"upstream\\\",\\n        filters=[\\\"age &gt;= 25\\\", \\\"status = 'active'\\\"]\\n    )\\n    ```\",\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\",\n      \"description\": \"Feature key. Accepts a slashed string ('a/b/c'), a sequence of strings, a FeatureKey instance, or a child class of BaseFeature\"\n    },\n    \"columns\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Columns\"\n    },\n    \"rename\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"title\": \"Rename\"\n    },\n    \"fields_mapping\": {\n      \"$ref\": \"#/$defs/FieldsMapping\"\n    },\n    \"filters\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"SQL-like filter strings applied to this dependency.\",\n      \"title\": \"Filters\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FeatureDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>ValidatedFeatureKey</code>)             </li> <li> <code>columns</code>                 (<code>tuple[str, ...] | None</code>)             </li> <li> <code>rename</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>fields_mapping</code>                 (<code>FieldsMapping</code>)             </li> <li> <code>sql_filters</code>                 (<code>tuple[str, ...] | None</code>)             </li> </ul> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    *,\n    feature: str | Sequence[str] | FeatureKey | type[BaseFeature],\n    columns: tuple[str, ...] | None = None,\n    rename: dict[str, str] | None = None,\n    fields_mapping: FieldsMapping | None = None,\n    filters: Sequence[str] | None = None,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.sql_filters","title":"metaxy.FeatureDep.sql_filters  <code>pydantic-field</code>","text":"<pre><code>sql_filters: tuple[str, ...] | None = None\n</code></pre> <p>SQL-like filter strings applied to this dependency.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.filters","title":"metaxy.FeatureDep.filters  <code>cached</code> <code>property</code>","text":"<pre><code>filters: tuple[Expr, ...]\n</code></pre> <p>Parse sql_filters into Narwhals expressions.</p>"},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature-spec/#metaxy.FeatureDep.table_name","title":"metaxy.FeatureDep.table_name","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature spec.</p> Source code in <code>src/metaxy/models/feature_spec.py</code> <pre><code>def table_name(self) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature spec.\"\"\"\n    return self.feature.table_name\n</code></pre>"},{"location":"reference/api/definitions/feature/","title":"Feature","text":"<p><code>BaseFeature</code> is the most important class in Metaxy. Features are defined by extending it.</p> <p>Code Version Access</p> <p>Retrieve a feature's code version from its spec: <code>MyFeature.spec().code_version</code>.</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature","title":"metaxy.BaseFeature  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"properties\": {\n    \"metaxy_provenance_by_field\": {\n      \"additionalProperties\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Field-level provenance hashes (maps field names to hashes)\",\n      \"title\": \"Metaxy Provenance By Field\",\n      \"type\": \"object\"\n    },\n    \"metaxy_provenance\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_provenance_by_field\",\n      \"title\": \"Metaxy Provenance\"\n    },\n    \"metaxy_feature_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the feature definition (dependencies + fields + code_versions)\",\n      \"title\": \"Metaxy Feature Version\"\n    },\n    \"metaxy_snapshot_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of the entire feature graph snapshot\",\n      \"title\": \"Metaxy Snapshot Version\"\n    },\n    \"metaxy_data_version_by_field\": {\n      \"anyOf\": [\n        {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Field-level data version hashes (maps field names to version hashes)\",\n      \"title\": \"Metaxy Data Version By Field\"\n    },\n    \"metaxy_data_version\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Hash of metaxy_data_version_by_field\",\n      \"title\": \"Metaxy Data Version\"\n    },\n    \"metaxy_created_at\": {\n      \"anyOf\": [\n        {\n          \"format\": \"date-time\",\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Timestamp when the metadata row was created (UTC)\",\n      \"title\": \"Metaxy Created At\"\n    },\n    \"metaxy_materialization_id\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"External orchestration run ID (e.g., Dagster Run ID)\",\n      \"title\": \"Metaxy Materialization Id\"\n    }\n  },\n  \"title\": \"BaseFeature\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>metaxy_provenance_by_field</code>                 (<code>dict[str, str]</code>)             </li> <li> <code>metaxy_provenance</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_feature_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_snapshot_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_data_version_by_field</code>                 (<code>dict[str, str] | None</code>)             </li> <li> <code>metaxy_data_version</code>                 (<code>str | None</code>)             </li> <li> <code>metaxy_created_at</code>                 (<code>AwareDatetime | None</code>)             </li> <li> <code>metaxy_materialization_id</code>                 (<code>str | None</code>)             </li> </ul> <p>Validators:</p> <ul> <li> <code>_validate_id_columns_exist</code> </li> </ul>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance_by_field","title":"metaxy.BaseFeature.metaxy_provenance_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance_by_field: dict[str, str]\n</code></pre> <p>Field-level provenance hashes (maps field names to hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_provenance","title":"metaxy.BaseFeature.metaxy_provenance  <code>pydantic-field</code>","text":"<pre><code>metaxy_provenance: str | None = None\n</code></pre> <p>Hash of metaxy_provenance_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_feature_version","title":"metaxy.BaseFeature.metaxy_feature_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_feature_version: str | None = None\n</code></pre> <p>Hash of the feature definition (dependencies + fields + code_versions)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_snapshot_version","title":"metaxy.BaseFeature.metaxy_snapshot_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_snapshot_version: str | None = None\n</code></pre> <p>Hash of the entire feature graph snapshot</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version_by_field","title":"metaxy.BaseFeature.metaxy_data_version_by_field  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version_by_field: dict[str, str] | None = None\n</code></pre> <p>Field-level data version hashes (maps field names to version hashes)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_data_version","title":"metaxy.BaseFeature.metaxy_data_version  <code>pydantic-field</code>","text":"<pre><code>metaxy_data_version: str | None = None\n</code></pre> <p>Hash of metaxy_data_version_by_field</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_created_at","title":"metaxy.BaseFeature.metaxy_created_at  <code>pydantic-field</code>","text":"<pre><code>metaxy_created_at: AwareDatetime | None = None\n</code></pre> <p>Timestamp when the metadata row was created (UTC)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.metaxy_materialization_id","title":"metaxy.BaseFeature.metaxy_materialization_id  <code>pydantic-field</code>","text":"<pre><code>metaxy_materialization_id: str | None = None\n</code></pre> <p>External orchestration run ID (e.g., Dagster Run ID)</p>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature-functions","title":"Functions","text":""},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.table_name","title":"metaxy.BaseFeature.table_name  <code>classmethod</code>","text":"<pre><code>table_name() -&gt; str\n</code></pre> <p>Get SQL-like table name for this feature.</p> <p>Converts feature key to SQL-compatible table name by joining parts with double underscores, consistent with IbisMetadataStore.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Table name string (e.g., \"my_namespace__my_feature\")</p> </li> </ul> Example <pre><code>class VideoFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"video\", \"processing\"]),\n    ...\n)):\n    pass\nVideoFeature.table_name()\n# 'video__processing'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef table_name(cls) -&gt; str:\n    \"\"\"Get SQL-like table name for this feature.\n\n    Converts feature key to SQL-compatible table name by joining\n    parts with double underscores, consistent with IbisMetadataStore.\n\n    Returns:\n        Table name string (e.g., \"my_namespace__my_feature\")\n\n    Example:\n        ```py\n        class VideoFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"video\", \"processing\"]),\n            ...\n        )):\n            pass\n        VideoFeature.table_name()\n        # 'video__processing'\n        ```\n    \"\"\"\n    return cls.spec().table_name()\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_version","title":"metaxy.BaseFeature.feature_version  <code>classmethod</code>","text":"<pre><code>feature_version() -&gt; str\n</code></pre> <p>Get hash of feature specification.</p> <p>Returns a hash representing the feature's complete configuration: - Feature key - Field definitions and code versions - Dependencies (feature-level and field-level)</p> <p>This hash changes when you modify: - Field code versions - Dependencies - Field definitions</p> <p>Used to distinguish current vs historical metafield provenance hashes. Stored in the 'metaxy_feature_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest (like git short hashes)</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_version()\n# 'a3f8b2c1...'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_version(cls) -&gt; str:\n    \"\"\"Get hash of feature specification.\n\n    Returns a hash representing the feature's complete configuration:\n    - Feature key\n    - Field definitions and code versions\n    - Dependencies (feature-level and field-level)\n\n    This hash changes when you modify:\n    - Field code versions\n    - Dependencies\n    - Field definitions\n\n    Used to distinguish current vs historical metafield provenance hashes.\n    Stored in the 'metaxy_feature_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest (like git short hashes)\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_version()\n        # 'a3f8b2c1...'\n        ```\n    \"\"\"\n    return cls.graph.get_feature_version(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.feature_spec_version","title":"metaxy.BaseFeature.feature_spec_version  <code>classmethod</code>","text":"<pre><code>feature_spec_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature specification.</p> <p>Returns a hash representing ALL specification properties including: - Feature key - Dependencies - Fields - Code versions - Any future metadata, tags, or other properties</p> <p>Unlike feature_version which only hashes computational properties (for migration triggering), feature_spec_version captures the entire specification for complete reproducibility and audit purposes.</p> <p>Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete specification</p> </li> </ul> Example <pre><code>class MyFeature(Feature, spec=FeatureSpec(\n    key=FeatureKey([\"my\", \"feature\"]),\n    fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n)):\n    pass\nMyFeature.feature_spec_version()\n# 'def456...'  # Different from feature_version\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef feature_spec_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature specification.\n\n    Returns a hash representing ALL specification properties including:\n    - Feature key\n    - Dependencies\n    - Fields\n    - Code versions\n    - Any future metadata, tags, or other properties\n\n    Unlike feature_version which only hashes computational properties\n    (for migration triggering), feature_spec_version captures the entire specification\n    for complete reproducibility and audit purposes.\n\n    Stored in the 'metaxy_feature_spec_version' column of metadata DataFrames.\n\n    Returns:\n        SHA256 hex digest of the complete specification\n\n    Example:\n        ```py\n        class MyFeature(Feature, spec=FeatureSpec(\n            key=FeatureKey([\"my\", \"feature\"]),\n            fields=[FieldSpec(key=FieldKey([\"default\"]), code_version=\"1\")],\n        )):\n            pass\n        MyFeature.feature_spec_version()\n        # 'def456...'  # Different from feature_version\n        ```\n    \"\"\"\n    return cls.spec().feature_spec_version\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.full_definition_version","title":"metaxy.BaseFeature.full_definition_version  <code>classmethod</code>","text":"<pre><code>full_definition_version() -&gt; str\n</code></pre> <p>Get hash of the complete feature definition including Pydantic schema.</p> <p>This method computes a hash of the entire feature class definition, including: - Pydantic model schema - Project name</p> <p>Used in the <code>metaxy_full_definition_version</code> column of system tables.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>SHA256 hex digest of the complete definition</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef full_definition_version(cls) -&gt; str:\n    \"\"\"Get hash of the complete feature definition including Pydantic schema.\n\n    This method computes a hash of the entire feature class definition, including:\n    - Pydantic model schema\n    - Project name\n\n    Used in the `metaxy_full_definition_version` column of system tables.\n\n    Returns:\n        SHA256 hex digest of the complete definition\n    \"\"\"\n    import json\n\n    hasher = hashlib.sha256()\n\n    # Hash the Pydantic schema (includes field types, descriptions, validators, etc.)\n    schema = cls.model_json_schema()\n    schema_json = json.dumps(schema, sort_keys=True)\n    hasher.update(schema_json.encode())\n\n    # Hash the feature specification\n    hasher.update(cls.feature_spec_version().encode())\n\n    # Hash the project name\n    hasher.update(cls.project.encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.provenance_by_field","title":"metaxy.BaseFeature.provenance_by_field  <code>classmethod</code>","text":"<pre><code>provenance_by_field() -&gt; dict[str, str]\n</code></pre> <p>Get the code-level field provenance for this feature.</p> <p>This returns a static hash based on code versions and dependencies, not sample-level field provenance computed from upstream data.</p> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>Dictionary mapping field keys to their provenance hashes.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef provenance_by_field(cls) -&gt; dict[str, str]:\n    \"\"\"Get the code-level field provenance for this feature.\n\n    This returns a static hash based on code versions and dependencies,\n    not sample-level field provenance computed from upstream data.\n\n    Returns:\n        Dictionary mapping field keys to their provenance hashes.\n    \"\"\"\n    return cls.graph.get_feature_version_by_field(cls.spec().key)\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.load_input","title":"metaxy.BaseFeature.load_input  <code>classmethod</code>","text":"<pre><code>load_input(joiner: Any, upstream_refs: dict[str, LazyFrame[Any]]) -&gt; tuple[LazyFrame[Any], dict[str, str]]\n</code></pre> <p>Join upstream feature metadata.</p> <p>Override for custom join logic (1:many, different keys, filtering, etc.).</p> <p>Parameters:</p> <ul> <li> <code>joiner</code>               (<code>Any</code>)           \u2013            <p>UpstreamJoiner from MetadataStore</p> </li> <li> <code>upstream_refs</code>               (<code>dict[str, LazyFrame[Any]]</code>)           \u2013            <p>Upstream feature metadata references (lazy where possible)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>(joined_upstream, upstream_column_mapping)</p> </li> <li> <code>dict[str, str]</code>           \u2013            <ul> <li>joined_upstream: All upstream data joined together</li> </ul> </li> <li> <code>tuple[LazyFrame[Any], dict[str, str]]</code>           \u2013            <ul> <li>upstream_column_mapping: Maps upstream_key -&gt; column name</li> </ul> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef load_input(\n    cls,\n    joiner: Any,\n    upstream_refs: dict[str, \"nw.LazyFrame[Any]\"],\n) -&gt; tuple[\"nw.LazyFrame[Any]\", dict[str, str]]:\n    \"\"\"Join upstream feature metadata.\n\n    Override for custom join logic (1:many, different keys, filtering, etc.).\n\n    Args:\n        joiner: UpstreamJoiner from MetadataStore\n        upstream_refs: Upstream feature metadata references (lazy where possible)\n\n    Returns:\n        (joined_upstream, upstream_column_mapping)\n        - joined_upstream: All upstream data joined together\n        - upstream_column_mapping: Maps upstream_key -&gt; column name\n    \"\"\"\n    from metaxy.models.feature_spec import FeatureDep\n\n    # Extract columns and renames from deps\n    upstream_columns: dict[str, tuple[str, ...] | None] = {}\n    upstream_renames: dict[str, dict[str, str] | None] = {}\n\n    deps = cls.spec().deps\n    if deps:\n        for dep in deps:\n            if isinstance(dep, FeatureDep):\n                dep_key_str = dep.feature.to_string()\n                upstream_columns[dep_key_str] = dep.columns\n                upstream_renames[dep_key_str] = dep.rename\n\n    return joiner.join_upstream(\n        upstream_refs=upstream_refs,\n        feature_spec=cls.spec(),\n        feature_plan=cls.graph.get_feature_plan(cls.spec().key),\n        upstream_columns=upstream_columns,\n        upstream_renames=upstream_renames,\n    )\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.BaseFeature.resolve_data_version_diff","title":"metaxy.BaseFeature.resolve_data_version_diff  <code>classmethod</code>","text":"<pre><code>resolve_data_version_diff(diff_resolver: Any, target_provenance: LazyFrame[Any], current_metadata: LazyFrame[Any] | None, *, lazy: bool = False) -&gt; Increment | LazyIncrement\n</code></pre> <p>Resolve differences between target and current field provenance.</p> <p>Override for custom diff logic (ignore certain fields, custom rules, etc.).</p> <p>Parameters:</p> <ul> <li> <code>diff_resolver</code>               (<code>Any</code>)           \u2013            <p>MetadataDiffResolver from MetadataStore</p> </li> <li> <code>target_provenance</code>               (<code>LazyFrame[Any]</code>)           \u2013            <p>Calculated target field provenance (Narwhals LazyFrame)</p> </li> <li> <code>current_metadata</code>               (<code>LazyFrame[Any] | None</code>)           \u2013            <p>Current metadata for this feature (Narwhals LazyFrame, or None). Should be pre-filtered by feature_version at the store level.</p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, return LazyIncrement. If False, return Increment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Increment | LazyIncrement</code>           \u2013            <p>Increment (eager) or LazyIncrement (lazy) with added, changed, removed</p> </li> </ul> <p>Example (default):     <pre><code>class MyFeature(Feature, spec=...):\n    pass  # Uses diff resolver's default implementation\n</code></pre></p> <p>Example (ignore certain field changes):     <pre><code>class MyFeature(Feature, spec=...):\n    @classmethod\n    def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n        # Get standard diff\n        result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n        # Custom: Only consider 'frames' field changes, ignore 'audio'\n        # Users can filter/modify the increment here\n\n        return result  # Return modified Increment\n</code></pre></p> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef resolve_data_version_diff(\n    cls,\n    diff_resolver: Any,\n    target_provenance: \"nw.LazyFrame[Any]\",\n    current_metadata: \"nw.LazyFrame[Any] | None\",\n    *,\n    lazy: bool = False,\n) -&gt; \"Increment | LazyIncrement\":\n    \"\"\"Resolve differences between target and current field provenance.\n\n    Override for custom diff logic (ignore certain fields, custom rules, etc.).\n\n    Args:\n        diff_resolver: MetadataDiffResolver from MetadataStore\n        target_provenance: Calculated target field provenance (Narwhals LazyFrame)\n        current_metadata: Current metadata for this feature (Narwhals LazyFrame, or None).\n            Should be pre-filtered by feature_version at the store level.\n        lazy: If True, return LazyIncrement. If False, return Increment.\n\n    Returns:\n        Increment (eager) or LazyIncrement (lazy) with added, changed, removed\n\n    Example (default):\n        ```py\n        class MyFeature(Feature, spec=...):\n            pass  # Uses diff resolver's default implementation\n        ```\n\n    Example (ignore certain field changes):\n        ```py\n        class MyFeature(Feature, spec=...):\n            @classmethod\n            def resolve_data_version_diff(cls, diff_resolver, target_provenance, current_metadata, **kwargs):\n                # Get standard diff\n                result = diff_resolver.find_changes(target_provenance, current_metadata, cls.spec().id_columns)\n\n                # Custom: Only consider 'frames' field changes, ignore 'audio'\n                # Users can filter/modify the increment here\n\n                return result  # Return modified Increment\n        ```\n    \"\"\"\n    # Diff resolver always returns LazyIncrement - materialize if needed\n    lazy_result = diff_resolver.find_changes(\n        target_provenance=target_provenance,\n        current_metadata=current_metadata,\n        id_columns=cls.spec().id_columns,  # Pass ID columns from feature spec\n    )\n\n    # Materialize to Increment if lazy=False\n    if not lazy:\n        from metaxy.versioning.types import Increment\n\n        return Increment(\n            added=lazy_result.added.collect(),\n            changed=lazy_result.changed.collect(),\n            removed=lazy_result.removed.collect(),\n        )\n\n    return lazy_result\n</code></pre>"},{"location":"reference/api/definitions/feature/#metaxy.get_feature_by_key","title":"metaxy.get_feature_by_key","text":"<pre><code>get_feature_by_key(key: CoercibleToFeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key from the active graph.</p> <p>Convenience function that retrieves Metaxy feature class from the currently active feature graph. Can be useful when receiving a feature key from storage or across process boundaries.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>from metaxy import get_feature_by_key, FeatureKey\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = get_feature_by_key(parent_key)\n\n# Or use string notation\nParentFeature = get_feature_by_key(\"examples/parent\")\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(key: CoercibleToFeatureKey) -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key from the active graph.\n\n    Convenience function that retrieves Metaxy feature class from the currently active [feature graph][metaxy.FeatureGraph]. Can be useful when receiving a feature key from storage or across process boundaries.\n\n    Args:\n        key: Feature key to look up. Accepts types that can be converted into a feature key..\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        from metaxy import get_feature_by_key, FeatureKey\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = get_feature_by_key(parent_key)\n\n        # Or use string notation\n        ParentFeature = get_feature_by_key(\"examples/parent\")\n        ```\n    \"\"\"\n    graph = FeatureGraph.get_active()\n    return graph.get_feature_by_key(key)\n</code></pre>"},{"location":"reference/api/definitions/field/","title":"Field","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey","title":"metaxy.FieldKey","text":"<pre><code>FieldKey(parts: str | Sequence[str] | FieldKey)\n</code></pre> <p>               Bases: <code>_Key</code></p> <p>Field key as a sequence of string parts.</p> <p>Hashable for use as dict keys in registries. Parts cannot contain forward slashes (/) or double underscores (__).</p> <p>Example:</p> <pre><code>```py\nFieldKey(\"a/b/c\")  # String format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey([\"a\", \"b\", \"c\"])  # List format\n# FieldKey(parts=['a', 'b', 'c'])\n\nFieldKey(FieldKey([\"a\", \"b\", \"c\"]))  # FieldKey copy\n# FieldKey(parts=['a', 'b', 'c'])\n```\n</code></pre> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    parts: str | Sequence[str] | FieldKey,\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.parts","title":"metaxy.FieldKey.parts  <code>property</code>","text":"<pre><code>parts: tuple[str, ...]\n</code></pre> <p>Backward compatibility property for accessing root as parts.</p>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.table_name","title":"metaxy.FieldKey.table_name  <code>property</code>","text":"<pre><code>table_name: str\n</code></pre> <p>Get SQL-like table name for this feature key.</p> <p>Replaces hyphens with underscores for SQL compatibility.</p>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_string","title":"metaxy.FieldKey.to_string","text":"<pre><code>to_string() -&gt; str\n</code></pre> <p>Convert to string representation with \"/\" separator.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert to string representation with \"/\" separator.\"\"\"\n    return KEY_SEPARATOR.join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_struct_key","title":"metaxy.FieldKey.to_struct_key","text":"<pre><code>to_struct_key() -&gt; str\n</code></pre> <p>Convert to a name that can be used as struct key in databases</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_struct_key(self) -&gt; str:\n    \"\"\"Convert to a name that can be used as struct key in databases\"\"\"\n    return \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.to_column_suffix","title":"metaxy.FieldKey.to_column_suffix","text":"<pre><code>to_column_suffix() -&gt; str\n</code></pre> <p>Convert to a suffix usable for database column names (typically temporary).</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def to_column_suffix(self) -&gt; str:\n    \"\"\"Convert to a suffix usable for database column names (typically temporary).\"\"\"\n    return \"__\" + \"_\".join(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__repr__","title":"metaxy.FieldKey.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__str__","title":"metaxy.FieldKey.__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Return string representation.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation.\"\"\"\n    return self.to_string()\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__lt__","title":"metaxy.FieldKey.__lt__","text":"<pre><code>__lt__(other: Any) -&gt; bool\n</code></pre> <p>Less than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Less than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__le__","title":"metaxy.FieldKey.__le__","text":"<pre><code>__le__(other: Any) -&gt; bool\n</code></pre> <p>Less than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Less than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &lt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__gt__","title":"metaxy.FieldKey.__gt__","text":"<pre><code>__gt__(other: Any) -&gt; bool\n</code></pre> <p>Greater than comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt; other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__ge__","title":"metaxy.FieldKey.__ge__","text":"<pre><code>__ge__(other: Any) -&gt; bool\n</code></pre> <p>Greater than or equal comparison for sorting.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Greater than or equal comparison for sorting.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts &gt;= other.parts\n    return NotImplemented\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__iter__","title":"metaxy.FieldKey.__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Return iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    \"\"\"Return iterator over parts.\"\"\"\n    return iter(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__getitem__","title":"metaxy.FieldKey.__getitem__","text":"<pre><code>__getitem__(index: int) -&gt; str\n</code></pre> <p>Get part by index.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __getitem__(self, index: int) -&gt; str:\n    \"\"\"Get part by index.\"\"\"\n    return self.parts[index]\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__len__","title":"metaxy.FieldKey.__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of parts.\"\"\"\n    return len(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__contains__","title":"metaxy.FieldKey.__contains__","text":"<pre><code>__contains__(item: str) -&gt; bool\n</code></pre> <p>Check if part is in key.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __contains__(self, item: str) -&gt; bool:\n    \"\"\"Check if part is in key.\"\"\"\n    return item in self.parts\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__reversed__","title":"metaxy.FieldKey.__reversed__","text":"<pre><code>__reversed__()\n</code></pre> <p>Return reversed iterator over parts.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __reversed__(self):\n    \"\"\"Return reversed iterator over parts.\"\"\"\n    return reversed(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.model_dump","title":"metaxy.FieldKey.model_dump","text":"<pre><code>model_dump(**kwargs: Any) -&gt; Any\n</code></pre> <p>Serialize to list format for backward compatibility.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; Any:\n    \"\"\"Serialize to list format for backward compatibility.\"\"\"\n    # When serializing this key, return it as a list of parts\n    # instead of the full Pydantic model structure\n    return list(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__hash__","title":"metaxy.FieldKey.__hash__","text":"<pre><code>__hash__() -&gt; int\n</code></pre> <p>Return hash for use as dict keys.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash for use as dict keys.\"\"\"\n    return hash(self.parts)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldKey.__eq__","title":"metaxy.FieldKey.__eq__","text":"<pre><code>__eq__(other: Any) -&gt; bool\n</code></pre> <p>Check equality with another instance.</p> Source code in <code>src/metaxy/models/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Check equality with another instance.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.parts == other.parts\n    return super().__eq__(other)\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec","title":"metaxy.FieldSpec  <code>pydantic-model</code>","text":"<pre><code>FieldSpec(key: CoercibleToFieldKey, code_version: str = DEFAULT_CODE_VERSION, deps: SpecialFieldDep | list[FieldDep] | None = None, *args, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldDep\": {\n      \"properties\": {\n        \"feature\": {\n          \"$ref\": \"#/$defs/FeatureKey\"\n        },\n        \"fields\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"$ref\": \"#/$defs/FieldKey\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"const\": \"__METAXY_ALL_DEP__\",\n              \"type\": \"string\"\n            }\n          ],\n          \"default\": \"__METAXY_ALL_DEP__\",\n          \"title\": \"Fields\"\n        }\n      },\n      \"required\": [\n        \"feature\"\n      ],\n      \"title\": \"FieldDep\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"SpecialFieldDep\": {\n      \"enum\": [\n        \"__METAXY_ALL_DEP__\"\n      ],\n      \"title\": \"SpecialFieldDep\",\n      \"type\": \"string\"\n    }\n  },\n  \"properties\": {\n    \"key\": {\n      \"$ref\": \"#/$defs/FieldKey\"\n    },\n    \"code_version\": {\n      \"default\": \"__metaxy_initial__\",\n      \"title\": \"Code Version\",\n      \"type\": \"string\"\n    },\n    \"deps\": {\n      \"anyOf\": [\n        {\n          \"$ref\": \"#/$defs/SpecialFieldDep\"\n        },\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldDep\"\n          },\n          \"type\": \"array\"\n        }\n      ],\n      \"title\": \"Deps\"\n    }\n  },\n  \"title\": \"FieldSpec\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>key</code>                 (<code>FieldKey</code>)             </li> <li> <code>code_version</code>                 (<code>str</code>)             </li> <li> <code>deps</code>                 (<code>SpecialFieldDep | list[FieldDep]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(\n    self,\n    key: CoercibleToFieldKey,\n    code_version: str = DEFAULT_CODE_VERSION,\n    deps: SpecialFieldDep | list[FieldDep] | None = None,\n    *args,\n    **kwargs: Any,\n) -&gt; None:\n    validated_key = FieldKeyAdapter.validate_python(key)\n\n    # Handle None deps - use empty list as default\n    if deps is None:\n        deps = []\n\n    super().__init__(\n        key=validated_key,\n        code_version=code_version,\n        deps=deps,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldSpec-functions","title":"Functions","text":""},{"location":"reference/api/definitions/field/#metaxy.FieldSpec.__get_pydantic_core_schema__","title":"metaxy.FieldSpec.__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(source_type, handler)\n</code></pre> <p>Add custom validator to coerce strings to FieldSpec.</p> Source code in <code>src/metaxy/models/field.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(cls, source_type, handler):\n    \"\"\"Add custom validator to coerce strings to FieldSpec.\"\"\"\n    from pydantic_core import core_schema\n\n    # Get the default schema\n    python_schema = handler(source_type)\n\n    # Wrap it with a before validator that converts strings\n    return core_schema.no_info_before_validator_function(\n        _validate_field_spec_from_string,\n        python_schema,\n    )\n</code></pre>"},{"location":"reference/api/definitions/field/#metaxy.FieldDep","title":"metaxy.FieldDep  <code>pydantic-model</code>","text":"<pre><code>FieldDep(feature: str | Sequence[str] | FeatureKey | FeatureSpec | type[BaseFeature], fields: list[CoercibleToFieldKey] | Literal[ALL] = ALL, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FeatureKey\": {\n      \"description\": \"Feature key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FeatureKey(\\\"a/b/c\\\")  # String format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n\\n    FeatureKey(FeatureKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FeatureKey copy\\n    # FeatureKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FeatureKey\",\n      \"type\": \"array\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"properties\": {\n    \"feature\": {\n      \"$ref\": \"#/$defs/FeatureKey\"\n    },\n    \"fields\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"const\": \"__METAXY_ALL_DEP__\",\n          \"type\": \"string\"\n        }\n      ],\n      \"default\": \"__METAXY_ALL_DEP__\",\n      \"title\": \"Fields\"\n    }\n  },\n  \"required\": [\n    \"feature\"\n  ],\n  \"title\": \"FieldDep\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>feature</code>                 (<code>FeatureKey</code>)             </li> <li> <code>fields</code>                 (<code>list[FieldKey] | Literal[ALL]</code>)             </li> </ul> Source code in <code>src/metaxy/models/field.py</code> <pre><code>def __init__(  # pyright: ignore[reportMissingSuperCall]\n    self,\n    feature: str\n    | Sequence[str]\n    | FeatureKey\n    | \"FeatureSpec\"\n    | type[\"BaseFeature\"],\n    fields: list[CoercibleToFieldKey]\n    | Literal[SpecialFieldDep.ALL] = SpecialFieldDep.ALL,\n    **kwargs: Any,\n) -&gt; None: ...  # pyright: ignore[reportMissingSuperCall]\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/","title":"Fields Mapping","text":"<p>Metaxy provides a few helpers when defining field-level dependencies:</p> <ul> <li>the default mapping that matches on field names or suffixes with [[metaxy.FieldsMapping.identity]] (the default one)</li> <li><code>specific</code> mapping with [[metaxy.FieldsMapping.default]]</li> <li><code>all</code> mapping with [[metaxy.FieldsMapping.all]]</li> </ul> <p>Always use these classmethods to create instances of lineage relationships. Under the hood, they use Pydantic's discriminated union to ensure that the correct type is constructed based on the provided data.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping","title":"metaxy.models.fields_mapping.FieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for field mapping configurations.</p> <p>Field mappings define how a field automatically resolves its dependencies based on upstream feature fields. This is separate from explicit field dependencies which are defined directly.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AllFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"all\",\n          \"default\": \"all\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"AllFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"DefaultFieldsMapping\": {\n      \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"default\",\n          \"default\": \"default\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"match_suffix\": {\n          \"default\": false,\n          \"title\": \"Match Suffix\",\n          \"type\": \"boolean\"\n        },\n        \"exclude_fields\": {\n          \"items\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Exclude Fields\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"DefaultFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    },\n    \"NoneFieldsMapping\": {\n      \"description\": \"Field mapping that never matches any upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"none\",\n          \"default\": \"none\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"NoneFieldsMapping\",\n      \"type\": \"object\"\n    },\n    \"SpecificFieldsMapping\": {\n      \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"specific\",\n          \"default\": \"specific\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"mapping\": {\n          \"additionalProperties\": {\n            \"items\": {\n              \"$ref\": \"#/$defs/FieldKey\"\n            },\n            \"type\": \"array\",\n            \"uniqueItems\": true\n          },\n          \"propertyNames\": {\n            \"$ref\": \"#/$defs/FieldKey\"\n          },\n          \"title\": \"Mapping\",\n          \"type\": \"object\"\n        }\n      },\n      \"required\": [\n        \"mapping\"\n      ],\n      \"title\": \"SpecificFieldsMapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Base class for field mapping configurations.\\n\\nField mappings define how a field automatically resolves its dependencies\\nbased on upstream feature fields. This is separate from explicit field\\ndependencies which are defined directly.\",\n  \"properties\": {\n    \"mapping\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"all\": \"#/$defs/AllFieldsMapping\",\n          \"default\": \"#/$defs/DefaultFieldsMapping\",\n          \"none\": \"#/$defs/NoneFieldsMapping\",\n          \"specific\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/AllFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/SpecificFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/NoneFieldsMapping\"\n        },\n        {\n          \"$ref\": \"#/$defs/DefaultFieldsMapping\"\n        }\n      ],\n      \"title\": \"Mapping\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"FieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>mapping</code>                 (<code>AllFieldsMapping | SpecificFieldsMapping | NoneFieldsMapping | DefaultFieldsMapping</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.FieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve field dependencies based on upstream feature fields.</p> <p>Invokes the provided mapping to resolve dependencies.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    \"\"\"Resolve field dependencies based on upstream feature fields.\n\n    Invokes the provided mapping to resolve dependencies.\n\n    Args:\n        context: The resolution context containing field key and upstream feature.\n\n    Returns:\n        Set of [FieldKey][metaxy.models.types.FieldKey] instances for matching fields\n    \"\"\"\n    return self.mapping.resolve_field_deps(context)\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.default","title":"metaxy.models.fields_mapping.FieldsMapping.default  <code>classmethod</code>","text":"<pre><code>default(*, match_suffix: bool = False, exclude_fields: list[FieldKey] | None = None) -&gt; Self\n</code></pre> <p>Create a default field mapping configuration.</p> <p>Parameters:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef default(\n    cls,\n    *,\n    match_suffix: bool = False,\n    exclude_fields: list[FieldKey] | None = None,\n) -&gt; Self:\n    \"\"\"Create a default field mapping configuration.\n\n    Args:\n        match_suffix: If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")\n        exclude_fields: List of field keys to exclude from auto-mapping\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(\n        mapping=DefaultFieldsMapping(\n            match_suffix=match_suffix,\n            exclude_fields=exclude_fields or [],\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.specific","title":"metaxy.models.fields_mapping.FieldsMapping.specific  <code>classmethod</code>","text":"<pre><code>specific(mapping: dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]) -&gt; Self\n</code></pre> <p>Create a field mapping that maps downstream field keys into specific upstream field keys.</p> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]</code>)           \u2013            <p>Mapping of downstream field keys to sets of upstream field keys. Keys and values can be strings, sequences, or FieldKey instances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef specific(\n    cls, mapping: dict[CoercibleToFieldKey, set[CoercibleToFieldKey]]\n) -&gt; Self:\n    \"\"\"Create a field mapping that maps downstream field keys into specific upstream field keys.\n\n    Args:\n        mapping: Mapping of downstream field keys to sets of upstream field keys.\n            Keys and values can be strings, sequences, or FieldKey instances.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    # Validate and coerce the mapping keys and values\n    validated_mapping: dict[FieldKey, set[FieldKey]] = {}\n    for key, value_set in mapping.items():\n        validated_key = ValidatedFieldKeyAdapter.validate_python(key)\n        validated_values = {\n            ValidatedFieldKeyAdapter.validate_python(v) for v in value_set\n        }\n        validated_mapping[validated_key] = validated_values\n\n    return cls(mapping=SpecificFieldsMapping(mapping=validated_mapping))\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.all","title":"metaxy.models.fields_mapping.FieldsMapping.all  <code>classmethod</code>","text":"<pre><code>all() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on all upstream fields.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Use in field specifications\n&gt;&gt;&gt; FieldSpec(\n...     key=\"combined\",\n...     fields_mapping=FieldsMapping.all()\n... )\n</code></pre> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef all(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on all upstream fields.\n\n    Returns:\n        Configured FieldsMapping instance.\n\n    Examples:\n        &gt;&gt;&gt; # Use in field specifications\n        &gt;&gt;&gt; FieldSpec(\n        ...     key=\"combined\",\n        ...     fields_mapping=FieldsMapping.all()\n        ... )\n    \"\"\"\n    return cls(mapping=AllFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMapping.none","title":"metaxy.models.fields_mapping.FieldsMapping.none  <code>classmethod</code>","text":"<pre><code>none() -&gt; Self\n</code></pre> <p>Create a field mapping that explicitly depends on no upstream fields.</p> <p>This is typically useful when explicitly defining FieldSpec.deps instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured FieldsMapping instance.</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>@classmethod\ndef none(cls) -&gt; Self:\n    \"\"\"Create a field mapping that explicitly depends on no upstream fields.\n\n    This is typically useful when explicitly defining [FieldSpec.deps][metaxy.models.field.FieldSpec] instead.\n\n    Returns:\n        Configured FieldsMapping instance.\n    \"\"\"\n    return cls(mapping=NoneFieldsMapping())\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.FieldsMappingType","title":"metaxy.models.fields_mapping.FieldsMappingType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of fields mapping between a field key and the upstream field keys.</p>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping","title":"metaxy.models.fields_mapping.DefaultFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Default automatic field mapping configuration.</p> <p>When used, automatically maps fields to matching upstream fields based on field keys.</p> <p>Attributes:</p> <ul> <li> <code>match_suffix</code>               (<code>bool</code>)           \u2013            <p>If True, allows suffix matching (e.g., \"french\" matches \"audio/french\")</p> </li> <li> <code>exclude_fields</code>               (<code>list[FieldKey]</code>)           \u2013            <p>List of field keys to exclude from auto-mapping</p> </li> </ul> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Default automatic field mapping configuration.\\n\\nWhen used, automatically maps fields to matching upstream fields based on field keys.\\n\\nAttributes:\\n    match_suffix: If True, allows suffix matching (e.g., \\\"french\\\" matches \\\"audio/french\\\")\\n    exclude_fields: List of field keys to exclude from auto-mapping\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"default\",\n      \"default\": \"default\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"match_suffix\": {\n      \"default\": false,\n      \"title\": \"Match Suffix\",\n      \"type\": \"boolean\"\n    },\n    \"exclude_fields\": {\n      \"items\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Exclude Fields\",\n      \"type\": \"array\"\n    }\n  },\n  \"title\": \"DefaultFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[DEFAULT]</code>)             </li> <li> <code>match_suffix</code>                 (<code>bool</code>)             </li> <li> <code>exclude_fields</code>                 (<code>list[FieldKey]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.DefaultFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    res = set()\n\n    for upstream_field_key in context.upstream_feature_fields:\n        # Skip excluded fields\n        if upstream_field_key in self.exclude_fields:\n            continue\n\n        # Check for exact match\n        if upstream_field_key == context.field_key:\n            res.add(upstream_field_key)\n        # Check for suffix match if enabled\n        elif self.match_suffix and self._is_suffix_match(\n            context.field_key, upstream_field_key\n        ):\n            res.add(upstream_field_key)\n\n    # If no fields matched, return ALL fields from this upstream feature\n    # (excluding any explicitly excluded fields)\n    if not res:\n        for upstream_field_key in context.upstream_feature_fields:\n            if upstream_field_key not in self.exclude_fields:\n                res.add(upstream_field_key)\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping","title":"metaxy.models.fields_mapping.SpecificFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on specific upstream fields.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"FieldKey\": {\n      \"description\": \"Field key as a sequence of string parts.\\n\\nHashable for use as dict keys in registries.\\nParts cannot contain forward slashes (/) or double underscores (__).\\n\\nExample:\\n\\n    ```py\\n    FieldKey(\\\"a/b/c\\\")  # String format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"])  # List format\\n    # FieldKey(parts=['a', 'b', 'c'])\\n\\n    FieldKey(FieldKey([\\\"a\\\", \\\"b\\\", \\\"c\\\"]))  # FieldKey copy\\n    # FieldKey(parts=['a', 'b', 'c'])\\n    ```\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"FieldKey\",\n      \"type\": \"array\"\n    }\n  },\n  \"description\": \"Field mapping that explicitly depends on specific upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"specific\",\n      \"default\": \"specific\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"mapping\": {\n      \"additionalProperties\": {\n        \"items\": {\n          \"$ref\": \"#/$defs/FieldKey\"\n        },\n        \"type\": \"array\",\n        \"uniqueItems\": true\n      },\n      \"propertyNames\": {\n        \"$ref\": \"#/$defs/FieldKey\"\n      },\n      \"title\": \"Mapping\",\n      \"type\": \"object\"\n    }\n  },\n  \"required\": [\n    \"mapping\"\n  ],\n  \"title\": \"SpecificFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[SPECIFIC]</code>)             </li> <li> <code>mapping</code>                 (<code>dict[FieldKey, set[FieldKey]]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.SpecificFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    desired_upstream_fields = self.mapping.get(context.field_key, set())\n    return desired_upstream_fields &amp; context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping","title":"metaxy.models.fields_mapping.AllFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that explicitly depends on all upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that explicitly depends on all upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"all\",\n      \"default\": \"all\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"AllFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[ALL]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.AllFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return context.upstream_feature_fields\n</code></pre>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping","title":"metaxy.models.fields_mapping.NoneFieldsMapping  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseFieldsMapping</code></p> <p>Field mapping that never matches any upstream fields.</p> Show JSON schema: <pre><code>{\n  \"description\": \"Field mapping that never matches any upstream fields.\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"none\",\n      \"default\": \"none\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"NoneFieldsMapping\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[NONE]</code>)             </li> </ul>"},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping-functions","title":"Functions","text":""},{"location":"reference/api/definitions/fields-mapping/#metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","title":"metaxy.models.fields_mapping.NoneFieldsMapping.resolve_field_deps","text":"<pre><code>resolve_field_deps(context: FieldsMappingResolutionContext) -&gt; set[FieldKey]\n</code></pre> <p>Resolve automatic field mapping to explicit FieldDep list.</p> <p>This method should be overridden by concrete implementations.</p> <p>Parameters:</p> <ul> <li> <code>context</code>               (<code>FieldsMappingResolutionContext</code>)           \u2013            <p>The resolution context containing field key and upstream feature.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[FieldKey]</code>           \u2013            <p>Set of FieldKey instances for matching fields</p> </li> </ul> Source code in <code>src/metaxy/models/fields_mapping.py</code> <pre><code>def resolve_field_deps(\n    self,\n    context: FieldsMappingResolutionContext,\n) -&gt; set[FieldKey]:\n    return set()\n</code></pre>"},{"location":"reference/api/definitions/filters/","title":"String Filters","text":""},{"location":"reference/api/definitions/filters/#metaxy.models.filter_expression.parse_filter_string","title":"metaxy.models.filter_expression.parse_filter_string","text":"<pre><code>parse_filter_string(filter_string: str) -&gt; Expr\n</code></pre> <p>Parse a SQL WHERE-like string into a Narwhals expression.</p> <p>The parser understands SQL <code>WHERE</code> clauses composed of comparison operators, logical operators, parentheses, dotted identifiers, and literal values (strings, numbers, booleans, <code>NULL</code>).</p> <p>This functionality is implemented with SQLGlot.</p> Example <pre><code>parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n# Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n</code></pre> Source code in <code>src/metaxy/models/filter_expression.py</code> <pre><code>def parse_filter_string(filter_string: str) -&gt; nw.Expr:\n    \"\"\"Parse a SQL WHERE-like string into a Narwhals expression.\n\n    The parser understands SQL `WHERE` clauses composed of comparison operators, logical operators, parentheses,\n    dotted identifiers, and literal values (strings, numbers, booleans, ``NULL``).\n\n    This functionality is implemented with [SQLGlot](https://sqlglot.com/).\n\n    Example:\n        ```python\n        parse_filter_string(\"NOT (status = 'deleted') AND deleted_at = NULL\")\n        # Returns: (~(nw.col(\"status\") == \"deleted\")) &amp; nw.col(\"deleted_at\").is_null()\n        ```\n    \"\"\"\n    return NarwhalsFilter.model_validate(filter_string).to_expr()\n</code></pre>"},{"location":"reference/api/definitions/graph/","title":"Feature Graph","text":"<p><code>FeatureGraph</code> is a global \"God\" object that holds all the features loaded by Metaxy via the feature discovery mechanism.</p> <p>Users may interact with <code>FeatureGraph</code> when writing custom migrations, otherwise they are not exposed to it.</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph","title":"metaxy.FeatureGraph","text":"<pre><code>FeatureGraph()\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def __init__(self):\n    self.features_by_key: dict[FeatureKey, type[BaseFeature]] = {}\n    self.feature_specs_by_key: dict[FeatureKey, FeatureSpec] = {}\n    # Standalone specs registered without Feature classes (for migrations)\n    self.standalone_specs_by_key: dict[FeatureKey, FeatureSpec] = {}\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.snapshot_version","title":"metaxy.FeatureGraph.snapshot_version  <code>property</code>","text":"<pre><code>snapshot_version: str\n</code></pre> <p>Generate a snapshot version representing the current topology + versions of the feature graph</p>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph-functions","title":"Functions","text":""},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.add_feature","title":"metaxy.FeatureGraph.add_feature","text":"<pre><code>add_feature(feature: type[BaseFeature]) -&gt; None\n</code></pre> <p>Add a feature to the graph.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to register</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a feature with the same key is already registered        or if duplicate column names would result from renaming operations</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def add_feature(self, feature: type[\"BaseFeature\"]) -&gt; None:\n    \"\"\"Add a feature to the graph.\n\n    Args:\n        feature: Feature class to register\n\n    Raises:\n        ValueError: If a feature with the same key is already registered\n                   or if duplicate column names would result from renaming operations\n    \"\"\"\n    if feature.spec().key in self.features_by_key:\n        existing = self.features_by_key[feature.spec().key]\n        raise ValueError(\n            f\"Feature with key {feature.spec().key.to_string()} already registered. \"\n            f\"Existing: {existing.__name__}, New: {feature.__name__}. \"\n            f\"Each feature key must be unique within a graph.\"\n        )\n\n    # Validate that there are no duplicate column names across dependencies after renaming\n    if feature.spec().deps:\n        self._validate_no_duplicate_columns(feature.spec())\n\n    self.features_by_key[feature.spec().key] = feature\n    self.feature_specs_by_key[feature.spec().key] = feature.spec()\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.remove_feature","title":"metaxy.FeatureGraph.remove_feature","text":"<pre><code>remove_feature(key: CoercibleToFeatureKey) -&gt; None\n</code></pre> <p>Remove a feature from the graph.</p> <p>Removes Feature class or standalone spec (whichever exists).</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to remove. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def remove_feature(self, key: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Remove a feature from the graph.\n\n    Removes Feature class or standalone spec (whichever exists).\n\n    Args:\n        key: Feature key to remove. Accepts types that can be converted into a feature key..\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    # Check both Feature classes and standalone specs\n    combined = {**self.feature_specs_by_key, **self.standalone_specs_by_key}\n\n    if validated_key not in combined:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in combined]}\"\n        )\n\n    # Remove from all relevant dicts\n    if validated_key in self.features_by_key:\n        del self.features_by_key[validated_key]\n    if validated_key in self.standalone_specs_by_key:\n        del self.standalone_specs_by_key[validated_key]\n    if validated_key in self.feature_specs_by_key:\n        del self.feature_specs_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_by_key","title":"metaxy.FeatureGraph.get_feature_by_key","text":"<pre><code>get_feature_by_key(key: CoercibleToFeatureKey) -&gt; type[BaseFeature]\n</code></pre> <p>Get a feature class by its key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to look up. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[BaseFeature]</code>           \u2013            <p>Feature class</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If no feature with the given key is registered</p> </li> </ul> Example <pre><code>graph = FeatureGraph.get_active()\nparent_key = FeatureKey([\"examples\", \"parent\"])\nParentFeature = graph.get_feature_by_key(parent_key)\n\n# Or use string notation\nParentFeature = graph.get_feature_by_key(\"examples/parent\")\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_by_key(self, key: CoercibleToFeatureKey) -&gt; type[\"BaseFeature\"]:\n    \"\"\"Get a feature class by its key.\n\n    Args:\n        key: Feature key to look up. Accepts types that can be converted into a feature key..\n\n    Returns:\n        Feature class\n\n    Raises:\n        KeyError: If no feature with the given key is registered\n\n    Example:\n        ```py\n        graph = FeatureGraph.get_active()\n        parent_key = FeatureKey([\"examples\", \"parent\"])\n        ParentFeature = graph.get_feature_by_key(parent_key)\n\n        # Or use string notation\n        ParentFeature = graph.get_feature_by_key(\"examples/parent\")\n        ```\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    if validated_key not in self.features_by_key:\n        raise KeyError(\n            f\"No feature with key {validated_key.to_string()} found in graph. \"\n            f\"Available keys: {[k.to_string() for k in self.features_by_key.keys()]}\"\n        )\n    return self.features_by_key[validated_key]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.list_features","title":"metaxy.FeatureGraph.list_features","text":"<pre><code>list_features(projects: list[str] | str | None = None, *, only_current_project: bool = True) -&gt; list[FeatureKey]\n</code></pre> <p>List all feature keys in the graph, optionally filtered by project(s).</p> <p>By default, filters features by the current project (first part of feature key). This prevents operations from affecting features in other projects.</p> <p>Parameters:</p> <ul> <li> <code>projects</code>               (<code>list[str] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Project name(s) to filter by. Can be: - None: Use current project from MetaxyConfig (if only_current_project=True) - str: Single project name - list[str]: Multiple project names</p> </li> <li> <code>only_current_project</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, filter by current/specified project(s). If False, return all features regardless of project.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys</p> </li> </ul> Example <pre><code># Get all features for current project\ngraph = FeatureGraph.get_active()\nfeatures = graph.list_features()\n\n# Get features for specific project\nfeatures = graph.list_features(projects=\"myproject\")\n\n# Get features for multiple projects\nfeatures = graph.list_features(projects=[\"project1\", \"project2\"])\n\n# Get all features regardless of project\nall_features = graph.list_features(only_current_project=False)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def list_features(\n    self,\n    projects: list[str] | str | None = None,\n    *,\n    only_current_project: bool = True,\n) -&gt; list[FeatureKey]:\n    \"\"\"List all feature keys in the graph, optionally filtered by project(s).\n\n    By default, filters features by the current project (first part of feature key).\n    This prevents operations from affecting features in other projects.\n\n    Args:\n        projects: Project name(s) to filter by. Can be:\n            - None: Use current project from MetaxyConfig (if only_current_project=True)\n            - str: Single project name\n            - list[str]: Multiple project names\n        only_current_project: If True, filter by current/specified project(s).\n            If False, return all features regardless of project.\n\n    Returns:\n        List of feature keys\n\n    Example:\n        ```py\n        # Get all features for current project\n        graph = FeatureGraph.get_active()\n        features = graph.list_features()\n\n        # Get features for specific project\n        features = graph.list_features(projects=\"myproject\")\n\n        # Get features for multiple projects\n        features = graph.list_features(projects=[\"project1\", \"project2\"])\n\n        # Get all features regardless of project\n        all_features = graph.list_features(only_current_project=False)\n        ```\n    \"\"\"\n    if not only_current_project:\n        # Return all features\n        return list(self.features_by_key.keys())\n\n    # Normalize projects to list\n    project_list: list[str]\n    if projects is None:\n        # Try to get from config context\n        try:\n            from metaxy.config import MetaxyConfig\n\n            config = MetaxyConfig.get()\n            project_list = [config.project]\n        except RuntimeError:\n            # Config not initialized - in tests or non-CLI usage\n            # Return all features (can't determine project)\n            return list(self.features_by_key.keys())\n    elif isinstance(projects, str):\n        project_list = [projects]\n    else:\n        project_list = projects\n\n    # Filter by project(s) using Feature.project attribute\n    return [\n        key\n        for key in self.features_by_key.keys()\n        if self.features_by_key[key].project in project_list\n    ]\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_plan","title":"metaxy.FeatureGraph.get_feature_plan","text":"<pre><code>get_feature_plan(key: CoercibleToFeatureKey) -&gt; FeaturePlan\n</code></pre> <p>Get a feature plan for a given feature key.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get plan for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeaturePlan</code>           \u2013            <p>FeaturePlan instance with feature spec and dependencies.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_plan(self, key: CoercibleToFeatureKey) -&gt; FeaturePlan:\n    \"\"\"Get a feature plan for a given feature key.\n\n    Args:\n        key: Feature key to get plan for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        FeaturePlan instance with feature spec and dependencies.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    spec = self.all_specs_by_key[validated_key]\n\n    return FeaturePlan(\n        feature=spec,\n        deps=[self.feature_specs_by_key[dep.feature] for dep in spec.deps or []]\n        or None,\n        feature_deps=spec.deps,  # Pass the actual FeatureDep objects with field mappings\n    )\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version_by_field","title":"metaxy.FeatureGraph.get_feature_version_by_field","text":"<pre><code>get_feature_version_by_field(key: CoercibleToFeatureKey) -&gt; dict[str, str]\n</code></pre> <p>Computes the field provenance map for a feature.</p> <p>Hash together field provenance entries with the feature code version.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get field versions for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>           \u2013            <p>dict[str, str]: The provenance hash for each field in the feature plan. Keys are field names as strings.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version_by_field(\n    self, key: CoercibleToFeatureKey\n) -&gt; dict[str, str]:\n    \"\"\"Computes the field provenance map for a feature.\n\n    Hash together field provenance entries with the feature code version.\n\n    Args:\n        key: Feature key to get field versions for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        dict[str, str]: The provenance hash for each field in the feature plan.\n            Keys are field names as strings.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    res = {}\n\n    plan = self.get_feature_plan(validated_key)\n\n    for k, v in plan.feature.fields_by_key.items():\n        res[k.to_string()] = self.get_field_version(\n            FQFieldKey(field=k, feature=validated_key)\n        )\n\n    return res\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_feature_version","title":"metaxy.FeatureGraph.get_feature_version","text":"<pre><code>get_feature_version(key: CoercibleToFeatureKey) -&gt; str\n</code></pre> <p>Computes the feature version as a single string.</p> <p>Parameters:</p> <ul> <li> <code>key</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature key to get version for. Accepts types that can be converted into a feature key..</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Truncated SHA256 hash representing the feature version.</p> </li> </ul> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_feature_version(self, key: CoercibleToFeatureKey) -&gt; str:\n    \"\"\"Computes the feature version as a single string.\n\n    Args:\n        key: Feature key to get version for. Accepts types that can be converted into a feature key..\n\n    Returns:\n        Truncated SHA256 hash representing the feature version.\n    \"\"\"\n    # Validate and coerce the key\n    validated_key = ValidatedFeatureKeyAdapter.validate_python(key)\n\n    hasher = hashlib.sha256()\n    provenance_by_field = self.get_feature_version_by_field(validated_key)\n    for field_key in sorted(provenance_by_field):\n        hasher.update(field_key.encode())\n        hasher.update(provenance_by_field[field_key].encode())\n\n    return truncate_hash(hasher.hexdigest())\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_downstream_features","title":"metaxy.FeatureGraph.get_downstream_features","text":"<pre><code>get_downstream_features(sources: Sequence[CoercibleToFeatureKey]) -&gt; list[FeatureKey]\n</code></pre> <p>Get all features downstream of sources, topologically sorted.</p> <p>Performs a depth-first traversal of the dependency graph to find all features that transitively depend on any of the source features.</p> <p>Parameters:</p> <ul> <li> <code>sources</code>               (<code>Sequence[CoercibleToFeatureKey]</code>)           \u2013            <p>List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of downstream feature keys in topological order (dependencies first).</p> </li> <li> <code>list[FeatureKey]</code>           \u2013            <p>Does not include the source features themselves.</p> </li> </ul> Example <pre><code># DAG: A -&gt; B -&gt; D\n#      A -&gt; C -&gt; D\ngraph.get_downstream_features([FeatureKey([\"A\"])])\n# [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n\n# Or use string notation\ngraph.get_downstream_features([\"A\"])\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def get_downstream_features(\n    self, sources: Sequence[CoercibleToFeatureKey]\n) -&gt; list[FeatureKey]:\n    \"\"\"Get all features downstream of sources, topologically sorted.\n\n    Performs a depth-first traversal of the dependency graph to find all\n    features that transitively depend on any of the source features.\n\n    Args:\n        sources: List of source feature keys. Each element can be string, sequence, FeatureKey, or BaseFeature class.\n\n    Returns:\n        List of downstream feature keys in topological order (dependencies first).\n        Does not include the source features themselves.\n\n    Example:\n        ```py\n        # DAG: A -&gt; B -&gt; D\n        #      A -&gt; C -&gt; D\n        graph.get_downstream_features([FeatureKey([\"A\"])])\n        # [FeatureKey([\"B\"]), FeatureKey([\"C\"]), FeatureKey([\"D\"])]\n\n        # Or use string notation\n        graph.get_downstream_features([\"A\"])\n        ```\n    \"\"\"\n    # Validate and coerce the source keys\n    validated_sources = ValidatedFeatureKeySequenceAdapter.validate_python(sources)\n\n    source_set = set(validated_sources)\n    visited = set()\n    post_order = []\n    source_set = set(sources)\n    visited = set()\n    post_order = []  # Reverse topological order\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS traversal.\"\"\"\n        if key in visited:\n            return\n        visited.add(key)\n\n        # Find all features that depend on this one\n        for feature_key, feature_spec in self.feature_specs_by_key.items():\n            if feature_spec.deps:\n                for dep in feature_spec.deps:\n                    if dep.feature == key:\n                        # This feature depends on 'key', so visit it\n                        visit(feature_key)\n\n        post_order.append(key)\n\n    # Visit all sources\n    for source in validated_sources:\n        visit(source)\n\n    # Remove sources from result, reverse to get topological order\n    result = [k for k in reversed(post_order) if k not in source_set]\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.topological_sort_features","title":"metaxy.FeatureGraph.topological_sort_features","text":"<pre><code>topological_sort_features(feature_keys: Sequence[CoercibleToFeatureKey] | None = None, *, descending: bool = False) -&gt; list[FeatureKey]\n</code></pre> <p>Sort feature keys in topological order.</p> <p>Uses stable alphabetical ordering when multiple nodes are at the same level. This ensures deterministic output for diff comparisons and migrations.</p> <p>Implemented using depth-first search with post-order traversal.</p> <p>Parameters:</p> <ul> <li> <code>feature_keys</code>               (<code>Sequence[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of feature keys to sort. Each element can be string, sequence, FeatureKey, or BaseFeature class. If None, sorts all features (both Feature classes and standalone specs) in the graph.</p> </li> <li> <code>descending</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If False (default), dependencies appear before dependents. For a chain A -&gt; B -&gt; C, returns [A, B, C]. If True, dependents appear before dependencies. For a chain A -&gt; B -&gt; C, returns [C, B, A].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FeatureKey]</code>           \u2013            <p>List of feature keys sorted in topological order</p> </li> </ul> Example <pre><code>graph = FeatureGraph.get_active()\n# Sort specific features (dependencies first)\nsorted_keys = graph.topological_sort_features([\n    FeatureKey([\"video\", \"raw\"]),\n    FeatureKey([\"video\", \"scene\"]),\n])\n\n# Or use string notation\nsorted_keys = graph.topological_sort_features([\"video/raw\", \"video/scene\"])\n\n# Sort all features in the graph (including standalone specs)\nall_sorted = graph.topological_sort_features()\n\n# Sort with dependents first (useful for processing leaf nodes before roots)\nreverse_sorted = graph.topological_sort_features(descending=True)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def topological_sort_features(\n    self,\n    feature_keys: Sequence[CoercibleToFeatureKey] | None = None,\n    *,\n    descending: bool = False,\n) -&gt; list[FeatureKey]:\n    \"\"\"Sort feature keys in topological order.\n\n    Uses stable alphabetical ordering when multiple nodes are at the same level.\n    This ensures deterministic output for diff comparisons and migrations.\n\n    Implemented using depth-first search with post-order traversal.\n\n    Args:\n        feature_keys: List of feature keys to sort. Each element can be string, sequence,\n            FeatureKey, or BaseFeature class. If None, sorts all features\n            (both Feature classes and standalone specs) in the graph.\n        descending: If False (default), dependencies appear before dependents.\n            For a chain A -&gt; B -&gt; C, returns [A, B, C].\n            If True, dependents appear before dependencies.\n            For a chain A -&gt; B -&gt; C, returns [C, B, A].\n\n    Returns:\n        List of feature keys sorted in topological order\n\n    Example:\n        ```py\n        graph = FeatureGraph.get_active()\n        # Sort specific features (dependencies first)\n        sorted_keys = graph.topological_sort_features([\n            FeatureKey([\"video\", \"raw\"]),\n            FeatureKey([\"video\", \"scene\"]),\n        ])\n\n        # Or use string notation\n        sorted_keys = graph.topological_sort_features([\"video/raw\", \"video/scene\"])\n\n        # Sort all features in the graph (including standalone specs)\n        all_sorted = graph.topological_sort_features()\n\n        # Sort with dependents first (useful for processing leaf nodes before roots)\n        reverse_sorted = graph.topological_sort_features(descending=True)\n        ```\n    \"\"\"\n    # Determine which features to sort\n    if feature_keys is None:\n        # Include both Feature classes and standalone specs\n        keys_to_sort = set(self.feature_specs_by_key.keys())\n    else:\n        # Validate and coerce the feature keys\n        validated_keys = ValidatedFeatureKeySequenceAdapter.validate_python(\n            feature_keys\n        )\n        keys_to_sort = set(validated_keys)\n\n    visited = set()\n    result = []  # Topological order (dependencies first)\n\n    def visit(key: FeatureKey):\n        \"\"\"DFS visit with post-order traversal.\"\"\"\n        if key in visited or key not in keys_to_sort:\n            return\n        visited.add(key)\n\n        # Get dependencies from feature spec\n        spec = self.feature_specs_by_key.get(key)\n        if spec and spec.deps:\n            # Sort dependencies alphabetically for deterministic ordering\n            sorted_deps = sorted(\n                (dep.feature for dep in spec.deps),\n                key=lambda k: k.to_string().lower(),\n            )\n            for dep_key in sorted_deps:\n                if dep_key in keys_to_sort:\n                    visit(dep_key)\n\n        # Add to result after visiting dependencies (post-order)\n        result.append(key)\n\n    # Visit all keys in sorted order for deterministic traversal\n    for key in sorted(keys_to_sort, key=lambda k: k.to_string().lower()):\n        visit(key)\n\n    # Post-order DFS gives topological order (dependencies before dependents)\n    if descending:\n        return list(reversed(result))\n    return result\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.to_snapshot","title":"metaxy.FeatureGraph.to_snapshot","text":"<pre><code>to_snapshot() -&gt; dict[str, SerializedFeature]\n</code></pre> <p>Serialize graph to snapshot format.</p> <p>Returns a dict mapping feature_key (string) to feature data dict, including the import path of the Feature class for reconstruction.</p> <p>Returns: dictionary mapping feature_key (string) to feature data dict</p> Example <pre><code>snapshot = graph.to_snapshot()\nsnapshot[\"video_processing\"][\"metaxy_feature_version\"]\n# 'abc12345'\nsnapshot[\"video_processing\"][\"metaxy_feature_spec_version\"]\n# 'def67890'\nsnapshot[\"video_processing\"][\"metaxy_full_definition_version\"]\n# 'xyz98765'\nsnapshot[\"video_processing\"][\"feature_class_path\"]\n# 'myapp.features.video.VideoProcessing'\nsnapshot[\"video_processing\"][\"project\"]\n# 'myapp'\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>def to_snapshot(self) -&gt; dict[str, SerializedFeature]:\n    \"\"\"Serialize graph to snapshot format.\n\n    Returns a dict mapping feature_key (string) to feature data dict,\n    including the import path of the Feature class for reconstruction.\n\n    Returns: dictionary mapping feature_key (string) to feature data dict\n\n    Example:\n        ```py\n        snapshot = graph.to_snapshot()\n        snapshot[\"video_processing\"][\"metaxy_feature_version\"]\n        # 'abc12345'\n        snapshot[\"video_processing\"][\"metaxy_feature_spec_version\"]\n        # 'def67890'\n        snapshot[\"video_processing\"][\"metaxy_full_definition_version\"]\n        # 'xyz98765'\n        snapshot[\"video_processing\"][\"feature_class_path\"]\n        # 'myapp.features.video.VideoProcessing'\n        snapshot[\"video_processing\"][\"project\"]\n        # 'myapp'\n        ```\n    \"\"\"\n    snapshot: dict[str, SerializedFeature] = {}\n\n    for feature_key, feature_cls in self.features_by_key.items():\n        feature_key_str = feature_key.to_string()\n        feature_spec_dict = feature_cls.spec().model_dump(mode=\"json\")  # type: ignore[attr-defined]\n        feature_schema_dict = feature_cls.model_json_schema()  # type: ignore[attr-defined]\n        feature_version = feature_cls.feature_version()  # type: ignore[attr-defined]\n        feature_spec_version = feature_cls.spec().feature_spec_version  # type: ignore[attr-defined]\n        full_definition_version = feature_cls.full_definition_version()  # type: ignore[attr-defined]\n        project = feature_cls.project  # type: ignore[attr-defined]\n\n        # Get class import path (module.ClassName)\n        class_path = f\"{feature_cls.__module__}.{feature_cls.__name__}\"\n\n        snapshot[feature_key_str] = {  # pyright: ignore\n            \"feature_spec\": feature_spec_dict,\n            \"feature_schema\": feature_schema_dict,\n            FEATURE_VERSION_COL: feature_version,\n            FEATURE_SPEC_VERSION_COL: feature_spec_version,\n            FEATURE_TRACKING_VERSION_COL: full_definition_version,\n            \"feature_class_path\": class_path,\n            \"project\": project,\n        }\n\n    return snapshot\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.from_snapshot","title":"metaxy.FeatureGraph.from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(snapshot_data: Mapping[str, Mapping[str, Any]], *, class_path_overrides: dict[str, str] | None = None, force_reload: bool = False) -&gt; FeatureGraph\n</code></pre> <p>Reconstruct graph from snapshot by importing Feature classes.</p> <p>Strictly requires Feature classes to exist at their recorded import paths. This ensures custom methods (like load_input) are available.</p> <p>If a feature has been moved/renamed, use class_path_overrides to specify the new location.</p> <p>Parameters:</p> <ul> <li> <code>snapshot_data</code>               (<code>Mapping[str, Mapping[str, Any]]</code>)           \u2013            <p>Dict of feature_key -&gt; dict containing feature_spec (dict), feature_class_path (str), and other fields as returned by to_snapshot() or loaded from DB</p> </li> <li> <code>class_path_overrides</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional dict mapping feature_key to new class path                  for features that have been moved/renamed</p> </li> <li> <code>force_reload</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, reload modules from disk to get current code state.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>New FeatureGraph with historical features</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If feature class cannot be imported at recorded path</p> </li> </ul> Example <pre><code># Load snapshot from metadata store\nhistorical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n# With override for moved feature\nhistorical_graph = FeatureGraph.from_snapshot(\n    snapshot_data,\n    class_path_overrides={\n        \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n    }\n)\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef from_snapshot(\n    cls,\n    snapshot_data: Mapping[str, Mapping[str, Any]],\n    *,\n    class_path_overrides: dict[str, str] | None = None,\n    force_reload: bool = False,\n) -&gt; \"FeatureGraph\":\n    \"\"\"Reconstruct graph from snapshot by importing Feature classes.\n\n    Strictly requires Feature classes to exist at their recorded import paths.\n    This ensures custom methods (like load_input) are available.\n\n    If a feature has been moved/renamed, use class_path_overrides to specify\n    the new location.\n\n    Args:\n        snapshot_data: Dict of feature_key -&gt; dict containing\n            feature_spec (dict), feature_class_path (str), and other fields\n            as returned by to_snapshot() or loaded from DB\n        class_path_overrides: Optional dict mapping feature_key to new class path\n                             for features that have been moved/renamed\n        force_reload: If True, reload modules from disk to get current code state.\n\n    Returns:\n        New FeatureGraph with historical features\n\n    Raises:\n        ImportError: If feature class cannot be imported at recorded path\n\n    Example:\n        ```py\n        # Load snapshot from metadata store\n        historical_graph = FeatureGraph.from_snapshot(snapshot_data)\n\n        # With override for moved feature\n        historical_graph = FeatureGraph.from_snapshot(\n            snapshot_data,\n            class_path_overrides={\n                \"video_processing\": \"myapp.features_v2.VideoProcessing\"\n            }\n        )\n        ```\n    \"\"\"\n    import importlib\n    import sys\n\n    graph = cls()\n    class_path_overrides = class_path_overrides or {}\n\n    # If force_reload, collect all module paths first to remove ALL features\n    # from those modules before reloading (modules can have multiple features)\n    modules_to_reload = set()\n    if force_reload:\n        for feature_key_str, feature_data in snapshot_data.items():\n            class_path = class_path_overrides.get(\n                feature_key_str\n            ) or feature_data.get(\"feature_class_path\")\n            if class_path:\n                module_path, _ = class_path.rsplit(\".\", 1)\n                if module_path in sys.modules:\n                    modules_to_reload.add(module_path)\n\n    # Use context manager to temporarily set the new graph as active\n    # This ensures imported Feature classes register to the new graph, not the current one\n    with graph.use():\n        for feature_key_str, feature_data in snapshot_data.items():\n            # Parse FeatureSpec for validation\n            feature_spec_dict = feature_data[\"feature_spec\"]\n            FeatureSpec.model_validate(feature_spec_dict)\n\n            # Get class path (check overrides first)\n            if feature_key_str in class_path_overrides:\n                class_path = class_path_overrides[feature_key_str]\n            else:\n                class_path = feature_data.get(\"feature_class_path\")\n                if not class_path:\n                    raise ValueError(\n                        f\"Feature '{feature_key_str}' has no feature_class_path in snapshot. \"\n                        f\"Cannot reconstruct historical graph.\"\n                    )\n\n            # Import the class\n            try:\n                module_path, class_name = class_path.rsplit(\".\", 1)\n\n                # Force reload module from disk if requested\n                # This is critical for migration detection - when code changes,\n                # we need fresh imports to detect the changes\n                if force_reload and module_path in modules_to_reload:\n                    # Before first reload of this module, remove ALL features from this module\n                    # (a module can define multiple features)\n                    if module_path in modules_to_reload:\n                        # Find all features from this module in snapshot and remove them\n                        for fk_str, fd in snapshot_data.items():\n                            fcp = class_path_overrides.get(fk_str) or fd.get(\n                                \"feature_class_path\"\n                            )\n                            if fcp and fcp.rsplit(\".\", 1)[0] == module_path:\n                                fspec_dict = fd[\"feature_spec\"]\n                                fspec = FeatureSpec.model_validate(fspec_dict)\n                                if fspec.key in graph.features_by_key:\n                                    graph.remove_feature(fspec.key)\n\n                        # Mark module as processed so we don't remove features again\n                        modules_to_reload.discard(module_path)\n\n                    module = importlib.reload(sys.modules[module_path])\n                else:\n                    module = __import__(module_path, fromlist=[class_name])\n\n                feature_cls = getattr(module, class_name)\n            except (ImportError, AttributeError):\n                # Feature class not importable - add as standalone spec instead\n                # This allows migrations to work even when old Feature classes are deleted/moved\n                import logging\n\n                logger = logging.getLogger(__name__)\n                logger.exception(\n                    f\"Cannot import Feature class '{class_path}' for '{feature_key_str}'. \"\n                    f\"Adding only the FeatureSpec. \"\n                )\n\n                feature_spec = FeatureSpec.model_validate(feature_spec_dict)\n                # Add the spec as a standalone spec\n                graph.add_feature_spec(feature_spec)\n                continue\n\n            # Validate the imported class matches the stored spec\n            if not hasattr(feature_cls, \"spec\"):\n                raise TypeError(\n                    f\"Imported class '{class_path}' is not a valid Feature class \"\n                    f\"(missing 'spec' attribute)\"\n                )\n\n            # Register the imported feature to this graph if not already present\n            # If the module was imported for the first time, the metaclass already registered it\n            # If the module was previously imported, we need to manually register it\n            if feature_cls.spec().key not in graph.features_by_key:\n                graph.add_feature(feature_cls)\n\n    return graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.get_active","title":"metaxy.FeatureGraph.get_active  <code>classmethod</code>","text":"<pre><code>get_active() -&gt; FeatureGraph\n</code></pre> <p>Get the currently active graph.</p> <p>Returns the graph from the context variable if set, otherwise returns the default global graph.</p> <p>Returns:</p> <ul> <li> <code>FeatureGraph</code>           \u2013            <p>Active FeatureGraph instance</p> </li> </ul> Example <pre><code># Normal usage - returns default graph\nreg = FeatureGraph.get_active()\n\n# With custom graph in context\nwith my_graph.use():\n    reg = FeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef get_active(cls) -&gt; \"FeatureGraph\":\n    \"\"\"Get the currently active graph.\n\n    Returns the graph from the context variable if set, otherwise returns\n    the default global graph.\n\n    Returns:\n        Active FeatureGraph instance\n\n    Example:\n        ```py\n        # Normal usage - returns default graph\n        reg = FeatureGraph.get_active()\n\n        # With custom graph in context\n        with my_graph.use():\n            reg = FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    return _active_graph.get() or graph\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.set_active","title":"metaxy.FeatureGraph.set_active  <code>classmethod</code>","text":"<pre><code>set_active(reg: FeatureGraph) -&gt; None\n</code></pre> <p>Set the active graph for the current context.</p> <p>This sets the context variable that will be returned by get_active(). Typically used in application setup code or test fixtures.</p> <p>Parameters:</p> <ul> <li> <code>reg</code>               (<code>FeatureGraph</code>)           \u2013            <p>FeatureGraph to activate</p> </li> </ul> Example <pre><code># In application setup\nmy_graph = FeatureGraph()\nFeatureGraph.set_active(my_graph)\n\n# Now all operations use my_graph\nFeatureGraph.get_active()  # Returns my_graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@classmethod\ndef set_active(cls, reg: \"FeatureGraph\") -&gt; None:\n    \"\"\"Set the active graph for the current context.\n\n    This sets the context variable that will be returned by get_active().\n    Typically used in application setup code or test fixtures.\n\n    Args:\n        reg: FeatureGraph to activate\n\n    Example:\n        ```py\n        # In application setup\n        my_graph = FeatureGraph()\n        FeatureGraph.set_active(my_graph)\n\n        # Now all operations use my_graph\n        FeatureGraph.get_active()  # Returns my_graph\n        ```\n    \"\"\"\n    _active_graph.set(reg)\n</code></pre>"},{"location":"reference/api/definitions/graph/#metaxy.FeatureGraph.use","title":"metaxy.FeatureGraph.use","text":"<pre><code>use() -&gt; Iterator[Self]\n</code></pre> <p>Context manager to temporarily use this graph as active.</p> <p>This is the recommended way to use custom registries, especially in tests. The graph is automatically restored when the context exits.</p> <p>Yields:</p> <ul> <li> <code>FeatureGraph</code> (              <code>Self</code> )          \u2013            <p>This graph instance</p> </li> </ul> Example <pre><code>test_graph = FeatureGraph()\n\nwith test_graph.use():\n    # All operations use test_graph\n    class TestFeature(Feature, spec=...):\n        pass\n\n# Outside context, back to previous graph\n</code></pre> Source code in <code>src/metaxy/models/feature.py</code> <pre><code>@contextmanager\ndef use(self) -&gt; Iterator[Self]:\n    \"\"\"Context manager to temporarily use this graph as active.\n\n    This is the recommended way to use custom registries, especially in tests.\n    The graph is automatically restored when the context exits.\n\n    Yields:\n        FeatureGraph: This graph instance\n\n    Example:\n        ```py\n        test_graph = FeatureGraph()\n\n        with test_graph.use():\n            # All operations use test_graph\n            class TestFeature(Feature, spec=...):\n                pass\n\n        # Outside context, back to previous graph\n        ```\n    \"\"\"\n    token = _active_graph.set(self)\n    try:\n        yield self\n    finally:\n        _active_graph.reset(token)\n</code></pre>"},{"location":"reference/api/definitions/relationship/","title":"Lineage Relationships","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship","title":"metaxy.models.lineage.LineageRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Wrapper class for lineage relationship configurations with convenient constructors.</p> <p>This provides a cleaner API for creating lineage relationships while maintaining type safety through discriminated unions.</p> Show JSON schema: <pre><code>{\n  \"$defs\": {\n    \"AggregationRelationship\": {\n      \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"N:1\",\n          \"default\": \"N:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"anyOf\": [\n            {\n              \"items\": {\n                \"type\": \"string\"\n              },\n              \"type\": \"array\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n          \"title\": \"On\"\n        }\n      },\n      \"title\": \"AggregationRelationship\",\n      \"type\": \"object\"\n    },\n    \"ExpansionRelationship\": {\n      \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:N\",\n          \"default\": \"1:N\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        },\n        \"on\": {\n          \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"title\": \"On\",\n          \"type\": \"array\"\n        },\n        \"id_generation_pattern\": {\n          \"anyOf\": [\n            {\n              \"type\": \"string\"\n            },\n            {\n              \"type\": \"null\"\n            }\n          ],\n          \"default\": null,\n          \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n          \"title\": \"Id Generation Pattern\"\n        }\n      },\n      \"required\": [\n        \"on\"\n      ],\n      \"title\": \"ExpansionRelationship\",\n      \"type\": \"object\"\n    },\n    \"IdentityRelationship\": {\n      \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n      \"properties\": {\n        \"type\": {\n          \"const\": \"1:1\",\n          \"default\": \"1:1\",\n          \"title\": \"Type\",\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"IdentityRelationship\",\n      \"type\": \"object\"\n    }\n  },\n  \"description\": \"Wrapper class for lineage relationship configurations with convenient constructors.\\n\\nThis provides a cleaner API for creating lineage relationships while maintaining\\ntype safety through discriminated unions.\",\n  \"properties\": {\n    \"relationship\": {\n      \"discriminator\": {\n        \"mapping\": {\n          \"1:1\": \"#/$defs/IdentityRelationship\",\n          \"1:N\": \"#/$defs/ExpansionRelationship\",\n          \"N:1\": \"#/$defs/AggregationRelationship\"\n        },\n        \"propertyName\": \"type\"\n      },\n      \"oneOf\": [\n        {\n          \"$ref\": \"#/$defs/IdentityRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/AggregationRelationship\"\n        },\n        {\n          \"$ref\": \"#/$defs/ExpansionRelationship\"\n        }\n      ],\n      \"title\": \"Relationship\"\n    }\n  },\n  \"required\": [\n    \"relationship\"\n  ],\n  \"title\": \"LineageRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Config:</p> <ul> <li><code>frozen</code>: <code>True</code></li> </ul> <p>Fields:</p> <ul> <li> <code>relationship</code>                 (<code>LineageRelationshipUnion</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.identity","title":"metaxy.models.lineage.LineageRelationship.identity  <code>classmethod</code>","text":"<pre><code>identity() -&gt; Self\n</code></pre> <p>Create an identity (1:1) relationship.</p> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:1 relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"feature\",\n...     lineage=LineageRelationship.identity()\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; Self:\n    \"\"\"Create an identity (1:1) relationship.\n\n    Returns:\n        Configured LineageRelationship for 1:1 relationship.\n\n    Examples:\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"feature\",\n        ...     lineage=LineageRelationship.identity()\n        ... )\n    \"\"\"\n    return cls(relationship=IdentityRelationship())\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.aggregation","title":"metaxy.models.lineage.LineageRelationship.aggregation  <code>classmethod</code>","text":"<pre><code>aggregation(on: Sequence[str] | None = None) -&gt; Self\n</code></pre> <p>Create an aggregation (N:1) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Columns to group by for aggregation. If None, uses all target ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for N:1 relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Aggregate on specific columns\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"hourly_stats\",\n...     id_columns=[\"sensor_id\", \"hour\"],\n...     lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Aggregate on all ID columns (default)\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"user_summary\",\n...     id_columns=[\"user_id\"],\n...     lineage=LineageRelationship.aggregation()\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef aggregation(cls, on: Sequence[str] | None = None) -&gt; Self:\n    \"\"\"Create an aggregation (N:1) relationship.\n\n    Args:\n        on: Columns to group by for aggregation. If None, uses all target ID columns.\n\n    Returns:\n        Configured LineageRelationship for N:1 relationship.\n\n    Examples:\n        &gt;&gt;&gt; # Aggregate on specific columns\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"hourly_stats\",\n        ...     id_columns=[\"sensor_id\", \"hour\"],\n        ...     lineage=LineageRelationship.aggregation(on=[\"sensor_id\", \"hour\"])\n        ... )\n\n        &gt;&gt;&gt; # Aggregate on all ID columns (default)\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"user_summary\",\n        ...     id_columns=[\"user_id\"],\n        ...     lineage=LineageRelationship.aggregation()\n        ... )\n    \"\"\"\n    return cls(relationship=AggregationRelationship(on=on))\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.expansion","title":"metaxy.models.lineage.LineageRelationship.expansion  <code>classmethod</code>","text":"<pre><code>expansion(on: Sequence[str], id_generation_pattern: str | None = None) -&gt; Self\n</code></pre> <p>Create an expansion (1:N) relationship.</p> <p>Parameters:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. Required - must explicitly specify which columns link parent to child.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Pattern for generating child IDs. Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>Configured LineageRelationship for 1:N relationship.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Sequential ID generation with explicit parent ID\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"video_frames\",\n...     id_columns=[\"video_id\", \"frame_id\"],\n...     lineage=LineageRelationship.expansion(\n...         on=[\"video_id\"],\n...         id_generation_pattern=\"sequential\"\n...     )\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom ID generation in load_input()\n&gt;&gt;&gt; spec = FeatureSpec(\n...     key=\"text_chunks\",\n...     id_columns=[\"doc_id\", \"chunk_id\"],\n...     lineage=LineageRelationship.expansion(on=[\"doc_id\"])\n... )\n</code></pre> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>@classmethod\ndef expansion(\n    cls,\n    on: Sequence[str],\n    id_generation_pattern: str | None = None,\n) -&gt; Self:\n    \"\"\"Create an expansion (1:N) relationship.\n\n    Args:\n        on: Parent ID columns that identify the parent record. Child records with\n            the same parent IDs will share the same upstream provenance.\n            Required - must explicitly specify which columns link parent to child.\n        id_generation_pattern: Pattern for generating child IDs.\n            Can be \"sequential\", \"hash\", or custom. If None, handled by load_input().\n\n    Returns:\n        Configured LineageRelationship for 1:N relationship.\n\n    Examples:\n        &gt;&gt;&gt; # Sequential ID generation with explicit parent ID\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"video_frames\",\n        ...     id_columns=[\"video_id\", \"frame_id\"],\n        ...     lineage=LineageRelationship.expansion(\n        ...         on=[\"video_id\"],\n        ...         id_generation_pattern=\"sequential\"\n        ...     )\n        ... )\n\n        &gt;&gt;&gt; # Custom ID generation in load_input()\n        &gt;&gt;&gt; spec = FeatureSpec(\n        ...     key=\"text_chunks\",\n        ...     id_columns=[\"doc_id\", \"chunk_id\"],\n        ...     lineage=LineageRelationship.expansion(on=[\"doc_id\"])\n        ... )\n    \"\"\"\n    return cls(\n        relationship=ExpansionRelationship(\n            on=on, id_generation_pattern=id_generation_pattern\n        )\n    )\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationship.get_aggregation_columns","title":"metaxy.models.lineage.LineageRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>Get columns to aggregate on for this relationship.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>Columns to group by for aggregation, or None if no aggregation needed.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self, target_id_columns: Sequence[str]\n) -&gt; Sequence[str] | None:\n    \"\"\"Get columns to aggregate on for this relationship.\n\n    Args:\n        target_id_columns: The target feature's ID columns.\n\n    Returns:\n        Columns to group by for aggregation, or None if no aggregation needed.\n    \"\"\"\n    return self.relationship.get_aggregation_columns(target_id_columns)\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.LineageRelationshipType","title":"metaxy.models.lineage.LineageRelationshipType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of lineage relationship between features.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship","title":"metaxy.models.lineage.IdentityRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-one relationship where each child row maps to exactly one parent row.</p> <p>This is the default relationship type. Parent and child features share the same ID columns and have the same cardinality. No aggregation is performed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default 1:1 relationship\n&gt;&gt;&gt; IdentityRelationship()\n</code></pre> <pre><code>&gt;&gt;&gt; # Or use the classmethod\n&gt;&gt;&gt; LineageRelationship.identity()\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-one relationship where each child row maps to exactly one parent row.\\n\\nThis is the default relationship type. Parent and child features share the same\\nID columns and have the same cardinality. No aggregation is performed.\\n\\nExamples:\\n    &gt;&gt;&gt; # Default 1:1 relationship\\n    &gt;&gt;&gt; IdentityRelationship()\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.identity()\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:1\",\n      \"default\": \"1:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    }\n  },\n  \"title\": \"IdentityRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[IDENTITY]</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","title":"metaxy.models.lineage.IdentityRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>No aggregation needed for identity relationships.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"No aggregation needed for identity relationships.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship","title":"metaxy.models.lineage.ExpansionRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>One-to-many relationship where one parent row expands to multiple child rows.</p> <p>Child features have more granular ID columns than the parent. Each parent row generates multiple child rows with additional ID columns.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str]</code>)           \u2013            <p>Parent ID columns that identify the parent record. Child records with the same parent IDs will share the same upstream provenance. If not specified, will be inferred from the available columns.</p> </li> <li> <code>id_generation_pattern</code>               (<code>str | None</code>)           \u2013            <p>Optional pattern for generating child IDs. Can be \"sequential\", \"hash\", or a custom pattern. If not specified, the feature's load_input() method is responsible for ID generation.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Video frames from video\n&gt;&gt;&gt; ExpansionRelationship(\n...     on=[\"video_id\"],  # Parent ID\n...     id_generation_pattern=\"sequential\"\n... )\n&gt;&gt;&gt; # Parent has: video_id\n&gt;&gt;&gt; # Child has: video_id, frame_id (generated)\n</code></pre> <pre><code>&gt;&gt;&gt; # Text chunks from document\n&gt;&gt;&gt; ExpansionRelationship(on=[\"doc_id\"])\n&gt;&gt;&gt; # Parent has: doc_id\n&gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"One-to-many relationship where one parent row expands to multiple child rows.\\n\\nChild features have more granular ID columns than the parent. Each parent row\\ngenerates multiple child rows with additional ID columns.\\n\\nAttributes:\\n    on: Parent ID columns that identify the parent record. Child records with\\n        the same parent IDs will share the same upstream provenance.\\n        If not specified, will be inferred from the available columns.\\n    id_generation_pattern: Optional pattern for generating child IDs.\\n        Can be \\\"sequential\\\", \\\"hash\\\", or a custom pattern. If not specified,\\n        the feature's load_input() method is responsible for ID generation.\\n\\nExamples:\\n    &gt;&gt;&gt; # Video frames from video\\n    &gt;&gt;&gt; ExpansionRelationship(\\n    ...     on=[\\\"video_id\\\"],  # Parent ID\\n    ...     id_generation_pattern=\\\"sequential\\\"\\n    ... )\\n    &gt;&gt;&gt; # Parent has: video_id\\n    &gt;&gt;&gt; # Child has: video_id, frame_id (generated)\\n\\n    &gt;&gt;&gt; # Text chunks from document\\n    &gt;&gt;&gt; ExpansionRelationship(on=[\\\"doc_id\\\"])\\n    &gt;&gt;&gt; # Parent has: doc_id\\n    &gt;&gt;&gt; # Child has: doc_id, chunk_id (generated in load_input)\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"1:N\",\n      \"default\": \"1:N\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"description\": \"Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"title\": \"On\",\n      \"type\": \"array\"\n    },\n    \"id_generation_pattern\": {\n      \"anyOf\": [\n        {\n          \"type\": \"string\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Pattern for generating child IDs. If None, handled by load_input().\",\n      \"title\": \"Id Generation Pattern\"\n    }\n  },\n  \"required\": [\n    \"on\"\n  ],\n  \"title\": \"ExpansionRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[EXPANSION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str]</code>)             </li> <li> <code>id_generation_pattern</code>                 (<code>str | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.on","title":"metaxy.models.lineage.ExpansionRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str]\n</code></pre> <p>Parent ID columns for grouping. Child records with same parent IDs share provenance. Required for expansion relationships.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.id_generation_pattern","title":"metaxy.models.lineage.ExpansionRelationship.id_generation_pattern  <code>pydantic-field</code>","text":"<pre><code>id_generation_pattern: str | None = None\n</code></pre> <p>Pattern for generating child IDs. If None, handled by load_input().</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","title":"metaxy.models.lineage.ExpansionRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str] | None\n</code></pre> <p>Get aggregation columns for the joiner phase.</p> <p>For expansion relationships, returns None because aggregation happens during diff resolution, not during joining. The joiner should pass through all parent records without aggregation.</p> <p>Parameters:</p> <ul> <li> <code>target_id_columns</code>               (<code>Sequence[str]</code>)           \u2013            <p>The target (child) feature's ID columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequence[str] | None</code>           \u2013            <p>None - no aggregation during join phase for expansion relationships.</p> </li> </ul> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str] | None:\n    \"\"\"Get aggregation columns for the joiner phase.\n\n    For expansion relationships, returns None because aggregation\n    happens during diff resolution, not during joining. The joiner\n    should pass through all parent records without aggregation.\n\n    Args:\n        target_id_columns: The target (child) feature's ID columns.\n\n    Returns:\n        None - no aggregation during join phase for expansion relationships.\n    \"\"\"\n    # Expansion relationships don't aggregate during join phase\n    # Aggregation happens later during diff resolution\n    return None\n</code></pre>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship","title":"metaxy.models.lineage.AggregationRelationship  <code>pydantic-model</code>","text":"<p>               Bases: <code>BaseLineageRelationship</code></p> <p>Many-to-one relationship where multiple parent rows aggregate to one child row.</p> <p>Parent features have more granular ID columns than the child. The child aggregates multiple parent rows by grouping on a subset of the parent's ID columns.</p> <p>Attributes:</p> <ul> <li> <code>on</code>               (<code>Sequence[str] | None</code>)           \u2013            <p>Columns to group by for aggregation. These should be a subset of the target feature's ID columns. If not specified, uses all target ID columns.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Aggregate sensor readings by hour\n&gt;&gt;&gt; AggregationRelationship(on=[\"sensor_id\", \"hour\"])\n&gt;&gt;&gt; # Parent has: sensor_id, hour, minute\n&gt;&gt;&gt; # Child has: sensor_id, hour\n</code></pre> <pre><code>&gt;&gt;&gt; # Or use the classmethod\n&gt;&gt;&gt; LineageRelationship.aggregation(on=[\"user_id\", \"session_id\"])\n</code></pre> Show JSON schema: <pre><code>{\n  \"description\": \"Many-to-one relationship where multiple parent rows aggregate to one child row.\\n\\nParent features have more granular ID columns than the child. The child aggregates\\nmultiple parent rows by grouping on a subset of the parent's ID columns.\\n\\nAttributes:\\n    on: Columns to group by for aggregation. These should be a subset of the\\n        target feature's ID columns. If not specified, uses all target ID columns.\\n\\nExamples:\\n    &gt;&gt;&gt; # Aggregate sensor readings by hour\\n    &gt;&gt;&gt; AggregationRelationship(on=[\\\"sensor_id\\\", \\\"hour\\\"])\\n    &gt;&gt;&gt; # Parent has: sensor_id, hour, minute\\n    &gt;&gt;&gt; # Child has: sensor_id, hour\\n\\n    &gt;&gt;&gt; # Or use the classmethod\\n    &gt;&gt;&gt; LineageRelationship.aggregation(on=[\\\"user_id\\\", \\\"session_id\\\"])\",\n  \"properties\": {\n    \"type\": {\n      \"const\": \"N:1\",\n      \"default\": \"N:1\",\n      \"title\": \"Type\",\n      \"type\": \"string\"\n    },\n    \"on\": {\n      \"anyOf\": [\n        {\n          \"items\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"array\"\n        },\n        {\n          \"type\": \"null\"\n        }\n      ],\n      \"default\": null,\n      \"description\": \"Columns to group by for aggregation. Defaults to all target ID columns.\",\n      \"title\": \"On\"\n    }\n  },\n  \"title\": \"AggregationRelationship\",\n  \"type\": \"object\"\n}\n</code></pre> <p>Fields:</p> <ul> <li> <code>type</code>                 (<code>Literal[AGGREGATION]</code>)             </li> <li> <code>on</code>                 (<code>Sequence[str] | None</code>)             </li> </ul>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-attributes","title":"Attributes","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.on","title":"metaxy.models.lineage.AggregationRelationship.on  <code>pydantic-field</code>","text":"<pre><code>on: Sequence[str] | None = None\n</code></pre> <p>Columns to group by for aggregation. Defaults to all target ID columns.</p>"},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship-functions","title":"Functions","text":""},{"location":"reference/api/definitions/relationship/#metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","title":"metaxy.models.lineage.AggregationRelationship.get_aggregation_columns","text":"<pre><code>get_aggregation_columns(target_id_columns: Sequence[str]) -&gt; Sequence[str]\n</code></pre> <p>Get columns to aggregate on.</p> Source code in <code>src/metaxy/models/lineage.py</code> <pre><code>def get_aggregation_columns(\n    self,\n    target_id_columns: Sequence[str],\n) -&gt; Sequence[str]:\n    \"\"\"Get columns to aggregate on.\"\"\"\n    return self.on if self.on is not None else target_id_columns\n</code></pre>"},{"location":"reference/api/metadata-stores/","title":"Metadata Stores","text":"<p>Metaxy abstracts interactions with metadata behind an interface called MetadaStore.</p> <p>Users can extend this class to implement support for arbitrary metadata storage such as databases, lakehouse formats, or really any kind of external system. Metaxy has built-in support for the following metadata store types:</p>"},{"location":"reference/api/metadata-stores/#databases","title":"Databases","text":"<ul> <li> <p>BigQuery</p> </li> <li> <p>ClickHouse</p> </li> <li> <p>DuckDB</p> </li> <li> <p>LanceDB</p> </li> <li> <p><code>IbisMetadataStore</code> (a base class) - see Ibis integration</p> </li> </ul>"},{"location":"reference/api/metadata-stores/#storage-only","title":"Storage Only","text":"<ul> <li> <p>DeltaMetadataStore</p> </li> <li> <p>InMemoryMetadataStore</p> </li> </ul>"},{"location":"reference/api/metadata-stores/#metadata-store-interface","title":"Metadata Store Interface","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore","title":"metaxy.MetadataStore","text":"<pre><code>MetadataStore(*, versioning_engine_cls: type[VersioningEngineT], hash_algorithm: HashAlgorithm | None = None, versioning_engine: VersioningEngineOptions = 'auto', fallback_stores: list[MetadataStore] | None = None, auto_create_tables: bool | None = None, materialization_id: str | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for metadata storage backends.</p> <p>Parameters:</p> <ul> <li> <code>hash_algorithm</code>               (<code>HashAlgorithm | None</code>, default:                   <code>None</code> )           \u2013            <p>Hash algorithm to use for the versioning engine.</p> </li> <li> <code>versioning_engine</code>               (<code>VersioningEngineOptions</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which versioning engine to use.</p> <ul> <li> <p>\"auto\": Prefer the store's native engine, fall back to Polars if needed</p> </li> <li> <p>\"native\": Always use the store's native engine, raise <code>VersioningEngineMismatchError</code>     if provided dataframes are incompatible</p> </li> <li> <p>\"polars\": Always use the Polars engine</p> </li> </ul> </li> <li> <code>fallback_stores</code>               (<code>list[MetadataStore] | None</code>, default:                   <code>None</code> )           \u2013            <p>Ordered list of read-only fallback stores. Used when upstream features are not in this store. <code>VersioningEngineMismatchError</code> is not raised when reading from fallback stores.</p> </li> <li> <code>auto_create_tables</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>If True, automatically create tables when opening the store. If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var). If False, never auto-create tables.</p> <p>Warning</p> <p>Auto-create is intended for development/testing only. Use proper database migration tools like Alembic for production deployments.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID. If provided, all metadata writes will include this ID in the <code>metaxy_materialization_id</code> column. Can be overridden per <code>MetadataStore.write_metadata</code> call.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If fallback stores use different hash algorithms or truncation lengths</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If a user-provided dataframe has a wrong implementation and versioning_engine is set to <code>native</code></p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __init__(\n    self,\n    *,\n    versioning_engine_cls: type[VersioningEngineT],\n    hash_algorithm: HashAlgorithm | None = None,\n    versioning_engine: VersioningEngineOptions = \"auto\",\n    fallback_stores: list[MetadataStore] | None = None,\n    auto_create_tables: bool | None = None,\n    materialization_id: str | None = None,\n):\n    \"\"\"\n    Initialize the metadata store.\n\n    Args:\n        hash_algorithm: Hash algorithm to use for the versioning engine.\n\n        versioning_engine: Which versioning engine to use.\n\n            - \"auto\": Prefer the store's native engine, fall back to Polars if needed\n\n            - \"native\": Always use the store's native engine, raise `VersioningEngineMismatchError`\n                if provided dataframes are incompatible\n\n            - \"polars\": Always use the Polars engine\n\n        fallback_stores: Ordered list of read-only fallback stores.\n            Used when upstream features are not in this store.\n            `VersioningEngineMismatchError` is not raised when reading from fallback stores.\n        auto_create_tables: If True, automatically create tables when opening the store.\n            If None (default), reads from global MetaxyConfig (which reads from METAXY_AUTO_CREATE_TABLES env var).\n            If False, never auto-create tables.\n\n            !!! warning\n                Auto-create is intended for development/testing only.\n                Use proper database migration tools like Alembic for production deployments.\n\n        materialization_id: Optional external orchestration ID.\n            If provided, all metadata writes will include this ID in the `metaxy_materialization_id` column.\n            Can be overridden per [`MetadataStore.write_metadata`][metaxy.MetadataStore.write_metadata] call.\n\n    Raises:\n        ValueError: If fallback stores use different hash algorithms or truncation lengths\n        VersioningEngineMismatchError: If a user-provided dataframe has a wrong implementation\n            and versioning_engine is set to `native`\n    \"\"\"\n    # Initialize state early so properties can check it\n    self._is_open = False\n    self._context_depth = 0\n    self._versioning_engine = versioning_engine\n    self._allow_cross_project_writes = False\n    self._materialization_id = materialization_id\n    self._open_cm: AbstractContextManager[Self] | None = (\n        None  # Track the open() context manager\n    )\n    self.versioning_engine_cls = versioning_engine_cls\n\n    # Resolve auto_create_tables from global config if not explicitly provided\n    if auto_create_tables is None:\n        from metaxy.config import MetaxyConfig\n\n        self.auto_create_tables = MetaxyConfig.get().auto_create_tables\n    else:\n        self.auto_create_tables = auto_create_tables\n\n    # Use store's default algorithm if not specified\n    if hash_algorithm is None:\n        hash_algorithm = self._get_default_hash_algorithm()\n\n    self.hash_algorithm = hash_algorithm\n\n    self.fallback_stores = fallback_stores or []\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-attributes","title":"Attributes","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.materialization_id","title":"metaxy.MetadataStore.materialization_id  <code>property</code>","text":"<pre><code>materialization_id: str | None\n</code></pre> <p>The external orchestration ID for this store instance.</p> <p>If set, all metadata writes include this ID in the <code>metaxy_materialization_id</code> column, allowing filtering of rows written during a specific materialization run.</p>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.config_model","title":"metaxy.MetadataStore.config_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[MetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef config_model(cls) -&gt; type[MetadataStoreConfig]:\n    \"\"\"Return the configuration model class for this store type.\n\n    Subclasses must override this to return their specific config class.\n\n    Returns:\n        The config class type (e.g., DuckDBMetadataStoreConfig)\n\n    Note:\n        Subclasses override this with a more specific return type.\n        Type checkers may show a warning about incompatible override,\n        but this is intentional - each store returns its own config type.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.from_config","title":"metaxy.MetadataStore.from_config  <code>classmethod</code>","text":"<pre><code>from_config(config: MetadataStoreConfig, **kwargs: Any) -&gt; Self\n</code></pre> <p>Create a store instance from a configuration object.</p> <p>This method creates a store by: 1. Converting the config to a dict 2. Resolving fallback store names to actual store instances 3. Calling the store's init with the config parameters</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>MetadataStoreConfig</code>)           \u2013            <p>Configuration object (should be the type returned by config_model())</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed directly to the store constructor (e.g., materialization_id for runtime parameters not in config)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>           \u2013            <p>A new store instance configured according to the config object</p> </li> </ul> Example <pre><code>from metaxy.metadata_store.duckdb import (\n    DuckDBMetadataStore,\n    DuckDBMetadataStoreConfig,\n)\n\nconfig = DuckDBMetadataStoreConfig(\n    database=\"metadata.db\",\n    fallback_stores=[\"prod\"],\n)\n\nstore = DuckDBMetadataStore.from_config(config)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MetadataStoreConfig, **kwargs: Any) -&gt; Self:\n    \"\"\"Create a store instance from a configuration object.\n\n    This method creates a store by:\n    1. Converting the config to a dict\n    2. Resolving fallback store names to actual store instances\n    3. Calling the store's __init__ with the config parameters\n\n    Args:\n        config: Configuration object (should be the type returned by config_model())\n        **kwargs: Additional arguments passed directly to the store constructor\n            (e.g., materialization_id for runtime parameters not in config)\n\n    Returns:\n        A new store instance configured according to the config object\n\n    Example:\n        ```python\n        from metaxy.metadata_store.duckdb import (\n            DuckDBMetadataStore,\n            DuckDBMetadataStoreConfig,\n        )\n\n        config = DuckDBMetadataStoreConfig(\n            database=\"metadata.db\",\n            fallback_stores=[\"prod\"],\n        )\n\n        store = DuckDBMetadataStore.from_config(config)\n        ```\n    \"\"\"\n    # Convert config to dict, excluding unset values\n    config_dict = config.model_dump(exclude_unset=True)\n\n    # Pop and resolve fallback store names to actual store instances\n    fallback_store_names = config_dict.pop(\"fallback_stores\", [])\n    fallback_stores = [\n        MetaxyConfig.get().get_store(name) for name in fallback_store_names\n    ]\n\n    # Create store with resolved fallback stores, config, and extra kwargs\n    return cls(fallback_stores=fallback_stores, **config_dict, **kwargs)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.resolve_update","title":"metaxy.MetadataStore.resolve_update","text":"<pre><code>resolve_update(feature: type[BaseFeature], *, samples: IntoFrame | Frame | None = None, filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None = None, global_filters: Sequence[Expr] | None = None, lazy: Literal[False] = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, skip_comparison: bool = False, **kwargs: Any) -&gt; Increment\n</code></pre><pre><code>resolve_update(feature: type[BaseFeature], *, samples: IntoFrame | Frame | None = None, filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None = None, global_filters: Sequence[Expr] | None = None, lazy: Literal[True], versioning_engine: Literal['auto', 'native', 'polars'] | None = None, skip_comparison: bool = False, **kwargs: Any) -&gt; LazyIncrement\n</code></pre> <pre><code>resolve_update(feature: type[BaseFeature], *, samples: IntoFrame | Frame | None = None, filters: Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None = None, global_filters: Sequence[Expr] | None = None, lazy: bool = False, versioning_engine: Literal['auto', 'native', 'polars'] | None = None, skip_comparison: bool = False, **kwargs: Any) -&gt; Increment | LazyIncrement\n</code></pre> <p>Calculate an incremental update for a feature.</p> <p>This is the main workhorse in Metaxy.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>type[BaseFeature]</code>)           \u2013            <p>Feature class to resolve updates for</p> </li> <li> <code>samples</code>               (<code>IntoFrame | Frame | None</code>, default:                   <code>None</code> )           \u2013            <p>A dataframe with joined upstream metadata and <code>\"metaxy_provenance_by_field\"</code> column set. When provided, <code>MetadataStore</code> skips loading upstream feature metadata and provenance calculations.</p> <p>Required for root features</p> <p>Metaxy doesn't know how to populate input metadata for root features, so <code>samples</code> argument for must be provided for them.</p> <p>Tip</p> <p>For non-root features, use <code>samples</code> to customize the automatic upstream loading and field provenance calculation. For example, it can be used to requires processing for specific sample IDs.</p> <p>Setting this parameter during normal operations is not required.</p> </li> <li> <code>filters</code>               (<code>Mapping[CoercibleToFeatureKey, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>A mapping from feature keys to lists of Narwhals filter expressions. Keys can be feature classes, FeatureKey objects, or string paths. Applied at read-time. May filter the current feature, in this case it will also be applied to <code>samples</code> (if provided). Example: <code>{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}</code></p> </li> <li> <code>global_filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of Narwhals filter expressions applied to all features. These filters are combined with any feature-specific filters from <code>filters</code>. Useful for filtering by common columns like <code>sample_uid</code> across all features. Example: <code>[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]</code></p> </li> <li> <code>lazy</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a metaxy.versioning.types.LazyIncrement or a metaxy.versioning.types.Increment.</p> </li> <li> <code>versioning_engine</code>               (<code>Literal['auto', 'native', 'polars'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Override the store's versioning engine for this operation.</p> </li> <li> <code>skip_comparison</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, skip the increment comparison logic and return all upstream samples in <code>Increment.added</code>. The <code>changed</code> and <code>removed</code> frames will be empty.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no <code>samples</code> dataframe has been provided when resolving an update for a root feature.</p> </li> <li> <code>VersioningEngineMismatchError</code>             \u2013            <p>If <code>versioning_engine</code> has been set to <code>\"native\"</code> and a dataframe of a different implementation has been encountered during <code>resolve_update</code>.</p> </li> </ul> <p>With a root feature</p> <pre><code>samples = pl.DataFrame({\n    \"sample_uid\": [1, 2, 3],\n    \"metaxy_provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n})\nresult = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def resolve_update(\n    self,\n    feature: type[BaseFeature],\n    *,\n    samples: IntoFrame | Frame | None = None,\n    filters: Mapping[CoercibleToFeatureKey, Sequence[nw.Expr]] | None = None,\n    global_filters: Sequence[nw.Expr] | None = None,\n    lazy: bool = False,\n    versioning_engine: Literal[\"auto\", \"native\", \"polars\"] | None = None,\n    skip_comparison: bool = False,\n    **kwargs: Any,\n) -&gt; Increment | LazyIncrement:\n    \"\"\"Calculate an incremental update for a feature.\n\n    This is the main workhorse in Metaxy.\n\n    Args:\n        feature: Feature class to resolve updates for\n        samples: A dataframe with joined upstream metadata and `\"metaxy_provenance_by_field\"` column set.\n            When provided, `MetadataStore` skips loading upstream feature metadata and provenance calculations.\n\n            !!! info \"Required for root features\"\n                Metaxy doesn't know how to populate input metadata for root features,\n                so `samples` argument for **must** be provided for them.\n\n            !!! tip\n                For non-root features, use `samples` to customize the automatic upstream loading and field provenance calculation.\n                For example, it can be used to requires processing for specific sample IDs.\n\n            Setting this parameter during normal operations is not required.\n\n        filters: A mapping from feature keys to lists of Narwhals filter expressions.\n            Keys can be feature classes, FeatureKey objects, or string paths.\n            Applied at read-time. May filter the current feature,\n            in this case it will also be applied to `samples` (if provided).\n            Example: `{UpstreamFeature: [nw.col(\"x\") &gt; 10], ...}`\n        global_filters: A list of Narwhals filter expressions applied to all features.\n            These filters are combined with any feature-specific filters from `filters`.\n            Useful for filtering by common columns like `sample_uid` across all features.\n            Example: `[nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]`\n        lazy: Whether to return a [metaxy.versioning.types.LazyIncrement][] or a [metaxy.versioning.types.Increment][].\n        versioning_engine: Override the store's versioning engine for this operation.\n        skip_comparison: If True, skip the increment comparison logic and return all\n            upstream samples in `Increment.added`. The `changed` and `removed` frames will\n            be empty.\n\n    Raises:\n        ValueError: If no `samples` dataframe has been provided when resolving an update for a root feature.\n        VersioningEngineMismatchError: If `versioning_engine` has been set to `\"native\"`\n            and a dataframe of a different implementation has been encountered during `resolve_update`.\n\n    !!! example \"With a root feature\"\n\n        ```py\n        samples = pl.DataFrame({\n            \"sample_uid\": [1, 2, 3],\n            \"metaxy_provenance_by_field\": [{\"field\": \"h1\"}, {\"field\": \"h2\"}, {\"field\": \"h3\"}],\n        })\n        result = store.resolve_update(RootFeature, samples=nw.from_native(samples))\n        ```\n    \"\"\"\n    import narwhals as nw\n\n    # Convert samples to Narwhals frame if not already\n    samples_nw: nw.DataFrame[Any] | nw.LazyFrame[Any] | None = None\n    if samples is not None:\n        if isinstance(samples, (nw.DataFrame, nw.LazyFrame)):\n            samples_nw = samples\n        else:\n            samples_nw = nw.from_native(samples)\n\n    # Normalize filter keys to FeatureKey\n    normalized_filters: dict[FeatureKey, list[nw.Expr]] = {}\n    if filters:\n        for key, exprs in filters.items():\n            feature_key = self._resolve_feature_key(key)\n            normalized_filters[feature_key] = list(exprs)\n\n    # Convert global_filters to a list for easy concatenation\n    global_filter_list = list(global_filters) if global_filters else []\n\n    graph = current_graph()\n    plan = graph.get_feature_plan(feature.spec().key)\n\n    # Root features without samples: error (samples required)\n    if not plan.deps and samples_nw is None:\n        raise ValueError(\n            f\"Feature {feature.spec().key} has no upstream dependencies (root feature). \"\n            f\"Must provide 'samples' parameter with sample_uid and {METAXY_PROVENANCE_BY_FIELD} columns. \"\n            f\"Root features require manual {METAXY_PROVENANCE_BY_FIELD} computation.\"\n        )\n\n    # Combine feature-specific filters with global filters\n    current_feature_filters = [\n        *normalized_filters.get(feature.spec().key, []),\n        *global_filter_list,\n    ]\n\n    current_metadata = self.read_metadata_in_store(\n        feature,\n        filters=[\n            nw.col(METAXY_FEATURE_VERSION)\n            == graph.get_feature_version(feature.spec().key),\n            *current_feature_filters,\n        ],\n    )\n\n    upstream_by_key: dict[FeatureKey, nw.LazyFrame[Any]] = {}\n    filters_by_key: dict[FeatureKey, list[nw.Expr]] = {}\n\n    # if samples are provided, use them as source of truth for upstream data\n    if samples_nw is not None:\n        # Apply filters to samples if any\n        filtered_samples = samples_nw\n        if current_feature_filters:\n            filtered_samples = samples_nw.filter(current_feature_filters)\n\n        # fill in METAXY_PROVENANCE column if it's missing (e.g. for root features)\n        samples_nw = self.hash_struct_version_column(\n            plan,\n            df=filtered_samples,\n            struct_column=METAXY_PROVENANCE_BY_FIELD,\n            hash_column=METAXY_PROVENANCE,\n        )\n\n        # For root features, add data_version columns if they don't exist\n        # (root features have no computation, so data_version equals provenance)\n        if METAXY_DATA_VERSION_BY_FIELD not in samples_nw.columns:\n            samples_nw = samples_nw.with_columns(\n                nw.col(METAXY_PROVENANCE_BY_FIELD).alias(\n                    METAXY_DATA_VERSION_BY_FIELD\n                ),\n                nw.col(METAXY_PROVENANCE).alias(METAXY_DATA_VERSION),\n            )\n    else:\n        for upstream_spec in plan.deps or []:\n            # Combine feature-specific filters with global filters for upstream\n            upstream_filters = [\n                *normalized_filters.get(upstream_spec.key, []),\n                *global_filter_list,\n            ]\n            upstream_feature_metadata = self.read_metadata(\n                upstream_spec.key,\n                filters=upstream_filters,\n            )\n            if upstream_feature_metadata is not None:\n                upstream_by_key[upstream_spec.key] = upstream_feature_metadata\n\n    # determine which implementation to use for resolving the increment\n    # consider (1) whether all upstream metadata has been loaded with the native implementation\n    # (2) if samples have native implementation\n\n    # Use parameter if provided, otherwise use store default\n    engine_mode = (\n        versioning_engine\n        if versioning_engine is not None\n        else self._versioning_engine\n    )\n\n    # If \"polars\" mode, force Polars immediately\n    if engine_mode == \"polars\":\n        implementation = nw.Implementation.POLARS\n        switched_to_polars = True\n    else:\n        implementation = self.native_implementation()\n        switched_to_polars = False\n\n        for upstream_key, df in upstream_by_key.items():\n            if df.implementation != implementation:\n                switched_to_polars = True\n                # Only raise error in \"native\" mode if no fallback stores configured.\n                # If fallback stores exist, the implementation mismatch indicates data came\n                # from fallback (different implementation), which is legitimate fallback access.\n                # If data were local, it would have the native implementation.\n                if engine_mode == \"native\" and not self.fallback_stores:\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but upstream feature `{upstream_key.to_string()}` \"\n                        f\"has implementation {df.implementation}, expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\" or (\n                    engine_mode == \"native\" and self.fallback_stores\n                ):\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=df.implementation,\n                        message=f\"Using Polars for resolving the increment instead. This was caused by upstream feature `{upstream_key.to_string()}`.\",\n                    )\n                implementation = nw.Implementation.POLARS\n                break\n\n        if (\n            samples_nw is not None\n            and samples_nw.implementation != self.native_implementation()\n        ):\n            if not switched_to_polars:\n                if engine_mode == \"native\":\n                    # Always raise error for samples with wrong implementation, regardless\n                    # of fallback stores, because samples come from user argument, not from fallback\n                    raise VersioningEngineMismatchError(\n                        f\"versioning_engine='native' but provided `samples` have implementation {samples_nw.implementation}, \"\n                        f\"expected {self.native_implementation()}\"\n                    )\n                elif engine_mode == \"auto\":\n                    PolarsMaterializationWarning.warn_on_implementation_mismatch(\n                        expected=self.native_implementation(),\n                        actual=samples_nw.implementation,\n                        message=f\"Provided `samples` have implementation {samples_nw.implementation}. Using Polars for resolving the increment instead.\",\n                    )\n            implementation = nw.Implementation.POLARS\n            switched_to_polars = True\n\n    if switched_to_polars:\n        if current_metadata:\n            current_metadata = switch_implementation_to_polars(current_metadata)\n        if samples_nw:\n            samples_nw = switch_implementation_to_polars(samples_nw)\n        for upstream_key, df in upstream_by_key.items():\n            upstream_by_key[upstream_key] = switch_implementation_to_polars(df)\n\n    with self.create_versioning_engine(\n        plan=plan, implementation=implementation\n    ) as engine:\n        if skip_comparison:\n            # Skip comparison: return all upstream samples as added\n            if samples_nw is not None:\n                # Root features or user-provided samples: use samples directly\n                # Note: samples already has metaxy_provenance computed\n                added = samples_nw.lazy()\n            else:\n                # Non-root features: load all upstream with provenance\n                added = engine.load_upstream_with_provenance(\n                    upstream=upstream_by_key,\n                    hash_algo=self.hash_algorithm,\n                    filters=filters_by_key,\n                )\n            changed = None\n            removed = None\n        else:\n            added, changed, removed = engine.resolve_increment_with_provenance(\n                current=current_metadata,\n                upstream=upstream_by_key,\n                hash_algorithm=self.hash_algorithm,\n                filters=filters_by_key,\n                sample=samples_nw.lazy() if samples_nw is not None else None,\n            )\n\n    # Convert None to empty DataFrames\n    if changed is None:\n        changed = empty_frame_like(added)\n    if removed is None:\n        removed = empty_frame_like(added)\n\n    if lazy:\n        return LazyIncrement(\n            added=added\n            if isinstance(added, nw.LazyFrame)\n            else nw.from_native(added),\n            changed=changed\n            if isinstance(changed, nw.LazyFrame)\n            else nw.from_native(changed),\n            removed=removed\n            if isinstance(removed, nw.LazyFrame)\n            else nw.from_native(removed),\n        )\n    else:\n        return Increment(\n            added=added.collect() if isinstance(added, nw.LazyFrame) else added,\n            changed=changed.collect()\n            if isinstance(changed, nw.LazyFrame)\n            else changed,\n            removed=removed.collect()\n            if isinstance(removed, nw.LazyFrame)\n            else removed,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata","title":"metaxy.MetadataStore.read_metadata","text":"<pre><code>read_metadata(feature: CoercibleToFeatureKey, *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, allow_fallback: bool = True, current_only: bool = True, latest_only: bool = True) -&gt; LazyFrame[Any]\n</code></pre> <p>Read metadata with optional fallback to upstream stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Explicit feature_version to filter by (mutually exclusive with current_only=True)</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>Sequence of Narwhals filter expressions to apply to this feature. Example: <code>[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]</code></p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to include. Metaxy's system columns are always included.</p> </li> <li> <code>allow_fallback</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, check fallback stores on local miss</p> </li> <li> <code>current_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If <code>True</code>, only return rows with current feature_version</p> </li> <li> <code>latest_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to deduplicate samples within <code>id_columns</code> groups ordered by <code>metaxy_created_at</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any]</code>           \u2013            <p>Narwhals LazyFrame with metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If feature not found in any store</p> </li> <li> <code>SystemDataNotFoundError</code>             \u2013            <p>When attempting to read non-existent Metaxy system data</p> </li> <li> <code>ValueError</code>             \u2013            <p>If both feature_version and current_only=True are provided</p> </li> </ul> <p>Info</p> <p>When this method is called with default arguments, it will return the latest (by <code>metaxy_created_at</code>) metadata for the current feature version. Therefore, it's perfectly suitable for most use cases.</p> <p>Warning</p> <p>The order of rows is not guaranteed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def read_metadata(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    allow_fallback: bool = True,\n    current_only: bool = True,\n    latest_only: bool = True,\n) -&gt; nw.LazyFrame[Any]:\n    \"\"\"\n    Read metadata with optional fallback to upstream stores.\n\n    Args:\n        feature: Feature to read metadata for\n        feature_version: Explicit feature_version to filter by (mutually exclusive with current_only=True)\n        filters: Sequence of Narwhals filter expressions to apply to this feature.\n            Example: `[nw.col(\"x\") &gt; 10, nw.col(\"y\") &lt; 5]`\n        columns: Subset of columns to include. Metaxy's system columns are always included.\n        allow_fallback: If `True`, check fallback stores on local miss\n        current_only: If `True`, only return rows with current feature_version\n        latest_only: Whether to deduplicate samples within `id_columns` groups ordered by `metaxy_created_at`.\n\n    Returns:\n        Narwhals LazyFrame with metadata\n\n    Raises:\n        FeatureNotFoundError: If feature not found in any store\n        SystemDataNotFoundError: When attempting to read non-existent Metaxy system data\n        ValueError: If both feature_version and current_only=True are provided\n\n    !!! info\n        When this method is called with default arguments, it will return the latest (by `metaxy_created_at`)\n        metadata for the current feature version. Therefore, it's perfectly suitable for most use cases.\n\n    !!! warning\n        The order of rows is not guaranteed.\n    \"\"\"\n    filters = filters or []\n    columns = columns or []\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate mutually exclusive parameters\n    if feature_version is not None and current_only:\n        raise ValueError(\n            \"Cannot specify both feature_version and current_only=True. \"\n            \"Use current_only=False with feature_version parameter.\"\n        )\n\n    # Add feature_version filter only when needed\n    if current_only or feature_version is not None and not is_system_table:\n        version_filter = nw.col(METAXY_FEATURE_VERSION) == (\n            current_graph().get_feature_version(feature_key)\n            if current_only\n            else feature_version\n        )\n        filters = [version_filter, *filters]\n\n    if columns and not is_system_table:\n        # Add only system columns that aren't already in the user's columns list\n        columns_set = set(columns)\n        missing_system_cols = [\n            c for c in ALL_SYSTEM_COLUMNS if c not in columns_set\n        ]\n        read_columns = [*columns, *missing_system_cols]\n    else:\n        read_columns = None\n\n    lazy_frame = None\n    try:\n        lazy_frame = self.read_metadata_in_store(\n            feature, filters=filters, columns=read_columns\n        )\n    except FeatureNotFoundError as e:\n        # do not read system features from fallback stores\n        if is_system_table:\n            raise SystemDataNotFoundError(\n                f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n            ) from e\n\n    # Handle case where read_metadata_in_store returns None (no exception raised)\n    if lazy_frame is None and is_system_table:\n        raise SystemDataNotFoundError(\n            f\"System Metaxy data with key {feature_key} is missing in {self.display()}. Invoke `metaxy graph push` before attempting to read system data.\"\n        )\n\n    if lazy_frame is not None and not is_system_table and latest_only:\n        from metaxy.models.constants import METAXY_CREATED_AT\n\n        # Apply deduplication\n        lazy_frame = self.versioning_engine_cls.keep_latest_by_group(\n            df=lazy_frame,\n            group_columns=list(\n                self._resolve_feature_plan(feature_key).feature.id_columns\n            ),\n            timestamp_column=METAXY_CREATED_AT,\n        )\n\n    if lazy_frame is not None:\n        # After dedup, filter to requested columns if specified\n        if columns:\n            lazy_frame = lazy_frame.select(columns)\n\n        return lazy_frame\n\n    # Try fallback stores\n    if allow_fallback:\n        for store in self.fallback_stores:\n            try:\n                # Use full read_metadata to handle nested fallback chains\n                return store.read_metadata(\n                    feature,\n                    feature_version=feature_version,\n                    filters=filters,\n                    columns=columns,\n                    allow_fallback=True,\n                    current_only=current_only,\n                    latest_only=latest_only,\n                )\n            except FeatureNotFoundError:\n                # Try next fallback store\n                continue\n\n    # Not found anywhere\n    raise FeatureNotFoundError(\n        f\"Feature {feature_key.to_string()} not found in store\"\n        + (\" or fallback stores\" if allow_fallback else \"\")\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata","title":"metaxy.MetadataStore.write_metadata","text":"<pre><code>write_metadata(feature: CoercibleToFeatureKey, df: IntoFrame, materialization_id: str | None = None) -&gt; None\n</code></pre> <p>Write metadata for a feature (append-only by design).</p> <p>Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to write metadata for</p> </li> <li> <code>df</code>               (<code>IntoFrame</code>)           \u2013            <p>Metadata DataFrame of any type supported by Narwhals. Must have <code>metaxy_provenance_by_field</code> column of type Struct with fields matching feature's fields. Optionally, may also contain <code>metaxy_data_version_by_field</code>.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for this write. Overrides the store's default <code>materialization_id</code> if provided. Useful for tracking which orchestration run produced this metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li> <p>Must be called within a <code>MetadataStore.open(mode=\"write\")</code> context manager.</p> </li> <li> <p>Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.</p> </li> <li> <p>Fallback stores are never used for writes.</p> </li> <li> <p>Features from other Metaxy projects cannot be written to, unless project validation has been disabled with MetadataStore.allow_cross_project_writes.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata(\n    self,\n    feature: CoercibleToFeatureKey,\n    df: IntoFrame,\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for a feature (append-only by design).\n\n    Automatically adds the Metaxy system columns, unless they already exist in the DataFrame.\n\n    Args:\n        feature: Feature to write metadata for\n        df: Metadata DataFrame of any type supported by [Narwhals](https://narwhals-dev.github.io/narwhals/).\n            Must have `metaxy_provenance_by_field` column of type Struct with fields matching feature's fields.\n            Optionally, may also contain `metaxy_data_version_by_field`.\n        materialization_id: Optional external orchestration ID for this write.\n            Overrides the store's default `materialization_id` if provided.\n            Useful for tracking which orchestration run produced this metadata.\n\n    Raises:\n        MetadataSchemaError: If DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"write\")` context manager.\n\n        - Metaxy always performs an \"append\" operation. Metadata is never deleted or mutated.\n\n        - Fallback stores are never used for writes.\n\n        - Features from other Metaxy projects cannot be written to, unless project validation has been disabled with [MetadataStore.allow_cross_project_writes][].\n\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    is_system_table = self._is_system_table(feature_key)\n\n    # Validate project for non-system tables\n    if not is_system_table:\n        self._validate_project_write(feature)\n\n    # Convert Polars to Narwhals to Polars if needed\n    # if isinstance(df_nw, (pl.DataFrame, pl.LazyFrame)):\n    df_nw = nw.from_native(df)\n\n    assert isinstance(df_nw, nw.DataFrame), \"df must be a Narwhal DataFrame\"\n\n    # For system tables, write directly without feature_version tracking\n    if is_system_table:\n        self._validate_schema_system_table(df_nw)\n        self.write_metadata_to_store(feature_key, df_nw)\n        return\n\n    if METAXY_PROVENANCE_BY_FIELD not in df_nw.columns:\n        from metaxy.metadata_store.exceptions import MetadataSchemaError\n\n        raise MetadataSchemaError(\n            f\"DataFrame must have '{METAXY_PROVENANCE_BY_FIELD}' column\"\n        )\n\n    # Add all required system columns\n    # warning: for dataframes that do not match the native MetadataStore implementation\n    # and are missing the METAXY_DATA_VERSION column, this call will lead to materializing the equivalent Polars DataFrame\n    # while calculating the missing METAXY_DATA_VERSION column\n    df_nw = self._add_system_columns(\n        df_nw, feature, materialization_id=materialization_id\n    )\n\n    self._validate_schema(df_nw)\n    self.write_metadata_to_store(feature_key, df_nw)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata_multi","title":"metaxy.MetadataStore.write_metadata_multi","text":"<pre><code>write_metadata_multi(metadata: Mapping[Any, IntoFrame], materialization_id: str | None = None) -&gt; None\n</code></pre> <p>Write metadata for multiple features in reverse topological order.</p> <p>Processes features so that dependents are written before their dependencies. This ordering ensures that downstream features are written first, which can be useful for certain data consistency requirements or when features need to be processed in a specific order.</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>               (<code>Mapping[Any, IntoFrame]</code>)           \u2013            <p>Mapping from feature keys to metadata DataFrames. Keys can be any type coercible to FeatureKey (string, sequence, FeatureKey, or BaseFeature class). Values must be DataFrames compatible with Narwhals, containing required system columns.</p> </li> <li> <code>materialization_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional external orchestration ID for all writes. Overrides the store's default <code>materialization_id</code> if provided. Applied to all feature writes in this batch.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>MetadataSchemaError</code>             \u2013            <p>If any DataFrame schema is invalid</p> </li> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> <li> <code>ValueError</code>             \u2013            <p>If writing to a feature from a different project than expected</p> </li> </ul> Note <ul> <li>Must be called within a <code>MetadataStore.open(mode=\"write\")</code> context manager.</li> <li>Empty mappings are handled gracefully (no-op).</li> <li>Each feature's metadata is written via <code>write_metadata</code>, so all   validation and system column handling from that method applies.</li> </ul> Example <pre><code>with store.open(mode=\"write\"):\n    store.write_metadata_multi({\n        ChildFeature: child_df,\n        ParentFeature: parent_df,\n    })\n# Features are written in reverse topological order:\n# ChildFeature first, then ParentFeature\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def write_metadata_multi(\n    self,\n    metadata: Mapping[Any, IntoFrame],\n    materialization_id: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write metadata for multiple features in reverse topological order.\n\n    Processes features so that dependents are written before their dependencies.\n    This ordering ensures that downstream features are written first, which can\n    be useful for certain data consistency requirements or when features need\n    to be processed in a specific order.\n\n    Args:\n        metadata: Mapping from feature keys to metadata DataFrames.\n            Keys can be any type coercible to FeatureKey (string, sequence,\n            FeatureKey, or BaseFeature class). Values must be DataFrames\n            compatible with Narwhals, containing required system columns.\n        materialization_id: Optional external orchestration ID for all writes.\n            Overrides the store's default `materialization_id` if provided.\n            Applied to all feature writes in this batch.\n\n    Raises:\n        MetadataSchemaError: If any DataFrame schema is invalid\n        StoreNotOpenError: If store is not open\n        ValueError: If writing to a feature from a different project than expected\n\n    Note:\n        - Must be called within a `MetadataStore.open(mode=\"write\")` context manager.\n        - Empty mappings are handled gracefully (no-op).\n        - Each feature's metadata is written via `write_metadata`, so all\n          validation and system column handling from that method applies.\n\n    Example:\n        ```py\n        with store.open(mode=\"write\"):\n            store.write_metadata_multi({\n                ChildFeature: child_df,\n                ParentFeature: parent_df,\n            })\n        # Features are written in reverse topological order:\n        # ChildFeature first, then ParentFeature\n        ```\n    \"\"\"\n    if not metadata:\n        return\n\n    # Build mapping from resolved keys to dataframes in one pass\n    resolved_metadata = {\n        self._resolve_feature_key(key): df for key, df in metadata.items()\n    }\n\n    # Get reverse topological order (dependents first)\n    graph = current_graph()\n    sorted_keys = graph.topological_sort_features(\n        list(resolved_metadata.keys()), descending=True\n    )\n\n    # Write metadata in reverse topological order\n    for feature_key in sorted_keys:\n        self.write_metadata(\n            feature_key,\n            resolved_metadata[feature_key],\n            materialization_id=materialization_id,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.native_implementation","title":"metaxy.MetadataStore.native_implementation","text":"<pre><code>native_implementation() -&gt; Implementation\n</code></pre> <p>Get the native Narwhals implementation for this store's backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def native_implementation(self) -&gt; nw.Implementation:\n    \"\"\"Get the native Narwhals implementation for this store's backend.\"\"\"\n    return self.versioning_engine_cls.implementation()\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.create_versioning_engine","title":"metaxy.MetadataStore.create_versioning_engine","text":"<pre><code>create_versioning_engine(plan: FeaturePlan, implementation: Implementation) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]\n</code></pre> <p>Creates an appropriate provenance engine.</p> <p>Falls back to Polars implementation if the required implementation differs from the store's native implementation.</p> <p>Parameters:</p> <ul> <li> <code>plan</code>               (<code>FeaturePlan</code>)           \u2013            <p>The feature plan.</p> </li> <li> <code>implementation</code>               (<code>Implementation</code>)           \u2013            <p>The desired engine implementation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[VersioningEngine | PolarsVersioningEngine]</code>           \u2013            <p>An appropriate provenance engine.</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef create_versioning_engine(\n    self, plan: FeaturePlan, implementation: nw.Implementation\n) -&gt; Iterator[VersioningEngine | PolarsVersioningEngine]:\n    \"\"\"\n    Creates an appropriate provenance engine.\n\n    Falls back to Polars implementation if the required implementation differs from the store's native implementation.\n\n    Args:\n        plan: The feature plan.\n        implementation: The desired engine implementation.\n\n    Returns:\n        An appropriate provenance engine.\n    \"\"\"\n\n    if implementation == nw.Implementation.POLARS:\n        cm = self._create_polars_versioning_engine(plan)\n    elif implementation == self.native_implementation():\n        cm = self._create_versioning_engine(plan)\n    else:\n        cm = self._create_polars_versioning_engine(plan)\n\n    with cm as engine:\n        yield engine\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.open","title":"metaxy.MetadataStore.open  <code>abstractmethod</code>","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open/initialize the store for operations.</p> <p>Context manager that opens the store with specified access mode. Called internally by <code>__enter__</code>. Child classes should implement backend-specific connection setup/teardown here.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode for this connection session.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance with connection open</p> </li> </ul> Note <p>Users should prefer using <code>with store:</code> pattern except when write access mode is needed.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:\n    \"\"\"Open/initialize the store for operations.\n\n    Context manager that opens the store with specified access mode.\n    Called internally by `__enter__`.\n    Child classes should implement backend-specific connection setup/teardown here.\n\n    Args:\n        mode: Access mode for this connection session.\n\n    Yields:\n        Self: The store instance with connection open\n\n    Note:\n        Users should prefer using `with store:` pattern except when write access mode is needed.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.__enter__","title":"metaxy.MetadataStore.__enter__","text":"<pre><code>__enter__() -&gt; Self\n</code></pre> <p>Enter context manager - opens store in READ mode by default.</p> <p>Use <code>MetadataStore.open</code> for write access mode instead.</p> <p>Returns:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The opened store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def __enter__(self) -&gt; Self:\n    \"\"\"Enter context manager - opens store in READ mode by default.\n\n    Use [`MetadataStore.open`][metaxy.metadata_store.base.MetadataStore.open] for write access mode instead.\n\n    Returns:\n        Self: The opened store instance\n    \"\"\"\n    # Determine mode based on auto_create_tables\n    mode = \"write\" if self.auto_create_tables else \"read\"\n\n    # Open the store (open() manages _context_depth internally)\n    self._open_cm = self.open(mode)\n    self._open_cm.__enter__()\n\n    return self\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.validate_hash_algorithm","title":"metaxy.MetadataStore.validate_hash_algorithm","text":"<pre><code>validate_hash_algorithm(check_fallback_stores: bool = True) -&gt; None\n</code></pre> <p>Validate that hash algorithm is supported by this store's components.</p> <p>Public method - can be called to verify hash compatibility.</p> <p>Parameters:</p> <ul> <li> <code>check_fallback_stores</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, also validate hash is supported by fallback stores (ensures compatibility for future cross-store operations)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If hash algorithm not supported by components or fallback stores</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def validate_hash_algorithm(\n    self,\n    check_fallback_stores: bool = True,\n) -&gt; None:\n    \"\"\"Validate that hash algorithm is supported by this store's components.\n\n    Public method - can be called to verify hash compatibility.\n\n    Args:\n        check_fallback_stores: If True, also validate hash is supported by\n            fallback stores (ensures compatibility for future cross-store operations)\n\n    Raises:\n        ValueError: If hash algorithm not supported by components or fallback stores\n    \"\"\"\n    # Validate hash algorithm support without creating a full engine\n    # (engine creation requires a graph which isn't available during store init)\n    self._validate_hash_algorithm_support()\n\n    # Check fallback stores\n    if check_fallback_stores:\n        for fallback in self.fallback_stores:\n            fallback.validate_hash_algorithm(check_fallback_stores=False)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.allow_cross_project_writes","title":"metaxy.MetadataStore.allow_cross_project_writes","text":"<pre><code>allow_cross_project_writes() -&gt; Iterator[None]\n</code></pre> <p>Context manager to temporarily allow cross-project writes.</p> <p>This is an escape hatch for legitimate cross-project operations like migrations, where metadata needs to be written to features from different projects.</p> Example <pre><code># During migration, allow writing to features from different projects\nwith store.allow_cross_project_writes():\n    store.write_metadata(feature_from_project_a, metadata_a)\n    store.write_metadata(feature_from_project_b, metadata_b)\n</code></pre> <p>Yields:</p> <ul> <li> <code>None</code> (              <code>None</code> )          \u2013            <p>The context manager temporarily disables project validation</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@contextmanager\ndef allow_cross_project_writes(self) -&gt; Iterator[None]:\n    \"\"\"Context manager to temporarily allow cross-project writes.\n\n    This is an escape hatch for legitimate cross-project operations like migrations,\n    where metadata needs to be written to features from different projects.\n\n    Example:\n        ```py\n        # During migration, allow writing to features from different projects\n        with store.allow_cross_project_writes():\n            store.write_metadata(feature_from_project_a, metadata_a)\n            store.write_metadata(feature_from_project_b, metadata_b)\n        ```\n\n    Yields:\n        None: The context manager temporarily disables project validation\n    \"\"\"\n    previous_value = self._allow_cross_project_writes\n    try:\n        self._allow_cross_project_writes = True\n        yield\n    finally:\n        self._allow_cross_project_writes = previous_value\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.write_metadata_to_store","title":"metaxy.MetadataStore.write_metadata_to_store  <code>abstractmethod</code>","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame, **kwargs: Any) -&gt; None\n</code></pre> <p>Internal write implementation (backend-specific).</p> <p>Backends may convert to their specific type if needed (e.g., Polars, Ibis).</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>Narwhals-compatible DataFrame with metadata to write</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Note: Subclasses implement this for their storage backend.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation (backend-specific).\n\n    Backends may convert to their specific type if needed (e.g., Polars, Ibis).\n\n    Args:\n        feature_key: Feature key to write to\n        df: [Narwhals](https://narwhals-dev.github.io/narwhals/)-compatible DataFrame with metadata to write\n        **kwargs: Backend-specific parameters\n\n    Note: Subclasses implement this for their storage backend.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.drop_feature_metadata","title":"metaxy.MetadataStore.drop_feature_metadata","text":"<pre><code>drop_feature_metadata(feature: CoercibleToFeatureKey) -&gt; None\n</code></pre> <p>Drop all metadata for a feature.</p> <p>This removes all stored metadata for the specified feature from the store. Useful for cleanup in tests or when re-computing feature metadata from scratch.</p> Warning <p>This operation is irreversible and will permanently delete all metadata for the specified feature.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature class or key to drop metadata for</p> </li> </ul> Example <pre><code>store.drop_feature_metadata(MyFeature)\nassert not store.has_feature(MyFeature)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def drop_feature_metadata(self, feature: CoercibleToFeatureKey) -&gt; None:\n    \"\"\"Drop all metadata for a feature.\n\n    This removes all stored metadata for the specified feature from the store.\n    Useful for cleanup in tests or when re-computing feature metadata from scratch.\n\n    Warning:\n        This operation is irreversible and will **permanently delete all metadata** for the specified feature.\n\n    Args:\n        feature: Feature class or key to drop metadata for\n\n    Example:\n        ```py\n        store.drop_feature_metadata(MyFeature)\n        assert not store.has_feature(MyFeature)\n        ```\n    \"\"\"\n    self._check_open()\n    feature_key = self._resolve_feature_key(feature)\n    self._drop_feature_metadata_impl(feature_key)\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.read_metadata_in_store","title":"metaxy.MetadataStore.read_metadata_in_store  <code>abstractmethod</code>","text":"<pre><code>read_metadata_in_store(feature: CoercibleToFeatureKey, *, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, **kwargs: Any) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from THIS store only without using any fallbacks stores.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read metadata for</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions for this specific feature.</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Subset of columns to return</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if feature not found in the store</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef read_metadata_in_store(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from THIS store only without using any fallbacks stores.\n\n    Args:\n        feature: Feature to read metadata for\n        filters: List of Narwhals filter expressions for this specific feature.\n        columns: Subset of columns to return\n        **kwargs: Backend-specific parameters\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if feature not found in the store\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.has_feature","title":"metaxy.MetadataStore.has_feature","text":"<pre><code>has_feature(feature: CoercibleToFeatureKey, *, check_fallback: bool = False) -&gt; bool\n</code></pre> <p>Check if feature exists in store.</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to check</p> </li> <li> <code>check_fallback</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, also check fallback stores</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if feature exists, False otherwise</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def has_feature(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    check_fallback: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Check if feature exists in store.\n\n    Args:\n        feature: Feature to check\n        check_fallback: If True, also check fallback stores\n\n    Returns:\n        True if feature exists, False otherwise\n    \"\"\"\n    self._check_open()\n\n    if self.read_metadata_in_store(feature) is not None:\n        return True\n\n    # Check fallback stores\n    if not check_fallback:\n        return self._has_feature_impl(feature)\n    else:\n        for store in self.fallback_stores:\n            if store.has_feature(feature, check_fallback=True):\n                return True\n\n    return False\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.display","title":"metaxy.MetadataStore.display  <code>abstractmethod</code>","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Return a human-readable display string for this store.</p> <p>Used in warnings, logs, and CLI output to identify the store.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>@abstractmethod\ndef display(self) -&gt; str:\n    \"\"\"Return a human-readable display string for this store.\n\n    Used in warnings, logs, and CLI output to identify the store.\n\n    Returns:\n        Display string (e.g., \"DuckDBMetadataStore(database=/path/to/db.duckdb)\")\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.get_store_metadata","title":"metaxy.MetadataStore.get_store_metadata","text":"<pre><code>get_store_metadata(feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]\n</code></pre> <p>Arbitrary key-value pairs with useful metadata like path in storage.</p> <p>Useful for logging purposes. This method should not expose sensitive information.</p> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def get_store_metadata(self, feature_key: CoercibleToFeatureKey) -&gt; dict[str, Any]:\n    \"\"\"Arbitrary key-value pairs with useful metadata like path in storage.\n\n    Useful for logging purposes. This method should not expose sensitive information.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.MetadataStore.copy_metadata","title":"metaxy.MetadataStore.copy_metadata","text":"<pre><code>copy_metadata(from_store: MetadataStore, features: list[CoercibleToFeatureKey] | None = None, *, from_snapshot: str | None = None, filters: Mapping[str, Sequence[Expr]] | None = None, incremental: bool = True) -&gt; dict[str, int]\n</code></pre> <p>Copy metadata from another store with fine-grained filtering.</p> <p>This is a reusable method that can be called programmatically or from CLI/migrations. Copies metadata for specified features, preserving the original snapshot_version.</p> <p>Parameters:</p> <ul> <li> <code>from_store</code>               (<code>MetadataStore</code>)           \u2013            <p>Source metadata store to copy from (must be opened)</p> </li> <li> <code>features</code>               (<code>list[CoercibleToFeatureKey] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of features to copy. Can be: - None: copies all features from source store - List of FeatureKey or Feature classes: copies specified features</p> </li> <li> <code>from_snapshot</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Snapshot version to filter source data by. If None, uses latest snapshot from source store. Only rows with this snapshot_version will be copied. The snapshot_version is preserved in the destination store.</p> </li> <li> <code>filters</code>               (<code>Mapping[str, Sequence[Expr]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions. These filters are applied when reading from the source store. Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}</p> </li> <li> <code>incremental</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True (default), filter out rows that already exist in the destination store by performing an anti-join on sample_uid for the same snapshot_version.</p> <p>The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid filtered by snapshot_version.</p> <p>Disabling incremental (incremental=False) may improve performance when: - You know the destination is empty or has no overlap with source - The destination store uses deduplication</p> <p>When incremental=False, it's the user's responsibility to avoid duplicates or configure deduplication at the storage layer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, int]</code>           \u2013            <p>Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If from_store or self (destination) is not open</p> </li> <li> <code>FeatureNotFoundError</code>             \u2013            <p>If a specified feature doesn't exist in source store</p> </li> </ul> <p>Examples:</p> <pre><code># Simple: copy all features from latest snapshot\nstats = dest_store.copy_metadata(from_store=source_store)\n</code></pre> <pre><code># Copy specific features from a specific snapshot\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[FeatureKey([\"my_feature\"])],\n    from_snapshot=\"abc123\",\n)\n</code></pre> <pre><code># Copy with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n)\n</code></pre> <pre><code># Copy specific features with filters\nstats = dest_store.copy_metadata(\n    from_store=source_store,\n    features=[\n        FeatureKey([\"feature_a\"]),\n        FeatureKey([\"feature_b\"]),\n    ],\n    filters={\n        \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n        \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n    },\n)\n</code></pre> Source code in <code>src/metaxy/metadata_store/base.py</code> <pre><code>def copy_metadata(\n    self,\n    from_store: MetadataStore,\n    features: list[CoercibleToFeatureKey] | None = None,\n    *,\n    from_snapshot: str | None = None,\n    filters: Mapping[str, Sequence[nw.Expr]] | None = None,\n    incremental: bool = True,\n) -&gt; dict[str, int]:\n    \"\"\"Copy metadata from another store with fine-grained filtering.\n\n    This is a reusable method that can be called programmatically or from CLI/migrations.\n    Copies metadata for specified features, preserving the original snapshot_version.\n\n    Args:\n        from_store: Source metadata store to copy from (must be opened)\n        features: List of features to copy. Can be:\n            - None: copies all features from source store\n            - List of FeatureKey or Feature classes: copies specified features\n        from_snapshot: Snapshot version to filter source data by. If None, uses latest snapshot\n            from source store. Only rows with this snapshot_version will be copied.\n            The snapshot_version is preserved in the destination store.\n        filters: Dict mapping feature keys (as strings) to sequences of Narwhals filter expressions.\n            These filters are applied when reading from the source store.\n            Example: {\"feature/key\": [nw.col(\"x\") &gt; 10], \"other/feature\": [...]}\n        incremental: If True (default), filter out rows that already exist in the destination\n            store by performing an anti-join on sample_uid for the same snapshot_version.\n\n            The implementation uses an anti-join: source LEFT ANTI JOIN destination ON sample_uid\n            filtered by snapshot_version.\n\n            Disabling incremental (incremental=False) may improve performance when:\n            - You know the destination is empty or has no overlap with source\n            - The destination store uses deduplication\n\n            When incremental=False, it's the user's responsibility to avoid duplicates or\n            configure deduplication at the storage layer.\n\n    Returns:\n        Dict with statistics: {\"features_copied\": int, \"rows_copied\": int}\n\n    Raises:\n        ValueError: If from_store or self (destination) is not open\n        FeatureNotFoundError: If a specified feature doesn't exist in source store\n\n    Examples:\n        ```py\n        # Simple: copy all features from latest snapshot\n        stats = dest_store.copy_metadata(from_store=source_store)\n        ```\n\n        ```py\n        # Copy specific features from a specific snapshot\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[FeatureKey([\"my_feature\"])],\n            from_snapshot=\"abc123\",\n        )\n        ```\n\n        ```py\n        # Copy with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            filters={\"my/feature\": [nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])]},\n        )\n        ```\n\n        ```py\n        # Copy specific features with filters\n        stats = dest_store.copy_metadata(\n            from_store=source_store,\n            features=[\n                FeatureKey([\"feature_a\"]),\n                FeatureKey([\"feature_b\"]),\n            ],\n            filters={\n                \"feature_a\": [nw.col(\"field_a\") &gt; 10, nw.col(\"sample_uid\").is_in([\"s1\", \"s2\"])],\n                \"feature_b\": [nw.col(\"field_b\") &lt; 30],\n            },\n        )\n        ```\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(__name__)\n\n    # Validate destination store is open\n    if not self._is_open:\n        raise ValueError(\n            'Destination store must be opened with store.open(\"write\") before use'\n        )\n\n    # Auto-open source store if not already open\n    if not from_store._is_open:\n        with from_store.open(\"read\"):\n            return self._copy_metadata_impl(\n                from_store=from_store,\n                features=features,\n                from_snapshot=from_snapshot,\n                filters=filters,\n                incremental=incremental,\n                logger=logger,\n            )\n    else:\n        return self._copy_metadata_impl(\n            from_store=from_store,\n            features=features,\n            from_snapshot=from_snapshot,\n            filters=filters,\n            incremental=incremental,\n            logger=logger,\n        )\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.types.AccessMode","title":"metaxy.metadata_store.types.AccessMode  <code>module-attribute</code>","text":"<pre><code>AccessMode = Literal['read', 'write']\n</code></pre>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.base.VersioningEngineOptions","title":"metaxy.metadata_store.base.VersioningEngineOptions  <code>module-attribute</code>","text":"<pre><code>VersioningEngineOptions = Literal['auto', 'native', 'polars']\n</code></pre>"},{"location":"reference/api/metadata-stores/#base-configuration-class","title":"Base Configuration Class","text":"<p>The following base configuration class is typically used by child metadata stores:</p>"},{"location":"reference/api/metadata-stores/#metaxy.metadata_store.base.MetadataStoreConfig","title":"metaxy.metadata_store.base.MetadataStoreConfig","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Base configuration class for metadata stores.</p> <p>This class defines common configuration fields shared by all metadata store types. Store-specific config classes should inherit from this and add their own fields.</p> Example <pre><code>from metaxy.metadata_store.duckdb import DuckDBMetadataStoreConfig\n\nconfig = DuckDBMetadataStoreConfig(\n    database=\"metadata.db\",\n    hash_algorithm=HashAlgorithm.MD5,\n)\n\nstore = DuckDBMetadataStore.from_config(config)\n</code></pre>"},{"location":"reference/api/metadata-stores/#configuration","title":"Configuration","text":"<p>The base <code>MetadataStoreConfig</code> class injects the following configuration options:</p>"},{"location":"reference/api/metadata-stores/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"reference/api/metadata-stores/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"reference/api/metadata-stores/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"reference/api/metadata-stores/#project-write-validation","title":"Project Write Validation","text":"<p>By default, <code>MetadataStore</code> raises a <code>ValueError</code> when attempting to write to a project that doesn't match the expected project from <code>MetaxyConfig.get().project</code>.</p> <p>For legitimate cross-project operations (such as migrations that need to update features across multiple projects), use <code>MetadataStore.allow_cross_project_writes</code>:</p> <pre><code>with store.open(\"write\"), store.allow_cross_project_writes():\n    store.write_metadata(ExternallyDefinedFeature, df)\n</code></pre>"},{"location":"reference/api/metadata-stores/exceptions/","title":"Metadata Store Exceptions","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions","title":"metaxy.metadata_store.exceptions","text":"<p>Exceptions for metadata store operations.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions-classes","title":"Classes","text":""},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataStoreError","title":"metaxy.metadata_store.exceptions.MetadataStoreError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for metadata store errors.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FeatureNotFoundError","title":"metaxy.metadata_store.exceptions.FeatureNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a feature is not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.SystemDataNotFoundError","title":"metaxy.metadata_store.exceptions.SystemDataNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when system features are not found in the store.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.FieldNotFoundError","title":"metaxy.metadata_store.exceptions.FieldNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a field is not found for a feature.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.MetadataSchemaError","title":"metaxy.metadata_store.exceptions.MetadataSchemaError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when metadata DataFrame has invalid schema.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.DependencyError","title":"metaxy.metadata_store.exceptions.DependencyError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when upstream dependencies are missing or invalid.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.StoreNotOpenError","title":"metaxy.metadata_store.exceptions.StoreNotOpenError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when attempting to use a store that hasn't been opened.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","title":"metaxy.metadata_store.exceptions.HashAlgorithmNotSupportedError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a hash algorithm is not supported by the store or its components.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.TableNotFoundError","title":"metaxy.metadata_store.exceptions.TableNotFoundError","text":"<p>               Bases: <code>MetadataStoreError</code></p> <p>Raised when a table does not exist and auto_create_tables is disabled.</p>"},{"location":"reference/api/metadata-stores/exceptions/#metaxy.metadata_store.exceptions.VersioningEngineMismatchError","title":"metaxy.metadata_store.exceptions.VersioningEngineMismatchError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when versioning_engine='native' is requested but data has wrong implementation.</p>"},{"location":"reference/api/metadata-stores/memory/","title":"In-Memory Metadata Store","text":"<p>This one is mostly useful for testing.</p>"},{"location":"reference/api/metadata-stores/memory/#configuration","title":"Configuration","text":""},{"location":"reference/api/metadata-stores/memory/#fallback_stores","title":"<code>fallback_stores</code>","text":"<p>List of fallback store names to search when features are not found in the current store.</p> <p>Type: <code>list[str]</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# fallback_stores = []\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__FALLBACK_STORES=...\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#hash_algorithm","title":"<code>hash_algorithm</code>","text":"<p>Hash algorithm for versioning. If None, uses store's default.</p> <p>Type: <code>metaxy.versioning.types.HashAlgorithm | None</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\n# Optional\n# hash_algorithm = null\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__HASH_ALGORITHM=...\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#versioning_engine","title":"<code>versioning_engine</code>","text":"<p>Which versioning engine to use: 'auto' (prefer native), 'native', or 'polars'.</p> <p>Type: <code>Literal['auto', 'native', 'polars']</code> | Default: <code>\"auto\"</code></p> metaxy.tomlpyproject.tomlEnvironment Variable <pre><code>[stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>[tool.metaxy.stores.dev.config]\nversioning_engine = \"auto\"\n</code></pre> <pre><code>export METAXY_STORES__DEV__CONFIG__VERSIONING_ENGINE=auto\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#api-reference","title":"API Reference","text":""},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore","title":"metaxy.InMemoryMetadataStore","text":"<pre><code>InMemoryMetadataStore(**kwargs: Any)\n</code></pre> <p>               Bases: <code>MetadataStore</code></p> <p>In-memory metadata store using dict-based storage.</p> <p>Features: - Simple dict storage: {FeatureKey: pl.DataFrame} - Fast for testing and prototyping - No persistence (data lost when process exits) - Schema validation on write - Uses Polars components for all operations</p> <p>Limitations: - Not suitable for production - Data lost on process exit - No concurrency support across processes - Memory-bound (all data in RAM)</p> Notes <p>Uses Narwhals LazyFrames (nw.LazyFrame) for all operations</p> Components <p>Components are created on-demand in resolve_update(). Uses Polars internally but exposes Narwhals interface. Only supports Polars components (no native backend).</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Passed to MetadataStore.init (e.g., fallback_stores, hash_algorithm)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialize in-memory store.\n\n    Args:\n        **kwargs: Passed to MetadataStore.__init__ (e.g., fallback_stores, hash_algorithm)\n    \"\"\"\n    # Use tuple as key (hashable) instead of string to avoid parsing issues\n    self._storage: dict[tuple[str, ...], pl.DataFrame] = {}\n    super().__init__(**kwargs, versioning_engine_cls=PolarsVersioningEngine)\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore-functions","title":"Functions","text":""},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.write_metadata_to_store","title":"metaxy.InMemoryMetadataStore.write_metadata_to_store","text":"<pre><code>write_metadata_to_store(feature_key: FeatureKey, df: Frame, **kwargs: Any) -&gt; None\n</code></pre> <p>Internal write implementation for in-memory storage.</p> <p>Parameters:</p> <ul> <li> <code>feature_key</code>               (<code>FeatureKey</code>)           \u2013            <p>Feature key to write to</p> </li> <li> <code>df</code>               (<code>Frame</code>)           \u2013            <p>Narwhals Frame (eager or lazy) with metadata (already validated)</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def write_metadata_to_store(\n    self,\n    feature_key: FeatureKey,\n    df: Frame,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Internal write implementation for in-memory storage.\n\n    Args:\n        feature_key: Feature key to write to\n        df: Narwhals Frame (eager or lazy) with metadata (already validated)\n        **kwargs: Backend-specific parameters (currently unused)\n    \"\"\"\n    df_polars: pl.DataFrame = collect_to_polars(df)\n\n    storage_key = self._get_storage_key(feature_key)\n\n    # Append or create\n    if storage_key in self._storage:\n        existing_df = self._storage[storage_key]\n\n        # Handle schema evolution: ensure both DataFrames have matching columns\n        # Add missing columns as null to the existing DataFrame\n        for col_name in df_polars.columns:\n            if col_name not in existing_df.columns:\n                # Get the data type from the new DataFrame\n                col_dtype = df_polars.schema[col_name]\n                # Add column with null values of the appropriate type\n                existing_df = existing_df.with_columns(\n                    pl.lit(None).cast(col_dtype).alias(col_name)\n                )\n\n        # Add missing columns to the new DataFrame\n        for col_name in existing_df.columns:\n            if col_name not in df_polars.columns:\n                # Get the data type from the existing DataFrame\n                col_dtype = existing_df.schema[col_name]\n                # Add column with null values of the appropriate type\n                df_polars = df_polars.with_columns(\n                    pl.lit(None).cast(col_dtype).alias(col_name)\n                )  # type: ignore[arg-type,union-attr]\n\n        # Ensure column order matches by selecting columns in consistent order\n        all_columns = sorted(set(existing_df.columns) | set(df_polars.columns))\n        existing_df = existing_df.select(all_columns)\n        df_polars = df_polars.select(all_columns)\n\n        # Now we can safely concat\n        self._storage[storage_key] = pl.concat(\n            [existing_df, df_polars],\n            how=\"vertical\",\n        )\n    else:\n        # Create new\n        self._storage[storage_key] = df_polars\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.read_metadata_in_store","title":"metaxy.InMemoryMetadataStore.read_metadata_in_store","text":"<pre><code>read_metadata_in_store(feature: CoercibleToFeatureKey, *, feature_version: str | None = None, filters: Sequence[Expr] | None = None, columns: Sequence[str] | None = None, **kwargs: Any) -&gt; LazyFrame[Any] | None\n</code></pre> <p>Read metadata from this store only (no fallback).</p> <p>Parameters:</p> <ul> <li> <code>feature</code>               (<code>CoercibleToFeatureKey</code>)           \u2013            <p>Feature to read</p> </li> <li> <code>feature_version</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by specific feature_version</p> </li> <li> <code>filters</code>               (<code>Sequence[Expr] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Narwhals filter expressions</p> </li> <li> <code>columns</code>               (<code>Sequence[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of columns to select</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Backend-specific parameters (currently unused)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LazyFrame[Any] | None</code>           \u2013            <p>Narwhals LazyFrame with metadata, or None if not found</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>StoreNotOpenError</code>             \u2013            <p>If store is not open</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def read_metadata_in_store(\n    self,\n    feature: CoercibleToFeatureKey,\n    *,\n    feature_version: str | None = None,\n    filters: Sequence[nw.Expr] | None = None,\n    columns: Sequence[str] | None = None,\n    **kwargs: Any,\n) -&gt; nw.LazyFrame[Any] | None:\n    \"\"\"\n    Read metadata from this store only (no fallback).\n\n    Args:\n        feature: Feature to read\n        feature_version: Filter by specific feature_version\n        filters: List of Narwhals filter expressions\n        columns: Optional list of columns to select\n        **kwargs: Backend-specific parameters (currently unused)\n\n    Returns:\n        Narwhals LazyFrame with metadata, or None if not found\n\n    Raises:\n        StoreNotOpenError: If store is not open\n    \"\"\"\n    self._check_open()\n\n    feature_key = self._resolve_feature_key(feature)\n    storage_key = self._get_storage_key(feature_key)\n\n    if storage_key not in self._storage:\n        return None\n\n    # Start with lazy Polars DataFrame, wrap with Narwhals\n    df_lazy = self._storage[storage_key].lazy()\n    nw_lazy = nw.from_native(df_lazy)\n\n    # Apply feature_version filter\n    if feature_version is not None:\n        nw_lazy = nw_lazy.filter(\n            nw.col(\"metaxy_feature_version\") == feature_version\n        )\n\n    # Apply generic Narwhals filters\n    if filters is not None:\n        for filter_expr in filters:\n            nw_lazy = nw_lazy.filter(filter_expr)\n\n    # Select columns\n    if columns is not None:\n        nw_lazy = nw_lazy.select(columns)\n\n    # Check if result would be empty (we need to check the underlying frame)\n    # For now, return the lazy frame - emptiness check happens when materializing\n    return nw_lazy\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.clear","title":"metaxy.InMemoryMetadataStore.clear","text":"<pre><code>clear() -&gt; None\n</code></pre> <p>Clear all metadata from store.</p> <p>Useful for testing.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all metadata from store.\n\n    Useful for testing.\n    \"\"\"\n    self._storage.clear()\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.open","title":"metaxy.InMemoryMetadataStore.open","text":"<pre><code>open(mode: AccessMode = 'read') -&gt; Iterator[Self]\n</code></pre> <p>Open the in-memory store (no-op for in-memory, but accepts mode for consistency).</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>AccessMode</code>, default:                   <code>'read'</code> )           \u2013            <p>Access mode (accepted for consistency but ignored).</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Self</code> (              <code>Self</code> )          \u2013            <p>The store instance</p> </li> </ul> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>@contextmanager\ndef open(self, mode: AccessMode = \"read\") -&gt; Iterator[Self]:\n    \"\"\"Open the in-memory store (no-op for in-memory, but accepts mode for consistency).\n\n    Args:\n        mode: Access mode (accepted for consistency but ignored).\n\n    Yields:\n        Self: The store instance\n    \"\"\"\n    # Increment context depth to support nested contexts\n    self._context_depth += 1\n\n    try:\n        # Only perform actual open on first entry\n        if self._context_depth == 1:\n            # No actual connection needed for in-memory\n            # Mark store as open and validate\n            self._is_open = True\n            self._validate_after_open()\n\n        yield self\n    finally:\n        # Decrement context depth\n        self._context_depth -= 1\n\n        # Only perform actual close on last exit\n        if self._context_depth == 0:\n            # Nothing to clean up\n            self._is_open = False\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.__repr__","title":"metaxy.InMemoryMetadataStore.__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    num_fallbacks = len(self.fallback_stores)\n    status = \"open\" if self._is_open else \"closed\"\n    return (\n        f\"InMemoryMetadataStore(status={status}, fallback_stores={num_fallbacks})\"\n    )\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.display","title":"metaxy.InMemoryMetadataStore.display","text":"<pre><code>display() -&gt; str\n</code></pre> <p>Display string for this store.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>def display(self) -&gt; str:\n    \"\"\"Display string for this store.\"\"\"\n    status = \"open\" if self._is_open else \"closed\"\n    return f\"InMemoryMetadataStore(status={status})\"\n</code></pre>"},{"location":"reference/api/metadata-stores/memory/#metaxy.InMemoryMetadataStore.config_model","title":"metaxy.InMemoryMetadataStore.config_model  <code>classmethod</code>","text":"<pre><code>config_model() -&gt; type[InMemoryMetadataStoreConfig]\n</code></pre> <p>Return the configuration model class for this store type.</p> <p>Subclasses must override this to return their specific config class.</p> <p>Returns:</p> <ul> <li> <code>type[MetadataStoreConfig]</code>           \u2013            <p>The config class type (e.g., DuckDBMetadataStoreConfig)</p> </li> </ul> Note <p>Subclasses override this with a more specific return type. Type checkers may show a warning about incompatible override, but this is intentional - each store returns its own config type.</p> Source code in <code>src/metaxy/metadata_store/memory.py</code> <pre><code>@classmethod\ndef config_model(cls) -&gt; type[InMemoryMetadataStoreConfig]:  # pyright: ignore[reportIncompatibleMethodOverride]\n    return InMemoryMetadataStoreConfig\n</code></pre>"}]}